<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Kubernetes 设计模式笔记 —— Daemon Service]]></title>
    <url>%2F2023%2F08%2F15%2Fkubernetes-patterns-reading-notes-daemon-service%2F</url>
    <content type="text"><![CDATA[Daemon Service 能够向目标节点放置和运行有优先级的、面向基础设施的 Pod，通常被管理员用来部署与节点相关联的 Pod 以增强 Kubernetes 平台的功能。 从操作系统层面来看，daemon 是一类长时间运行、能够自行恢复的后台进程，通常在计算机启动时即自动加载，不会与前台进行任何交互。这类概念也存在于应用层面。比如运行在后台的 JVM deamon 线程为用户线程提供支持服务，有着较低的优先级，执行诸如 GC 等任务。 与上述场景类似，Kubernetes 也提供了 DaemonSet 功能。DaemonSet 与 ReplicaSet 有一个相同点，都是负责确保特定数量的 Pod 是一直运行着的。不同点在于，ReplicaSet 的具体配置通常取决于应用对于高可用和工作负载的需求，与节点数量无关；DaemonSet 则并不关注负载方面的因素，它的主要目的是在每一个节点（或部分特定的节点）上保持运行一个 Pod。 1234567891011121314151617181920212223242526272829303132apiVersion: extensions/v1beta1kind: DaemonSetmetadata: name: random-refresherspec: selector: matchLabels: app: random-refresher template: metadata: labels: app: random-refresher spec: nodeSelector: feature: hw-rng containers: - image: k8spatterns/random-generator:1.0 name: random-generator command: - sh - -c - &gt;- "while true; do java -cp / RandomRunner /host_dev/random 100000; sleep 30; done" volumeMounts: - mountPath: /host_dev name: devices volumes: - name: devices hostPath: path: /dev DaemonSet 的适用场景比如日志收集、监控数据导出甚至 kube-proxy 等。其与 ReplicaSet 的主要区别如下： 默认情况下，DaemonSet 会向每一个节点都部署一个 Pod 实例，可以通过 nodeSelector 字段只选取部分节点 DaemonSet 创建的 Pod 能够在 Scheduler 启动之前运行，因此，节点上还没有任何其他 Pod 被部署时，DaemonSet 创建的 Pod 就已经可以运行 由于 Scheduler 并没有参与 DaemonSet 的部署，节点上的 unschedulable 字段对 DaemonSet 控制器不起作用 DaemonSet 管理的 Pod 通常只运行在特定的节点上，因而很多控制器会对这些 Pod 区别对待，给与更高的优先级。比如 descheduler 会避免销毁这类 Pod，cluster autoscaler 会对它们独立地进行管理 DaemonSet 和 CronJob 是两个非常优秀的例子，Kubernetes 将单节点的概念比如 Crontab 和 daemon 脚本，转换成多节点的、集群化的原语，应用到分布式系统的管理中。 参考资料Kubernetes Patterns]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Container</tag>
        <tag>Kubernetes</tag>
        <tag>Pod</tag>
        <tag>k8s</tag>
        <tag>Virtualization</tag>
        <tag>Daemon</tag>
        <tag>Service</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kubernetes 设计模式笔记 —— Singleton Service]]></title>
    <url>%2F2023%2F08%2F15%2Fkubernetes-patterns-reading-notes-singleton-service%2F</url>
    <content type="text"><![CDATA[Singleton Service 模式会保证在某个特定的时间点，有且只有一个应用实例是活跃的。这个模式可以在应用内部实现，也可以完全交给 Kubernetes 去处理。 Kubernetes 最大的优势之一就是能够轻松、透明地扩展应用，只需要一条命令式的语句 kubectl scale 就能伸缩 Pod，或者通过声明式的方式修改 controller（比如 ReplicaSet）定义来实现，甚至基于应用负载动态地完成应用扩展。在 Kubernetes 中，多实例指的就是 Pod 的多个副本，能够提升应用的吞吐量和可用性，Service 则负责分配请求。 但是在一些特殊的场景中，同一时间只允许唯一一个应用实例运行。比如某个周期性执行的定时任务，若存在该任务的多个实例同时运行，则其中每一个实例都会在规定的时间间隔后触发一次该任务，导致预期之外的重复执行。另一个例子比如，某个应用服务需要轮询特定的系统资源（文件系统或数据库等），我们想要确保只有一个应用实例甚至只有一个线程在执行这类操作。还比如，我们必须按照特定的顺序消费来自消息代理的消息时，这里的单线程消费者也是一种单例。 运行同一个 Pod 实例的多个副本并令它们同时处于活跃状态，属于 active-active 拓扑结构，单例模式需要的是 active-possive（或者叫做 master-slave）结构。即只有一个实例是活跃的，其他所有的实例都是被动的，等待需要的时候被唤醒。 Out-of-Application Locking顾名思义，通过外部的管理程序确保应用只有唯一一个实例在运行，应用本身并不知晓自己是否是单例。 Kubernetes 实现单例模式的方式是，启动一个副本唯一的 Pod，但这并不能确保 Pod 是高可用的。因此还必须通过控制器（比如 ReplicaSet）来管理单例，令其具备高可用的能力。这种结构严格来说并不是 active-passive （没有配置 passive 实例）的，但是具有同样的效果。Kubernetes 会确保任何时候都会有一个 Pod 实例在运行，控制器会持续进行健康检查，修复失败的 Pod。 最需要额外注意的是副本数量，避免意外地被修改成大于 1 的值。事实上任何时候都只有唯一的应用实例在运行的说法不是完全准确的。Kubernetes 中的组件比如 ReplicaSet，会优先考虑可用性而非一致性。这意味着对于副本数量来说，ReplicaSet 会实行至少一个而不是最多一个的策略。在某些特殊的情况下，即便有 replicas: 1 的配置，也会出现多个实例同时在运行的情况。最常见的情形比如当某个节点失效时，与整个 Kubernetes 集群的连接丢失，ReplicaSet 控制器就会在另一个健康的节点上创建一个新的 Pod 实例，并且不会提前确认断连的节点上的原 Pod 是否已经关闭。类似的情况也会在修改副本数量或者将 Pod 重新分配给另一个节点时出现。 单例模式可以具有弹性和恢复能力，但是从定义来看，并不具备高可用性。它通常更倾向于一致性而非可用性。同样更倾向于一致性的 Kubernetes 资源是 StatefulSet。如果需要严格意义上的单例模式，StatefulSet 是更好的选择，但是它同样会增加系统的复杂度。 In-Application Locking在分布式环境中，控制服务实例数量的一种方式就是分布式锁。当实例中的服务组件被激活时，它会尝试获取一个锁，成功获取到锁则服务处于活跃状态。此时任何其他未获取到锁的服务实例则等待并不断地尝试获取锁，直到占用的锁被释放。 在面向对象的概念中，单例就是一个保存在类的静态变量中的对象实例。类本身知晓该实例是单例，并且在定义中不允许为同一个进程实例化多个实例。在分布式系统中，就意味着容器化应用本身在设计上，就不允许同一时间下有多于一个的实例处于活跃状态，不管实际上启动了多少个 Pod。上述实现需要借助分布式锁，比如 ZooKeeper，Consul，Redis 或 Etcd 等。 结论如果使用场景需要强 singleton 保证，就不能借助 ReplicaSet 实现的应用外部的锁机制。ReplicaSet 的设计目标在于保证 Pod 的可用性而不是满足 at-most-one 语义。会有很多错误场景，同一个 Pod 的两个副本短时间内并发地运行。若上述情况是不可接受的，则可以使用 StatefulSet 或者引入应用内的锁机制。在另外一些场景中，只有容器化应用的一部分是需要作为 singleton 运行的。比如一个容器化应用提供 HTTP 服务（能够安全地扩展），同时还包含必须是 singleton 的轮询组件。这时候使用应用外部的锁机制，会阻止整个应用被扩展。我们因此必须将 singleton 组件从原本的部署中分离出来。或者借助应用内的锁机制，只对 singleton 组件加锁。此时就可以透明地扩展整个应用，多个 HTTP 服务副本提供访问，singleton 组件则以 active-passive 模式运行。 参考资料Kubernetes Patterns]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Container</tag>
        <tag>Kubernetes</tag>
        <tag>k8s</tag>
        <tag>Virtualization</tag>
        <tag>Cluster</tag>
        <tag>Singleton</tag>
        <tag>Scale</tag>
        <tag>StatefulSet</tag>
        <tag>Consistency</tag>
        <tag>Locking</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kubernetes 设计模式笔记 —— Job & CronJob]]></title>
    <url>%2F2023%2F06%2F19%2Fkubernetes-patterns-reading-notes-job-and-cronjob%2F</url>
    <content type="text"><![CDATA[Batch JobBatch Job 模式适合处理隔离的、原子化的工作任务，能够在分布式的环境中，可靠地运行 short-lived Pods，直到工作任务成功地结束。 在 Kubernetes 中，可以通过不同的方式创建 Pod： Bare Pod：可以手动创建 Pod 来运行容器应用，但是当此类 Pod 所在的节点失效时，Pod 不会自动重启。除非用于开发或测试目的，此类方式并不推荐 ReplicaSet：当 Pod 应该长时间持续运行时（比如 web server），就适合用此方式来创建 Pod 和管理其生命周期。它会确保在任意时刻，运行着的 Pod 副本数量都是稳定的 DaemonSet：负责在每一个节点上都部署一个 Pod。通常情况下用于平台管理工作，比如监控、日志聚合、存储等 上述 Pod 有一个共同点，它们都代表着长时间运行的进程，并不是在一段时间后就需要被关掉。但是在某些场景下，仍需要执行一类预先定义好的、有限的工作流，当该工作流程可靠地完成后，再关闭对应的容器。 Kubernetes Job 类似于 ReplicaSet，它也会创建 1 个或者多个 Pods 并确保它们成功运行。区别在于，当特定数量的 Pods 成功终止后，Job 就变为完成状态，不会再有额外的 Pod 被启动。12345678910111213141516apiVersion: batch/v1kind: Jobmetadata: name: random-generatorspec: completions: 5 parallelism: 2 template: metadata: name: random-generator spec: restartPolicy: OnFailure containers: - image: k8spatterns/random-generator:1.0 name: random-generator command: [ "java", "-cp", "/", "RandomRunner", "/numbers.txt", "10000" ] 比如上面配置的 Job，会确保有 5 个 Pod 成功执行完毕，可以有两个 Pod 同时运行。此外，Job 配置文件中的 restartPolicy 是必需的，且其值只能是 OnFailure 或 Never，不能是 Always。 为什么不通过 bare Pods 来执行 Job 对应的任务呢？因为 Job 相比于 bare Pods，能够提供更多可靠性和扩展性方面的好处。 Job 并不是临时的 in-memory 任务，而是一个持久化的能够在集群重启后幸存的任务 Job 完成后并不会被删除，而是继续保留，方便以后追踪问题。只有当 bare Pods 是 restartPolicy: OnFailure 时，其才会拥有同样的特性 Job 可能需要执行多次，可以通过 .spec.completions 指定 当任务确实需要完成多次时，Job 还支持扩展，即同一时间开启多个 Pods。可以通过 .spec.parallelism 指定 若节点失效，或者 Pod 正在运行时因为某些原因被移除，由 Job 创建的 Pods 会被 scheduler 重新分配给健康的节点 两个字段对控制 Job 的行为发挥着关键作用： .spec.completions：指定 Pod 的数量。当特定数量的 Pod 执行完毕后，当前 Job 才算完成 .spec.parallelism：指定可以并行执行的 Pod 副本数量 基于上述两个参数，Job 可以分为如下几种类型： Single Pod Job：不设置 .spec.completions 和 .spec.parallelism 的值，或者将它们设置为默认值 1。此类 Job 只会启动一个 Pod，当 Pod 成功退出后，Job 完成 Fixed completions count Jobs：.spec.completions 的值大于 1。当特定数量（.spec.completions）的 Pod 执行完毕后，Job 完成 Work queue Job：.spec.completions 不设置或者设为默认值，.spec.parallelism 大于 1。适用于工作队列中的 Job。当至少有一个 Pod 成功终止时，所有其他 Pod 也会自行终止。比如，一堆固定数量的待处理项目保存在某个队列中，并行的 Pod 可以按顺序获取并处理它们，当某个 Pod 检测到队列为空并成功退出后，Job controller 等待其他 Pod 终止运行 总结Job 帮助我们将隔离的工作单元变成一个可靠的、可扩展的执行单元。并不是所有的服务都需要一直运行，比如某些服务可能需要按需运行，某些必须在特定的时间窗口运行，某些必须按照计划重复执行。通过 Job 可以只在需要的时候运行 Pod，且任务完成后就退出。使用 Job 处理 short-lived 任务可以节约系统资源。 Periodic JobPeriodic Job 是对 Batch Job 的扩展，为其添加了时间维度，同时允许临时的事件触发工作流的执行。在分布式系统的世界里，有一种比较清晰的倾向，借助 HTTP 和轻量的消息系统实现实时、事件驱动的应用。不考虑软件开发中的此类倾向，计划任务仍然是一种历史悠久且至今常用的手段。它们通常用于自动化的系统维护工作或者管理员任务，在商业应用方面的场景比如文件同步、发送邮件、清理和归档旧文件等。 传统的处理 Periodic Job 的方式是借助专门的计划任务软件比如 Cron。但是 Cron jobs 运行在单一的服务器上，难以维护且有发生单点故障的风险。这也是为什么很多开发者会尝试实现自己的方案，比如 Java 中的 Quartz、Spring Batch 等。但是类似于 Cron，它们也会遇到弹性和高可用性方面的挑战，导致较高的资源使消耗。此外在这类方案里，Job 调度器是应用的一部分，为了获得高可用，通常就需要运行多个应用实例，同时还需要确保同一时刻下只有一个实例是活跃的。从而引入 leader election 等分布式系统问题。 面对以上的一些问题，Kubernetes 实现了 CronJob，允许开发者以广为熟知的 Cron 格式将 Job 设置为计划任务。 12345678910111213141516apiVersion: batch/v1beta1kind: CronJobmetadata: name: random-generatorspec: # Every three minutes schedule: "*/3 * * * *" jobTemplate: spec: template: spec: containers: - image: k8spatterns/random-generator:1.0 name: random-generator command: [ "java", "-cp", "/", "RandomRunner", "/numbers.txt", "10000" ]restartPolicy: OnFailure 与 Job 相比，CronJob 有一些额外的字段： .spec.schedule：指定 Job 的 schedule 模式（如 0 * * * * 表示每个小时触发一次） .spec.startingDeadlineSeconds：Job 启动时的截止时间。有些时候由于资源不够或者缺少其他依赖，Job 错过了预定的触发时间。此字段用于指定错过多少秒后就直接跳过此次执行 .spec.concurrencyPolicy：用于控制同一个 CronJob 的并发执行。默认值为 Allow，即使前一个 Job 并未结束，也允许新的 Job 实例被创建；可以指定为 Forbid，若当前 Job 并未结束，则跳过下一次执行；或者改为 Replace，取消当前还未结束的 Job 并启动一个新的 Job 实例 .spec.suspend：暂停所有后续执行，但不影响已经开始的执行 .spec.successfulJobsHistoryLimit 和 .spec.failedJobsHistoryLimit：应保留多少已完成和失败的 Job 作为审计数据 总结CronJob 其实是一个非常简单的原语，在现有的 Job 定义中添加类似 Cron 的行为。但是当它与 Kubernetes 提供的其他原语比如 Pods、资源隔离结合起来时，就成为一个非常强大的任务调度系统。它的调度行为是平台的一部分，实现在应用的外部，使得开发者能够专注于应用的业务逻辑，无需在应用内部额外设计一套调度逻辑。同时提供了高可用、高弹性、高容积以及由策略驱动的 Pod 部署等特性。当然，和 Job 一样，CronJob 容器在部署时，也需要考虑所有的特殊情况，比如重复执行、未触发、并发执行和任务取消等。 参考资料Kubernetes Patterns]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Docker</tag>
        <tag>Container</tag>
        <tag>Kubernetes</tag>
        <tag>k8s</tag>
        <tag>Virtualization</tag>
        <tag>Job</tag>
        <tag>CronJob</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kubernetes 设计模式笔记 —— Automated Placement]]></title>
    <url>%2F2023%2F06%2F19%2Fkubernetes-patterns-reading-notes-automated-placement%2F</url>
    <content type="text"><![CDATA[Automated Placement 是 Kubernetes 中 scheduler 的核心功能，负责将新的 Pod 分配给合适的节点，满足容器的资源需求，同时遵守设定好的调度策略。 基于微服务的系统通常会包含数十个甚至数百个隔离的进程，容器和 Pod 为它们提供了很好的打包和部署机制，但并没有解决将众多的进程分配给适当的节点这项工作。容器之间存在依赖关系，有些还需要关联到特定的节点，容器自身也有一定的资源需求。这些都会随着时间发生变化。同时集群本身的资源也不是恒定的，它会执行缩容或者扩容，其特定时刻下的容量也取决于已经放置的容器数量。这些因素都会左右容器的调度。 Available Node Resources首先需要考虑的就是节点上是否有足够的可用资源。Scheduler 会确保 Pod 申请的资源总和小于可分配节点上的可用容量。节点可用容量的计算公式：1234Allocatable [capacity for application pods] = Node Capacity [available capacity on a node] - Kube-Reserved [Kubernetes daemons like kubelet, container runtime] - System-Reserved [OS system daemons like sshd, udev] Container Resource DemandsPod 在调度时，另一个重要的考虑因素就是容器有着自己的运行时依赖和资源需求。比如：123456789101112131415apiVersion: v1kind: Podmetadata: name: random-generatorspec: containers: - image: k8spatterns/random-generator:1.0 name: random-generator resources: requests: cpu: 100m memory: 100Mi limits: cpu: 200m memory: 200Mi Placement PoliciesScheduler 配置了一组默认的优先级策略，适用于绝大多数场景。这个策略可以在 scheduler 启动时被替换掉。 scheduler 策略示例：123456789101112131415161718&#123; "kind" : "Policy", "apiVersion" : "v1", "predicates" : [ &#123;"name" : "PodFitsHostPorts"&#125;, &#123;"name" : "PodFitsResources"&#125;, &#123;"name" : "NoDiskConflict"&#125;, &#123;"name" : "NoVolumeZoneConflict"&#125;, &#123;"name" : "MatchNodeSelector"&#125;, &#123;"name" : "HostName"&#125; ], "priorities" : [ &#123;"name" : "LeastRequestedPriority", "weight" : 2&#125;, &#123;"name" : "BalancedResourceAllocation", "weight" : 1&#125;, &#123;"name" : "ServiceSpreadingPriority", "weight" : 2&#125;, &#123;"name" : "EqualPriority", "weight" : 1&#125; ]&#125; 其中 Predicate 规则用于过滤掉不合格的节点。比如 PodFitsHostsPorts 关注特定的固定主机端口，只有在这些端口可用时对应的节点才会作为候选。Priorities 用于根据一些偏好设置来对候选的节点进行排序。比如 LeastRequestedPriority 会赋予请求了较少资源的节点更高的优先级。 可以同时运行多个 scheduler，让 Pod 自己去指定使用哪一个。只需要在 Pod 的配置中添加一条 .spec.schedulerName，其值为自定义 scheduler 的名字。 调度流程 只要 Pod 创建完成且还没有被分配给任何节点，scheduler 就会挑选出该 Pod，连同所有可用的节点及优先级策略。第一阶段借助过滤策略移除所有不满足要求的节点，剩余的节点在第二阶段有权重地排序。最后一个阶段得到最终的胜出节点。 在绝大多数情况下，最好都只让 scheduler 去做 Pod-to-Node 的分配工作，不要去尝试“微操”调度逻辑。在某些特殊场景下，如果需要强制某个 Pod 只能分配给特定的一个或一组节点，可以借助 Pod 的 .spec.nodeSelector 字段。该字段可以指定一些键值对，对应节点身上的标签。比如想要 Pod 运行在拥有 SSD 磁盘的硬件上：12345678910apiVersion: v1kind: Podmetadata: name: random-generatorspec: containers: - image: k8spatterns/random-generator:1.0 name: random-generator nodeSelector: disktype: ssd 除了通过自定义标签指定节点，还可以通过每个节点上都有的默认标签来筛选，比如 kubernetes.io/hostname。 Node AffinityKubernetes 还支持更为灵活的配置调度流程的方式，比如 node affinity。其实它相当于 nodeSelector 机制的泛化，其规则可以被指定为“必需”或者“优先”。“必需”表示相应的规则必须被满足，否则节点无法作为候选；“优先”则并不强制，只是提高匹配节点的权重。此外，node affinity 支持多种操作符，如 In、NotIn、Exists、DoesNotExist、Gt、Lt 等，从而获得更强的表达能力。 1234567891011121314151617181920212223apiVersion: v1kind: Podmetadata: name: random-generatorspec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: numberCores operator: Gt values: [ "3" ] preferredDuringSchedulingIgnoredDuringExecution: - weight: 1 preference: matchFields: - key: metadata.name operator: NotIn values: [ "master" ] containers: - image: k8spatterns/random-generator:1.0 name: random-generator 其中 requiredDuringSchedulingIgnoredDuringExecution 用来指定节点必须具备的条件，此规则不会在执行过程中重新计算。结合后面的 nodeSelectorTerms 配置，筛选出核心数大于 3 的节点。preferredDuringSchedulingIgnoredDuringExecution 用于指定非必须的条件，表现为一个带有权重的 selector 列表。对于每一个节点，计算出所有匹配项的权重总和，结果最高的节点被选中，只要该节点已经满足了前面的“必需”条件。 PS：matchFields 只支持 In 和 NotIn 操作符，values 指定的列表中也只允许有一个值。 诚然，node affinity 相比于 nodeSelector 功能更为强大。它允许通过标签或者字段为 Pod 选择合适的节点，但不能够用来表达 Pod 之间的依赖关系，比如无法根据某个节点上已经部署的 Pod 判断某个新 Pod 是否也应该部署到该节点。这类需求可以通过 Pod affinity 实现。 Pod AffinityNode affinity 工作在节点层级上，Pod affinity 则可以在多个拓扑层级上表达规则，达到粒度更细的控制。1234567891011121314151617181920212223apiVersion: v1kind: Podmetadata: name: random-generatorspec: affinity: podAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchLabels: confidential: high topologyKey: security-zone podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 100 podAffinityTerm: labelSelector: matchLabels: confidential: none topologyKey: kubernetes.io/hostname containers: - image: k8spatterns/random-generator:1.0 name: random-generator 其中 podAffinity 部分的配置表示，符合条件的节点上必须有带有 confidential=high 标签的 Pod 在运行，且该节点有 security-zone 标签。podAntiAffinity 定义的规则用于过滤掉匹配的节点。结合其配置，即节点上有带 confidential=none 标签的 Pod 在运行时，该节点不会用来部署当前 Pod。 Taints and TolerationsTaints 和 Tolerations 是一类更高级的用于控制调度策略的特性。简单来说，node affinity 允许 Pod 根据规则选择合适的节点，taints 和 tolerations 则正相反，它允许节点自身去控制 Pod 是否应该分配给自己。Taint 是节点自身的一种属性，当它存在时，会阻止 Pod 分配给自己，除非该 Pod 拥有针对 taint 的 tolerations。 Taint 可以使用 kubectl 命令添加。如 kubectl taint nodes master noderole.kubernetes.io/master=&quot;true&quot;:NoSchedule，等效于下面的配置。 Tainted 节点：12345678apiVersion: v1kind: Nodemetadata: name: masterspec: taints: - effect: NoSchedule key: node-role.kubernetes.io/master 拥有此 taint 的节点不会被分配任何 Pod，除非有 Pod 指定了对应的 toleration。比如： 123456789101112apiVersion: v1kind: Podmetadata: name: random-generatorspec: containers: - image: k8spatterns/random-generator:1.0 name: random-generator tolerations: - key: node-role.kubernetes.io/master operator: Exists effect: NoSchedule 在生产级别的集群中，带有 noderole.kubernetes.io/master 配置的 taint 一般会指定给 master 节点，阻止 Pod 部署到 master 上。这里给 Pod 添加的 toleration 会覆盖 taint 的 NoSchedule 效果，即无论如何都允许此 Pod 分配给 master 节点。 Taint 可以是硬性的，阻止节点作为候选（effect=NoSchedule），也可以是软性的，尝试避免节点作为候选（effect=PreferNoSchedule），还可以强制移除节点上已经在运行的 Pod（effect=NoExecute）。 当 Pod 已经分配给某个节点，scheduler 的工作就已经算完成了，它不会再对完成的分配进行调整。除非该 Pod 被删除或者重建。随着时间的推移，这一定会导致资源的碎片化，集群利用率降低。另一个潜在的问题是，Pod 被创建后具体分配给哪一个节点，依赖于当时集群的状态。而集群本身是动态的，节点的资源配置会更改，或者有新的节点加入进来，scheduler 并不会纠正已经存在的部署。此外，节点上的标签也有可能会变动，影响到之后的调度，但之前已经完成的调度依旧保持不变。 以上所有的场景都可以通过 descheduler 去解决。Kubernetes 的 descheduler 是一个可选的特性，通常作为 Job 执行，当管理员觉得是时候通过重新调度 Pod 来整理集群的碎片。Descheduler 有一些预先定义的策略，可以被启用或者禁用： RemoveDuplicates：该策略会确保 ReplicaSet 或 Deployment 关联的单一 Pod 只运行在唯一一个节点上。当某个节点不健康时，controller 会在其他健康的节点上启动新的 Pod。此时若之前不健康的节点恢复正常重新加入集群，正在运行的 Pod 就会大于需要的数量。此策略就可以应用于这类的场景。同时 RemoveDuplicates 还可以在策略或集群架构发生变化后，将 Pod 更均匀地分散在更多的节点上 LowNodeUtilization：该策略会找到使用率低的节点，并将高使用率节点上的 Pod 移除掉，希望这些移除的 Pod 可以重新分配到未充分利用的节点上。使用率低指 CPU、内存或 Pod 数量小于 thresholds 配置；使用率高指的是 CPU、内存或 Pod 数量大于 targetThresholds 配置 RemovePodsViolatingInterPodAntiAffinity：该策略会移除违反了 pod antiaffinity 规则的 Pod。这种情况可能发生在，添加规则时一些不符合规则的 Pod 就已经存在了 RemovePodsViolatingNodeAffinity：移除违反了 node affinity 规则的 Pod 不管使用何种配置的策略，descheduler 会避免移除如下类型的 Pod： 在 annotation 中标记为 scheduler.alpha.kubernetes.io/criticalpod 的关键 Pod 不由 ReplicaSet、Deployment 或 Job 管理的 Pod 由 DaemonSet 管理的 Pod 拥有本地存储的 Pod 配置了 PodDisruptionBudget 的 Pod，且移除时会违反此规则 Descheduler Pod 本身 总结容器调度是一个我们希望尽可能少干预的领域。从简单到复杂，以下方法控制着调度的具体策略： nodeName：最简单的分配方式，将 Pod 到节点的关系硬编码到配置中。理想的情况下，此字段应该由 scheduler 填充，策略去驱动，而不是手动指定 nodeSelector：键值对映射。符合条件的节点必须包含此键值对指向的标签。在控制调度策略的可接受的方式中，最简单的一种 Default scheduling alteration：必要的情况下，可以修改 default scheduler 的过滤规则和优先级策略、顺序、权重等 Pod affinity 和 antiaffinity：此机制允许 Pod 表达自身对其他 Pod 的依赖关系 Node affinity：允许 Pod 表达自身对节点的依赖关系，比如节点的硬件配置、地理位置等 Taints 和 tolerations：允许节点去控制哪些 Pod 允许哪些不允许分配给自己。比如为一组 Pod 分配一个专用节点，甚至在运行时移除 Pod Custom scheduler：若上述方案都不能符合需求，还可以编写自定义的 scheduler。自定义 scheduler 可以替换掉标准的 Kubernetes scheduler，也可以两者一起运行 参考资料Kubernetes Patterns]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Docker</tag>
        <tag>Container</tag>
        <tag>Kubernetes</tag>
        <tag>Pod</tag>
        <tag>k8s</tag>
        <tag>Virtualization</tag>
        <tag>Node</tag>
        <tag>Schedule</tag>
        <tag>Policy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 借助 asyncio 实现并发编程]]></title>
    <url>%2F2023%2F01%2F12%2Fpython-concurrency-with-coroutine-and-asyncio-basics%2F</url>
    <content type="text"><![CDATA[asyncio 基础创建协程使用 async 关键字创建 coroutine 12345678910111213141516171819async def coroutine_add_one(number: int) -&gt; int: return number + 1def add_one(number: int) -&gt; int: return number + 1function_result = add_one(1)coroutine_result = coroutine_add_one(1)print( f'Function result is &#123;function_result&#125; and the type is &#123;type(function_result)&#125;')# =&gt; Function result is 2 and the type is &lt;class 'int'&gt;print( f'Coroutine result is &#123;coroutine_result&#125; and the type is &#123;type(coroutine_result)&#125;')# =&gt; Coroutine result is &lt;coroutine object coroutine_add_one at 0x7f9f495f20a0&gt; and the type is &lt;class 'coroutine'&gt;# =&gt; sys:1: RuntimeWarning: coroutine 'coroutine_add_one' was never awaited 创建 coroutine 和创建普通的函数一样直接，唯一的区别在于使用 async def 而不是 def。当我们直接调用协程 coroutine_add_one 时，传入的参数并没有被加 1 然后返回计算结果，我们只是得到了一个 coroutine object。即我们只是创建了一个能够在之后的某个时间运行的 coroutine 对象，为了运行它，我们总是需要显式地将其放入 event loop 中。最简单的方式就是使用 asyncio.run 函数。 运行 coroutine 12345678910import asyncioasync def coroutine_add_one(number: int) -&gt; int: return number + 1result = asyncio.run(coroutine_add_one(1))print(result)# =&gt; 2 asyncio.run 是 asyncio 应用程序的入口。 使用 await 关键字暂停执行asyncio 的真正用处，在于能够在一个长时间运行的操作过程中，暂停执行，从而令 event loop 有机会处理其他任务。“暂停”的动作通过 await 关键字触发。await 后面通常紧跟着一个对 coroutine （更严谨地说，一个 awaitable 对象）的调用。 1234567891011121314151617import asyncioasync def add_one(number: int) -&gt; int: return number + 1async def main() -&gt; None: one_plus_one = await add_one(1) two_plus_one = await add_one(2) print(one_plus_one) # =&gt; 2 print(two_plus_one) # =&gt; 3asyncio.run(main()) 首先 await 对协程 add_one(1) 的调用，此时父协程（即 main()）被暂停，add_one(1) 执行并获取结果（2），main() 协程恢复执行，将结果赋值给 one_plus_one；同样地，对协程 add_one(2) 的 await 也会导致 main() 被暂停和恢复。 sleep前面的例子只是为了介绍协程的基本语法，并没有涉及任何 long-running 操作，因而也没有享受到 asyncio 在并发方面的作用。我们可以借助 asyncio.sleep 函数模拟 web API 请求或者数据库查询等长时间运行的操作，asyncio.sleep 能够令某个协程“睡眠”指定的时间（秒）。asyncio.sleep 本身就是一个协程，因而当我们在某个协程中 await asyncio.sleep 时，其他部分代码就得到了执行的机会。 sleep 实现 delay 函数 123456789# util.pyimport asyncioasync def delay(delay_seconds: int) -&gt; int: print(f'sleeping for &#123;delay_seconds&#125; second(s)') await asyncio.sleep(delay_seconds) print(f'finished sleeping for &#123;delay_seconds&#125; second(s)') return delay_seconds 运行两个协程 12345678910111213141516171819202122232425import asynciofrom util import delayasync def add_one(number: int) -&gt; int: return number + 1async def hello_world_message() -&gt; str: await delay(1) return 'Hello Wrold!'async def main() -&gt; None: message = await hello_world_message() one_plus_one = await add_one(1) print(one_plus_one) print(message) # =&gt; sleeping for 1 second(s) # =&gt; finished sleeping for 1 second(s) # =&gt; 2 # =&gt; Hello Wrold!asyncio.run(main()) 运行上面的代码，先是等待 1 秒钟，之后才是两个函数调用的结果被打印出来。我们本来希望看到的是，两个协程并发地执行，add_one(1) 的结果直接被输出，并不需要等待 hello_world_message() 中的 sleep 结束。实际上 await 会暂停其所在的协程（这里是 main），并且不会执行当前协程中的任何其他代码，直到 await 表达式获得一个结果。hello_world_message 需要 1 秒钟才能返回结果，因而 main 协程也会被暂停 1 秒钟。排在它后面的 add_one(1) 在暂停结束后执行并返回结果。 上面的代码和同步、顺序执行的代码没有表现出任何区别。为了实现并发，我们需要引入一个新的概念 task。 tasksTask 是对协程的一种包装，能够将一个协程调度至 event loop 并争取尽快执行。这种调度是以一种非阻塞的方式发生的，即 task 被创建后会立即返回，不必等待其运行结束，从而我们能够有机会执行其他代码。 并发地执行多个 task 12345678910111213141516171819202122232425import asynciofrom util import delayasync def hello_every_second(): for i in range(2): await asyncio.sleep(1) print("I'm running other code while I'm waiting!")async def main(): first_delay = asyncio.create_task(delay(3)) second_delay = asyncio.create_task(delay(3)) await hello_every_second() await first_delay await second_delayasyncio.run(main())# =&gt; sleeping for 3 second(s)# =&gt; sleeping for 3 second(s)# =&gt; I'm running other code while I'm waiting!# =&gt; I'm running other code while I'm waiting!# =&gt; finished sleeping for 3 second(s)# =&gt; finished sleeping for 3 second(s) 上述代码创建了 2 个 task，每个都需要 3 秒钟才能执行完毕。两次对 create_task 的调用都会立即返回。由于 task 调度的原则是尽快执行，当后面的 await 代码刷新了一次 event loop 之后，前面创建的 2 个 task 会立即被执行（非阻塞）。两个 delay task 在 sleep 过程中，应用是闲置的，我们得以有机会运行其他代码。协程 hello_every_second 每隔 1 秒输出一条消息。整个应用总的运行时间大约是 3 秒，即大约等于耗时最长的异步任务的时间，而不是像顺序执行的程序那样，等于多个任务运行时间的总和。 协程和任务的陷阱将一些长时间运行的任务并发的执行，能够带来很大程度上的性能提升。因而我们会倾向于在应用的任何地方使用协程和 task。事实上，仅仅将函数用 async 修饰，将其封装进 task，并不总是带来性能上的提升。甚至有些情况下还会降低程序的效率。最主要的情形有两种，一个是在不借助多进程的情况下，尝试在 task 或协程中运行 CPU-bound 代码；另一种是在不借助多线程的情况下调用阻塞式 I/O-bound API。 CPU 密集型任务有时候我们需要一些函数执行 CPU 密集型的任务，比如对一个很大的字典执行循环或者数学计算。为了提升效率，我们会想着将它们放置在单独的 task 中运行。然而现实是，asyncio 使用单线程并发模型，我们依然会受到单个线程和 GIL 的限制。 计算协程运行时间 1234567891011121314151617181920212223242526272829# util.pyimport asyncioimport functoolsimport timefrom typing import Callable, Anydef async_timed(): def wrapper(func: Callable) -&gt; Callable: @functools.wraps(func) async def wrapped(*args, **kwargs) -&gt; Any: print(f'Starting &#123;func&#125; with &#123;args&#125; &#123;kwargs&#125;') start = time.time() try: return await func(*args, **kwargs) finally: end = time.time() total = end - start print(f'finished &#123;func&#125; in &#123;total:.4f&#125; second(s)') return wrapped return wrapper@async_timed()async def delay(delay_seconds: int) -&gt; int: print(f'sleeping for &#123;delay_seconds&#125; second(s)') await asyncio.sleep(delay_seconds) print(f'finished sleeping for &#123;delay_seconds&#125; second(s)') return delay_seconds 运行 CPU-bound 代码 123456789101112131415161718192021222324252627282930313233import asynciofrom util import delay, async_timed@async_timed()async def cpu_bound_work() -&gt; int: counter = 0 for i in range(100000000): counter = counter + 1 return counter@async_timed()async def main(): task_one = asyncio.create_task(cpu_bound_work()) task_two = asyncio.create_task(cpu_bound_work()) delay_task = asyncio.create_task(delay(4)) await task_one await task_two await delay_taskasyncio.run(main())# =&gt; Starting &lt;function main at 0x7f2d6b85bc70&gt; with () &#123;&#125;# =&gt; Starting &lt;function cpu_bound_work at 0x7f2d6c2bba30&gt; with () &#123;&#125;# =&gt; finished &lt;function cpu_bound_work at 0x7f2d6c2bba30&gt; in 2.7423 second(s)# =&gt; Starting &lt;function cpu_bound_work at 0x7f2d6c2bba30&gt; with () &#123;&#125;# =&gt; finished &lt;function cpu_bound_work at 0x7f2d6c2bba30&gt; in 2.7430 second(s)# =&gt; Starting &lt;function delay at 0x7f2d6b85a0e0&gt; with (4,) &#123;&#125;# =&gt; sleeping for 4 second(s)# =&gt; finished sleeping for 4 second(s)# =&gt; finished &lt;function delay at 0x7f2d6b85a0e0&gt; in 4.0048 second(s)# =&gt; finished &lt;function main at 0x7f2d6b85bc70&gt; in 9.4903 second(s) 上述代码创建了 3 个 task，但实际执行时依然是顺序的而非并发的，耗费的时间并没有变少。两个 CPU-bound task 是依次执行的，甚至 delay_task 也并没有与其他两个任务呈现并发性。原因在于我们先创建了两个 CPU-bound 任务，这两个任务会阻塞 event loop，阻止其调度执行任何其他任务。因此，总的运行时间等于两个 CPU-bound 任务执行完毕的时间加上 delay 任务运行的 4 秒。即 asyncio 并没有为 CPU-bound 的任务带来任何性能上的提升。假如我们需要在执行 CPU-bound 任务的同时仍使用 async 语法，就必须借助多进程，告诉 asyncio 在 process pool 中执行任务。 阻塞式 API我们也会倾向于使用现有的库执行 I/O-bound 操作，再将其封装进协程。然而，这会引起与 CPU-bound 操作同样的问题。因为这些 API 会阻塞 main 线程。当我们在协程内部调用一个阻塞的 API，我们会阻塞 event loop 线程本身，线程被阻塞请求占据，导致 event loop 无法调度任何其他协程和任务。阻塞式 API 请求包括 requests 库和 time.sleep 等。通常来说，任何执行 I/O 操作且不是协程的函数，以及执行 CPU 密集型任务的函数，都可以认为是阻塞的。 协程内部调用阻塞式 API 1234567891011121314151617181920212223242526272829import asyncioimport requestsfrom util import async_timed@async_timed()async def get_example_status() -&gt; int: return requests.get('http://www.example.com').status_code@async_timed()async def main(): task_1 = asyncio.create_task(get_example_status()) task_2 = asyncio.create_task(get_example_status()) task_3 = asyncio.create_task(get_example_status()) await task_1 await task_2 await task_3asyncio.run(main())# =&gt; Starting &lt;function main at 0x7f4335080790&gt; with () &#123;&#125;# =&gt; Starting &lt;function get_example_status at 0x7f4335186170&gt; with () &#123;&#125;# =&gt; finished &lt;function get_example_status at 0x7f4335186170&gt; in 0.5144 second(s)# =&gt; Starting &lt;function get_example_status at 0x7f4335186170&gt; with () &#123;&#125;# =&gt; finished &lt;function get_example_status at 0x7f4335186170&gt; in 0.5163 second(s)# =&gt; Starting &lt;function get_example_status at 0x7f4335186170&gt; with () &#123;&#125;# =&gt; finished &lt;function get_example_status at 0x7f4335186170&gt; in 0.5177 second(s)# =&gt; finished &lt;function main at 0x7f4335080790&gt; in 1.5488 second(s) main 协程运行的总时间基本上等于所有 task 运行的时间之和。即我们并没有获取到任何并发上的收益。原因在于 requests 库是阻塞的，任何调用都会阻塞当前线程，而 asyncio 只有一个线程，在阻塞调用结束之前，线程中的 event loop 没有机会以异步的形式运行任何任务。当你使用的库并没有返回协程，你并没有在自己的协程中使用 await 关键字，很大可能你就是在进行阻塞的函数调用。当前我们使用的大多数 API 都是阻塞的，并不支持与 asyncio 开箱即用。要想体验到 asyncio 带来的异步和并发特性，就必须使用原生支持协程和非阻塞 socket 的库，比如 aiohttp。或者你坚持使用 requests 库，同时又需要 async 语法，就必须显式地告诉 asyncio 使用多线程的方式，通过 thread pool executor 执行阻塞调用。 借助支持协程的库 aiohttp 实现并发 1234567891011121314151617181920212223242526272829303132import asynciofrom aiohttp import ClientSessionfrom util import async_timed@async_timed()async def get_example_status() -&gt; int: session = ClientSession() resp = await session.get('http://example.com') await session.close() return resp.status@async_timed()async def main(): task_1 = asyncio.create_task(get_example_status()) task_2 = asyncio.create_task(get_example_status()) task_3 = asyncio.create_task(get_example_status()) await task_1 await task_2 await task_3asyncio.run(main())# =&gt; Starting &lt;function main at 0x7fd9f90b6a70&gt; with () &#123;&#125;# =&gt; Starting &lt;function get_example_status at 0x7fd9f90b63b0&gt; with () &#123;&#125;# =&gt; Starting &lt;function get_example_status at 0x7fd9f90b63b0&gt; with () &#123;&#125;# =&gt; Starting &lt;function get_example_status at 0x7fd9f90b63b0&gt; with () &#123;&#125;# =&gt; finished &lt;function get_example_status at 0x7fd9f90b63b0&gt; in 0.5191 second(s)# =&gt; finished &lt;function get_example_status at 0x7fd9f90b63b0&gt; in 0.5191 second(s)# =&gt; finished &lt;function get_example_status at 0x7fd9f90b63b0&gt; in 0.5191 second(s)# =&gt; finished &lt;function main at 0x7fd9f90b6a70&gt; in 0.5196 second(s) 可以看到所有 task 执行的总时间，基本上只比一个 task 运行的时间多一点点。此时的程序是并发执行的。 取消任务取消任务 每个 task 对象都有一个 cancel 方法可以帮助我们随时终止该任务。当我们 await 取消的任务时，会报出 CancelledError 异常。比如我们调度执行某个任务，又不希望该任务运行的时间超过 5 秒： 12345678910111213141516171819202122232425262728293031323334import asynciofrom asyncio import CancelledErrorfrom util import delayasync def main(): long_task = asyncio.create_task(delay(10)) seconds_elapsed = 0 while not long_task.done(): print('Task not finished, checking again in a second.') await asyncio.sleep(1) seconds_elapsed = seconds_elapsed + 1 if seconds_elapsed == 5: long_task.cancel() try: await long_task except CancelledError: print('Our task was cancelled')asyncio.run(main())# =&gt; Task not finished, checking again in a second.# =&gt; Starting &lt;function delay at 0x7fdb383ae0e0&gt; with (10,) &#123;&#125;# =&gt; sleeping for 10 second(s)# =&gt; Task not finished, checking again in a second.# =&gt; Task not finished, checking again in a second.# =&gt; Task not finished, checking again in a second.# =&gt; Task not finished, checking again in a second.# =&gt; Task not finished, checking again in a second.# =&gt; finished &lt;function delay at 0x7fdb383ae0e0&gt; in 5.0079 second(s)# =&gt; Our task was cancelled 需要注意的是，CancelledError 只会在 await 语句处抛出，调用 cancel 方法并不会神奇地强行关闭正在运行的任务，只有你刚好遇到 await 时任务才会被终止，不然就等待下一个 await。 使用 wait_for 设置超时时间每隔一段时间手动进行检查，以确定是否取消某个任务，并不算一种简单的处理方式。asyncio 提供了一个 wait_for 函数，它接收一个协程或者任务，以及超时的秒数作为参数，返回一个协程对象。若任务运行超时，一个 TimeoutException 就会被抛出，任务自动被终止。 12345678910111213141516171819import asynciofrom util import delayasync def main(): delay_task = asyncio.create_task(delay(2)) try: result = await asyncio.wait_for(delay_task, timeout=1) print(result) except asyncio.exceptions.TimeoutError: print('Got a timeout') print(f'Was the task cancelled? &#123;delay_task.cancelled()&#125;')asyncio.run(main())# =&gt; Starting &lt;function delay at 0x7f71e18160e0&gt; with (2,) &#123;&#125;# =&gt; sleeping for 2 second(s)# =&gt; finished &lt;function delay at 0x7f71e18160e0&gt; in 1.0016 second(s)# =&gt; Got a timeout# =&gt; Was the task cancelled? True asyncio.shield在另外一些情况下，我们有可能并不希望直接取消某个超时的任务，而是当任务运行时间过长时，提醒用户这个情况，但是并不执行任何 cancel 操作。shield 可以帮助我们实现这样的功能。 12345678910111213141516171819202122from util import delayasync def main(): task = asyncio.create_task(delay(10)) try: result = await asyncio.wait_for(asyncio.shield(task), 5) print(result) except asyncio.exceptions.TimeoutError: print("Task took longer than five seconds, it will finish soon!") result = await task print(result)asyncio.run(main())# =&gt; Starting &lt;function delay at 0x7ff344d120e0&gt; with (10,) &#123;&#125;# =&gt; sleeping for 10 second(s)# =&gt; Task took longer than five seconds, it will finish soon!# =&gt; finished sleeping for 10 second(s)# =&gt; finished &lt;function delay at 0x7ff344d120e0&gt; in 10.0063 second(s)# =&gt; 10 参考资料Python Concurrency with asyncio]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Concurrency</tag>
        <tag>Thread</tag>
        <tag>Async</tag>
        <tag>Coroutine</tag>
        <tag>Asyncio</tag>
        <tag>EventLoop</tag>
        <tag>Non-blocking</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Node.js 设计模式笔记 —— 消息中间件及其应用模式（任务分发）]]></title>
    <url>%2F2023%2F01%2F12%2Fnode-js-design-patterns-message-broker-pattern-task-distribution%2F</url>
    <content type="text"><![CDATA[将高成本的任务委派给多个工作节点，这种类型的应用并不适合由 Pub/Sub 模式实现。因为我们并不想同一个任务被多个消费者收到，相反我们更需要一种类似负载均衡的消息分发模式。在消息系统术语中，也被称为 competing consumers，fanout distribution 或 ventilator。与 HTTP 负载均衡器不同的是，任务分发系统中的消费者是一种更活跃的角色。绝大多数时候都是消费者连接到任务队列，请求新的任务。这一点在可扩展系统中非常关键，允许我们在不修改生产者部分的情况下，直接平滑地增加工作节点的数量。此外，在一个通用的消息系统中，我们没有必要强调生产者和消费者之间的请求/响应通信。多数情况下，更优先的选择是使用单向的异步通信，从而获得更优异的并行能力和扩展性。消息基本上总是沿着一个方向流动，这样的管道允许我们构建复杂的信息处理架构，又不必承受同步通信带来的开销。 ZeroMQ Fanout/Fanin 模式分布式 hashsum 破解器需要以下组件实现一个标准的并行管线： 一个协调节点负责在多个工作节点间分发任务 多个工作节点承担具体的计算任务 一个用于收集计算结果的节点 即一个节点负责生成所有可能的字符串组合，并将它们分发给不同的工作节点；工作节点则负责计算接收到的字符串，比较 hash 值；最后一个节点负责收集暴力破解的结果。 实现 producer为了表示所有可能的字符组合，这里使用 N 维索引树。每个节点包含一个当前位置下可能出现的字母，比如只有 a、b 两个字母的话，长度为 3 的字符串组合共有图示的以下几种： indexed-string-variation 包可以帮助我们由索引计算出对应的字符串，这项工作可以在工作节点完成，因此 producer 这里只需要将分好组的索引值分发给工作节点。generateTasks.js： 123456789101112131415161718192021222324export function* generateTasks(searchHash, alphabet, maxWordLength, batchSize) &#123; let nVariations = 0 for (let n = 1; n &lt;= maxWordLength; n++) &#123; nVariations += Math.pow(alphabet.length, n) &#125; console.log('Finding the hashsum source string over ' + `$&#123;nVariations&#125; possible variations`) let batchStart = 1 while (batchStart &lt;= nVariations) &#123; const batchEnd = Math.min( batchStart + batchSize - 1, nVariations) yield &#123; searchHash, alphabet: alphabet, batchStart, batchEnd &#125; batchStart = batchEnd + 1 &#125;&#125; producer.js：123456789101112131415161718192021import zmq from 'zeromq'import delay from 'delay'import &#123; generateTasks &#125; from './generateTasks.js'const ALPHABET = 'abcdefghijklmnopqrstuvwxyz'const BATCH_SIZE = 10000const [, , maxLength, searchHash] = process.argvasync function main() &#123; const ventilator = new zmq.Push() await ventilator.bind('tcp://*:5016') await delay(1000) const generatorObj = generateTasks(searchHash, ALPHABET, maxLength, BATCH_SIZE) for (const task of generatorObj) &#123; await ventilator.send(JSON.stringify(task)) &#125;&#125;main().catch(err =&gt; console.log(err)) 创建一个 PUSH socket 并绑定给本地的 5016 端口，工作节点的 PULL socket 会连接到此端口并接收任务 将每一个生成的任务字符串化，通过 PUSH socket 的 send() 方法发送给工作节点。工作节点以轮询的方式接收不同的任务 实现 workerprocess Task.js：1234567891011121314151617181920import isv from 'indexed-string-variation'import &#123; createHash &#125; from 'crypto'export function processTask(task) &#123; const variationGen = isv.generator(task.alphabet) console.log('processing from ' + `$&#123;variationGen(task.batchStart)&#125; ($&#123;task.batchStart&#125;)` + `to $&#123;variationGen(task.batchEnd)&#125; ($&#123;task.batchEnd&#125;`) for (let idx = task.batchStart; idx &lt;= task.batchEnd; idx++) &#123; const word = variationGen(idx) const shasum = createHash('sha1') shasum.update(word) const digest = shasum.digest('hex') if (digest === task.searchHash) &#123; return word &#125; &#125;&#125; processTask() 遍历给定区间内的所有索引值，对每一个索引生成对应的字符串，再计算其 SHA1 值，与传入的 task 对象中的 searchHash 比较。 worker.js：1234567891011121314151617181920import zmq from 'zeromq'import &#123; processTask &#125; from './processTask.js'async function main() &#123; const fromVentilator = new zmq.Pull() const toSink = new zmq.Push() fromVentilator.connect('tcp://localhost:5016') toSink.connect('tcp://localhost:5017') for await (const rawMessage of fromVentilator) &#123; const found = processTask(JSON.parse(rawMessage.toString())) if (found) &#123; console.log(`Found! =&gt; $&#123;found&#125;`) await toSink.send(`Found: $found`) &#125; &#125;&#125;main().catch(err =&gt; console.error(err)) worker.js 创建了两个 socket。PULL socket 负责连接到任务发布方（Ventilator），接收任务；PUSH socket 负责连接到结果收集方（sink），传递任务执行的结果。 实现 results collectorcollector.js：123456789101112import zmq from 'zeromq'async function main() &#123; const sink = new zmq.Pull() await sink.bind('tcp://*:5017') for await (const rawMessage of sink) &#123; console.log('Message from worker: ', rawMessage.toString()) &#125;&#125;main().catch(err =&gt; console.error(err)) 运行以下命令测试结果：1234node worker.jsnode worker.jsnode collector.jsnode producer.js 4 f8e966d1e207d02c44511a58dccff2f5429e9a3b AMQP 实现 pipeline 和 competing consumers 像前面那样在点对点的模式下，实现 pipeline 是非常直观的。假设我们需要借助 AMQP 这类系统实现任务分配模式，就必须确保每条消息都只会被一个消费者接收到。可以直接将任务发布到目标 queue，不经过 exchange。避免了 exchange 有可能绑定了多个 queue 的情况。之后，多个消费者同时监听这一个 queue，消息即会以 fanout 的方式均匀地分发给所有的消费者。 hashsum 破解器的 AMQP 实现producer-amqp.js：12345678910111213141516171819202122232425import amqp from 'amqplib'import &#123; generateTasks &#125; from './generateTasks.js'const ALPHABET = 'abcdefghijklmnopqrstuvwxyz'const BATCH_SIZE = 10000const [, , maxLength, searchHash] = process.argvasync function main() &#123; const connection = await amqp.connect('amqp://localhost') const channel = await connection.createConfirmChannel() await channel.assertQueue('tasks_queue') const generatorObj = generateTasks(searchHash, ALPHABET, maxLength, BATCH_SIZE) for (const task of generatorObj) &#123; channel.sendToQueue('tasks_queue', Buffer.from(JSON.stringify(task))) &#125; await channel.waitForConfirms() channel.close() connection.close()&#125;main().catch(err =&gt; console.error(err)) 此处创建的是一个 confirmChannel，它提供了一个 waitForConfirms() 函数，可以在 broker 确认收到消息前等待，确保应用不会过早地关闭到 broker 的连接 channel.sendToQueue() 负责将一条消息直接发送给某个 queue，跳过任何 exchange 或者路由 worker-amqp.js：1234567891011121314151617181920import amqp from 'amqplib'import &#123; processTask &#125; from './processTask.js'async function main() &#123; const connection = await amqp.connect('amqp://localhost') const channel = await connection.createChannel() const &#123; queue &#125; = await channel.assertQueue('tasks_queue') channel.consume(queue, async (rawMessage) =&gt; &#123; const found = processTask( JSON.parse(rawMessage.content.toString())) if (found) &#123; console.log(`Found! =&gt; $&#123;found&#125;`) await channel.sendToQueue('results_queue', Buffer.from(`Found: $&#123;found&#125;`)) &#125; await channel.ack(rawMessage) &#125;)&#125;main().catch(err =&gt; console.error(err)) collector-amqp.js：123456789101112import amqp from 'amqplib'async function main() &#123; const connection = await amqp.connect('amqp://localhost') const channel = await connection.createChannel() const &#123; queue &#125; = await channel.assertQueue('results_queue') channel.consume(queue, msg =&gt; &#123; console.log(`Message from worker: $&#123;msg.content.toString()&#125;`) &#125;)&#125;main().catch(err =&gt; console.error(err)) 运行如下命令测试效果：1234node worker-amqp.jsnode worker-amqp.jsnode collector-amqp.jsnode producer-amqp.js 4 f8e966d1e207d02c44511a58dccff2f5429e9a3b 通过 Redis Streams 实现任务分发Redis Stream 可以借助一种叫做 consumer groups 的特性实现任务分发模式。Consumer group 是一个有状态的实体，由一组名称标识的消费者组成，组中的消费者会以 round-robin 的方式接收记录。每条记录都必须被显式地确认，否则该记录会一直处于 pending 状态。每个消费者都只能访问它自己的 pending 记录，假如消费者突然崩溃，在其回到线上后会先尝试获取其 pending 的记录。 Consumer group 也会记录其读取的上一条消息的 ID，因而在连续的读取操作中，consumer group 知道下一条要读取的记录时是哪个。 producer-redis.js： 123456789101112131415161718192021import Redis from 'ioredis'import &#123; generateTasks &#125; from './generateTasks.js'const ALPHABET = 'abcdefghijklmnopqrstuvwxyz'const BATCH_SIZE = 10000const redisClient = new Redis()const [, , maxLength, searchHash] = process.argvasync function main() &#123; const generatorObj = generateTasks(searchHash, ALPHABET, maxLength, BATCH_SIZE) for (const task of generatorObj) &#123; await redisClient.xadd('tasks_stream', '*', 'task', JSON.stringify(task)) &#125; redisClient.disconnect()&#125;main().catch(err =&gt; console.error(err)) worker-redis.js： 12345678910111213141516171819202122232425262728293031323334353637383940import Redis from 'ioredis'import &#123; processTask &#125; from './processTask.js'const redisClient = new Redis()const [, , consumerName] = process.argvasync function main() &#123; await redisClient.xgroup('CREATE', 'tasks_stream', 'workers_group', '$', 'MKSTREAM') .catch(() =&gt; console.log('Consumer group already exists')) const [[, records]] = await redisClient.xreadgroup( 'GROUP', 'workers_group', consumerName, 'STREAMS', 'tasks_stream', '0') for (const [recordId, [, rawTask]] of records) &#123; await processAndAck(recordId, rawTask) &#125; while (true) &#123; const [[, records]] = await redisClient.xreadgroup( 'GROUP', 'workers_group', consumerName, 'BLOCK', '0', 'COUNT', '1', 'STREAMS', 'tasks_stream', '&gt;') for (const [recordId, [, rawTask]] of records) &#123; await processAndAck(recordId, rawTask) &#125; &#125;&#125;async function processAndAck(recordId, rawTask) &#123; const found = processTask(JSON.parse(rawTask)) if (found) &#123; console.log(`Found! =&gt; $&#123;found&#125;`) await redisClient.xadd('results_stream', '*', 'result', `Found: $&#123;found&#125;`) &#125; await redisClient.xack('tasks_stream', 'workers_group', recordId)&#125;main().catch(err =&gt; console.error(err)) xgroup 命令用来确保 consumer group 存在。 CREATE 表示我们希望创建一个 consumer group tasks_stream 表示我们想要读取的 stream 的名字 workers_group 是 consumer group 的名字 第四个参数表示 consumer group 开始读取的记录的位置。$ 表示当前 stream 中最后一条记录的 ID MKSTREAM 表示如果 stream 不存在则创建它 通过 xreadgroup 命令读取属于当前 consumer 的所有 pending 的记录。 &#39;GROUP&#39;、&#39;workers_group&#39;、consumerName 用来指代 consumer group 和 consumer 的名字 STREAMS 和 tasks_stream 用来指代我们想要读取的 stream 的名字 0 用来表示我们想要开始读取的记录的位置。这里表示从属于当前 consumer 的第一条记录开始，读取所有 pending 的消息 通过另外一条 xreadgroup 命令读取 stream 里新增加的记录。 &#39;BLOCK&#39; 和 &#39;0&#39; 两个参数表示如果没有新的消息，就一直阻塞等待。&#39;0&#39; 具体表示一直等待永不超时 &#39;COUNT&#39; 和 &#39;1&#39; 表示一次请求只获取一条记录 特殊 ID &gt; 表示只获取还没有被当前的 consumer group 处理过的消息 processAndAck() 函数负责当 xreadgroup() 返回的记录被处理完成时，调用 xack 命令进行确认，将该记录从当前 consumer 的 pending 列表里移除 collector-redis.js：12345678910111213141516171819import Redis from 'ioredis'const redisClient = new Redis()async function main() &#123; let lastRecordId = '$' while (true) &#123; const data = await redisClient.xread( 'BLOCK', '0', 'STREAMS', 'results_stream', lastRecordId) for (const [, logs] of data) &#123; for (const [recordId, [, message]] of logs) &#123; console.log(`Message from worker: $&#123;message&#125;`) lastRecordId = recordId &#125; &#125; &#125;&#125;main().catch(err =&gt; console.error(err)) 运行程序测试效果：1234node worker-redis.js workerAnode worker-redis.js workerBnode collector-redis.jsnode producer-redis.js 4 f8e966d1e207d02c44511a58dccff2f5429e9a3b 参考资料Node.js Design Patterns: Design and implement production-grade Node.js applications using proven patterns and techniques, 3rd Edition]]></content>
      <categories>
        <category>Program</category>
      </categories>
      <tags>
        <tag>Pattern</tag>
        <tag>JavaScript</tag>
        <tag>Pipeline</tag>
        <tag>Message</tag>
        <tag>Node.js</tag>
        <tag>AMQP</tag>
        <tag>Redis</tag>
        <tag>RabbitMQ</tag>
        <tag>Stream</tag>
        <tag>Task</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Node.js 设计模式笔记 —— 消息中间件及其应用模式（发布订阅）]]></title>
    <url>%2F2023%2F01%2F03%2Fnode-js-design-patterns-message-queue-pub-sub-pattern%2F</url>
    <content type="text"><![CDATA[主要有两类技术可以用来整合分布式应用：一类是通过共享存储作为一个中心化的协调者，跟踪和保存所有需要共享的信息；另一类则是通过消息中间件，向系统中的所有节点散布数据、事件和命令等。消息存在于软件系统的各个层级。我们通过互联网交换消息完成通信；通过管道发送消息给其他进程；设备驱动通过消息与硬件进行交互等等。任何用于在组件和系统之间交换信息的离散或结构化数据都可以视为消息。 消息系统基础对于消息系统，有以下四个基本要素需要考虑： 通讯的方向。可以是单向的，也可以是“请求 - 响应”模式 通讯的目的。同时决定了消息本身的内容 消息的时效性。可以同步或者异步地发送与接收 消息的投递方式。可以直接投递也可以通过某个中间件 单向 vs “请求 - 应答”模式单向模式：消息从源头推送到目的地。常见的应用比如邮件系统、将工作任务分派给一系列工作节点的系统。 “请求 - 响应”模式：一方发出的消息总能够与对方发出的消息匹配。比如 web 服务的调用、向数据库请求数据等。 包含多个响应节点的“请求 - 响应”模式： 消息类型消息内容主要取决于通信的目的。通常有以下三种： 命令消息 事件消息 文档消息 命令消息用来令接收者触发某个动作或者任务。借助它可以实现远程过程调用（RPC）系统，分布式计算等。RESTful HTTP 请求就是简单的命令消息的例子。事件消息用来通知另一个组件发生了某些情况。事件在分布式系统中是一种很重要的整合机制，用来确保系统的各个组件保持同样的步调。文档消息基本上就是在组件之间传输数据。比如数据库请求的结果。 异步队列和流同步通信类似于打电话。电话的双方必须同时在线，连接到同一个通道，实时地交流信息。当我们需要打给另一个人时，通常就得搞一部新的手机或者挂掉当前正在进行的通话，拨打新的号码。异步通信类似于发短信。我们发送短信的时刻，并不需要接收方已经接入了网络。我们可以一条接一条地发送多条短信给不同的人，以任意顺序接收对方的回复（如果有的话）。 另一个异步通信的重要特性就是，消息可以被临时存储在某个地方，再在之后的某个时间送达。当接收方非常忙碌无法处理新的消息，或者我们需要确保投递的成功率时，这个特性就非常有用了。消息队列就是这样一种在生产者和消费者之间存储消息的中间组件。若消费者因为某种原因崩溃、断开连接等，消息会在队列中累积，待消费者重新上线时立即进行分发。 另外一种类似的数据结构是 log。log 是一种只能追加的结构，它是持久的，其消息可以在到达时被读取，也可以通过访问其历史记录来获取。在消息系统中，也常被叫做 stream。不同于队列，在 stream 中，消息被消费后不会被移除，意味着 stream 在消息的获取方面有着更高的自由度。队列通常一次只暴露一条消息给消费者，而一个 stream 能够被多个消费者共享（甚至是同一份消息）。 消息队列： 流： 点对点 vs 消息中间件 “发布 - 订阅” 模式就是一种分布式的观察者模式。 一个最小化的实时聊天应用package.json：1234567891011121314151617&#123; "type": "module", "dependencies": &#123; "amqplib": "^0.10.3", "ioredis": "^5.2.4", "JSONStream": "^1.3.5", "level": "^8.0.0", "leveldown": "^6.1.1", "levelup": "^5.1.1", "monotonic-timestamp": "^0.0.9", "serve-handler": "^6.1.5", "superagent": "^8.0.6", "ws": "^8.11.0", "yargs": "^17.6.2", "zeromq": "^6.0.0-beta.16" &#125;&#125; index.js：1234567891011121314151617181920212223242526import ws, &#123; WebSocketServer &#125; from 'ws'import &#123; createServer &#125; from 'http'import staticHandler from 'serve-handler'const server = createServer((req, res) =&gt; &#123; return staticHandler(req, res, &#123; public: 'www' &#125;)&#125;)const wss = new WebSocketServer(&#123; server &#125;)wss.on('connection', client =&gt; &#123; console.log('Client connected') client.on('message', msg =&gt; &#123; console.log(`Message: $&#123;msg&#125;`) broadcast(`$&#123;msg&#125;`) &#125;)&#125;)function broadcast(msg) &#123; for (const client of wss.clients) &#123; if (client.readyState == ws.OPEN) &#123; client.send(msg) &#125; &#125;&#125;server.listen(process.argv[2] || 8000) 首先创建一个 HTTP 服务，将所有请求转发给一个特别的 handler（staticHandler），该 handler 负责 serve 所有的静态文件 创建一个 WebSocket 服务实例，绑定到 HTTP 服务。同时监听来自 WebSocket 客户端的连接请求，以及客户端发送的消息 当某个客户端发送的新消息到达时，通过 broadcast() 函数将消息广播给所有的客户端 www/index.html：1234567891011121314151617181920212223242526272829&lt;!DOCTYPE html&gt;&lt;html&gt; &lt;body&gt; Messages: &lt;div id="messages"&gt;&lt;/div&gt; &lt;form id="msgForm"&gt; &lt;input type="text" placeholder="Send a message" id="msgBox"/&gt; &lt;input type="submit" value="Send"/&gt; &lt;/form&gt; &lt;script&gt; const ws = new WebSocket( `ws://$&#123;window.document.location.host&#125;` ) ws.onmessage = function (message) &#123; const msgDiv = document.createElement('div') msgDiv.innerHTML = message.data document.getElementById('messages').appendChild(msgDiv) &#125; const form = document.getElementById('msgForm') form.addEventListener('submit', (event) =&gt; &#123; event.preventDefault() const message = document.getElementById('msgBox').value ws.send(message) document.getElementById('msgBox').value = '' &#125;) &lt;/script&gt; &lt;/body&gt;&lt;/html&gt; 通过 node index.js 8002 命令运行应用，打开两个浏览器页面访问 Web 服务，测试聊天效果： 但我们的应用是无法进行横向扩展的。比如再启动一个新的服务实例 node index.js 8003，此时连接到 8002 的客户端无法与连接到 8003 的客户端通信。可以自行测试。 使用 Redis 作为消息中间件架构图如下所示。每个服务实例都会把从客户端收到的消息发布到消息中间件，同时也会通过中间件订阅从其他服务实例发布的消息。 通过客户端网页发送的消息传递给对应的 chat server chat server 把收到的消息发布到 Redis Redis 将收到的消息分发给所有的订阅方（chat server） chat server 将收到的消息再分发给所有连接的客户端 index-redis.js：1234567891011121314151617181920212223242526272829303132import ws, &#123; WebSocketServer &#125; from 'ws'import &#123; createServer &#125; from 'http'import staticHandler from 'serve-handler'import Redis from 'ioredis'const redisSub = new Redis()const redisPub = new Redis()const server = createServer((req, res) =&gt; &#123; return staticHandler(req, res, &#123; public: 'www' &#125;)&#125;)const wss = new WebSocketServer(&#123; server &#125;)wss.on('connection', client =&gt; &#123; console.log('Client connected') client.on('message', msg =&gt; &#123; console.log(`Message: $&#123;msg&#125;`) redisPub.publish('chat_message', `$&#123;msg&#125;`) &#125;)&#125;)redisSub.subscribe('chat_message')redisSub.on('message', (channel, msg) =&gt; &#123; for (const client of wss.clients) &#123; if (client.readyState === ws.OPEN) &#123; client.send(msg) &#125; &#125;&#125;)server.listen(process.argv[2] || 8000) 运行 node index-redis.js 8002、node index-redis.js 8003 两条命令启动两个服务实例，此时连接到不同服务器的客户端相互之间也能够进行通信。 点对点 Pub/Sub 模式通过 ZeroMQ 创建两种类型的 socket：PUB 和 SUB。PUB socket 绑定到本地机器的某个端口，负责监听来自其他机器上 SUB socket 的订阅请求。当一条消息通过 PUB socket 发送时，该消息会被广播到所有连接的 SUB socket。 index-zeromq.js：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051import &#123; createServer &#125; from 'http'import staticHandler from 'serve-handler'import ws, &#123; WebSocketServer &#125; from 'ws'import yargs from 'yargs'import zmq from 'zeromq'const server = createServer((req, res) =&gt; &#123; return staticHandler(req, res, &#123; public: 'www' &#125;)&#125;)let pubSocketasync function initializeSockets() &#123; pubSocket = new zmq.Publisher() await pubSocket.bind(`tcp://127.0.0.1:$&#123;yargs(process.argv).argv.pub&#125;`) const subSocket = new zmq.Subscriber() const subPorts = [].concat(yargs(process.argv).argv.sub) for (const port of subPorts) &#123; console.log(`Subscribing to $&#123;port&#125;`) subSocket.connect(`tcp://127.0.0.1:$&#123;port&#125;`) &#125; subSocket.subscribe('chat') for await (const [msg] of subSocket) &#123; console.log(`Message from another server: $&#123;msg&#125;`) broadcast(msg.toString().split(' ')[1]) &#125;&#125;initializeSockets()const wss = new WebSocketServer(&#123; server &#125;)wss.on('connection', client =&gt; &#123; console.log('Client connected') client.on('message', msg =&gt; &#123; console.log(`Message: $&#123;msg&#125;`) broadcast(`$&#123;msg&#125;`) pubSocket.send(`chat $&#123;msg&#125;`) &#125;)&#125;)function broadcast(msg) &#123; for (const client of wss.clients) &#123; if (client.readyState === ws.OPEN) &#123; client.send(msg) &#125; &#125;&#125;server.listen(yargs(process.argv).argv.http || 8000) 通过 yargs 模块解析命令行参数 通过 initializeSocket() 函数创建 Publisher，并绑定到由 --pub 命令行参数提供的端口上 创建 Subscriber socket 并将其连接到其他应用实例的 Publisher socket。被连接的 Publisher 端口由 --sub 命令行参数提供。之后创建以 chat 为过滤器的订阅，即只接收以 chat 开头的消息 通过 for 循环监听到达 Subscriber 的消息，去除消息中的 chat 前缀，通过 broadcast() 函数将处理后的消息广播给所有连接的客户端 当有消息到达当前实例的 WebSocket 服务时，广播此消息到所有客户端，同时通过 Publisher 发布该消息 运行服务测试效果：123node index-zeromq.js --http 8002 --pub 5000 --sub 5001 --sub 5002node index-zeromq.js --http 8003 --pub 5001 --sub 5000 --sub 5002node index-zeromq.js --http 8004 --pub 5002 --sub 5000 --sub 5001 通过队列实现可靠的消息投递消息队列是消息系统中的一种重要抽象。借助消息队列，通信中的发送方和接收方不必同时处于活跃的连接状态。队列系统会负责存储未投递的消息，直到目标处于能够接收的状态。 消息系统的投递机制可以简单概况为以下 3 类： 最多一次：fire-and-forget。消息不会被持久化，投递状态也不会被确认。意味着在接收者崩溃或者断开连接时，消息有可能丢失 最少一次：消息会确保至少被收到一次。但是重复收取同一条消息的情况有可能出现，比如接收者在收到消息后突然崩溃，没有来得及告知发送者消息已经收到。 只有一次：这是最可靠的投递机制，保证消息只会被接收一次。但由于需要更复杂的确认机制，会牺牲一部分消息投递的效率。 当消息投递机制可以实现“最少一次”或者“只有一次”时，我们就有了 durable subscriber。 AMQPAMQP 是一个被很多消息系统支持的开放标准协议。除了定义一个通用的传输协议以外，他还提供了用于描述 routing、filtering、queuing、reliability 和 security 的模型。 Queue：用于存储消息的数据结构。假如多个消费者绑定了同一个队列，消息在它们之间是负载均衡的。队列可以是以下任意一种类型： Durable：当中间件重启时队列会自动重建。但这并不意味着其内容也会被保留。实际上只有标记为持久化消息的内容才会被保存到磁盘，并在重启时恢复 Exclusive：队列只绑定给唯一一个特定的订阅者，当连接关闭时，队列即被销毁 Auto-delete：当最后一个订阅者断开连接时，队列被删除 Exchange：消息发布的地方。Exchange 会将消息路由至一个或者多个 queue。路由规则取决于具体的实现： Direct exchange：通过完整匹配一个 routing key 来对消息进行路由（如 chat.msg） Topic exchange：对 routing key 进行模糊匹配（如 chat.# 匹配所有以 chat 开头的 key） Fanout exchange：将消息广播至所有连接的 queue，忽略提供的任何 routing key Binding：Exchange 和 queue 之间的链接，定义了用于过滤消息的 routing key 或模式 上述所有组件由中间件进行维护，同时对外暴露用于创建和维护的 API。当连接到某个中间件时，客户端会创建一个 channel 对象负责维护通信的状态。 AMQP 和 RabbitMQ 实现 durable subscriberchat 应用和消息历史记录服务的架构图： AMQP 和数据库实现 history service此模块由两部分组成：一个 HTTP 服务负责将聊天历史记录暴露给客户端；一个 AMQP 消费者负责获取聊天消息并将它们保存在本地数据库中。 historySvc.js：1234567891011121314151617181920212223242526272829303132import &#123; createServer &#125; from 'http'import levelup from 'levelup'import leveldown from 'leveldown'import timestamp from 'monotonic-timestamp'import JSONStream from 'JSONStream'import amqp from 'amqplib'async function main() &#123; const db = levelup(leveldown('./msgHistory')) const connection = await amqp.connect('amqp://localhost') const channel = await connection.createChannel() await channel.assertExchange('chat', 'fanout') const &#123; queue &#125; = channel.assertQueue('chat_history') await channel.bindQueue(queue, 'chat') channel.consume(queue, async msg =&gt; &#123; const content = msg.content.toString() console.log(`Saving message: $&#123;content&#125;`) await db.put(timestamp(), content) channel.ack(msg) &#125;) createServer((req, res) =&gt; &#123; res.writeHead(200) db.createValueStream() .pipe(JSONStream.stringify()) .pipe(res) &#125;).listen(8090)&#125;main().catch(err =&gt; console.error(err)) 创建一个到 AMQP 中间件的连接 设置一个名为 chat 的 fanout 模式的 exchange。assertExchange() 函数会确保相应的 exchange 存在，否则就创建 创建一个名为 chat_history 的 queue，绑定给上一步中创建的 exchange 开始监听来自 queue 的消息，将收到的每一条消息保存至 LevelDB 数据库，以时间戳作为键。消息保存成功后由 channel.ack(msg) 进行确认。若确认动作未被中间件收到，则该条消息会保留在队列中再次被处理 index-amqp.js1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556import &#123; createServer &#125; from 'http'import staticHandler from 'serve-handler'import ws, &#123; WebSocketServer &#125; from 'ws'import amqp from 'amqplib'import JSONStream from 'JSONStream'import superagent from 'superagent'const httpPort = process.argv[2] || 8000async function main() &#123; const connection = await amqp.connect('amqp://localhost') const channel = await connection.createChannel() await channel.assertExchange('chat', 'fanout') const &#123; queue &#125; = await channel.assertQueue( `chat_srv_$&#123;httpPort&#125;`, &#123; exclusive: true &#125;) await channel.bindQueue(queue, 'chat') channel.consume(queue, msg =&gt; &#123; msg = msg.content.toString() console.log(`From queue: $&#123;msg&#125;`) broadcast(msg) &#125;, &#123; noAck: true &#125;) const server = createServer((req, res) =&gt; &#123; return staticHandler(req, res, &#123; public: 'www' &#125;) &#125;) const wss = new WebSocketServer(&#123; server &#125;) wss.on('connection', client =&gt; &#123; console.log('Client connected') client.on('message', msg =&gt; &#123; console.log(`Message: $&#123;msg&#125;`) channel.publish('chat', '', Buffer.from(msg)) &#125;) superagent .get('http://localhost:8090') .on('error', err =&gt; console.log(err)) .pipe(JSONStream.parse('*')) .on('data', msg =&gt; &#123; client.send(Buffer(msg).toString()) &#125;) &#125;) function broadcast(msg) &#123; for (const client of wss.clients) &#123; if (client.readyState === ws.OPEN) &#123; client.send(msg) &#125; &#125; &#125; server.listen(httpPort)&#125;main().catch(err =&gt; console.log(err)) 我们的聊天服务没必要是 durable subscriber，fire-and-forget 机制就足够了，因而有 { exclusive: true } 选项 确认机制也是不需要的。{ noAck: true } 发布消息也很简单，只需要指定目标 exchange（chat）和一个 routing key 即可，这里我们使用的是 fanout exchange，不需要路由，routing key 为空 发布到 exchange 的消息被转发到所有绑定的 queue，再到达所有订阅了 queue 的服务实例，每个实例再将消息发送到所有连接的客户端 通过 superagent 请求 history 微服务，将获取到的所有历史消息发送给刚连接的客户端 运行服务测试效果：123node index-amqp.js 8002node index-amqp.js 8003node historySvc.js 通过 streams 实现可靠的消息投递在系统集成的范畴里，stream（或 log）是一种有序的、只能追加的持久化的数据结构。Stream 概念里的 message 更应该叫做 record，总是被添加到 stream 末尾，且不会在被消费之后自动删除（不同于 queue）。这种特性令 stream 更像是一种数据仓库而不是消息中间件。Stream 的另一个重要特性在于，record 是被消费者从 stream 中“拉取”的，因而消费者可以按照自己的节奏处理 record。Stream 可以用来实现可靠的消息投递，一旦消费者崩溃，它可以在恢复后从中断的地方继续拉取消息。 Streams vs 消息队列Stream 明显的应用场景在于处理顺序的流数据，也支持批量处理或者根据之前的消息确定相关性，并可以跨多个节点分发数据。Stream 和消息队列都可以实现 Pub/Sub 模式，但消息队列更适合复杂的系统集成任务，它可以提供更复杂的路由机制，允许我们为不同的消息提供不同的优先级，而 Stream 中 record 的顺序是一定的。 通过 Redis Streams 实现 chat 应用index-stream.js：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354import &#123; createServer &#125; from 'http'import staticHandler from 'serve-handler'import ws, &#123; WebSocketServer &#125; from 'ws'import Redis from 'ioredis'const redisClient = new Redis()const redisClientXRead = new Redis()const server = createServer((req, res) =&gt; &#123; return staticHandler(req, res, &#123; public: 'www' &#125;)&#125;)const wss = new WebSocketServer(&#123; server &#125;)wss.on('connection', async client =&gt; &#123; console.log('Client connected') client.on('message', msg =&gt; &#123; console.log(`Message: $&#123;msg&#125;`) redisClient.xadd('chat_stream', '*', 'message', msg) &#125;) const logs = await redisClient.xrange( 'chat_stream', '-', '+') for (const [, [, message]] of logs) &#123; client.send(message) &#125;&#125;)function broadcast(msg) &#123; for (const client of wss.clients) &#123; if (client.readyState === ws.OPEN) &#123; client.send(msg) &#125; &#125;&#125;let lastRecordId = '$'async function processStreamMessages() &#123; while (true) &#123; const [[, records]] = await redisClientXRead.xread( 'BLOCK', '0', 'STREAMS', 'chat_stream', lastRecordId) for (const [recordId, [, message]] of records) &#123; console.log(`Message from stream: $&#123;message&#125;`) broadcast(message) lastRecordId = recordId &#125; &#125;&#125;processStreamMessages().catch(err =&gt; console.error(err))server.listen(process.argv[2] || 8080) xadd 负责在收到来自客户端的消息时，向 stream 添加一条新的 record。它接收 3 个参数： Stream 的名字，这里是 chat_stream record 的 ID。这里传入的是星号（*），令 Redis 为我们生成一个 ID。ID 必须是单调递增的，以保持 record 的顺序，而 Redis 可以替我们处理这些 key-value 的列表。这里只提供 value msg（从客户端收到的消息）的 ‘message’ key 使用 xrange 检索 stream 的过往记录，以获取聊天历史。我们在每次有客户端连接时就进行一次检索。其中 - 表示最小的 ID 值，+ 表示最大的 ID 值，因而整个 xrange 会获取当前 stream 中所有的消息 最后一部分的逻辑是等待新的记录被添加到 stream 中，从而每个应用实例都能读取到更新的消息。这里使用一个无线循环和 xread 命令： 其中 BLOCK 表示在新消息到达前阻塞 0 用来指定超时时间，超过这个时间则直接返回 null。0 代表不超时 STREAMS 是一个关键字，告诉 Redis 我们接下来会指定想要读取的 stream 的细节 chat_stream 是 stream 的名字 最后我们提供 record ID（lastRecordId）作为读取新消息的节点。初始情况下是 $，表示当前 stream 中最大的 ID。当我们读取第一条消息后，更新 lastRecordId 为最近读取到的消息的 ID 此外，解包消息的代码 for (const [, [, message]] of logs) {...} 实际上等同于 for (const [recordId, [propertyId, message]] of logs) {...}，由 xrange 命令查询到的消息的格式如下：1234[ [&quot;1588590110918-0&quot;, [&quot;message&quot;, &quot;This is a message&quot;]], [&quot;1588590130852-0&quot;, [&quot;message&quot;, &quot;This is another message&quot;]]] 参考资料Node.js Design Patterns: Design and implement production-grade Node.js applications using proven patterns and techniques, 3rd Edition]]></content>
      <categories>
        <category>Program</category>
      </categories>
      <tags>
        <tag>Pattern</tag>
        <tag>JavaScript</tag>
        <tag>Queue</tag>
        <tag>Message</tag>
        <tag>Node.js</tag>
        <tag>Observer</tag>
        <tag>Broker</tag>
        <tag>Pub/Sub</tag>
        <tag>AMQP</tag>
        <tag>Redis</tag>
        <tag>RabbitMQ</tag>
        <tag>Stream</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Programming with Types —— 组合类型]]></title>
    <url>%2F2022%2F12%2F15%2Fprogramming-with-types-compound-types%2F</url>
    <content type="text"><![CDATA[复合类型最直观的创造新的复合类型的方式，就是直接将多个类型组合在一起。比如平面上的点都有 X 和 Y 两个坐标，各自都属于 number 类型。因此可以说，平面上的点是由两个 number 类型组合成的新类型。通常来说，将多个类型直接组合在一起形成新的类型，这样的类型最终的取值范围，就是全部成员类型所有可能的组合值的集合。 元组假如我们需要一个函数来计算两个点之间的距离，可以这样实现：123function distance(x1: number, y1: number, x2: number, y2: number): number &#123; return Math.sqrt((x1 - x1) ** 2 + (y1 - y2) ** 2)&#125; 上述实现能够正常工作，但并不算完美。x1 在没有对应的 Y 坐标一起出现的情况下，是没有任何实际含义的。同时在应用的其他地方，我们很可能也会遇到很多针对坐标点的其他操作，因此相对于将 X 坐标和 Y 坐标独立地进行表示和传递，我们可以将两者组合在一起，成为一个新的元组类型。元组能够帮助我们将单独的 X 和 Y 坐标组合在一起作为“点”对待，从而令代码更方便阅读和书写。123456type Point = [number, number]function distance(point1: Point, point2: Point): number &#123; return Math.sqrt( (point1[0] - point2[0]) ** 2 + (point1[1] - point2[1]) ** 2);&#125; DIY 元组大部分语言都提供了元组作为内置语法，这里假设在标准库里没有元组的情况下，如何自己实现包含两个元素的元组类型：12345678910111213141516class Pair&lt;T1, T2&gt; &#123; m0: T1; m1: T2; constructor(m0: T1, m1: T2) &#123; this.m0 = m0; this.m1 = m1; &#125;&#125;type Point = Pair&lt;number, number&gt;;function distance(point1: Point, point2: Point): number &#123; return Math.sqrt( (point1.m0 - point2.m0) ** 2 + (point1.m1 - point2.m1) ** 2);&#125; Record 类型将坐标点定义为数字对，是可以正常工作的。但是我们也因此失去了在代码中包含更多含义的机会。在前面的例子中，我们假定第一个数字是 X 坐标，第二个数字是 Y 坐标。但最好是借助类型系统，在代码中编入更精确的含义。从而彻底消除将 X 错认为是 Y 或者将 Y 错认为是 X 的机会。可以借助 Record 类型来实现：1234567891011121314class Point &#123; x: number; y: number; constructor(x: number, y: number) &#123; this.x = x; this.y = y; &#125;&#125;function distance(point1: Point, point2: Point): number &#123; return Math.sqrt( (point1.x - point2.x) ** 2 + (point1.y - point2.y) ** 2);&#125; 首要的原则是，最好优先使用含义清晰的 Record 类型，它包含的元素是有明确的命名的。而不是直接将元组传来传去。元组并不会为自己的元素提供名称，只是靠数字索引访问，因而会存在很大的误解的可能性。当然另一方面，元组是内置的，而 Record 类型通常需要额外进行定义。但大多数情况下，这样的额外工作是值得的。 维持不可变性类的成员函数和成员变量可以被定义为 public（能够被公开访问），也可以被定义为 private（只允许内部访问）。在 TypeScript 中，成员默认都是公开的。通常情况下我们定义 Record 类型，如果其成员变量是独立的，比如之前的 Point，X 坐标和 Y 坐标都可以独立的进行修改，不会影响到对方。且它们的值可以在不引起问题的情况下变化。像这样的成员被定义成公开的一般不会出现问题。但是也存在另外一些情况。比如下面这个由 dollar 值和 cents 值组成的 Currency 类型： dollar 值必须是一个大于或者等于 0 的整数 cent 值也必须是一个大于或者等于 0 的整数 cent 值不能大于 99，每 100 cents 都必须转换成 1 dollar 如果我们允许 dollars 和 cents 变量被公开访问，就有可能导致出现不规范的对象：123456789101112131415161718192021class Currency &#123; dollars: number; cents: number; constructor(dollars: number, cents: number) &#123; if (!Number.isSafeInteger(cents) || cents &lt; 0) throw new Error(); dollars = dollars + Math.floor(cents / 100); cents = cents % 100; if (!Number.isSafeInteger(dollars) || dollars &lt; 0) throw new Error(); this.dollars = dollars; this.cents = cents; &#125;&#125;let amount: Currency = new Currency(5, 50);amount.cents = 300; // 由于属性是公开的，外部代码可以直接修改。从而产生非法对象 上述情况可以通过将成员变量定义为 private 来避免。同时为了维护方便，一般还需要提供公开的方法对私有的属性进行修改。这些方法通常会包含一定的验证规则，确保修改后的对象状态是合法的。1234567891011121314151617181920212223242526272829303132class Currency &#123; private dollars: number = 0; private cents: number = 0; constructor(dollars: number, cents: number) &#123; this.assignDollars(dollars); this.assignCents(cents); &#125; getDollars(): number &#123; return this.dollars; &#125; assignDollars(dollars: number) &#123; if (!Number.isSafeInteger(dollars) || dollars &lt; 0) throw new Error(); this.dollars = dollars; &#125; getCents(): number &#123; return this.cents; &#125; assignCents(cents: number) &#123; if (!Number.isSafeInteger(cents) || cents &lt; 0) throw new Error(); this.assignDollars(this.dollars + Math.floor(cents / 100)); this.cents = cents % 100; &#125;&#125; 外部代码只能通过 assignDollars() 和 assignCents() 两个公开的方法，对私有的属性 dollars 和 cents 进行修改。同时这两个方法也会确保对象的状态一直符合我们定义的规则。 另外一种观点是，可以将属性定义成不可变（只读）的。这样属性就可以直接被外部访问，因为只读属性会阻止自身被修改。从而对象状态保持合法。123456789101112131415161718class Currency &#123; readonly dollars: number; readonly cents: number; constructor(dollars: number, cents: number) &#123; if (!Number.isSafeInteger(cents) || cents &lt; 0) throw new Error(); dollars = dollars + Math.floor(cents / 100); cents = cents % 100; if (!Number.isSafeInteger(dollars) || dollars &lt; 0) throw new Error(); this.dollars = dollars; this.cents = cents; &#125;&#125; 不可变对象还有一个优势，从不同的线程对这类数据并发地访问是保证安全的。可变性会导致数据竞争。但其劣势在于，每次我们需要一个新的值，就必须创建一个新的实例，无法通过修改现有对象得到。而创建新对象有时候是很昂贵的操作。 最终的目的在于，阻止外部代码直接修改属性，以至于跳过验证规则。可以将属性变为私有，对属性的访问完全通过包含验证规则的公开方法；也可以将属性声明为不可变的，在构造对象时执行验证。 either-or 类型either-or 是另外一种基础的将类型组合在一起的方式，即某个值有可能是多个类型所有合法取值中的任何一个。比如 Rust 语言中的 Result&lt;T, E&gt;，可能是成功的值 Ok(T)，也可能是失败值 Err(E)。 枚举先从一个简单的例子开始，通过类型系统编码周一到周日。我们可以用 0-6 的数字来表示一周的七天，0 表示一周里的第一天。但这样表示并不理想，因为不同的工程师可能对这些数字有不同的理解。有些国家第一天是周日，有些国家第一天是周一。1234567function isWeekend(dayOfWeek: number): boolean &#123; return dayOfWeek == 5 || dayOfWeek == 6;&#125; // 欧洲国家判断是否是周末function isWeekday(dayOfWeek: number): boolean &#123; return dayOfWeek &gt;= 1 &amp;&amp; dayOfWeek &lt;= 5;&#125; // 美国判断是否是工作日 上述两个函数是冲突的。若 0 表示周日，则 isWeekend() 是不正确的；若 0 表示周一，则 isWeekday() 是不正确的。 其他的方案是定义一系列常量用来表示一周七天。123456789101112131415const Sunday: number = 0;const Monday: number = 1;const Tuesday: number = 2;const Wednesday: number = 3;const Thursday: number = 4;const Friday: number = 5;const Saturday: number = 0;function isWeekend(dayOfWeek: number): boolean &#123; return dayOfWeek == Saturday || dayOfWeek == Sunday;&#125;function isWeekday(dayOfWeek: number): boolean &#123; return dayOfWeek &gt;= Monday &amp;&amp; dayOfWeek &lt;= Friday;&#125; 现在的实现看上去好了一些，但仍有问题。单看函数的签名，无法清楚的知道 number 类型的参数的期待值具体是什么。假如一个新接手代码的人刚看到 dayOfWeek: number，他可能不会意识到存在 Sunday 这类常量在某个模块的某处。因而他们会倾向于自己解释此处的数字。甚至一些人会传入非法的数字参数比如 -1 或 10。 更好的方案是借助枚举类型。12345678910111213141516171819enum DayOfWeek &#123; Sunday, Monday, Tuesday, Wednesday, Thursday, Friday, Saturday&#125;function isWeekend(dayOfWeek: DayOfWeek): boolean &#123; return dayOfWeek == DayOfWeek.Saturday || dayOfWeek == DayOfWeek.Sunday;&#125;function isWeekday(dayOfWeek: DayOfWeek): boolean &#123; return dayOfWeek &gt;= DayOfWeek.Monday &amp;&amp; dayOfWeek &lt;= DayOfWeek.Friday;&#125; Optional 类型假设我们需要将一个用户输入的 string 值转换为 DayOfWeek，若该 string 值是合法的，则返回对应的 DayOfWeek；若该 string 值非法，则显式地返回 undefined。在 TypeScript 中，可以通过 | 类型操作符来实现，| 允许我们组合多个类型。 1234567891011121314151617181920212223function parseDayOfWeek(input: string): DayOfWeek | undefined &#123; switch (input.toLowerCase()) &#123; case "sunday": return DayOfWeek.Sunday; case "monday": return DayOfWeek.Monday; case "tuesday": return DayOfWeek.Tuesday; case "Wednesday": return DayOfWeek.Wednesday; case "thursday": return DayOfWeek.Thursday; case "friday": return DayOfWeek.Friday; case "saturday": return DayOfWeek.Saturday; default: return undefined; &#125;&#125;function useInput(input: string) &#123; let result: DayOfWeek | undefined = parseDayOfWeek(input); if (result === undefined) &#123; console.log(`Failed to parse "$&#123;input&#125;"`); &#125; else &#123; let dayOfWeek: DayOfWeek = result; /* Use dayOfWeek */ &#125;&#125; 上述 parseDayOfWeek() 函数返回一个 DayOfWeek 或者 undefined。useInput() 函数在调用 parseDayOfWeek() 后再对返回值进行解包操作，输出错误信息或者得到合法值。 Optional 类型：也常被叫做 Maybe 类型，表示一个可能存在的 T 类型值。一个 Optional 类型的实例，可能会包含一个 T 类型的任意值；也可能是一个特殊值，用来表示 T 类型的值不存在。 DIY Optional1234567891011121314151617181920212223class Optional&lt;T&gt; &#123; private value: T | undefined; private assigned: boolean; constructor(value?: T) &#123; if (value) &#123; this.value = value; this.assigned = true; &#125; else &#123; this.value = undefined; this.assigned = false; &#125; &#125; hasValue(): boolean &#123; return this.assigned; &#125; getValue(): T &#123; if (!this.assigned) throw Error(); return &lt;T&gt;this.value; &#125;&#125; Optional 类型的优势在于，直接使用 null 空类型非常容易出错。因为判断一个变量什么时候能够为空或者不能为空是非常困难的，我们必须在所有代码中添加非空检查，否则就会有引用指向空值的风险，进一步导致运行时错误。Optional 背后的逻辑在于，将 null 值从合法的取值范围中解耦出来。Optional 明确了哪些变量有可能为空值。类型系统知晓 Optional 类型（比如 DayOfWeek | undefined，可能为空）和对应的非空类型（DayOfWeek）是不一样的。两者是不兼容的类型，因而我们不会将 Optional 类型及其非空类型相混淆，在需要非空类型的地方错误地使用有可能为空值的 Optional。一旦需要取出 Optional 中包含的值，就必须显式地进行解包操作，对空值进行检查。 Result or error现在尝试扩展前面的 DayOfWeek 例子。当 DayOfWeek 值无法正常识别时，我们不是简单地返回 undefined，而是输出包含更多内容的错误信息。常见的一个反模式就是同时返回 DayOfWeek 和错误码。 123456789101112131415161718192021222324252627282930313233343536373839enum InputError &#123; OK, NoInput, Invalid&#125;class Result &#123; error: InputError; value: DayOfWeek; constructor(error: InputError, value: DayOfWeek) &#123; this.error = error; this.value = value &#125;&#125;function parseDayOfWeek(input: string): Result &#123; if (input == "") return new Result(InputError.NoInput, DayOfWeek.Sunday); switch (input.toLowerCase()) &#123; case "sunday": return new Result(InputError.OK, DayOfWeek.Sunday); case "monday": return new Result(InputError.OK, DayOfWeek.Monday); case "tuesday": return new Result(InputError.OK, DayOfWeek.Tuesday); case "wednesday": return new Result(InputError.OK, DayOfWeek.Wednesday); case "thursday": return new Result(InputError.OK, DayOfWeek.Thursday); case "friday": return new Result(InputError.OK, DayOfWeek.Friday); case "saturday": return new Result(InputError.OK, DayOfWeek.Saturday); default: return new Result(InputError.Invalid, DayOfWeek.Sunday); &#125;&#125; 上述实现并不是理想的，原因在于，一旦我们忘记了检查错误代码，没有任何机制阻止我们继续使用 DayOfWeek 值。即便错误代码表明有问题出现，我们仍然可以忽视该错误并直接取用 DayOfWeek。将类型看作值的集合，则上述 Result 类型实际上是 InputError 和 DayOfWeek 所有可能值的组合。 我们应该实现一种 either-or 类型，返回值要么是错误类型，要么是合法的值。 DIY EitherEither 类型包含了 TLeft 和 TRight 另外两种类型。TLeft 用来存储错误类型，TRight 保存合法的值。 1234567891011121314151617181920212223242526272829303132333435class Either&lt;TLeft, TRight&gt; &#123; private readonly value: TLeft | TRight; private readonly left: boolean; private constructor(value: TLeft | TRight, left: boolean) &#123; this.value = value; this.left = left; &#125; isLeft(): boolean &#123; return this.left; &#125; getLeft(): TLeft &#123; if (!this.isLeft()) throw new Error(); return &lt;TLeft&gt;this.value; &#125; isRight(): boolean &#123; return !this.left; &#125; getRight(): TRight &#123; if (!this.isRight()) throw new Error(); return &lt;TRight&gt;this.value; &#125; static makeLeft&lt;TLeft, TRight&gt;(value: TLeft) &#123; return new Either&lt;TLeft, TRight&gt;(value, true); &#125; static makeRight&lt;TLeft, TRight&gt;(value: TRight) &#123; return new Either&lt;TLeft, TRight&gt;(value, false); &#125;&#125; 借助上面的 Either 实现，我们可以将 parseDayOfWeek() 更新为返回 Either&lt;InputError, DayOfWeek&gt;。若函数返回 InputError，则结果中就不会包含 DayOfWeek；若函数返回 DayOfWeek，就可以肯定没有错误发生。当然，我们需要显式地将结果（或 Error）从 Either 中解包出来。 123456789101112131415161718192021222324252627282930enum InputError &#123; NoInput, Invalid&#125;type Result = Either&lt;InputError, DayOfWeek&gt;function parseDayOfWeek(input: string): Result &#123; if (input == "") return Either.makeLeft(InputError.NoInput) switch (input.toLowerCase()) &#123; case "sunday": return Either.makeRight(DayOfWeek.Sunday); case "monday": return Either.makeRight(DayOfWeek.Monday); case "tuesday": return Either.makeRight(DayOfWeek.Tuesday); case "wednesday": return Either.makeRight(DayOfWeek.Wednesday); case "thursday": return Either.makeRight(DayOfWeek.Thursday); case "friday": return Either.makeRight(DayOfWeek.Friday); case "saturday": return Either.makeRight(DayOfWeek.Saturday); default: return Either.makeLeft(InputError.Invalid); &#125;&#125; 当错误本身并不是“异常的”（大部分情况下，处理用户输入的时候），或者调用某个会返回错误码的系统 API，我们并不想直接抛出异常，但仍旧需要传递正确值或者错误码这类信息。这些时候，最好将这类信息编码到 either value or error 中。 参考资料Programming with Types]]></content>
      <categories>
        <category>Program</category>
      </categories>
      <tags>
        <tag>Type</tag>
        <tag>JavaScript</tag>
        <tag>Tuple</tag>
        <tag>TypeScript</tag>
        <tag>Optional</tag>
        <tag>Either</tag>
        <tag>Enum</tag>
        <tag>Immutable</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Programming with Types —— 高阶类型（Functor、Monad）]]></title>
    <url>%2F2022%2F11%2F29%2Fprogramming-with-types-higher-kinded-types-functor-monad%2F</url>
    <content type="text"><![CDATA[通用的 map 实现map 是函数式编程中非常常见的一类接口，可以将某个函数操作应用到一系列元素上。一个通用的 map() 实现如下： 123456function* map&lt;T, U&gt;(iter: Iterable&lt;T&gt;, func: (item: T) =&gt; U): IterableIterator&lt;U&gt; &#123; for (const value of iter) &#123; yield func(value); &#125;&#125; 上述实现主要针对可迭代对象，可以将函数 func（类型为 (item: T) =&gt; U）应用给可迭代对象 iter 中的每一个元素。为了使 map() 函数的场景更为通用，func 的参数 item: T 理应能够接收更多类型的值，比如 Option&lt;T&gt;。1234567891011121314151617181920212223class Optional&lt;T&gt; &#123; private value: T | undefined; private assigned: boolean; constructor(value?: T) &#123; if (value) &#123; this.value = value; this.assigned = true; &#125; else &#123; this.value = undefined; this.assigned = false; &#125; &#125; hasValue(): boolean &#123; return this.assigned; &#125; getValue(): T &#123; if (!this.assigned) throw Error(); return &lt;T&gt;this.value; &#125;&#125; 从逻辑上看，将一个类型为 (value: T) =&gt; U 的函数 map 到 Optional&lt;T&gt; 类型，如果该 Optional&lt;T&gt; 里面包含一个类型为 T 的值，则返回值应该是包含 U 的 Optional&lt;U&gt; 类型；若 Optional&lt;T&gt; 并不包含任何值，则 map 操作应该返回一个空的 Optional&lt;U&gt;。 下面是支持 Optional 类型的 map 实现：12345678910namespace Optional &#123; export function map&lt;T, U&gt;( optional: Optional&lt;T&gt;, func: (value: T) =&gt; U): Optional&lt;U&gt; &#123; if (optional.hasValue()) &#123; return new Optional&lt;U&gt;(func(optional.getValue())); &#125; else &#123; return new Optional&lt;U&gt;(); &#125; &#125;&#125; 另一种简单的通用类型 Box&lt;T&gt; 及其 map 实现：1234567891011121314class Box&lt;T&gt; &#123; value: T; constructor(value: T) &#123; this.value = value &#125;&#125;namespace Box &#123; export function map&lt;T, U&gt;( box: Box&lt;T&gt;, func: (value: T) =&gt; U): Box&lt;U&gt; &#123; return new Box&lt;U&gt;(func(box.value)); &#125;&#125; 将类型为 (value: T) =&gt; U 的函数 map 到 Box&lt;T&gt;，返回一个 Box&lt;U&gt;。Box&lt;T&gt; 中 T 类型的值会被取出来，传递给被 map 的函数，再将结果放入 Box&lt;U&gt; 中返回。 处理结果 or 传递错误假设我们需要实现一个 square() 函数来计算某个数字的平方，以及一个 stringify 函数将数字转换为字符串。示例如下：1234567function square(value: number): number &#123; return value ** 2;&#125;function stringify(value: number): string &#123; return value.toString();&#125; 还有一个 readNumber() 函数负责从文件中读取数字。当我们需要处理输入数据时，有可能会遇到某些问题，比如文件不存在或者无法打开等。在上述情况下，readNumber() 函数会返回 undefined。1234function readNumber(): number | undefined &#123; /* Implementation omitted */ return 2&#125; 如果我们想通过 readNumber() 读取一个数字，再将其传递给 square() 处理，就必须确保 readNumber() 返回的值是一个实际的数字，而不是 undefined。一种可行的方案就是借助 if 语句将 number | undefined 转换为 number。12345function process(): string | undefined &#123; let value: number | undefined = readNumber(); if (value == undefined) return undefined; return stringify(square(value));&#125; square() 接收数字类型的参数，因而当输入有可能是 undefined 时，我们需要显式地处理这类情况。但通常意义上讲，代码的分支越少，其复杂性就越低，就更易于理解和维护。另一种实现 process() 的方式就是，并不对 undefined 做任何处理，只是将其简单地传递下去。即只让 process() 负责数字的处理工作，error 则交给后续的其他人。 可以借助 为 sum type 实现的 map()： 12345678910111213141516namespace SumType &#123; export function map&lt;T, U&gt;( value: T | undefined, func: (value: T) =&gt; U): U | undefined &#123; if (value == undefined) &#123; return undefined; &#125; else &#123; return func(value); &#125; &#125;&#125;function process(): string | undefined &#123; let value: number | undefined = readNumber(); let squaredValue: number | undefined = SumType.map(value, square) return SumType.map(squaredValue, stringify);&#125; 此时的 process() 实现不再包含分支逻辑。将 number | undefined 解包为 number 并对 underfined 进行检查的操作由 map() 负责。 同时 map() 是通用的函数，可以直接在其他 process 函数中对更多不同类型的数据使用（如 string | undefined），减少重复代码。 版本一（不借助 map）：1234567891011121314151617function squareSumType(value: number | undefined): number | undefined &#123; if (value == undefined) return undefined; return square(value);&#125;function squareBox(box: Box&lt;number&gt;): Box&lt;number&gt; &#123; return new Box(square(box.value))&#125;function stringifySumType(value: number | undefined): string | undefined &#123; if (value == undefined) return undefined; return stringify(value)&#125;function stringifyBox(box: Box&lt;number&gt;): Box&lt;string&gt; &#123; return new Box(stringify(box.value));&#125; 版本二（借助 map）：123456let x: number | undefined = 1;let y: Box&lt;number&gt; = new Box(42);console.log(SumType.map(x, stringify))console.log(Box.map(y, stringify))console.log(SumType.map(x, square))console.log(Box.map(y, square)) Functor 定义Functor：对于任意的泛型，比如 Box&lt;T&gt;，能够通过 map() 操作将函数 (value: T) =&gt; U 应用给 Box&lt;T&gt;，并返回一个 Box&lt;U&gt;。 又或者说，Functor 是支持某种 map() 函数的任意类型 H&lt;T&gt;。该 map() 函数接收 H&lt;T&gt; 作为参数，一个从 T 到 U 的函数作为另一个参数，最终返回 H&lt;U&gt;。以更面向对象一点的形式来表现的话，参考如下代码（当然这段代码是编译不通过的，因为 TypeScript 不支持高阶类型，如 &lt;H&lt;T&gt;&gt;）：123456789101112131415interface Functor&lt;H&lt;T&gt;&gt; &#123; map&lt;U&gt;(func: (value: T) =&gt; U): H&lt;U&gt;;&#125;class Box&lt;T&gt; implements Functor&lt;Box&lt;T&gt;&gt; &#123; value: T; constructor(value: T) &#123; this.value = value; &#125; map&lt;U&gt;(func: (value: T) =&gt; U): Box&lt;U&gt; &#123; return new Box(func(this.value)); &#125;&#125; Functors for functions实际上还存在针对函数的 Functor。 123456789101112131415161718namespace Function &#123; export function map&lt;T, U&gt;( f: (arg1: T, arg2: T) =&gt; T, func: (value: T) =&gt; U) : (arg1: T, arg2: T) =&gt; U &#123; return (arg1: T, arg2: T) =&gt; func(f(arg1, arg2)); &#125;&#125;function add(x: number, y: number): number &#123; return x + y;&#125;function stringify(value: number): string &#123; return value.toString();&#125;const result: string = Function.map(add, stringify)(40, 2);console.log(result) Monads在前面的例子中，只有第一个函数 readNumber() 有可能返回错误（undefined）。借助 Functor，square() 和 stringify() 可以不经修改地正常调用，若 readNumber() 返回 undefined，该 undefined 不会被处理，只是简单地传递下去。但是假如链条中的每一个函数都有可能返回错误，又该如何处理呢？ 假设我们需要打开某个文件，读取其内容，再将读取到的字符串反序列化为一个 Cat 对象。负责打开文件的 openFile() 函数可能返回一个 Error 或者 FileHandle。比如当文件不存在、文件被其他进程锁定或者用户没有权限读取文件，都会导致返回 Error。还需要一个 readFile() 函数，接收 FileHandle 作为参数，返回一个 Error 或者 String。比如有可能内存不足导致文件无法被读取，返回 Error。最后还需要一个 deserializeCat() 函数接收 string 作为参数，返回一个 Error 或者 Cat 对象。同样的道理，string 有可能格式不符合要求，无法被反序列化为 Cat 对象，返回 Error。 所有上述函数都遵循一种“返回一个正常结果或者一个错误对象”的模式，其返回值类型为 Either&lt;Error, ...&gt;。123declare function openFile(path: string): Either&lt;Error, FileHandle&gt;;declare function readFile(handle: FileHandle): Either&lt;Error, string&gt;;declare function deserializeCat(serializedCat: string): Either&lt;Error, Cat&gt;; 只是为了方便举例，上述函数并不包含具体的实现代码。同时 Either 类型的实现如下：1234567891011121314151617181920212223242526272829303132333435class Either&lt;TLeft, TRight&gt; &#123; private readonly value: TLeft | TRight; private readonly left: boolean; private constructor(value: TLeft | TRight, left: boolean) &#123; this.value = value; this.left = left; &#125; isLeft(): boolean &#123; return this.left; &#125; getLeft(): TLeft &#123; if (!this.isLeft()) throw new Error(); return &lt;TLeft&gt;this.value; &#125; isRight(): boolean &#123; return !this.left; &#125; getRight(): TRight &#123; if (this.isRight()) throw new Error(); return &lt;TRight&gt;this.value; &#125; static makeLeft&lt;TLeft, TRight&gt;(value: TLeft) &#123; return new Either&lt;TLeft, TRight&gt;(value, true); &#125; static makeRight&lt;TLeft, TRight&gt;(value: TRight) &#123; return new Either&lt;TLeft, TRight&gt;(value, false) &#125;&#125; 最终将上述各个函数连接起来的 process 函数类似下面这样：1234567function readCatFromFile(path: string): Either&lt;Error, Cat&gt; &#123; let handle: Either&lt;Error, FileHandle&gt; = openFile(path); if (handle.isLeft()) return Either.makeLeft(handle.getLeft()); let content: Either&lt;Error, string&gt; = readFile(handle.getRight()); if (content.isLeft()) return Either.makeLeft(content.getLeft()); return deserializeCat(content.getRight());&#125; 就像在上一个例子中对 process 函数做的那样，我们可以实现一个类似的 map() 函数，将 readCatFromFile() 中的所有分支结构和错误检查都转移到通用的 map() 中。按照普遍的约定，Either&lt;TLeft, TRight&gt; 中的 TLeft 包含错误对象，map() 只会将其不做改动地传递下去。只有当 TRight 存在时，map() 才会对 Either 应用给定的函数。12345678namespace Either &#123; export function map&lt;TLeft, TRight, URight&gt;( value: Either&lt;TLeft, TRight&gt;, func: (value: TRight) =&gt; URight): Either&lt;TLeft, URight&gt; &#123; if (value.isLeft()) return Either.makeLeft(value.getLeft()); return Either.makeRight(func(value.getRight())); &#125;&#125; 上述 map() 实现的问题在于，当我们调用 openFile() 得到返回值 Either&lt;Error, FileHandle&gt;，接下来就需要一个类型为 (value: FileHandle) =&gt; string 的函数从 FileHandle 读取文件内容。但是实际上的 readFile() 函数的返回类型不是 string，而是 Either&lt;Error, string&gt;。 当我们调用12let handle: Either&lt;Error, FileHandle&gt; = openFile(path);let content: Either&lt;Error, string&gt; = Either.map(handle, readFile); 会导致爆出 Type &#39;Either&lt;Error, Either&lt;Error, string&gt;&gt;&#39; is not assignable to type &#39;Either&lt;Error, string&gt;&#39;. 错误。 正确的实现应该是如下形式的 bind() 方法：123456789namespace Either &#123; export function bind&lt;TLeft, TRight, URight&gt;( value: Either&lt;TLeft, TRight&gt;, func: (value: TRight) =&gt; Either&lt;TLeft, URight&gt; ): Either&lt;TLeft, URight&gt; &#123; if (value.isLeft()) return Either.makeLeft(value.getLeft()); return func(value.getRight()); &#125;&#125; 借助 bind() 实现的 readCatFromFile() 函数：12345function readCatFromFile(path: string): Either&lt;Error, Cat&gt; &#123; let handle: Either&lt;Error, FileHandle&gt; = openFile(path); let content: Either&lt;Error, string&gt; = Either.bind(handle, readFile); return Either.bind(content, deserializeCat);&#125; Functor vs Monad对于 Box&lt;T&gt;，Functor（map()）会接收一个 Box&lt;T&gt; 值和一个从 T 到 U 的函数（(value: T) =&gt; U）作为参数，将 T 值取出并应用给传入的函数，最终返回 Box&lt;U&gt;。Monad（bind()）接收一个 Box&lt;T&gt; 值和一个从 T 到 Box&lt;U&gt; 的函数（(value: T) =&gt; Box&lt;U&gt;）作为参数，将 T 值取出并应用给传入的函数，最终返回 Box&lt;U&gt;。 1234567891011121314151617181920212223242526272829303132333435363738class Box&lt;T&gt; &#123; value: T; constructor(value: T) &#123; this.value = value &#125;&#125;namespace Box &#123; export function map&lt;T, U&gt;( box: Box&lt;T&gt;, func: (value: T) =&gt; U): Box&lt;U&gt; &#123; return new Box&lt;U&gt;(func(box.value)); &#125; export function bind&lt;T, U&gt;( box: Box&lt;T&gt;, func: (value: T) =&gt; Box&lt;U&gt;): Box&lt;U&gt; &#123; return func(box.value); &#125;&#125;function stringify(value: number): string &#123; return value.toString();&#125;const s: Box&lt;string&gt; = Box.map(new Box(42), stringify);console.log(s)// =&gt; Box &#123; value: '42' &#125;function boxify(value: number): Box&lt;string&gt; &#123; return new Box(value.toString());&#125;const b: Box&lt;string&gt; = Box.bind(new Box(42), boxify);console.log(b)// =&gt; Box &#123; value: '42' &#125; Monad 定义Monad 表示对于泛型 H&lt;T&gt;，我们有一个 unit() 函数能够接收 T 作为参数，返回类型为 H&lt;T&gt; 的值；同时还有一个 bind() 函数接收 H&lt;T&gt; 和一个从 T 到 H&lt;U&gt; 的函数作为参数，返回 H&lt;U&gt;。现实中能够将 Promise 串联起来的 then() 方法实际上就等同于 bind()，能够从值创建 Promise 的 resolve() 方法等同于 unit()。 借助 Monad，函数调用序列可以表示为一条抽离了数据管理、控制流程或副作用的管道。 参考资料Programming with Types]]></content>
      <categories>
        <category>Program</category>
      </categories>
      <tags>
        <tag>Functional</tag>
        <tag>Type</tag>
        <tag>Map</tag>
        <tag>TypeScript</tag>
        <tag>Functor</tag>
        <tag>Monad</tag>
        <tag>generic</tag>
        <tag>Optional</tag>
        <tag>Either</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kubernetes 设计模式笔记 —— 生命周期管理]]></title>
    <url>%2F2022%2F11%2F29%2Fkubernetes-patterns-reading-notes-managing-lifecycle%2F</url>
    <content type="text"><![CDATA[由云原生平台管理的容器化应用，并不能控制其自身的生命周期。它们必须监听由管理平台发出的事件，再对生命周期做出相对应的变更。所谓的生命周期管理，即代表应用该如何读取和响应这些由管理平台发出的生命周期事件。 鉴于某些管理策略或者外部环境因素，在任意时间点，管理平台都有可能需要启动或者终止容器中的应用。容器化应用可以决定平台发出的哪些事件是重要的，应该以怎样的行为去响应。这实际上是一个平台用来同应用进行沟通、向应用发送命令的 API，应用可以选择接受或者忽略。 仅仅使用进程模型来启动和终止应用通常是不够的，现实世界中的应用往往需要更细粒度的交互控制和生命周期管理。有些应用需要 warm up，有些需要一个平滑、干净的关闭流程。因而 Kubernetes 设计了如下几种由平台发出的事件，容器可以选择监听和响应这些事件。 SIGTERM Signal当 Kubernetes 决定关闭某个容器时，该容器会收到一个 SIGTERM 信号，之后容器会尝试尽快完成关闭流程。对于某些应用来说，干净的快速终止是可行的。但另外一些应用有可能需要完成处理中的请求，释放打开的连接，清理临时文件等。这会耗费更多的时间。 SIGKILL Signal当某个容器进程在收到 SIGTERM 信号后并没有关闭，接下来它会再收到一个 SIGKILL 信号强制终止进程。默认情况下，Kubernetes 会在发出 SIGTERM 信号 30 秒后再发送 SIGKILL。这个 30 秒的过渡时间可以通过 Pod 的 .spec.terminationGracePeriodSeconds 字段进行配置。 Poststart Hook仅仅使用进程信号来管理生命周期有一定程度的限制。因而 Kubernetes 又提供了 postStart 和 postStop。postStart 示例：123456789101112131415apiVersion: v1kind: Podmetadata: name: post-start-hookspec: containers: - image: k8spatterns/random-generator:1.0 name: random-generator lifecycle: postStart: exec: command: - sh - -c - sleep 30 &amp;&amp; echo "Wake up!" &gt; /tmp/postStart_done postStart 指定的命令会在容器创建后执行，与容器的基础进程异步。postStart 是一种阻塞请求，其 handler 完成之前，容器会一直处于 Waiting 状态，同时 Pod 处于 Pending 状态。postStart 的这种特性可以用来延迟容器的启动，为容器的主进程的初始化争取时间。 另一个 postStart 的使用场景就是在 Pod 不满足特定的前提条件时，阻止容器完成启动。当 postStart 命令返回了一个非 0 的返回值，主进程会被 Kubernetes 杀掉。 类似于 Health Probe，postStart 和 preStop 有如下两种 handler 类型： exec：在容器中直接运行一个命令 httpGet：向容器开放的某个端口发送 HTTP GET 请求 对于 postStart 执行的逻辑，需要注意以下几点： postStart 与容器进程是并行的关系，因而这个 hook 有可能在容器启动前执行 postStart 有至少执行一次的目标，需要考虑重复执行的情况 对于失败的 HTTP 请求，postStart 不会重复尝试 preStoppreStop hook 是一个在容器终止前发送给容器的阻塞请求。12345678910111213apiVersion: v1kind: Podmetadata: name: pre-stop-hookspec: containers: - image: k8spatterns/random-generator:1.0 name: random-generator lifecycle: preStop: httpGet: port: 8080 path: /shutdown 虽然 preStop 是阻塞的，但若它挂起或者返回一个非成功的结果，并不会阻止进程被杀掉、容器被删除。preStop 只是为了能够平滑地关闭应用，是除 SIGTERM 之外的另一种方便的选择。 总结云原生平台能提供的最大的好处之一，就是在不够可靠的云计算基础设施上，可靠地运行和扩展应用。这类平台设计了一系列应用必须遵守的协议和约束。处理和响应协议中的事件，能够确保应用平稳地启动和关闭，对接受服务的客户端只有最小的影响。应用的生命周期不再由个人所控制，而是完全由平台自动化管理。 参考资料Kubernetes Patterns]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Docker</tag>
        <tag>Container</tag>
        <tag>Kubernetes</tag>
        <tag>Pod</tag>
        <tag>k8s</tag>
        <tag>lifecycle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Programming with Types —— 类型及类型系统]]></title>
    <url>%2F2022%2F11%2F29%2Fprogramming-with-types-type-and-type-system%2F</url>
    <content type="text"><![CDATA[为什么要有类型从硬件和机器码这类底层视角来看，程序逻辑（代码）和代码操作的数据都是通过比特（bits）来表示，没有任何区别。当系统没办法正确地将这两者区分开来，错误就很容易发生。这类松散解析的一个例子就是 JavaScript 中的 eval() 函数。1234console.log(eval("40+2"));# =&gt; 42console.log(eval("Hello world!"));# =&gt; Uncaught SyntaxError: Unexpected identifier 除了正确区分代码和数据以外，我们还需要知道如何将一串字节序列中的数据解释出来。比如一个 16 位的字节序列 1100001010100011 既可以表示无符号 16 位整数 49827，又可以表示有符号 16 位整数 -15709，还可以表示 UTF-8 编码的字符 £，或者其他完全不同的数据。 类型赋予数据现实的意义。从而我们的软件能够在特定的上下文中，从一串给定的字节序列中解析出正确的值，不会将其误解成其他的含义。此外，类型还能够限定变量的取值范围。比如一个有符号的 16 位整数，只能是 -32768 到 32767 之间的任意整数，不能超过这个范围。类型可以看作是由合法的值构成的集合。这种对于取值的限制，很大程度上可以帮助我们减少代码中的错误。 类型的定义类型是一种对数据的分类，它定义了某类数据上能够执行的操作，允许的取值以及数据本身的意义。编译器或者运行时能够对类型进行检测，确保数据的完整性、访问控制，以及本身的含义没有被曲解。 类型系统的作用从根本上说，所有的数据都是一堆零和一组成的字节序列。数据本身的属性，比如怎样表示、是否可变、是否对于外部可见等，都是类型级别的性质。我们将某个变量声明为数字类型，类型检查器会确保不会将数据解析为字符串。我们将某个变量声明为私有或只读的，即便数据本身在内存中和公开的、可变的数据并没有任何区别，类型检查器会确保私有的变量不会在作用域外部被引用，只读的变量不会被修改。 Correctness类型能够帮助我们向代码中添加更加严格的限制条件，确保其行为正确。123456function scriptAt(s: any): number &#123; return s.indexOf(s);&#125;console.log(scriptAt("TypeScript"));console.log(scriptAt(42)); 上述代码运行时会报出 TypeError 错误，因为 42 并不是 scriptAt 函数的合法参数。但是编译器并没有发现这个错误，因为它没有获得足够的类型信息。将参数 s 的类型从 any 改为 string，修改后的代码会在编译时报出类型错误：123456function scriptAt(s: string): number &#123; return s.indexOf(s);&#125;console.log(scriptAt("TypeScript"));console.log(scriptAt(42)); // Argument of type '42' is not assignable to parameter of type 'string' 借助类型系统，我们可以将原来在运行时爆发的错误提前到影响相对较小的编译期，从而在代码正式运行或发布之前发现和修复 bug。 当程序进入到 bad state 状态时，错误就会发生。bad state 意味着当前所有存活着的变量的状态组合，由于某种原因是非法的。消除这类 bad state 的一种方式，就是通过限制变量可以接受的可能值的数量来减少状态空间。即更精确的类型定义。 不可变性不可变性的概念同样来自于将软件系统视为变化的状态空间。当我们处于一个已知的好的状态时，我们能维持该状态的某些部分不发生变化，就能降低出现错误的可能性。123456function safeDivide(): number &#123; let x: number = 42; if (x == 0) throw new Error("x should not be 0"); x = x - 42; return 42 / x;&#125; 上述代码中，变量 x 在检查除数不为零的语句之后发生了改变，导致前面的检查语句变得毫无意义。这类情况在现实中经常出现，比如变量被某一个并发的线程所修改。可以将变量声明为不可变的常量。若代码中尝试对常量进行修改，编译时就会报出错误。123456function safeDivide(): number &#123; const x: number = 42; if (x == 0) throw new Error("x should not be 0"); x = x - 42; // error TS2588: Cannot assign to 'x' because it is a constant return 42 / x;&#125; 变量和常量在内存中的表示并无任何异同，只是对编译器而言有不同的意义。优化编译器在处理不可变变量时能够生成更高效的代码，此外当涉及到并发时，不可变性的用处非常大。当数据不可变时，数据竞争的情形就不会存在。 封装性封装代表一种隐藏代码内部细节的能力，它能够帮助我们应对复杂性问题。我们将代码分割成一个个相对较小的组件，每个组件只向外部暴露有限的功能，其内部的实现细节则被隐藏和隔离。12345678910111213141516class SafeDivisor &#123; divisor: number = 1; setDivisor(value: number) &#123; if (value == 0) throw new Error("Value should not be 0"); this.divisor = value; &#125; divide(x: number): number &#123; return x / this.divisor; &#125;&#125;let sd = new SafeDivisor();sd.divisor = 0;console.log(sd.divide(42)); 在上述代码中，divisor 不再是不可变的常量，而是可以通过公开的 API setDivisor() 更新的变量。但新的问题在于，调用者可以绕过包含检查功能的赋值接口，直接访问实例的 divisor 属性并将其改为任意值。因为 divisor 属性对于外部世界是可以公开访问的。为了使 divisor 属性只对实例内部可见，外部对该属性的访问只能通过 setDivisor() 这类刻意公开的方法，可以将属性声明为 private：12345678910111213141516class SafeDivisor &#123; private divisor: number = 1; setDivisor(value: number) &#123; if (value == 0) throw new Error("Value should not be 0"); this.divisor = value; &#125; divide(x: number): number &#123; return x / this.divisor; &#125;&#125;let sd = new SafeDivisor();sd.divisor = 0; // error TS2341: Property 'divisor' is private and only accessible within class 'SafeDivisor'console.log(sd.divide(42)); 封装性能够帮助我们将逻辑和数据分派给公开的接口和非公开的实现，这对于构建大型系统非常有利。面向接口（抽象）编程可以减轻我们理解特定代码片段的心智负担，当我们引用某个功能时，我们只需要知晓公开的接口如何工作、如何使用，不必掌握任何内部的实现细节。同时封装性能够帮助我们将非公开的信息封锁在特定的边界内，保证没有任何外部代码会对其进行改动，提高了代码的安全性。 组合性1234567891011function findFirstNegativeNumber(numbers: number[]): number | undefined &#123; for (let i of numbers) &#123; if (i &lt; 0) return i; &#125;&#125;function findFirstOneCharacterString(strings: string[]): string | undefined &#123; for (let str of strings) &#123; if (str.length == 1) return str; &#125;&#125; 上述两个函数有着几乎一致的逻辑，这造成了一定程度的冗余代码。可以将它们之间通用的逻辑抽象成一个共享的算法，将变化的部分（操作的类型、判断条件）作为参数传入：12345678910111213function first&lt;T&gt;(range: T[], p: (elem: T) =&gt; boolean): T | undefined &#123; for (let elem of range) &#123; if (p(elem)) return elem; &#125;&#125;function findFirstNegativeNumber(numbers: number[]): number | undefined &#123; return first(numbers, n =&gt; n &lt; 0);&#125;function findFirstOneCharacterString(strings: string[]): string | undefined &#123; return first(strings, str =&gt; str.length == 1);&#125; 假如我们需要为上述所有的实现添加自定义的 logging，只更新 first 函数的实现即可。又或者我们发现了一种更加高效的算法，只需要更新 first 函数，所有 first 的调用者就都能享受到性能的提升。 将相互独立的组件组合成一个灵活的模块化的系统，各部分组件松散地结合在一起，相互之间有着更少的冗余代码。整体的代码量也会大大降低。新需求的添加往往只需要独立地修改特定的组件，而不会影响到整个系统。同时这样的模块化系统理解起来也更加容易，因为其中的每一个组件都可以拆下来，独立地进行分析。 可读性读代码的动作远远多于写代码。类型提供了额外的非常有价值的信息，能够令代码更加清晰、易读。 1234declare function find(range: any, pred: any): any;declare function first&lt;T&gt;(range: T[], p: (elem: T) =&gt; boolean): T | undefined; 类型系统的分类动态类型 vs 静态类型动态类型不会在编译期强加任何类型约束，类型只会在运行时生效。静态类型正相反，会在编译期执行类型检查，任何不恰当的类型都会导致编译错误。能够令类型错误在编译期就爆出来，不至于导致正在运行的程序发生故障，是静态类型的主要优势。 JavaScript、Python 属于动态类型，TypeScript、Java 属于静态类型。 弱类型 vs 强类型类型系统的强弱，用来描述系统在强制执行类型约束时的严格程度。一个弱的类型系统会隐式地尝试将某个值从实际的类型转换成期待的类型。 在强类型系统中，“牛奶”不等于“白色”。牛奶是一种液体，而颜色是另外一种不同的事物，两者无法进行比较；在弱类型的世界里，我们可以直接说，因为牛奶的颜色是白色，牛奶等于白色。不需要像强类型那样，对类型显式地进行转换。 JavaScript 是弱类型的：1234&gt; '2' == 2true&gt; 1 + '1''11' 隐式类型转换非常危险。 参考资料Programming with Types]]></content>
      <categories>
        <category>Program</category>
      </categories>
      <tags>
        <tag>Type</tag>
        <tag>TypeScript</tag>
        <tag>Static</tag>
        <tag>Immutability</tag>
        <tag>Encapsulation</tag>
        <tag>Composability</tag>
        <tag>Readability</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Node.js 设计模式笔记 —— Streams 流编程]]></title>
    <url>%2F2022%2F11%2F12%2Fnode-js-design-patterns-streams%2F</url>
    <content type="text"><![CDATA[Streams 是 Node.js 的组件和模式中最重要的几个之一。在 Node.js 这类基于 event 的平台上，最高效的实时地处理 I/O 的方式，就是当有输入时就立即接收数据，应用产生输出时就立即发送数据。 Buffering vs streaming对于输入数据的处理，buffer 模式会将来自资源的所有数据收集到 buffer 中，待操作完成再将数据作为单一的 blob of data 传递给调用者；相反地，streams 允许我们一旦接收到数据就立即对其进行处理。单从效率上说，streams 在空间（内存使用）和时间（CPU 时钟）的使用上都更加高效。此外 Node.js 中的 streams 还有另一个重要的优势：组合性。 空间效率使用 buffered API 完成 Gzip 压缩：123456789101112131415import &#123;promises as fs&#125; from 'fs'import &#123;gzip&#125; from 'zlib'import &#123;promisify&#125; from 'util'const gzipPromise = promisify(gzip)const filename = process.argv[2]async function main() &#123; const data = await fs.readFile(filename) const gzippedData = await gzipPromise(data) await fs.writeFile(`$&#123;filename&#125;.gz`, gzippedData) console.log('File successfully compressed')&#125;main() node gzip-buffer.js &lt;path to file&gt; 如果我们使用上述代码压缩一个足够大的文件（比如说 8G），我们很有可能会收到一个错误信息，类似文件大小超过了允许的最大 buffer 大小。12RangeError [ERR_FS_FILE_TOO_LARGE]: File size (8130792448) is greaterthan possible Buffer: 2147483647 bytes 即便没有超过 V8 的 buffer 大小限制，也有可能出现物理内存不够用的情况。 使用 streams 实现 Gzip 压缩：123456789import &#123;createReadStream, createWriteStream&#125; from 'fs'import &#123;createGzip&#125; from 'zlib'const filename = process.argv[2]createReadStream(filename) .pipe(createGzip()) .pipe(createWriteStream(`$&#123;filename&#125;.gz`)) .on('finish', () =&gt; console.log('File successfully compressed')) streams 的优势来自于其接口和可组合性，允许我们实现干净、优雅、简洁的代码。对于此处的示例，它可以对任意大小的文件进行压缩，只需要消耗常量的内存。 时间效率假设我们需要创建一个应用，能够压缩一个文件并将其上传到一个远程的 HTTP 服务器。而服务器端则负责将接收到的文件解压缩并保存。如果我们使用 buffer API 实现客户端组件，则只有当整个文件读取和压缩完成之后，上传操作才开始触发。同时在服务器端，也只有当所有数据都接收完毕之后才开始解压缩操作。 更好一些的方案是使用 streams。在客户端，streams 允许我们以 chunk 为单位从文件系统逐个、分段地读取数据，并立即进行压缩和发送。同时在服务器端，每个 chunk 被接收到后会立即进行解压缩。 服务端程序：1234567891011121314151617181920import &#123;createServer&#125; from 'http'import &#123;createWriteStream&#125; from 'fs'import &#123;createGunzip&#125; from 'zlib'import &#123;basename, join&#125; from 'path'const server = createServer((req, res) =&gt; &#123; const filename = basename(req.headers['x-filename']) const destFilename = join('received_files', filename) console.log(`File request received: $&#123;filename&#125;`) req .pipe(createGunzip()) .pipe(createWriteStream(destFilename)) .on('finish', () =&gt; &#123; res.writeHead(201, &#123;'Content-Type': 'text/plain'&#125;) res.end('OK\n') console.log(`File saved: $&#123;destFilename&#125;`) &#125;)&#125;)server.listen(3000, () =&gt; console.log('Listening on http://localhost:3000')) 客户端程序：123456789101112131415161718192021222324252627282930import &#123;request&#125; from 'http'import &#123;createGzip&#125; from 'zlib'import &#123;createReadStream&#125; from 'fs'import &#123;basename&#125; from 'path'const filename = process.argv[2]const serverHost = process.argv[3]const httpRequestOptions = &#123; hostname: serverHost, port: 3000, path: '/', method: 'PUT', headers: &#123; 'Content-Type': 'application/octet-stream', 'Content-Encoding': 'gzip', 'X-Filename': basename(filename) &#125;&#125;const req = request(httpRequestOptions, (res) =&gt; &#123; console.log(`Server response: $&#123;res.statusCode&#125;`)&#125;)createReadStream(filename) .pipe(createGzip()) .pipe(req) .on('finish', () =&gt; &#123; console.log('File successfully sent') &#125;) mkdir received_filesnode gzip-receive.jsnode gzip-send.js &lt;path to file&gt; localhost 借助 streams，整套流程的流水线在我们接收到第一个数据块的时候就开始启动了，完全不需要等待整个文件被读取。除此之外，下一个数据块能够被读取时，不需要等到之前的任务完成就能被处理。即另一条流水线被并行地被装配执行，Node.js 可以将这些异步的任务并行化地执行。只需要保证数据块最终的顺序是固定的，而 Node.js 中 streams 的内部实现机制保证了这一点。 组合性借助于 pipe() 方法，不同的 stream 能够被组合在一起。每个处理单元负责各自的单一功能，最终被 pipe() 连接起来。因为 streams 拥有统一的接口，它们彼此之间在 API 层面是互通的。只需要 pipeline 支持前一个 stream 生成的数据类型（可以是二进制、纯文本甚至对象等）。 客户端加密1234567891011121314151617181920212223242526272829303132333435import &#123;createCipheriv, randomBytes&#125; from 'crypto'import &#123;request&#125; from 'http'import &#123;createGzip&#125; from 'zlib'import &#123;createReadStream&#125; from 'fs'import &#123;basename&#125; from 'path'const filename = process.argv[2]const serverHost = process.argv[3]const secret = Buffer.from(process.argv[4], 'hex')const iv = randomBytes(16)const httpRequestOptions = &#123; hostname: serverHost, port: 3000, path: '/', method: 'PUT', headers: &#123; 'Content-Type': 'application/octet-stream', 'Content-Encoding': 'gzip', 'X-Filename': basename(filename), 'X-Initialization-Vector': iv.toString('hex') &#125;&#125;const req = request(httpRequestOptions, (res) =&gt; &#123; console.log(`Server response: $&#123;res.statusCode&#125;`)&#125;)createReadStream(filename) .pipe(createGzip()) .pipe(createCipheriv('aes192', secret, iv)) .pipe(req) .on('finish', () =&gt; &#123; console.log('File successfully sent') &#125;) 服务端加密12345678910111213141516171819202122232425262728import &#123;createServer&#125; from 'http'import &#123;createWriteStream&#125; from 'fs'import &#123;createGunzip&#125; from 'zlib'import &#123;basename, join&#125; from 'path'import &#123;createDecipheriv, randomBytes&#125; from 'crypto'const secret = randomBytes(24)console.log(`Generated secret: $&#123;secret.toString('hex')&#125;`)const server = createServer((req, res) =&gt; &#123; const filename = basename(req.headers['x-filename']) const iv = Buffer.from( req.headers['x-initialization-vector'], 'hex' ) const destFilename = join('received_files', filename) console.log(`File request received: $&#123;filename&#125;`) req .pipe(createDecipheriv('aes192', secret, iv)) .pipe(createGunzip()) .pipe(createWriteStream(destFilename)) .on('finish', () =&gt; &#123; res.writeHead(201, &#123;'Content-Type': 'text/plain'&#125;) res.end('OK\n') console.log(`File saved: $&#123;destFilename&#125;`) &#125;)&#125;)server.listen(3000, () =&gt; console.log('Listening on http://localhost:3000')) Streams 详解实际上在 Node.js 中的任何地方都可见到 streams。比如核心模块 fs 有 createReadStream() 方法用来读取文件内容，createWriteStream() 方法用来向文件写入数据；HTTP request 和 response 对象本质上也是 stream；zlib 模块允许我们通过流接口压缩和解压缩数据；甚至 crypto 模块也提供了一些有用的流函数比如 createCipheriv 和 createDecipheriv。 streams 的结构Node.js 中的每一个 stream 对象，都是对以下四种虚拟基类里任意一种的实现，这四个虚拟类都属于 stream 核心模块： Readable Writable Duplex Transform 每一个 stream 类同时也是 EventEmitter 的实例，实际上 Streams 可以生成几种类型的 event。比如当一个 Readable 流读取完毕时触发 end 事件，Writable 流吸入完毕时触发 finish 事件，或者当任意错误发生时抛出 error。 Steams 之所以足够灵活，一个重要的原因就是它们不仅仅能够处理 binary data，还支持几乎任意的 JavaScript 值。实际上 streams 有以下两种操作模式： Binary mode：以 chunk 的形式（比如 buffers 或 strings）传输数据 Object mode：通过由独立对象（可以包含任意 JavaScript 值）组成的序列传输数据 上述两种模式使得我们不仅仅可以利用 streams 处理 I/O 操作，还能够帮助我们以函数式的方式将多个处理单元优雅地组合起来。 从 Readable streams 读取数据non-flowing mode默认模式。readable 事件表示有新的数据可供读取，再通过 read() 方法同步地从内部 buffer 读取数据，返回一个 Buffer 对象。即从 stream 按需拉取数据。当 stream 以 Binary 模式工作时，我们还可以给 read() 方法指定一个 size 值，以读取特定数量的数据。1234567891011process.stdin .on('readable', () =&gt; &#123; let chunk console.log('New data available') while ((chunk = process.stdin.read()) !== null) &#123; console.log( `Chunk read ($&#123;chunk.length&#125; bytes): "$&#123;chunk.toString()&#125;"` ) &#125; &#125;) .on('end', () =&gt; console.log('End of stream')) flowing mode此模式下，数据并不会像之前那样通过 read() 方法拉取，而是一旦有数据可用，就主动推送给 data 事件的 listener。flowing 模式对于数据流的控制，相对而言灵活性较低一些。由于默认是 non-flowing 模式，为了使用 flowing 模式，需要绑定一个 listener 给 data 事件或者显式地调用 resume() 方法。调用 pause() 方法会导致 stream 暂时停止发送 data 事件，任何传入的数据会先被缓存到内部 buffer。即 stream 又切换回 non-flowing 模式。1234567891011process.stdin .on('readable', () =&gt; &#123; let chunk console.log('New data available') while ((chunk = process.stdin.read()) !== null) &#123; console.log( `Chunk read ($&#123;chunk.length&#125; bytes): "$&#123;chunk.toString()&#125;"` ) &#125; &#125;) .on('end', () =&gt; console.log('End of stream')) Async iteratorsReadable 流同时也是 async iterators。1234567891011async function main() &#123; for await (const chunk of process.stdin) &#123; console.log('New data available') console.log( `Chunk read ($&#123;chunk.length&#125; bytes): "$&#123;chunk.toString()&#125;"` ) &#125; console.log('End of stream')&#125;main() 实现 Readable streams1234567891011121314151617181920212223242526import &#123;Readable&#125; from 'stream'import Chance from 'chance'const chance = Chance()export class RandomStream extends Readable &#123; constructor(options) &#123; super(options) this.emittedBytes = 0 &#125; _read(size) &#123; const chunk = chance.string(&#123;length: size&#125;) this.push(chunk, 'utf8') this.emittedBytes += chunk.length if (chance.bool(&#123;likelihood: 5&#125;)) &#123; this.push(null) &#125; &#125;&#125;const randomStream = new RandomStream()randomStream .on('data', (chunk) =&gt; &#123; console.log(`Chunk received ($&#123;chunk.length&#125; bytes): $&#123;chunk.toString()&#125;`) &#125;) 为了实现一个自定义的 Readable stream，首先必须创建一个新的类，该类继承自 stream 模块中的 Readable。其次新创建的类中必须包含 _read() 方法的实现。上面代码中的 _read() 方法做了以下几件事： 借助第三方的 chance 模块，生成一个长度为 size 的随机字符串 通过 push() 方法将字符传推送到内部 buffer 依据 5% 的几率自行终止，终止时推送 null 到内部 buffer，作为 stream 的结束标志 简化版实现123456789101112131415161718192021import &#123;Readable&#125; from 'stream'import Chance from 'chance'const chance = new Chance()let emittedBytes = 0const randomStream = new Readable(&#123; read(size) &#123; const chunk = chance.string(&#123;length: size&#125;) this.push(chunk, 'utf8') emittedBytes += chunk.length if (chance.bool(&#123;likelihood: 5&#125;)) &#123; this.push(null) &#125; &#125;&#125;)randomStream .on('data', (chunk) =&gt; &#123; console.log(`Chunk received ($&#123;chunk.length&#125; bytes): $&#123;chunk.toString()&#125;`) &#125;) 从可迭代对象创建 Readable streamsReadable.from() 方法支持从数组或者其他可迭代对象（比如 generators, iterators, async iterators）创建 Readable streams。1234567891011121314import &#123;Readable&#125; from 'stream'const mountains = [ &#123;name: 'Everest', height: 8848&#125;, &#123;name: 'K2', height: 8611&#125;, &#123;name: 'Kangchenjunga', height: 8586&#125;, &#123;name: 'Lhotse', height: 8516&#125;, &#123;name: 'Makalu', height: 8481&#125;]const mountainsStream = Readable.from(mountains)mountainsStream.on('data', (mountain) =&gt; &#123; console.log(`$&#123;mountain.name.padStart(14)&#125;\t$&#123;mountain.height&#125;m`)&#125;) Writable streams向流写入数据write() 方法可以向 Writable stream 写入数据。writable.write(chunk, [encoding], [callback]) end() 方法可以向 stream 表明没有更多的数据需要写入。writable.end([chunk], [encoding], [callback]) callback 回调函数等同于为 finish 事件注册了一个 listener，会在流中写入的所有数据刷新到底层资源中时触发。 12345678910111213141516import &#123;createServer&#125; from 'http'import Chance from 'chance'const chance = new Chance()const server = createServer((req, res) =&gt; &#123; res.writeHead(200, &#123;'Content-Type': 'text/plain'&#125;) while (chance.bool(&#123;likelihood: 95&#125;)) &#123; res.write(`$&#123;chance.string()&#125;\n`) &#125; res.end('\n\n') res.on('finish', () =&gt; console.log('All data sent'))&#125;)server.listen(8080, () =&gt; &#123; console.log('listening on http://localhost:8080')&#125;) 上面代码中 HTTP 服务里的 res 对象是一个 http.ServerResponse 对象，实际上也是一个 Writable stream。 实现 Writable stream123456789101112131415161718192021import &#123;Writable&#125; from 'stream'import &#123;promises as fs&#125; from 'fs'class ToFileStream extends Writable &#123; constructor(options) &#123; super(&#123;...options, objectMode: true&#125;) &#125; _write(chunk, encoding, cb) &#123; fs.writeFile(chunk.path, chunk.content) .then(() =&gt; cb()) .catch(cb) &#125;&#125;const tfs = new ToFileStream()tfs.write(&#123;path: 'file1.txt', content: 'Hello'&#125;)tfs.write(&#123;path: 'file2.txt', content: 'Node.js'&#125;)tfs.write(&#123;path: 'file3.txt', content: 'streams'&#125;)tfs.end(() =&gt; console.log('All files created')) 简化形式12345678910111213141516import &#123;Writable&#125; from 'stream'import &#123;promises as fs&#125; from 'fs'const tfs = new Writable(&#123; objectMode: true, write(chunk, encoding, cb) &#123; fs.writeFile(chunk.path, chunk.content) .then(() =&gt; cb()) .catch(cb) &#125;&#125;)tfs.write(&#123;path: 'file1.txt', content: 'Hello'&#125;)tfs.write(&#123;path: 'file2.txt', content: 'Node.js'&#125;)tfs.write(&#123;path: 'file3.txt', content: 'streams'&#125;)tfs.end(() =&gt; console.log('All files created')) Duplex streamsDuplex 流，既 Readable 又 Writable 的流。它的场景在于，有时候我们描述的实体既是数据源，也是数据的接收者，比如网络套接字。Duplex 流同时继承来着 stream.Readable 和 stream.Writable 的方法。为了创建一个自定义的 Duplex 流，我们必须同时提供 _read() 和 _write() 的实现。 Transform streamsTransform 流是一种特殊类型的 Duplex 流，主要针对数据的转换。对于 Duplex 流来说，流入和流出的数据之间并没有直接的联系。比如一个 TCP 套接字，只是从远端接收或者发送数据，套接字本身不知晓输入输出之间的任何关系。 而 Transform 流则会对收到的每一段数据都应用某种转换操作，从 Writable 端接收数据，进行某种形式地转换后再通过 Readable 端提供给外部。 实现 Transform 流1234567891011121314151617181920212223242526272829303132import &#123;Transform&#125; from 'stream'class ReplaceStream extends Transform &#123; constructor(searchStr, replaceStr, options) &#123; super(&#123;...options&#125;) this.searchStr = searchStr this.replaceStr = replaceStr this.tail = '' &#125; _transform(chunk, encoding, callback) &#123; const pieces = (this.tail + chunk).split(this.searchStr) const lastPiece = pieces[pieces.length - 1] const tailLen = this.searchStr.length - 1 this.tail = lastPiece.slice(-tailLen) pieces[pieces.length - 1] = lastPiece.slice(0, -tailLen) this.push(pieces.join(this.replaceStr)) callback() &#125; _flush(callback) &#123; this.push(this.tail) callback() &#125;&#125;const replaceStream = new ReplaceStream('World', 'Node.js')replaceStream.on('data', chunk =&gt; console.log(chunk.toString()))replaceStream.write('Hello W')replaceStream.write('orld')replaceStream.end() 其中核心的 _transform() 方法，其有着和 Writable 流的 _write() 方法基本一致的签名，但并不会将处理后的数据写入底层资源，而是通过 this.push() 推送给内部 buffer，正如 Readable 流中 _read() 方法的行为。所以形成了 Transform 流整体上接收、转换、发送的行为。_flush() 则会在流结束前调用。 简化形式123456789101112131415161718192021222324252627import &#123;Transform&#125; from 'stream'const searchStr = 'World'const replaceStr = 'Node.js'let tail = ''const replaceStream = new Transform(&#123; defaultEncoding: 'utf-8', transform(chunk, encoding, cb) &#123; const pieces = (tail + chunk).split(searchStr) const lastPiece = pieces[pieces.length - 1] const tailLen = searchStr.length - 1 tail = lastPiece.slice(-tailLen) pieces[pieces.length - 1] = lastPiece.slice(0, -tailLen) this.push(pieces.join(replaceStr)) cb() &#125;, flush(cb) &#123; this.push(tail) cb() &#125;&#125;)replaceStream.on('data', chunk =&gt; console.log(chunk.toString()))replaceStream.write('Hello W')replaceStream.write('orld')replaceStream.end() Transform 流筛选和聚合数据数据源 data.csv：12345678type,country,profitHousehold,Namibia,597290.92Baby Food,Iceland,808579.10Meat,Russia,277305.60Meat,Italy,413270.00Cereal,Malta,174965.25Meat,Indonesia,145402.40Household,Italy,728880.54 package.json:1234567891011&#123; "type": "module", "main": "index.js", "dependencies": &#123; "csv-parse": "^4.10.1" &#125;, "engines": &#123; "node": "&gt;=14" &#125;, "engineStrict": true&#125; FilterByCountry Transform 流 filter-by-country.js：12345678910111213141516import &#123;Transform&#125; from 'stream'export class FilterByCountry extends Transform &#123; constructor(country, options = &#123;&#125;) &#123; options.objectMode = true super(options) this.country = country &#125; _transform(record, enc, cb) &#123; if (record.country === this.country) &#123; this.push(record) &#125; cb() &#125;&#125; SumProfit Transform 流 sum-profit.js：12345678910111213141516171819import &#123;Transform&#125; from 'stream'export class SumProfit extends Transform &#123; constructor(options = &#123;&#125;) &#123; options.objectMode = true super(options) this.total = 0 &#125; _transform(record, enc, cb) &#123; this.total += Number.parseFloat(record.profit) cb() &#125; _flush(cb) &#123; this.push(this.total.toString()) cb() &#125;&#125; index.js：123456789101112import &#123;createReadStream&#125; from 'fs'import parse from 'csv-parse'import &#123;FilterByCountry&#125; from './filter-by-conutry.js'import &#123;SumProfit&#125; from './sum-profit.js'const csvParser = parse(&#123;columns: true&#125;)createReadStream('data.csv') .pipe(csvParser) .pipe(new FilterByCountry('Italy')) .pipe(new SumProfit()) .pipe(process.stdout) 参考资料Node.js Design Patterns: Design and implement production-grade Node.js applications using proven patterns and techniques, 3rd Edition]]></content>
      <categories>
        <category>Program</category>
      </categories>
      <tags>
        <tag>Program</tag>
        <tag>Functional</tag>
        <tag>JavaScript</tag>
        <tag>Node.js</tag>
        <tag>Stream</tag>
        <tag>Pipe</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Node.js 设计模式笔记 —— 由 Promises 和 Async、Await 实现的异步模式]]></title>
    <url>%2F2022%2F11%2F12%2Fnode-js-design-patterns-promises-and-async%2F</url>
    <content type="text"><![CDATA[回调函数（Callbacks）是 Node.js 中异步编程的底层构件，但它们远远达不到对用户友好的程度。对于实现代码中最常见的串行控制流，一个未经训练的开发者很容易陷入到 callback hell 问题中。即便实现是正确的，该串行控制流也会显得不必要的复杂和脆弱。 为了获得更好的异步编程体验，第一个出现的就是 promise，一种保存了异步操作的状态和最终结果的对象。Promise 可以轻易地被串联起来，实现串行控制流，可以像其他任何对象一样自由地转移。Pormise 大大简化了异步代码，后来在此基础上又有了 async 和 await，能够令异步代码看起来就像是同步代码一样。 PromisesPromises 是 ECMAScript 2015 标准（ES6）的一部分，为传递异步结果提供了一种健壮的解决方案，替代原本的 CPS 样式的回调函数。Promise 能够令所有主要的异步控制流更加易读、简洁和健壮。 Promise 是一种用来代表异步操作的最终结果（或错误）的对象。在专业术语中，当异步操作未完成时，我们称 Promise 是 pending 的；当异步操作成功结束时，Promise 是 fulfilled 的；当异步操作因为错误终止时，Promise 是 rejected 的；当 Promise 或者是 fulfilled 或者是 rejected，则将其认定为 settled。 Promise 对象的 then() 方法可以获取成功执行后的结果或者终止时报出的错误：1promise.then(onFulfilled, onRejected) 其中 onFulfilled 是一个回调函数，最终会接收到 Promise 成功时的值；onRejected是另一个回调函数，最终会接收 Promise 异常终止时的值（如果有的话）。 基于回调函数的如下代码：123456asyncOperation(arg, (err, result) =&gt; &#123; if (err) &#123; // handle the error &#125; // do stuff with the result&#125;) Promise 实现上述同样的功能，则更加优雅、结构化：123456asyncOperationPromise(arg) .then(result =&gt; &#123; // do stuff with result &#125;, err =&gt; &#123; // handle the error &#125;) asyncOperationPromise() 会返回一个 Promise，可以被用来获取最终结果的值或者失败的原因。但最为关键的属性是，then() 方法会同步地返回另一个 Promise。更进一步地，如果 onFulfilled 或者 onRejected 函数返回一个值 x，那么 then() 方法返回的 Promise 会有以下行为： 若 x 是一个值，则 then() 返回的 Promise 使用 x 作为自身完成时的值 若 x 是一个 Promise 且成功完成，则 x 完成时返回的值作为 then() 返回的 Promise 完成时的值 若 x 是一个 Promise 且因为错误终止，则 x 终止的原因作为 then() 返回的 Promise 终止的原因 上述行为能够令我们将多个 promise 连接成链，轻松地将异步操作聚合在一起。如果我们没有指定一个 onFulfilled 或者 onRejected handler，Promise 完成时的值或者终止时的原因都会自动地传递给链条中的下一个 Promise。通过 Promise 链，任务的执行顺序突然变得很简单。123456789101112asyncOperationPromise(arg) .then(result1 =&gt; &#123; // return another promise return asyncOperationPromise(arg2) &#125;) .then(result2 =&gt; &#123; // return a value return 'done' &#125;) .then(undefined, err =&gt; &#123; // any error in the chain is caught here &#125;) promise APIPromise 构造函数（new Promise((resolve, reject) =&gt; {})）会创建一个新的 Promise 实例，其完成还是终止取决于作为参数传入的函数的行为。作为参数传入的函数接收如下两个参数： resolve(obj)：resolve 是一个函数，在调用时为 Promise 提供完成时的值。当 obj 是值时，则 obj 本身作为 Promise 完成时的值；当 obj 是另一个 Promise 时，则 obj 完成时的值作为当前 Promise 完成时的值 reject(err)：Promise 因为 err 终止 12345678910111213function delay(milliseconds) &#123; return new Promise((resolve, reject) =&gt; &#123; setTimeout(() =&gt; &#123; resolve(new Date()) &#125;, milliseconds) &#125;)&#125;console.log(`$&#123;new Date().getSeconds()&#125;s\nDelaying...`)delay(1000) .then(newDate =&gt; &#123; console.log(`$&#123;newDate.getSeconds()&#125;s`) &#125;) Promise 最重要的静态方法： Promise.resolve(obj)：从另一个 Promise、thenable 对象或者值创建一个新的 Promise Promise.reject(err)：创建一个 Promise，该 Promise 会因为 err 终止 Promise.all(iterable)：从一个可迭代对象创建 Promise，若该 iterable 中的每一项都提供了一个 fulfill 值，则 Promise 最终以包含这些值的列表作为 fulfill 值；若其中有任意一项 reject，则 Promise.all() 返回的 Promise 以第一个 reject 的 err 终止 Promise.allSettled(iterable)：此方法会等待所有输入的 Promise 或者 fulfill 或者 reject，之后返回一个包含所有 fulfill 值和 reject 原因的列表 Promise.race(iterable)：返回可迭代对象中第一个 fulfill 或 reject 的 Promise Promise 关键的实例方法： promise.catch(onRejected)：实际上就是 promise.then(undefined, onRejected) 的语法糖 promise.finally(onFinally)：允许我们设置一个 onFinally 回调函数，在 promise fulfill 或者 reject 时调用 顺序执行顺序执行意味着，每次只执行一系列任务中的一个，完成后再依次执行后面的任务。这一系列任务的先后顺序必须是预先定义好的，因为一个任务的结果有可能影响后续任务的执行。 上述执行流程有着不同形式的变种： 顺序执行一系列已知的任务，不需要在它们之间传递数据 前一个任务的输出作为后一个任务的输入（chain、pipeline、waterfall） 迭代任务集合，同时在每个元素上一个接一个地运行异步任务 package.json：12345678910111213141516171819&#123; "name": "03-promises-web-spider-v2", "version": "1.0.0", "private": true, "type": "module", "scripts": &#123; "test": "echo \"Error: no test specified\" &amp;&amp; exit 1" &#125;, "dependencies": &#123; "cheerio": "^1.0.0-rc.3", "mkdirp": "^0.5.1", "superagent": "^5.2.2", "slug": "^1.1.0" &#125;, "engines": &#123; "node": "&gt;=14" &#125;, "engineStrict": true&#125; spider.js：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950import &#123;promises as fsPromises&#125; from 'fs'import &#123;dirname&#125; from 'path'import superagent from 'superagent'import mkdirp from 'mkdirp'import &#123;urlToFilename, getPageLinks&#125; from './utils.js'import &#123;promisify&#125; from 'util'const mkdirpPromises = promisify(mkdirp)function download(url, filename) &#123; console.log(`Downloading $&#123;url&#125;`) let content return superagent.get(url) .then((res) =&gt; &#123; content = res.text return mkdirpPromises(dirname(filename)) &#125;) .then(() =&gt; fsPromises.writeFile(filename, content)) .then(() =&gt; &#123; console.log(`Downloaded and saved: $&#123;url&#125;`) return content &#125;)&#125;function spiderLinks(currentUrl, content, nesting) &#123; let promise = Promise.resolve() if (nesting === 0) &#123; return promise &#125; const links = getPageLinks(currentUrl, content) for (const link of links) &#123; promise = promise.then(() =&gt; spider(link, nesting - 1)) &#125; return promise&#125;export function spider(url, nesting) &#123; const filename = urlToFilename(url) return fsPromises.readFile(filename, 'utf8') .catch((err) =&gt; &#123; if (err.code !== 'ENOENT') &#123; throw err &#125; // The file doesn't exist, so let’s download it return download(url, filename) &#125;) .then(content =&gt; spiderLinks(url, content, nesting))&#125; spider-cli.js：12345678import &#123;spider&#125; from './spider.js'const url = process.argv[2]const nesting = Number.parseInt(process.argv[3], 10) || 1spider(url, nesting) .then(() =&gt; console.log('Download complete')) .catch(err =&gt; console.error(err)) utils.js：12345678910111213141516171819202122232425262728293031323334353637383940import &#123;join, extname&#125; from 'path'import &#123;URL&#125; from 'url'import slug from 'slug'import cheerio from 'cheerio'function getLinkUrl(currentUrl, element) &#123; const parsedLink = new URL(element.attribs.href || '', currentUrl) const currentParsedUrl = new URL(currentUrl) if (parsedLink.hostname !== currentParsedUrl.hostname || !parsedLink.pathname) &#123; return null &#125; return parsedLink.toString()&#125;export function urlToFilename(url) &#123; const parsedUrl = new URL(url) const urlPath = parsedUrl.pathname.split('/') .filter(function (component) &#123; return component !== '' &#125;) .map(function (component) &#123; return slug(component, &#123;remove: null&#125;) &#125;) .join('/') let filename = join(parsedUrl.hostname, urlPath) if (!extname(filename).match(/htm/)) &#123; filename += '.html' &#125; return filename&#125;export function getPageLinks(currentUrl, body) &#123; return Array.from(cheerio.load(body)('a')) .map(function (element) &#123; return getLinkUrl(currentUrl, element) &#125;) .filter(Boolean)&#125; node spider-cli.js http://www.baidu.com 2 其中的 spiderLinks() 函数通过循环动态地构建了一条 Promise 链： 先定义一个“空的” Promise 对象（resovle 到 undefined），这个空 Promise 只是作为链条的起点 在循环中，不断将 promise 变量更新为新的 Promise 对象（通过调用上一个 Promise 的 then() 方法得到）。这就是 Promise 的异步遍历模式 在 for 循环的最后，promise 变量会是最后一个 then() 方法返回的 Promise，因而只有当链条中的所有 Promise 都 resolve 时，promise 才会 resolve。 纵观所有代码，我们可以不需要像使用 callback 那样，强制地包含众多错误传递逻辑。因而大大减少了代码量和出错的机会。 并行执行在某些情况下，一系列异步任务的执行顺序并不重要，我们需要的只是当所有的任务都完成后能收到通知。 虽然 Node.js 是单线程的，但得益于其 non-blocking nature，我们仍可以实现并发行为。 比如我们有一个 Main 函数需要执行两个异步任务： Main 函数首先触发异步任务 Task1 和 Task2 的执行。异步任务触发后，会将程序控制权立即交还给 Main 函数，再转交给 event loop 当 Task1 中的异步任务结束时，event loop 调用 Task1 的回调函数，将控制权交给 Task1。Task1 执行完成自身内部的同步指令，通知 Main 函数并返还控制权 当 Task2 中的异步任务结束时，event loop 调用 Task2 的回调函数，将控制权交给 Task2。在 Task2 的终点，Main 函数再次被通知。Main 函数得知 Task1 和 Task2 全部结束，继续执行或者返回结果 简单来说，在 Node.js 中，我们只能并发地执行异步操作，因为它们的并发行为是由内部的非阻塞 API 控制的。同步（阻塞）操作无法并发地执行，除非它们的执行与异步操作交织在一起，或者由 setTimeout()、setImmediate() 包裹。 Promise 实现并发执行流，可以借助内置的 Promise.all() 方法。该方法会返回一个新的 Promise，只有当所有传入的 Promise 都 fulfill 时，新 Promise 才会 fulfill。如果传入的 Promise 之间没有因果关系，这些 Promise 就会并发地执行。 对于前面的 spider 应用，只需要将 spiderLinks() 函数改为如下形式：12345678function spiderLinks(currentUrl, content, nesting) &#123; if (nesting === 0) &#123; return Promise.resolve() &#125; const links = getPageLinks(currentUrl, content) const promises = links.map(link =&gt; spider(link, nesting - 1)) return Promise.all(promises)&#125; Async/awaitPromise 链相对于 callback hell 来说肯定是要好太多的，但是我们仍然需要调用 then() 方法，以及为链条中的每一个任务创建新的函数，对于日常编程中非常普遍的控制流来说还是比较麻烦。而 Async/await 可以帮助我们写出像同步代码一样可读性强、容易理解的异步代码。Async 函数是一种特殊的函数，在函数体里面可以使用 await 表达式“暂停”任意一个 Promise 的执行，将控制权交还给 async 函数的调用者，等该 Promise revolve 后再返回到暂停的地方继续执行。 12345678910111213141516171819202122function delay(milliseconds) &#123; return new Promise((resolve, reject) =&gt; &#123; setTimeout(() =&gt; &#123; resolve(new Date()) &#125;, milliseconds) &#125;)&#125;async function playingWithDelays() &#123; console.log('Initial date: ', new Date()) const dateAfterOneSecond = await delay(1000) console.log('Date after one second: ', dateAfterOneSecond) const dateAfterThreeSeconds = await delay(3000) console.log('Date after 3 secnods: ', dateAfterThreeSeconds) return 'done'&#125;playingWithDelays() .then(result =&gt; &#123; console.log(`After 4 seconds: $&#123;result&#125;`) &#125;) 错误处理Async/await 的另一个巨大的优势在于，它能够标准化 try...catch 代码块的行为，不管是针对同步代码中的 throw，抑或是异步代码中的 Promise reject。1234567891011121314151617181920212223function delayError(milliseconds) &#123; return new Promise((resolve, reject) =&gt; &#123; setTimeout(() =&gt; &#123; reject(new Error(`Error after $&#123;milliseconds&#125;ms`)) &#125;) &#125;)&#125;async function playingWithErrors(throwSyncError) &#123; try &#123; if (throwSyncError) &#123; throw new Error('This is a synchronous error') &#125; await delayError(1000) &#125; catch (err) &#123; console.log(`We have an error: $&#123;err.message&#125;`) &#125; finally &#123; console.log('Done') &#125;&#125;// playingWithErrors(true)playingWithErrors(false) 串行执行借助 Async/await，可以对之前的 spider 应用实现很多优化。比如 download() 函数：12345678async function download(url, filename) &#123; console.log(`Downloading $&#123;url&#125;`) const &#123;text: content&#125; = await superagent.get(url) await mkdirpPromises(dirname(filename)) await fsPromises.writeFile(filename, content) console.log(`Downloaded and saved: $&#123;url&#125;`) return content&#125; 整段代码行数大大减少，看起来也很“平整”，没有任何层级和缩进。 接下来是 spiderLinks() 函数，使用 async/await 异步地遍历一个列表：123456789async function spiderLinks(currentUrl, content, nesting) &#123; if (nesting === 0) &#123; return &#125; const links = getPageLinks(currentUrl, content) for (const link of links) &#123; await spider(link, nesting - 1) &#125;&#125; 然后是 spider() 函数，如何简单地通过 try...catch 处理错误，令异步代码更加易读：12345678910111213export async function spider(url, nesting) &#123; const filename = urlToFilename(url) let content try &#123; content = await fsPromises.readFile(filename, 'utf8') &#125; catch (err) &#123; if (err.code !== 'ENOENT') &#123; throw err &#125; content = await download(url, filename) &#125; return spiderLinks(url, content, nesting)&#125; 并行执行使用纯 async/await 实现并行的异步执行流程，可以参考如下代码：12345678910async function spiderLinks(currentUrl, content, nesting) &#123; if (nesting === 0) &#123; return &#125; const links = getPageLinks(currentUrl, content) const promises = links.map(link =&gt; spider(link, nesting - 1)) for (const promise of promises) &#123; await promise &#125;&#125; 然而上述代码存在一定的问题。如果列表中有一个 Promise reject 了，我们不得不等待列表中其他所有的 Promise 都 resolve，spiderLinks() 函数返回的 Promise 才会 reject。这种行为在多数情况下都是不理想的。我们通常都会想要在操作发生错误的第一时间捕获错误信息。因而并行执行异步操作，最后仍建议使用下面形式的代码：12345678async function spiderLinks(currentUrl, content, nesting) &#123; if (nesting === 0) &#123; return &#125; const links = getPageLinks(currentUrl, content) const promises = links.map(link =&gt; spider(link, nesting - 1)) return Promise.all(promises)&#125; 参考资料Node.js Design Patterns: Design and implement production-grade Node.js applications using proven patterns and techniques, 3rd Edition]]></content>
      <categories>
        <category>Program</category>
      </categories>
      <tags>
        <tag>Program</tag>
        <tag>JavaScript</tag>
        <tag>Node.js</tag>
        <tag>Event</tag>
        <tag>Async</tag>
        <tag>Promise</tag>
        <tag>Concurrent</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Node.js 设计模式笔记 —— Callbacks 和 Events]]></title>
    <url>%2F2022%2F09%2F07%2Fnode-js-design-patterns-callbacks-and-events%2F</url>
    <content type="text"><![CDATA[在同步式编程中，为了解决特定的问题，代码被组织成一系列连贯的计算步骤。其中每一个步骤都是阻塞的，即只有当某个操作完成以后，才有可能继续执行下一个步骤。这种方式形成的代码非常容易阅读、理解和调试。 而在异步式编程中，某些操作比如读取文件或者处理一个网络请求，是在“后台”启动和执行的。当我们调用某个异步操作后，即使其并没有执行完毕，该异步操作之后的代码指令也会立刻继续执行。在这种情况下，我们就需要一种“通知”机制。当异步操作执行完毕，我们会收到通知，获取该操作的结果并继续之前定义的执行流程。在 Node.js 中，最基础的通知机制就是回调函数。它本质上就是一种由 runtime 调用的带有异步操作结果的函数。 Callback 模式回调函数是一种能够传递操作结果的函数，正是异步编程所需要的。JavaScript 对于回调函数来说是一种理想的语言，函数是第一等对象，可以轻松地赋值给变量、作为参数传递给另一个函数、作为函数的返回值，以及存储到数据结构中。 The continuation-passing style在 JavaScript 中，回调函数会作为参数传递给另一个函数，并且在操作完成时连同结果一起被调用。即执行结果被传递给另一个函数（callback），而不是直接返回给调用者。这种方式在函数式编程里称作 continuation-passing style (CPS)。 下面是一个非常简单的同步函数：123function add(a, b) &#123; return a + b&#125; 和上述函数等效的 CPS 形式：12345678910function addCps(a, b, callback) &#123; callback(a + b)&#125;console.log('before')addCps(1, 2, result =&gt; console.log(`Result: $result`))console.log('after')// =&gt; before// =&gt; Result: $result// =&gt; after addCps 就是一个同步的 CPS 函数。 Asynchronous CPSaddCps 函数的异步版本：12345678910function additionAsync(a, b, callback) &#123; setTimeout(() =&gt; callback(a + b), 100)&#125;console.log('before')additionAsync(1, 2, result =&gt; console.log(`Result: $&#123;result&#125;`))console.log('after')// =&gt; before// =&gt; after// =&gt; Result: 3 上面的代码使用 setTimeout 来模拟回调函数的异步调用。由于 setTimeout 触发的是异步操作，它并不会等待回调函数 callback 执行，而是立即返回。将控制权交还给 additionAsync 进而回到调用者身上，执行主程序中的第二个 console.log。当异步操作执行完毕后，程序从之前控制权转移时的位置起恢复执行，callback 中的 console.log 被执行。 总结一下就是，同步函数会阻塞其他操作步骤，直到其自身执行完毕；异步函数会立即返回，它的执行结果会在 event loop 的后续周期中传递给 handler（即回调函数）。 同步 or 异步指令的执行顺序取决于函数的自然属性——同步还是异步，这对于整个应用流程的正确性和效率都有很大的影响。所以需要时刻注意避免制造矛盾和困惑。 Unleashing Zalgo一个 API 最危险的情形之一，就是有些时候表现为同步另一些情况下表现为异步。12345678910111213141516import &#123;readFile&#125; from 'fs'const cache = new Map()function inconsistentRead(filename, cb) &#123; if (cache.has(filename)) &#123; // invoked synchronously cb(cache.get(filename)) &#125; else &#123; // asynchronous function readFile(filename, 'utf8', (err, data) =&gt; &#123; cache.set(filename, data) cb(data) &#125;) &#125;&#125; 上述程序就是危险的。假如某个文件是第一次被读取，它会表现为异步操作，读取文件设置缓存；当某个文件的内容已经存在于缓存中时，它会表现为同步操作。 参考下面的示例：1234567891011121314151617181920function createFileReader(filename) &#123; const listeners = [] inconsistentRead(filename, value =&gt; &#123; listeners.forEach(listener =&gt; listener(value)) &#125;) return &#123; onDataReady: listener =&gt; listeners.push(listener) &#125;&#125;const reader1 = createFileReader('data.txt')reader1.onDataReady(data =&gt; &#123; console.log(`First call data: $&#123;data&#125;`) const reader2 = createFileReader('data.txt') reader2.onDataReady(data =&gt; &#123; console.log(`Second call data: $&#123;data&#125;`) &#125;)&#125;) 其中 createFileReader 函数会创建一个新的 { onDataReady: function() } 对象作为通知器，以帮助我们为文件读取操作设置多个 listener。若 inconsistentRead 是纯异步操作，实际上 onDataReady 会先被调用，将传入的 listener 添加到 listeners 列表中。之后 inconsistentRead 读取文件内容完毕，回调函数 cb 执行，遍历 listeners 列表并将读取到的文件内容传给 listener。 实际的执行结果为：1First call data: some data 第二次读取同一个文件并没有获取到任何内容。 原因在于，当 reader1 创建时，inconsistentRead 函数表现为异步的，因为该文件是第一次被读取。因而 onDataReady 会在刚开始读取文件时就将传入的 listener 添加到 listeners 列表中。文件读取完毕后 listeners 中注册的 listener 被调用。reader2 创建时同一个文件的缓存内容已经存在，inconsistentRead 表现为同步的。它的回调函数会立即调用，遍历 listeners 列表。然而我们是先创建的 reader2 再添加的 listener，这就导致遍历 listeners 列表时，向 listeners 添加 listener 的操作还没有执行，我们传入的 listener 并没有来得及注册。 在实际的应用中，上述类型的 bug 会非常难以定位和复现。npm 的创造者 Isaac Z. Schlueter 将类似的使用不可预测函数的行为，叫做 unleashing Zalgo。 使用同步 API想修复前面的 inconsistentRead 函数，一种可能的方案就是令其彻底变成同步的。实际上 Node.js 针对基础的 I/O 操作提供了一系列同步的 API。比如 fs.readFileSync。12345678910111213import &#123;readFileSync&#125; from 'fs'const cache = new Map()function consistentReadSync(filename) &#123; if (cache.has(filename)) &#123; return cache.get(filename) &#125; else &#123; const data = readFileSync(filename) cache.set(filename, data) return data &#125;&#125; 但是，使用同步 API 而不是异步 API 也有一定的风险： 针对特定功能的同步 API 有可能不存在 同步 API 会阻塞 event loop，暂停任何并发请求。从而破坏 Node.js 的并发模型并拖慢整个应用 在很多情况下，使用同步 I/O 操作在 Node.js 里都是非常不推荐的。但在一些场景下，同步 I/O 可能是最简单和高效的方案。比如在应用启动时使用同步阻塞 API 加载配置文件。 通过延迟执行保证异步性另一种修复 inconsistentRead 函数的方案就是，将其变成纯异步操作。诀窍就是将同步的回调函数延期到“未来”执行，而不是在同一个 event loop 周期里立即被调用。在 Node.js 中，可以通过 process.nextTick() 来实现。它会接收一个回调函数作为参数，将其推入到事件队列顶部，位于所有 pending 的 I/O 事件之前，然后立即返回。回调函数会在 event loop 再次收回控制权时立即被调用。12345678910111213141516import &#123;readFile&#125; from 'fs'const cache = new Map()function inconsistentRead(filename, callback) &#123; if (cache.has(filename)) &#123; // deferred callback invocation process.nextTick(() =&gt; callback(cache.get(filename))) &#125; else &#123; // asynchronous function readFile(filename, 'utf8', (err, data) =&gt; &#123; cache.set(filename, data) callback(data) &#125;) &#125;&#125; Node.js 回调函数的最佳实践回调函数出现在最后在所有核心的 Node.js 函数中，当其接收一个回调函数作为输入时，回调函数必须作为最后一个参数传入。1readFile(filename, [options], callback) error 总是出现在前面在 Node.js 中，任何 CPS 函数产生的错误都必须作为回调函数的第一个参数传递，任何实际的执行结果都从第二个参数开始。1234567readFile('foo.txt', 'utf8', (err, data) =&gt; &#123; if (err) &#123; handleError(err) &#125; else &#123; processData(data) &#125;&#125;) 最佳实践还在于总是检查 error 是否存在，以及 error 的定义必须是 Error 类型。 传递 error在同步的函数中，传递 error 可以通过常用的 throw 语句。而在异步的 CPS 函数中，则可以简单地将 error 传递给链条上的下一个回调函数。123456789101112131415161718192021import &#123;readFile&#125; from 'fs'function readJSON(filename, callack) &#123; readFile(filename, 'utf8', (err, data) =&gt; &#123; let parsed if (err) &#123; // propagate the error and exit the current function return callack(err) &#125; try &#123; // parse the file contents parsed = JSON.parse(data) &#125; catch (err) &#123; // catch parsing errors return callack(err) &#125; // no errors, propagate just the data callack(null, parsed) &#125;)&#125; 观察者模式在 Node.js 中另外一种非常重要和基础的模式就是观察者（Ovserver）模式。同 Reactor 模式、回调函数一起，它们都是掌握 Node.js 异步编程的绝对要求。观察者模式定义了一类称为 subject 的对象，它们可以在状态改变时向一系列称为观察者的对象发送通知。它是对回调函数的完美补充。主要区别在于 subject 能够通知多个观察者，而传统的 CPS 回调函数通常只会将结果传递给一个 listener。 EventEmitter观察者模式实际上已经通过 EventEmitter 类内置到 Node.js 的核心中了。EventEmitter 类允许我们注册一个或者多个函数作为 listener，这些 listener 会在特定的事件触发时自动被调用。 EventEmitter 类的基础方法如下： on(event, listener)：该方法允许我们为指定的事件类型（一个字符串）注册一个新的 listener（一个函数） once(event, listener)：该方法允许我们注册一个新的 listener，并且该 listener 会在事件触发一次之后自动被移除 emit(event, [arg1], [...])：该方法会产生一个新的事件，并向指定向 listeners 传递的额外的参数 removeListener(event, listener)：该方法用来移除某个 listener 上述所有的方法都会返回一个 EventEmitter 实例并允许被串联起来。 创建和使用 EventEmitter12345678910111213141516171819202122232425import &#123;EventEmitter&#125; from 'events'import &#123;readFile&#125; from 'fs'function findRegex(files, regex) &#123; const emitter = new EventEmitter() for (const file of files) &#123; readFile(file, 'utf8', (err, content) =&gt; &#123; if (err) &#123; return emitter.emit('error', err) &#125; emitter.emit('fileread', file) const match = content.match(regex) if (match) &#123; match.forEach(elem =&gt; emitter.emit('found', file, elem)) &#125; &#125;) &#125; return emitter&#125;findRegex(['fileA.txt', 'fileB.json'], /hello \w+/g) .on('fileread', file =&gt; console.log(`$&#123;file&#125; was read`)) .on('found', (file, match) =&gt; console.log(`Matched "$&#123;match&#125;" in $&#123;file&#125;`)) .on('error', err =&gt; console.error(`Error emitted $&#123;err.message&#125;`)) 令任意对象变得“可监测”在 Node.js 的世界里，EventEmitter 很少像上面的例子那样被直接使用。更为常见的情况是其他类继承 EventEmitter 从而变成一个可监测的对象。1234567891011121314151617181920212223242526272829303132333435363738394041import &#123;EventEmitter&#125; from 'events'import &#123;readFile&#125; from 'fs'class FindRegex extends EventEmitter &#123; constructor(regex) &#123; super() this.regex = regex this.files = [] &#125; addFile(file) &#123; this.files.push(file) return this &#125; find() &#123; for (const file of this.files) &#123; readFile(file, 'utf8', (err, content) =&gt; &#123; if (err) &#123; return this.emit('error', err) &#125; this.emit('fileread', file) const match = content.match(this.regex) if (match) &#123; match.forEach(elem =&gt; this.emit('found', file, elem)) &#125; &#125;) &#125; return this &#125;&#125;const findRegexInstance = new FindRegex(/hello \w+/g)findRegexInstance .addFile('fileA.txt') .addFile('fileB.json') .find() .on('found', (file, match) =&gt; console.log(`Matched "$&#123;match&#125;" in file $&#123;file&#125;`)) .on('error', err =&gt; console.error(`Error emitted $&#123;err.message&#125;`)) EventEmitter vs Callback以下的几点可以作为选择 EventEmitter 还是 Callback 的依据： 当涉及到需要支持不同类型的事件时，Callback 会有一定的限制。实际上 Callback 也可以区分多个事件，只需要将事件类型作为参数传给回调函数，或者接收多个回调函数。但在这样的情况下，EventEmitter 可以提供更优雅的接口和更精简的代码 当同样的事件可能多次发生或者根本不会发生时，应该使用 EventEmitter。而无论操作是否成功，回调函数都只会被调用一次 回调函数机制只支持通知一个特定的 listener，而 EventEmitter 允许我们为同一个事件注册多个 listener 参考资料Node.js Design Patterns: Design and implement production-grade Node.js applications using proven patterns and techniques, 3rd Edition]]></content>
      <categories>
        <category>Program</category>
      </categories>
      <tags>
        <tag>Design</tag>
        <tag>Pattern</tag>
        <tag>JavaScript</tag>
        <tag>Node.js</tag>
        <tag>Callback</tag>
        <tag>Event</tag>
        <tag>Observer</tag>
        <tag>Asynchronous</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Node.js 设计模式笔记 —— Node.js 的设计哲学和原理]]></title>
    <url>%2F2022%2F09%2F05%2Fnode-js-design-patterns-nodejs-philosophy-and-internal-pattern%2F</url>
    <content type="text"><![CDATA[一、Node.js 哲学每种编程语言平台都有其特定的“哲学”，即一系列被社区普遍接受的指导原则和规范。这些规范对语言平台本身的演进以及如何设计和开发应用都有着深刻的影响。 小的核心Node.js 核心，包含 Node.js 运行时以及所有内置的模块。这个核心遵循一系列基本的设计原则。其中一个就是尽可能只实现所需功能的最小集合，在此之外的非核心功能则由用户自行实现。用户自己开发的模块围绕在核心周围，形成了一个自由开放的软件生态。将核心的功能限制在最小程度，其他的需求则给与社区足够的自由度，去验证和实现更广泛的解决方案。不仅提升了核心本身的可维护性，同时也给整个生态环境带来了正向的文化氛围。 小的模块Node.js 使用模块（module）这个概念来表示程序代码的基础构件，模块是构成应用和库的基本单位。在 Node.js 中，一个广受推崇的原则就是，设计小的模块和包。这里的“小”不仅仅是指代码本身的规模，更为关键的是功能上的“小”和集中。上述原则深受 Unix 设计哲学的影响，即： Small is beautiful（小即为美） Make each program do one thing well（只做好一件事） 小的模块具有以下特点： 更容易理解和使用 更容易测试和维护 体积小，在浏览器上运行有优势 更小、更集中的模块鼓励每一个人共享和重用每一段哪怕是最小的代码块，在一定程度上提升了代码的可重用性。牢记 Don’t Repeat Yourself (DRY) 原则。 Small surface area除了在代码量和功能性上更小以外，Node.js 模块的另一个理想特征就是，尽可能向外界公开一组最小的功能集合。这可以帮助我们实现更清晰、不容易被错误使用的 API。模块只向外暴露单一的功能，只向外提供唯一一个清晰的、明确无误的入口。 很多 Node.js 模块的另一个特点是，模块本身被创建出来，是为了被使用而不是被扩展。通过禁止任何扩展来锁定模块内部，听起来缺乏一定的灵活性。但同时也带来了减少用例、简化实现、增强可维护性、提升可用性等优势。在实践中，这意味着更倾向于对外暴露函数而不是类，避免向外部世界泄露任何内部的细节。 简单性和实用主义Keep It Simple, Stupid (KISS)设计简单的，而不是“完美”的、功能完备的软件，在实践中往往是更优的选择： 更少的时间和资源去实现 更快地完成交付 更容易适应不断变化和增加的需求 更容易理解和维护 JavaScript 是一种非常“现实”的语言。在实践中，经常见到使用更简单的类、函数和闭包替换复杂的层级结构的类。纯粹的 OO 设计常常致力于使用数学模型完整地复制现实世界，并没有考虑到现实本身的“不完美”和复杂性。事实上，我们的软件一直都是对现实世界的接近，如果能够放弃创建“完美”软件的执念，尝试构造一个有着合理复杂度、能够快速工作的成品，有可能会获取到更大的成功。 二、Node.js 核心原理I/O 很慢在计算机的世界里，I/O 算得上基础操作里最慢的一种了。比如访问 RAM 的速度处于纳秒（10^-9）量级，而访问磁盘或者网络数据的速度则处于毫秒（10^-3）量级。I/O 操作通常并不消耗多少 CPU 资源，但它实际上在请求发出和操作完成之间增添了很大的延迟。此外，我们还必须考虑人为因素。很多场景下应用的输入依赖于具体的个人，比如点击鼠标等。从而导致现实里的 I/O 速度，有可能比纯技术层面的磁盘和网络读写要慢得多。 阻塞式 I/O在传统的阻塞式 I/O 编程中，I/O 请求关联的函数调用会阻塞线程的执行，直到 I/O 操作完成。这会导致一定程度的延迟，有可能是毫秒级别，比如 I/O 操作涉及到磁盘读写；也有可能长达几分钟甚至更久，比如等待用户提供某些输入。 1234// blocks the thread until the data is availabledata = socket.read()// data is availableprint(data) 很明显，由阻塞式 I/O 实现的 Web 服务无法在同一个线程中同时处理多个连接请求，因为 socket 上的每个 I/O 操作都会阻塞任何其他连接的访问。解决此问题的传统方法就是借助多线程，每个独立的线程分别处理并发连接中的一个请求。一个线程被 I/O 操作阻塞，并不会影响其他的线程继续提供服务。 多线程的缺点在于，从资源消耗的角度看，线程并不是一个低廉的选择。他会消耗内存，引发上下文切换等。一个长时间运行的只处理一个网络请求的线程，实际上有可能大部分时间并没有在工作，这意味着对内存和 CPU 资源的浪费。 非阻塞式 I/O除了阻塞式 I/O 外，现代的操作系统还支持另一种访问资源的机制，称为非阻塞式 I/O。在这种模式下，系统调用会立即返回，无需等待读写操作彻底完成。若返回时还没有获取到任何结果，则返回一个预定义的对象，该对象表明此时没有任何数据可以获取到。处理非阻塞式 I/O 的最基本的模式就是，通过循环主动轮询资源池中的资源，直到某个对象返回了实际的数据。这种方式称为 busy-waiting。 123456789101112131415161718resources = [socketA, socketB, fileA]while (!resources.isEmpty()) &#123; for (resource of resources) &#123; // try to read data = resource.read() if (data === NO_DATA_AVAILABLE) &#123; // there is no data to read at the moment continue &#125; if (data === RESOURCE_CLOSED) &#123; // the resource was closed, remove it from the list resources.remove(i) &#125; else &#123; //some data was received, process it consumeData(data) &#125; &#125;&#125; 通过上述简单的机制，不同的资源即能够在同一个线程中被处理。但仍然不够高效。实际上，CPU 的大部分时钟都被循环用来查询还没有准备好的资源，轮询算法通常意味着 CPU 资源的大量浪费。 解多路复用Busy-waiting 并不是处理非阻塞资源的理想技术，好在现代的大部分操作系统还提供了一种高效的原生机制，专门用来处理并发的非阻塞需求。该机制称为 synchronous event demultiplexer 或 event notification interface。multiplexing 是指将多路信号合并到一条通信链路中进行传输；demultiplexing 则是指相反的操作，将合并到一条链路中的数据重新还原成原本的多路信号。 synchronous event demultiplexer 会同时监听多个资源，当其中任何一个资源对应的读写操作完成时，就会返回一个或一系列新的事件。它的优势在于 synchronous，即它是同步的，当没有任何新的事件需要处理时，它会一直处于阻塞状态。因而我们可以在同一个线程中处理多个 I/O 操作，同时不至于像 busy-waiting 那样持续轮询消耗资源。 12345678910111213141516watchedList.add(socketA, FOR_READ)watchedList.add(fileB, FOR_READ)while (events = demultiplexer.watch(watchedList)) &#123; // event loop for (event of events) &#123; // This read will never block and will always return data data = event.resource.read() if (data === RESOURCE_CLOSED) &#123; // the resource was closed, remove it from the watched list demultiplexer.unwatch(event.resource) &#125; else &#123; // some actual data was received, process it consumeData(data) &#125; &#125;&#125; 其中的 demultiplexer.watch() 方法是同步的，当它监听的资源没有任何一个准备好时，它会一直处于阻塞状态。直到有任意资源准备好后，才会返回一系列新的事件。这个时间点返回的事件及其关联的资源由于是已经“准备好”的，可以被直接读取而不会阻塞。 Reactor patternReactor 模式背后的主要理念，就是给每一个 I/O 操作绑定一个 handler。在 Node.js 中可以使用回调函数来表示 handler。当某个事件被 event loop 生产和处理完之后，对应的 handler 就会立即被触发。 应用首先向 Event Demultiplexer 提交一个请求，由此生成一个新的 I/O 操作。与此同时应用会为该请求绑定一个 handler，当 I/O 操作结束时自动被调用。向 Event Demultiplexer 提交请求的操作是非阻塞的，该操作提交后程序控制权会立即返还给应用 当一系列 I/O 操作完成后，Event Demultiplexer 会向 Event Queue 中推入对应的事件 Event Loop 会不断遍历 Event Queue 中的事件，调用每一个事件对应的 handler handler 代码实际上是应用本身的一部分，它在执行完毕后又会把控制权给到 Event Loop。在 handler 执行的过程中，应用仍然可以向 Event Demultiplexer 提交新的异步操作请求 简单来说，所谓的异步行为，就是应用先在某个时间点表达出想要访问某个资源的兴趣（这个操作是非阻塞的），并给这个资源定义一个 handler。在另一个时间节点当资源能够被访问之后，绑定的 handler 自动被调用，处理对应的资源。 参考资料Node.js Design Patterns: Design and implement production-grade Node.js applications using proven patterns and techniques, 3rd Edition]]></content>
      <categories>
        <category>Program</category>
      </categories>
      <tags>
        <tag>Program</tag>
        <tag>Functional</tag>
        <tag>JavaScript</tag>
        <tag>Concurrency</tag>
        <tag>Node.js</tag>
        <tag>Event</tag>
        <tag>KISS</tag>
        <tag>DRY</tag>
        <tag>Reactor</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Node.js 设计模式笔记 —— Template 模式]]></title>
    <url>%2F2022%2F07%2F18%2Fnode-js-design-patterns-template-pattern%2F</url>
    <content type="text"><![CDATA[Template 模式与 Strategy 模式有很多相似之处。Template 模式首先会定义一个虚拟基类，描述某个组件的骨架（即通用的部分），同时令骨架中存在的某些步骤处于未定义状态。之后由虚拟基类的子类来实现上述组件中缺失的未定义部分，这部分之前未定义的方法称为 template methods。此模式的目的在于，定义一系列属于“同一家族”的类，能够囊括某个组件的所有变体。 Template 和 Strategy 模式的目的是非常相似的，它们之间的区别主要在于结构和实现。两者都允许我们在共享组件中通用部分的同时，修改组件中变化的部分，以此形成不同的变体。不同的地方在于，Strategy 是在运行时动态实现的，而 Template 则在子类定义的时刻就已经被确定了。 配置管理模板123mkdir template &amp;&amp; cd templatenpm install ininpm install objec-path package.json：123&#123; "type": "module"&#125; configTemplate.js：1234567891011121314151617181920212223242526272829303132import &#123;promises as fsPromises&#125; from 'fs'import objectPath from 'object-path'export class ConfigTemplate &#123; async load(file) &#123; console.log(`Deserializing from $&#123;file&#125;`) this.data = this._deserialize( await fsPromises.readFile(file, 'utf-8') ) &#125; async save(file) &#123; console.log(`Serializing to $&#123;file&#125;`) await fsPromises.writeFile(file, this._serialize(this.data)) &#125; get(path) &#123; return objectPath.get(this.data, path) &#125; set(path, value) &#123; return objectPath.set(this.data, path, value) &#125; _serialize() &#123; throw new Error('_serialize() must be implemented') &#125; _deserialize() &#123; throw new Error(`_deserialize() must be implemented`) &#125;&#125; 其中 configTemplate 虚拟基类实现了配置管理逻辑中的通用部分，即加载和保存文件、获取和设置属性。同时不对序列化和反序列化部分的逻辑进行定义，从而可以通过再创建不同的 Config 子类（即后面的 jsonConfig 和 iniConfig）来实现具体的序列化逻辑，进而支持特定的配置文件格式。 jsonConfig.js：1234567891011import &#123;ConfigTemplate&#125; from './configTemplate.js'export class JsonConfig extends ConfigTemplate &#123; _deserialize(data) &#123; return JSON.parse(data) &#125; _serialize(data) &#123; return JSON.stringify(data, null, ' ') &#125;&#125; iniConfig.js：123456789101112import &#123;ConfigTemplate&#125; from './configTemplate.js'import ini from 'ini'export class IniConfig extends ConfigTemplate &#123; _deserialize(data) &#123; return ini.parse(data) &#125; _serialize(data) &#123; return ini.stringify(data) &#125;&#125; index.js：12345678910111213141516import &#123;JsonConfig&#125; from './jsonConfig.js'import &#123;IniConfig&#125; from './iniConfig.js'async function main() &#123; const jsonConfig = new JsonConfig() await jsonConfig.load('samples/conf.json') jsonConfig.set('nodejs', 'design patterns') await jsonConfig.save('samples/conf_mod.json') const iniConifg = new IniConfig() await iniConifg.load('samples/conf.ini') iniConifg.set('nodejs', 'design patterns') await iniConifg.save('samples/conf_mod.ini')&#125;main() 参考资料Node.js Design Patterns: Design and implement production-grade Node.js applications using proven patterns and techniques, 3rd Edition]]></content>
      <categories>
        <category>Program</category>
      </categories>
      <tags>
        <tag>Program</tag>
        <tag>OOP</tag>
        <tag>Class</tag>
        <tag>Design</tag>
        <tag>Javascript</tag>
        <tag>Node.js</tag>
        <tag>Template</tag>
        <tag>Inheritance</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Node.js 设计模式笔记 —— State 模式]]></title>
    <url>%2F2022%2F07%2F18%2Fnode-js-design-patterns-state-pattern%2F</url>
    <content type="text"><![CDATA[State 模式是一种特殊形式的 Strategy 模式：Context 选择的具体策略根据不同的 state 发生变化。对于 Strategy 模式，可以基于不同的变量比如传入的参数来决定选择具体哪个策略，一旦选择确定后，直到 context 剩余的整个生命周期结束，该策略都保持不变。相反在 State 模式中，策略（或者在这里的语境下，叫做状态）在 context 的生命周期里是动态变化的，从而允许对象的行为可以根据内部状态的变化自适应地更改。 举例来说，我们需要创建一个宾馆预定系统，由一个 Reservation 类对预定房间的行为进行建模。 考虑如下一系列事件： 当 reservation 对象初次创建后，其处于未确认状态。用户可以通过一个 confirm() 方法对此次预定进行确认。但不能通过 cancel() 方法取消预订，因为此次预定还并没有被确认。可以使用 delete() 方法删除这条记录 一旦该 reservation 被确认，订单处于已确认状态。confirm() 方法将不能再次被调用（不能重复确认）；但该 reservation 支持通过 cancel() 方法进行取消，同时该记录无法被删除（已经有人确认预定） 在 reservation 日期的前一天，订单处于已生效状态。上述所有 3 个方法都不再支持，用户只能办理入住 参考上图，可以实现 3 种 不同的策略，他们都实现了 confirm()、cancel()、delete() 这几个方法。每种策略的具体逻辑由不同的状态决定。Reservation 对象只需要在每次状态切换时，激活对应的策略。 实例：failsafe socket12mkdir state &amp;&amp; cd statenpm install json-over-tcp-2 package.json：123456&#123; "type": "module", "dependencies": &#123; "json-over-tcp-2": "^0.3.5" &#125;&#125; failsafeSocket.js：1234567891011121314151617181920212223242526import &#123;OfflineState&#125; from './offlineState.js'import &#123;OnlineState&#125; from './onlineState.js'export class FailsafeSocket &#123; constructor(options) &#123; this.options = options this.queue = [] this.currentState = null this.socket = null this.states = &#123; offline: new OfflineState(this), online: new OnlineState(this) &#125; this.changeState('offline') &#125; changeState(state) &#123; console.log(`Activating state: $&#123;state&#125;`) this.currentState = this.states[state] this.currentState.activate() &#125; send(data) &#123; this.currentState.send(data) &#125;&#125; 上述 FailsafeSocket 类主要由以下几个组件构成： 构造函数 constructor 会初始化一个 queue 队列来存储 socket 离线时发送的数据，还创建了 offline 和 online 两种不同的状态，分别对应离线时和在线时 socket 的不同行为 changeState() 方法负责不同状态的切换。它会更新当前状态 currentState 并调用该状态的 activate() 方法 send() 方法包含 FailsafeSocket 类的主要功能，它会基于在线或离线状态触发不同的行为。这里它将具体的操作指派给了当前激活的状态对象去实现 offlineState.js：12345678910111213141516171819202122232425262728import jsonOverTcp from 'json-over-tcp-2'export class OfflineState &#123; constructor(failsafeSocket) &#123; this.failsafeSocket = failsafeSocket &#125; send(data) &#123; this.failsafeSocket.queue.push(data) &#125; activate() &#123; const retry = () =&gt; &#123; setTimeout(() =&gt; this.activate(), 1000) &#125; console.log('Trying to connect...') this.failsafeSocket.socket = jsonOverTcp.connect( this.failsafeSocket.options, () =&gt; &#123; console.log('Connection established') this.failsafeSocket.socket.removeListener('error', retry) this.failsafeSocket.changeState('online') &#125; ) this.failsafeSocket.socket.once('error', retry) &#125;&#125; 上述模块负责定义 socket 处于离线状态时的行为。 send() 方法只负责将接受到的数据存储到队列中，因为此时是离线状态，队列中的数据会在 socket 在线时取出并发送 activate() 方法会尝试建立连接，连接失败则隔一秒重试。成功建立连接后，failsafeSocket 的状态变为在线状态，触发在线状态的 activate() 方法 onlineState.js：12345678910111213141516171819202122232425262728293031export class OnlineState &#123; constructor(failsafeSocket) &#123; this.failsafeSocket = failsafeSocket this.hasDisconnected = false &#125; send(data) &#123; this.failsafeSocket.queue.push(data) this._safeWrite(data) &#125; _safeWrite(data) &#123; this.failsafeSocket.socket.write(data, (err) =&gt; &#123; if (!this.hasDisconnected &amp;&amp; !err) &#123; this.failsafeSocket.queue.pop() &#125; &#125;) &#125; activate() &#123; this.hasDisconnected = false for (const data of this.failsafeSocket.queue) &#123; this._safeWrite(data) &#125; this.failsafeSocket.socket.once('error', () =&gt; &#123; this.hasDisconnected = true this.failsafeSocket.changeState('offline') &#125;) &#125;&#125; OnlineState 模块实现了当 socket 处于在线状态时的行为。 send() 方法会将数据放入队列，并立即尝试将其写入到 socket，因为此时是在线状态。若该数据成功写入，则将其从队列中移除 activate() 方法会在连接成功建立时触发，会尝试发送在 socket 离线时排队的所有数据并监听任意错误事件。若有错误发生，转换到离线状态，触发离线状态的 activate 方法，继续尝试建立连接 server.js：12345678910import jsonOverTcp from 'json-over-tcp-2'const server = jsonOverTcp.createServer(&#123;port: 5000&#125;)server.on('connection', socket =&gt; &#123; socket.on('data', data =&gt; &#123; console.log('Client data', data) &#125;)&#125;)server.listen(5000, () =&gt; console.log('Server started')) client.js：123456import &#123;FailsafeSocket&#125; from './failsafeSocket.js'const failsafeSocket = new FailsafeSocket(&#123;port: 5000&#125;)setInterval(() =&gt; &#123; failsafeSocket.send(process.memoryUsage())&#125;, 1000) 参考资料Node.js Design Patterns: Design and implement production-grade Node.js applications using proven patterns and techniques, 3rd Edition]]></content>
      <categories>
        <category>Program</category>
      </categories>
      <tags>
        <tag>Program</tag>
        <tag>OOP</tag>
        <tag>Class</tag>
        <tag>Design</tag>
        <tag>Pattern</tag>
        <tag>Javascript</tag>
        <tag>Node.js</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Node.js 设计模式笔记 —— Strategy 模式]]></title>
    <url>%2F2022%2F05%2F26%2Fnode-js-design-patterns-strategy-pattern%2F</url>
    <content type="text"><![CDATA[Strategy 模式的主体是一个 context 对象，再把逻辑中有变化的部分抽取到独立的可相互替换的 strategy 对象中，从而使 context 支持不同的策略。即 context 实现通用的逻辑，strategy 实现可替换的部分。context 与 不同的 strategy 相组合即产生了多种不同的实现。 就像雨天穿胶鞋，打篮球穿运动鞋，短跑比赛穿跑鞋。这些不同的鞋子对应的就是一系列策略，它们是同一类对象的不同变种，彼此之间可以相互替换。面对不同的使用场景，选择对应的策略即可，这带来了更多的灵活性。首先鞋子和人不能是绑定的，这样的话，换鞋子就需要同时换掉整个人了；其次也没有任何一双鞋可以同时满足所有的使用场景。让鞋子作为可替换的插件无疑是最直观和方便的。总的来说，策略代表了一个对象中可替换的部分。不同的策略应对同一个问题的不同变种。静态与动态分离。 比如需要实现一个 Order 对象，代表在线商城中的订单。该对象有一个 pay() 方法，负责支付行为，将用户的钱转移到商户手中。为了能够支持多种不同的支付方式，可以有以下两种选项： 在 pay() 方法中使用 if...else，根据不同的支付方式，完成对应的支付动作 将支付的具体逻辑移交给独立的 strategy 对象，用户选择支付方式后，将对应的 strategy 注入到 Order 中 对于第一种方案，当 Order 对象需要支持更多的支付方式时，就必须要修改 Order 本身的代码。这会使代码变得非常复杂，难以维护。当使用第二种 Strategy 模式时，理论上可以支持无限多的支付方式。Order 只负责维护用户、商品条目、价格等信息，具体的支付逻辑则由另一个 Strategy 对象来实现。Order 本身不会由于支付方式的增加而发生任何变更。 实例：支持 JSON、INI 等多种格式的 config 对象123mkdir strategy &amp;&amp; cd strategynpm install ininpm install object-path package.json1234567&#123; "type": "module", "dependencies": &#123; "ini": "^3.0.0", "object-path": "^0.11.8" &#125;&#125; config.js：123456789101112131415161718192021222324252627282930import &#123;promises as fs&#125; from 'fs'import objectPath from 'object-path'export class Config &#123; constructor(formatStrategy) &#123; this.data = &#123;&#125; this.formatStrategy = formatStrategy &#125; get(configPath) &#123; return objectPath.get(this.data, configPath) &#125; set(configPath, value) &#123; return objectPath.set(this.data, configPath, value) &#125; async load(filePath) &#123; console.log(`Deserializing from $&#123;filePath&#125;`) this.data = this.formatStrategy.deserialize( await fs.readFile(filePath, 'utf-8') ) &#125; async save(filePath) &#123; console.log(`Serializing to $&#123;filePath&#125;`) await fs.writeFile(filePath, this.formatStrategy.serialize(this.data)) &#125;&#125; 其中构造函数 constructor 接收一个具体的策略对象 formStrategy 作为参数，之后的 load 和 save 方法又使用这个 formStrategy 去执行与格式相关的序列化和反序列化操作。不同的 formStrategy 有着不同的实现，从而 Config 类可以凭借 construcotr 接收的不同参数，与不同的策略整合，灵活地应对不同的需求场景。 strategy.js：1234567891011import ini from 'ini'export const iniStrategy = &#123; deserialize: data =&gt; ini.parse(data), serialize: data =&gt; ini.stringify(data)&#125;export const jsonStrategy = &#123; deserialize: data =&gt; JSON.parse(data), serialize: data =&gt; JSON.stringify(data, null, ' ')&#125; 此处的代码实现了两种不同的策略：iniStrategy 和 jsonStrategy，分别针对不同的文件格式。它们有着一致的接口，符合策略之间可以相互替换的原则，从而都可以被前面 Config 类的 load 和 save 方法调用。 index.js：12345678910111213141516import &#123;Config&#125; from './config.js'import &#123;jsonStrategy, iniStrategy&#125; from './strategy.js'async function main() &#123; const iniConfig = new Config(iniStrategy) await iniConfig.load('samples/conf.ini') iniConfig.set('book.nodejs', 'design patterns') await iniConfig.save('samples/conf_mod.ini') const jsonConfig = new Config(jsonStrategy) await jsonConfig.load('samples/conf.json') jsonConfig.set('book.nodejs', 'design patterns') await jsonConfig.save('samples/conf_mod.json')&#125;main() 参考资料Node.js Design Patterns: Design and implement production-grade Node.js applications using proven patterns and techniques, 3rd Edition]]></content>
      <categories>
        <category>Program</category>
      </categories>
      <tags>
        <tag>Program</tag>
        <tag>OOP</tag>
        <tag>Class</tag>
        <tag>Design</tag>
        <tag>Pattern</tag>
        <tag>JavaScript</tag>
        <tag>Node.js</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Node.js 设计模式笔记 —— Proxy 模式]]></title>
    <url>%2F2022%2F05%2F13%2Fnode-js-design-patterns-proxy-pattern%2F</url>
    <content type="text"><![CDATA[代理（proxy） 可以理解为一种对象，其能够控制客户端对另一个对象（subject）的访问。代理（proxy）和目标对象（subject）拥有完全相同的接口，可以自由地进行替换。proxy 会拦截所有或者部分本应该直接交给 subject 执行的操作，通过额外的预处理或后处理增强其行为，再转发给 subject。 Proxy 的主要应用场景： Data validation：proxy 对输入数据进行验证，再转发给 subject Security：proxy 检查客户端是否有权限执行请求的操作，若检查通过则将请求转发给 subject Caching：proxy 负责维护一份内部缓存，只有当请求的数据不在缓存中时，才将该请求转发给 subject 处理 Lazy initialization：若创建某个对象代价很高，proxy 可以延迟该创建操作直到必要的时候 Logging：proxy 拦截函数和对应的参数，在函数执行的同时记录日志信息 Remote objects：proxy 可以接收一个远程对象并令其表现为本地对象 示例代码：StackCalculator123456789101112131415161718192021222324252627282930313233343536373839404142434445class StackCalculator &#123; constructor() &#123; this.stack = [] &#125; putValue(value) &#123; this.stack.push(value) &#125; getValue() &#123; return this.stack.pop() &#125; peekValue() &#123; return this.stack[this.stack.length - 1] &#125; clear() &#123; this.stack = [] &#125; divide() &#123; const divisor = this.getValue() const dividend = this.getValue() const result = dividend / divisor this.putValue(result) return result &#125; multiply() &#123; const multiplicand = this.getValue() const multiplier = this.getValue() const result = multiplier * multiplicand this.putValue(result) return result &#125;&#125;const calculator = new StackCalculator()calculator.putValue(3)calculator.putValue(2)console.log(calculator.multiply()) // 3 * 2 = 6calculator.putValue(2)console.log(calculator.multiply()) // 6 * 2 = 12 现代的计算器基本上都遵循类似的逻辑，即上一个式子的计算结果可以作为下一次计算的输入。在 JavaScript 中，当用户尝试除以 0 时，并不会报错而是返回 Infinity。现在我们尝试借助 Proxy 模式来增强 StackCalculator 除以 0 时的行为。 Object composition组合（Composition）表示一个对象通过引用另一个对象，来扩展或者使用后者的功能。借助组合可以实现 Proxy 模式。创建一个新的对象，令其有着和 subject 完全一致的接口，同时内部还保存着一个对 subject 的引用。参考如下代码：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455class StackCalculator &#123; // see above&#125;class SafeCalculator &#123; constructor(calculator) &#123; this.calculator = calculator &#125; divide() &#123; const divisor = this.calculator.peekValue() if (divisor === 0) &#123; throw Error('Division by 0') &#125; return this.calculator.divide() &#125; putValue(value) &#123; return this.calculator.putValue(value) &#125; getValue() &#123; return this.calculator.getValue() &#125; peekValue() &#123; return this.calculator.peekValue() &#125; clear() &#123; return this.calculator.clear() &#125; multiply() &#123; return this.calculator.multiply() &#125;&#125;const calculator = new StackCalculator()const safeCalculator = new SafeCalculator(calculator)calculator.putValue(3)calculator.putValue(2)console.log(calculator.multiply()) // 3 * 2 = 6safeCalculator.putValue(2)console.log(safeCalculator.multiply()) // 6 * 2 = 12calculator.putValue(0)console.log(calculator.divide()) // 12 / 0 = InfinitysafeCalculator.clear()safeCalculator.putValue(4)safeCalculator.putValue(0)console.log(safeCalculator.divide()) // 4 / 0 -&gt; Error 在这次的实现中，proxy 拦截了感兴趣的方法（divide()），为其实现了新的行为（除以 0），而其他的操作（如 putValue()、getValue()、peekValue()、clear() 和 multiply()）则是简单地分派给 subject 去做。计算器的状态（栈中存放的值）仍由 calculator 实例在维护，SafeCalculator 只是调用 calculator 的方法来读取或者修改这些状态。 上面的实现方式，需要我们显式地将很多方法指派给 subject。即需要写出很多如下形式的代码片段：123getValue() &#123; return this.calculator.getValue()&#125; 这在很大程度上增加了代码的冗余度。 Object augmentation对象增强（Object augmentation）又叫做猴子补丁（monkey patching），能够只代理某个对象的部分方法，并且可能是所有方案中最简单、最常见的一种。它可以将 subject 的某个方法直接替换为 proxy 版本的实现，即直接修改 subject 对象本身。 参考如下代码：123456789101112131415161718192021222324252627class StackCalculator &#123; // see above&#125;function patchToSafeCalculator(calculator) &#123; const divideOrig = calculator.divide calculator.divide = () =&gt; &#123; // additional validation logic const divisor = calculator.peekValue() if (divisor === 0) &#123; throw Error('Division by 0') &#125; // if valid, delegates to the subject return divideOrig.apply(calculator) &#125; return calculator&#125;const calculator = new StackCalculator()const safeCalculator = new patchToSafeCalculator(calculator)safeCalculator.putValue(4)safeCalculator.putValue(0)// console.log(calculator.divide()) // Error, not Infinityconsole.log(safeCalculator.divide()) // 4 / 0 -&gt; Error 当只需要代理某一个或几个方法的时候，上述方案会非常方便。用户不需要再手动重新实现一遍 putValue() 等方法。不幸的是，简单化也带来了一定的代价，像上面那样直接修改 subject 对象是一种危险的行为。当该 subject 对象被其他部分的代码共享时，修改行为必须尽一切可能避免，从而不至于引发意想不到的 side effect。尝试将代码中的 // console.log(calculator.divide()) 取消注释，会发现 calculator 并没有像之前那样输出 Infinity，而是跟 safeCalculator 一样报出错误。即原来的 calculator 对象已经被猴子补丁所改变。 内置的 Proxy 对象ES2015 引入了一种原生的创建 proxy 对象的方式。其语法如下：const proxy = new Proxy(target, handler) 其中 target 代表被 proxy 代理的对象（即 subject），handler 对象则用来定义 proxy 的具体行为。它包含一系列可选的预定义方法（如 get、set、apply 等），叫做 trap methods，在 subject 上执行对应的操作时会自动触发这些方法。 示例代码：12345678910111213141516171819202122232425262728293031323334353637383940class StackCalculator &#123; // see above&#125;const safeCalculatorHandler = &#123; get: (target, property) =&gt; &#123; if (property === 'divide') &#123; // proxied method return function () &#123; // additional validation logic const divisor = target.peekValue() if (divisor === 0) &#123; throw Error('Division by 0') &#125; // if valid, delegates to the subject return target.divide() &#125; &#125; // delegated methods and properties return target[property] &#125;&#125;const calculator = new StackCalculator()const safeCalculator = new Proxy( calculator, safeCalculatorHandler)calculator.putValue(4)calculator.putValue(0)console.log(calculator.divide()) // InfinitysafeCalculator.clear()safeCalculator.putValue(4)safeCalculator.putValue(0)console.log(safeCalculator.divide()) // 4 / 0 -&gt; Error 在上面的代码中，通过 get trap method 捕获对于原本的 calculator 对象的属性和方法的访问，当访问的方法是 divide() 时，proxy 就会返回一个添加了额外验证逻辑的新函数。之后又简单地使用 target[property] 返回了所有未修改过的属性和方法。 总的来说，Proxy 对象为我们提供了一个非常简单的方法，只代理 subject 的一部分功能，且不需要显式地将未代理的方法移交给 subject。同时也不会对原本的 subject 做出任何改动。 几种 proxy 实现机制的比较 Composition：最直观和安全，subject 不会被修改。但需要手动将未代理的方法指派给 subject。冗余代码 Object augmentation：会直接修改原本的 subject 对象，不够安全。不需要手动处理未代理的方法 Proxy 对象：提供了更高级的访问控制。支持更多类型的属性访问，比如可以拦截 subject 对自身属性的删除等操作。不会修改 subject 本身，只需要使用一句代码处理未代理的方法 实例：logging Writable stream1mkdir logwritting &amp;&amp; cd logwritting package.json：123&#123; "type": "module"&#125; logging-writable.js：1234567891011121314export function createLoggingWritable(writable) &#123; return new Proxy(writable, &#123; get(target, propKey) &#123; if (propKey === 'write') &#123; return function (...args) &#123; const [chunk] = args console.log('Writing', chunk) return writable.write(...args) &#125; &#125; return target[propKey] &#125; &#125;)&#125; index.js：123456789101112import &#123;createWriteStream&#125; from 'fs'import &#123;createLoggingWritable&#125; from './logging-writable.js'const writable = createWriteStream('test.txt')const writableProxy = createLoggingWritable(writable)writableProxy.write('First chunk')writableProxy.write('Second chunk')writable.write('This is not logged')writableProxy.end()// =&gt; Writing First chunk// =&gt; Writing Second chunk 参考资料Node.js Design Patterns: Design and implement production-grade Node.js applications using proven patterns and techniques, 3rd Edition]]></content>
      <categories>
        <category>Program</category>
      </categories>
      <tags>
        <tag>OOP</tag>
        <tag>Design</tag>
        <tag>Pattern</tag>
        <tag>JavaScript</tag>
        <tag>Node.js</tag>
        <tag>Proxy</tag>
        <tag>Composition</tag>
        <tag>Monkey Patching</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kubernetes 设计模式笔记 —— Health Probe]]></title>
    <url>%2F2022%2F05%2F10%2Fkubernetes-patterns-reading-notes-health-probe%2F</url>
    <content type="text"><![CDATA[Health Probe 模式主要关注 Kubernetes 如何获取某个应用的健康状态。为了实现完全自动化，一个云原生应用必须是高度可观测的，从而 Kubernetes 能够推断应用的状态，检测应用是否已经启动，是否已经准备好接收请求。这些观测结果会影响 Pod 的生命周期管理，以及网络流量被路由到应用的具体路径。 Kubernetes 会定期检测容器中进程的状态，如果有错误发生，就立即重启该容器。然而在实践中，通过检查进程状态来确定应用是否健康，并不总是有效。很多情况下应用提供的服务中断了，但进程仍旧在运行。比如 Java 应用有可能抛出 OutOfMemoryError 同时 JVM 进程仍在运行。此外，应用还有可能因为无限循环、死锁或者缓存异常等原因冻结。因此 Kubernetes 需要一种可靠的方式来检查应用的健康状态，不关注应用的内部工作流程，而是通过某些指标，衡量应用能否对外提供服务。 软件行业已经接受了这样一个事实，即写出完全没有 bug 的软件是不现实的。因此当面对 failures 时，可以把关注点从避免 bug 转移到如何快速检测到失效并自动恢复。但错误检测并不是一个简单的对所有应用通用的任务，存在很多不同的对于错误的定义，并且不同类型的错误也需要不同的应对方式。 Process Health Checksprocess health check 是最简单的一种 health check 方式，由 Kubelet 持续对容器进程进行检测。若容器进程没有处于运行状态，即对其进行重启。如果应用本身能够检测到任意类型的错误并自行终止，凭借 process health check 就足够完成健康检查任务。 Liveness Probes假如应用会进入某种死锁状态，进程并未停止运行，因而从 process health check 的角度看应用仍然是健康的。Kubernetes 可以通过 liveness probes 来检测此类错误。能够从应用外部执行健康检测，而不是仅仅依靠应用本身，这一点是非常重要的。因为有些错误有可能会阻止应用本身的 watchdog 对外报告异常。liveness probes 看上去和 process health check 非常相似，它们都会在检测到异常时重启容器。但前者提供了更多的灵活性： HTTP probe：向容器的 IP 地址发起 HTTP GET 请求，期待获取一个成功的 HTTP 响应码（200 - 399） TCP Socket probe：测试是否能完成完整的 TCP 连接 Exec probe：在容器内部执行任意的命令，期待获取一个成功的退出码（0） 基于 HTTP 的 liveness probe 示例：12345678910111213141516171819apiVersion: v1kind: Podmetadata: name: pod-with-liveness-checkspec: containers: - image: k8spatterns/random-generator:1.0 name: random-generator env: - name: DELAY_STARTUP value: "20" ports: - containerPort: 8080 protocol: TCP livenessProbe: httpGet: path: /actuator/health port: 8080 initialDelaySeconds: 30 其中 httpGet 中的 path 项用于配置 HTTP probe 执行健康检测时请求的端点；initialDelaySeconds 用于配置执行第一次检测前等待的时间，以等待应用启动后完成 warm up。 需要注意的是，未通过 liveness probe 检查的后果就是容器被重启，若容器重启对于解决问题没有任何效果，则 liveness probe 本身也不会再有任何其他作用。 Readiness ProbesLiveness 检查通过杀掉不健康的容器并将它们替换为新的容器实例，来确保应用处于健康状态。但有些时候容器遇到问题，重启它们并不会令其恢复健康。最常见的情况就是容器正处于启动过程中，还没有准备好处理任何请求。或者有可能容器负载过高导致延迟极度增长。 在上述场景下，Kubernetes 提供了 readiness probe 特性。Readiness 检查和 Liveness 检查提供的检测方法是一样的（都是 HTTP、TCP 和 Exec），只有触发的操作不同。失败的 Readiness 检查会将容器从 Service 端点移除，确保其不再对外提供任何服务。它关注的重点在于容器是否已经准备好，有些容器在启动时需要一定的 warm up 时间才能处理请求。Readiness probe 在容器启动后依然会定期运行，从而将不能对外提供服务的容器屏蔽掉，保证未准备好的容器不会接收到外部的请求。 即 Liveness probe 触发的操作是重启容器，目的是不健康的容器尽可能恢复服务；Readiness probe 触发的操作是将容器从 Service 移除，目的是确保不健康的容器不会对外提供服务，用户的请求只会转发到健康的容器。当然在容器重启时，Kubernetes 也会尽力确保该容器不会再收到用户请求，不管 Readiness probe 是否通过。 Readiness probe 示例：1234567891011apiVersion: v1kind: Podmetadata: name: pod-with-readiness-checkspec: containers: - image: k8spatterns/random-generator:1.0 name: random-generator readinessProbe: exec: command: [ "stat", "/var/run/random-generator-ready" ] Process health check 和 liveness probe 的目的都是通过重启容器来使应用能从错误中自动恢复。Readiness probe 则力求为处于恢复中的容器争取足够的时间。 总结容器技术为打包和运行应用实现了一系列统一的接口，从而可以将应用作为黑盒（black box）看待。然而任何致力于成为云原生应用的容器，必须为运行时环境提供一系列必要的 API，对容器的健康状态进行统一的观测，并执行对应的操作。这是容器能统一地实现自动化升级和生命周期管理的基础需求，从而提高系统的稳定性和用户体验。这意味着容器化应用必须为多种不同的健康检测（liveness 和 readiness）提供需要的 API。甚至更优异的应用还必须为管理平台提供其他手段，以方便更好地观测容器化应用的状态，比如与 Prometheus 进行整合。将应用视为一种黑盒，同时实现必须的 API 接口，方便平台对其进行监控和管理。 参考资料Kubernetes Patterns]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Automation</tag>
        <tag>Docker</tag>
        <tag>Container</tag>
        <tag>Kubernetes</tag>
        <tag>Service</tag>
        <tag>Recovery</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Node.js 设计模式笔记 —— 单例模式]]></title>
    <url>%2F2022%2F05%2F09%2Fnode-js-design-patterns-singleton-pattern%2F</url>
    <content type="text"><![CDATA[Singleton单例（Singleton）模式是面向对象编程中最常见的设计模式之一，Node.js 已经有了很简单的实现。使用单例模式的目的在于确保某个类只有一个实例存在，并对该实例的访问进行统一的控制。其主要运用场景如下： 共享有状态的信息 优化资源消耗 同步对某个资源的访问 比如，一个标准的 Database 类会提供对数据库的访问：1234567// 'Database.js'export class Database &#123; constructor(dbName, connectionDetails) &#123; // ... &#125; // ...&#125; 在上述类的标准实现中，通常需要维护一个数据库连接池，毕竟为每一次数据库请求都分别创建一个新的 Database 实例显得不太现实。此外，Database 实例可能会保存部分有状态的数据，比如 pending 的事务列表。因此，一般只在应用开始运行时初始化一个 Database 实例，此后其作为一个唯一的共享实例被所有其他组件使用。 Node.js 的新用户可能会思考该如何从逻辑层面实现单例模式，事实上远比想象中更简单。将某个实例从模块中导入，即可实现单例模式的所有需求。 1234567// file 'dbInstance.js'import &#123;Database&#125; from './Database.js'export const dbInstance = new Database('my-app-db', &#123; url: 'localhost:5432', username: 'user', password: 'password'&#125;) 只需要简单地导出 Database 类的一个新实例（dbInstance），在当前的整个包中就可以认为只存在这一个 dbInstance 对象（单例），这得益于 Node.js 的模块系统。Node.js 会对模块进行缓存，保证不会在每次导入时都再执行一遍代码。 再通过如下一行代码即可简单地获取上面创建的共享的 dbInstance 实例：1import &#123; dbInstance &#125; from './dbInstance.js' 例外情况Node.js 中缓存的模块以完整路径作为对其进行查找的 key，所以前面实现的 Singleton 只在当前的包中生效。每个包都有可能包含其私有的依赖，放置在它自己的 node_modules 路径下。因而就可能导致同一个模块存在多个实例，前面实现的 Singleton 不能再保证唯一性。 例如，前面的 Database.js 和 dbInstance.js 同属于 mydb 包，其 package.json 内容如下：123456&#123; "name": "mydb", "version": "2.0.0", "type": "module", "main": "dbInstance.js"&#125; 又假设有两个包（package-a 和 package-b）各自都拥有包含如下内容的 index.js 文件：12345import &#123;dbInstance&#125; from 'mydb'export function getDbInstance() &#123; return dbInstance&#125; package-a 和 package-b 都依赖包 mydb，但 package-a 依赖版本 1.0.0，package-b 依赖版本 2.0.0。结果就会出现如下结构的依赖关系：12345678app/`-- node_modules |-- package-a | `-- node_modules | `-- mydb `-- package-b `-- node_modules `-- mydb 当 package-a 和 package-b 依赖两个不兼容版本的 mydb 模块时，包管理器不会将 mydb 放置在 node_modules 的根路径下，而是在 package-a 和 package-b 下面各自放一个私有的 mydb 副本，从而解决版本冲突。 此时假如 app/ 路径下有一个如下内容的 index.js：123456import &#123;getDbInstance as getDbFromA&#125; from 'package-a'import &#123;getDbInstance as getDbFromB&#125; from 'package-b'const isSame = getDbFromA() === getDbFromB()console.log('Is the db instance in package-a the same ' + `as package-b? $&#123;isSame ? 'YES' : 'NO'&#125;`) getDbFromA() 和 getDbFromB() 并不会获得同一个 dbInstance 实例，打破了 Singleton 模式的假设。 当然了，大多数情况下我们并不需要一个 pure Singleton。事实上，通常也只会在应用的 main 包中创建和导入 Singleton。 Singleton dependencies最简单地将两个模块组合在一起的方式，就是直接利用 Node.js 的模块系统。如前面所说，这样组合起来的有状态的依赖关系其实就是单例模式。 实现下面一个博客系统：mkdir blog &amp;&amp; cd blognpm install sqlite3 blog/package.json:123456&#123; "type": "module", "dependencies": &#123; "sqlite3": "^5.0.8" &#125;&#125; blog/db.js：12345678import &#123;dirname, join&#125; from 'path'import &#123;fileURLToPath&#125; from 'url'import sqlite3 from 'sqlite3'const __dirname = dirname(fileURLToPath(import.meta.url))export const db = new sqlite3.Database( join(__dirname, 'data.sqlite')) blog/blog.js：12345678910111213141516171819202122232425import &#123;promisify&#125; from 'util'import &#123;db&#125; from './db.js'const dbRun = promisify(db.run.bind(db))const dbAll = promisify(db.all.bind(db))export class Blog &#123; initialize() &#123; const initQuery = `CREATE TABLE IF NOT EXISTS posts ( id TEXT PRIMARY KEY, title TEXT NOT NULL, content TEXT, created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP );` return dbRun(initQuery) &#125; createPost(id, title, content, createdAt) &#123; return dbRun('INSERT INTO posts VALUES (?, ?, ?, ?)', id, title, content, createdAt) &#125; getAllPosts() &#123; return dbAll('SELECT * FROM posts ORDER BY created_at DESC') &#125;&#125; blog/index.js：1234567891011121314151617181920import &#123;Blog&#125; from './blog.js'async function main() &#123; const blog = new Blog() await blog.initialize() const posts = await blog.getAllPosts() if (posts.length === 0) &#123; console.log('No posts available.') &#125; for (const post of posts) &#123; console.log(post.title) console.log('-'.repeat(post.title.length)) console.log(`Published on $&#123;new Date(post.created_at).toISOString()&#125;`) console.log(post.content) &#125;&#125;main().catch(console.error) db.js 创建了一个 db 数据库实例并导出，blog.js 从 db.js 中导入 db 实例并直接在代码中使用。形成了一种简单直观的 blog.js 依赖于 db.js 模块的关系。同时整个项目中的数据库连接都由唯一的 db 单例进行控制。 运行效果：12$ node index.jsNo posts available. 可以运行下面的命令插入测试数据：12345678910111213141516171819202122232425262728293031323334353637383940414243// import-posts.jsimport &#123;Blog&#125; from './blog.js'const posts = [ &#123; id: 'my-first-post', title: 'My first post', content: 'Hello World!\nThis is my first post', created_at: new Date('2020-02-03') &#125;, &#123; id: 'iterator-patterns', title: 'Node.js iterator patterns', content: 'Let\'s talk about some iterator patterns in Node.js\n\n...', created_at: new Date('2020-02-06') &#125;, &#123; id: 'dependency-injection', title: 'Dependency injection in Node.js', content: 'Today we will discuss about dependency injection in Node.js\n\n...', created_at: new Date('2020-02-29') &#125; // ...]async function main() &#123; const blog = new Blog() await blog.initialize() await Promise.all( posts.map( (post) =&gt; blog.createPost( post.id, post.title, post.content, post.created_at ) ) ) console.log('All posts imported')&#125;main().catch(console.error) 1234567891011121314151617181920$ node import-posts.jsAll posts imported$ node index.jsDependency injection in Node.js-------------------------------Published on 2020-02-29T00:00:00.000ZToday we will discuss about dependency injection in Node.js...Node.js iterator patterns-------------------------Published on 2020-02-06T00:00:00.000ZLet's talk about some iterator patterns in Node.js...My first post-------------Published on 2020-02-03T00:00:00.000ZHello World!This is my first post 就如上面的代码所示，借助 Singleton 模式，将 db 实例自由地在文件之间传递，可以实现一个很简单的命令行博客管理系统。这也是大多数情况下我们管理有状态的依赖的方式。使用 Singleton 诚然是最简单、即时，可读性最好的方案。但是，假如我们需要在测试过程中 mock 数据库，或者需要终端用户能够自主选择另一个数据库后端，而不是默认提供的 SQLite。对于以上需求，Singleton 反而成为了一个设计更好结构的阻碍。可以在 db.js 中引入 if 语句根据某些条件来选择不同的实现，显然这种方式并不是很美观。 Dependency InjectionNode.js 的模块系统以及 Singleton 模式可以作为一个很好的管理和组合应用组件的工具，它们非常简单，容易上手。但另一方面，它们也可能会使各组件之间的耦合程度加深。在前面的例子中，blog.js 和 db.js 模块是耦合度很高的，blog.js 没有了 db.js 就无法工作，当然也无法使用另一个不同的数据库模块。可以借助 Dependency Injection 来弱化模块之间的耦合度。 依赖注入表示将某个组件的依赖模块由外部实体（injector）作为输入提供。DI 的主要优势在于能够降低耦合度，尤其当模块依赖于有状态的实例（比如数据库连接）时。每个依赖项并不是硬编码进主体代码，而是由外部传入，意味着这些依赖项可以被替换成任意相互兼容的实例。使得主体代码本身可以以最小的改动在不同的背景下重用。 修改 blog.js：123456789101112131415161718192021222324252627import &#123;promisify&#125; from 'util'export class Blog &#123; constructor(db) &#123; this.db = db this.dbRun = promisify(db.run.bind(db)) this.dbAll = promisify(db.all.bind(db)) &#125; initialize() &#123; const initQuery = `CREATE TABLE IF NOT EXISTS posts ( id TEXT PRIMARY KEY, title TEXT NOT NULL, content TEXT, created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP );` return this.dbRun(initQuery) &#125; createPost(id, title, content, createdAt) &#123; return this.dbRun('INSERT INTO posts VALUES (?, ?, ?, ?)', id, title, content, createdAt) &#125; getAllPosts() &#123; return this.dbAll('SELECT * FROM posts ORDER BY created_at DESC') &#125;&#125; 最主要的改动在于为 Blog 类添加了 constructor (db) 构造方法，该方法的参数 db 即为 Dependency，Blog 的依赖项，需要在运行时由 Blog 的客户端提供。 修改 db.js：12345import sqlite3 from 'sqlite3'export function createDb(dbFile) &#123; return new sqlite3.Database(dbFile)&#125; 此版本的 db 模块提供了一个 createDB() 工厂函数，可以在运行时返回一个新的数据库实例。 修改 index.js：1234567891011121314151617181920212223242526import &#123;dirname, join&#125; from 'path'import &#123;fileURLToPath&#125; from 'url'import &#123;Blog&#125; from './blog.js'import &#123;createDb&#125; from './db.js'const __dirname = dirname(fileURLToPath(import.meta.url))async function main() &#123; const db = createDb(join(__dirname, 'data.sqlite')) const blog = new Blog(db) await blog.initialize() const posts = await blog.getAllPosts() if (posts.length === 0) &#123; console.log('No posts available.') &#125; for (const post of posts) &#123; console.log(post.title) console.log('-'.repeat(post.title.length)) console.log(`Published on $&#123;new Date(post.created_at).toISOString()&#125;`) console.log(post.content) &#125;&#125;main().catch(console.error) 使用 createDB() 工厂函数创建数据库实例 db，然后在初始化 Blog 实例时，将 db 作为 Blog 的依赖进行注入。从而 blog.js 与具体的数据库实现进行了分离。 依赖注入可以提供松耦合和代码重用等优势，但也存在一定的代价。比如无法在编码时解析依赖项，使得理解模块之间的逻辑关系变得更加困难，尤其当应用很大很复杂的时候。此外，我们还必须确保数据库实例（依赖）在 Blog 实例之前创建，从而迫使我们手动构建整个应用的依赖图，以保证顺序正确。 参考资料Node.js Design Patterns: Design and implement production-grade Node.js applications using proven patterns and techniques, 3rd Edition]]></content>
      <categories>
        <category>Program</category>
      </categories>
      <tags>
        <tag>OOP</tag>
        <tag>Design</tag>
        <tag>Pattern</tag>
        <tag>JavaScript</tag>
        <tag>Singleton</tag>
        <tag>Node.js</tag>
        <tag>Dependency Injection</tag>
        <tag>Decoupling</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kubernetes 设计模式笔记 —— 声明式部署]]></title>
    <url>%2F2022%2F05%2F05%2Fkubernetes-patterns-reading-notes-declarative-deployment%2F</url>
    <content type="text"><![CDATA[声明式部署（Declarative Deployment）模式主要体现在 Kubernetes 对其 Deployment 对象的应用上。 升级某个服务意味着，需要平滑地关闭旧版本的 Pod，启动新版本的 Pod，然后等待和确认其部署成功，有时候在部署失败时还需要执行回滚操作。这些步骤或者需要一定的 downtime，同时不会有多个版本的服务在并行地运行（旧版本完全停止后再启动新版本）；或者不允许有 downtime，但在升级过程中，新旧版本的服务同时在线会造成资源消耗的增长（旧版本开始停止的时候就添加新版本的实例）。 手动执行上述操作难免会有错误发生，写脚本来自动化处理又需要付出较大的时间成本。好在 Kubernetes 通过 Deployment 概念自动化了这些升级、回滚流程，可以在 Deployment 中定义替换的策略（如 RollingUpdate 和 Recreate）以及其他升级过程中的细节。 Rolling Deployment下面是一个采用滚动升级（rolling update）策略的 Deployment 示例：12345678910111213141516171819202122232425apiVersion: apps/v1kind: Deploymentmetadata: name: random-generatorspec: replicas: 3 strategy: type: RollingUpdate rollingUpdate: maxSurge: 1 maxUnavailable: 1 selector: matchLabels: app: random-generator template: metadata: labels: app: random-generator spec: containers: - image: k8spatterns/random-generator:1.0 name: random-generator readinessProbe: exec: command: [ "stat", "/random-generator-ready"] 其中 replicas: 3 表示副本的总数量是 3，在执行 rolling update 时，声明的副本数量必须大于 1。maxSurge: 1 表示在升级过程中，允许临时添加的 Pod 的最大数量。滚动升级有一个切换的过程，必然导致某个时间段内新旧版本的应用同时运行，从而实际运行的 Pod 数量大于声明的副本数量。此处的配置最多允许 4 个副本同时运行。maxUnavailable: 1 表示升级过程中可能无法访问的 Pods 数量。此处的配置有可能导致在升级的某个阶段，只有两个 Pods 可用。readinessProbe 配置对于执行无 downtime 的滚动升级非常关键，它用来判断某个 Pod 副本是否已经在线 RollingUpdate 策略会确保升级过程中没有 downtime。Deployment 负责创建新的 ReplicaSet 并用新的容器替换掉旧容器，用户则可以通过配置 maxSurge 和 maxUnavailable 字段来控制切换的速率。 Deployment 的优势 Deployment 是完全被 Kubernetes 内部管理的资源对象，整个的升级过程由 Server 端执行，无需客户端介入 Deployment 的声明式性质，使得用户更加关注期望达到的状态而不是达到该状态需要执行的操作步骤 整个升级过程会以版本的方式进行记录，还提供了 pause、continue 和 rollback 等选项 Fixed DeploymentRollingUpdate 策略在需要确保无 downtime 时非常有用，但该方式也有一些负面影响。比如在升级过程中，会有两个版本的容器同时运行，这有可能导致接收服务的客户端出现一些 issue，尤其当更新引入了没有向后兼容的特性时。在此类场景下，可以使用 Recreate 策略。 Recreate 策略的效果类似于 RollingUpdate 将 maxUnavailable 的值设置成了副本的数量。这意味着所有当前版本的容器都会先被终止掉，在旧容器被全部清理干净之后才开始同步启动所有的新容器。结果就是升级过程中会存在一定的 downtime。 Blue-Green ReleaseBlue-Green deployment 发布策略致力于在生产环境中最小化部署时的 downtime。借助 Kubernetes 对于发布行为的抽象（Deployment），用户可以自行定义和实现将不可变的容器从一个版本转换到另一个版本的具体方式，比如 Blue-Green 部署方式。如果 Kubernetes 集群中并未安装 Service Mesh 或者 Knative 等扩展组件，Blue-Green deployment 就需要手动实现。其原理就是创建一个新的 Deployment，包含最新版本的容器（green），但是并不向外提供任何服务。旧的 Pod 副本（blue）依然在运行和处理请求。一旦用户确认新版本的 Pod 是健康的，可以提供服务，就将入站流量从旧的 Pod 副本切换到新版本的副本。在 Kubernetes 中可以通过修改 Service selector 来完成切换动作。当新版本的容器运行稳定后，旧版本即可被删除。 Blue-Green 方案的优势在于，同一时间只会有一个特定版本的应用对外提供服务，不需要接收服务的客户端处理多个并行的服务版本；其劣势则在于需要双倍的资源去运行 blue 和 green 容器。此外，有些时候切换的过程会非常复杂。 Canary ReleaseCanary release 是一种软部署方式，一开始只会将旧版本实例的一个较小的子集替换为新版本。先只允许一小部分用户能够访问更新后的服务，从而降低新版本向生产环境引入的风险。当能够确认新版本的服务对一小部分用户没有产生负面影响，再用新版全面替换掉旧版。 在 Kubernetes 中，上述部署方式可以通过创建一个包含新版本容器的 ReplicaSet 来实现，只不过副本的数量可以设置得很小，作为 Canary 实例。同时 Service 对象负责将一部分用户的请求转发给 Canary 容器。当我们确信新版本应用可以正常提供服务，则横向扩展新的 ReplicaSet 到期望的副本数量，同时收缩旧 ReplicaSet 的副本数量至 0。 总结Deployment 很好地向我们展示了，Kubernetes 将复杂的手动升级应用的流程，转化为可以重复执行、支持自动化编排的声明式部署。 上述四种部署方式的总结： 参考资料Kubernetes Patterns]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Docker</tag>
        <tag>Container</tag>
        <tag>Kubernetes</tag>
        <tag>Deployment</tag>
        <tag>ReplicaSet</tag>
        <tag>Update</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Node.js 设计模式笔记 —— Builder 模式]]></title>
    <url>%2F2022%2F05%2F05%2Fnode-js-design-patterns-builder-pattern%2F</url>
    <content type="text"><![CDATA[Builder 是一种创建型设计模式，可以通过提供简单平滑的接口来简化复杂对象的创建，允许我们一步一步的构建新对象。最明显的需要使用 Builder 模式的时候，就是当某个类的构造函数包含了太多的参数。 比如下面的一个 Boat 类：1234567class Boat &#123; constructor(hasMotor, motorCount, motorBrand, motorModel, hasSails, sailsCount, sailsMaterial, sailsColor, hullColor, hasCabin) &#123; // ... &#125;&#125; 调用上述 Boat 类的构造方法会导致出现非常难以阅读的代码：12const myBoat = new Boat(true, 2, 'Best Motor Co. ', 'OM123', true, 1, 'fabric', 'white', 'blue', false) 想要提升上述构造函数的设计，首先可以将所有的参数整合到一个单一的对象中，如下：123456789101112131415161718class Boat &#123; constructor(allParameters) &#123; // ... &#125;&#125;const myBoat = new Boat(&#123; hasMotor: true, motorCount: 2, motorBrand: 'Best Motor Co. ', motorModel: 'OM123', hasSails: true, sailsCount: 1, sailsMaterial: 'fabric', sailsColor: 'white', hullColor: 'blue', hasCabin: false,&#125;) 新版本的构造函数跟原来相比提升了不少，比如用户可以清晰地看到每个传入的参数所代表的具体含义。但是，将所有参数都放入同一个对象后再传入构造函数的方式，也有其自身的缺点。比如要想知道每个参数的具体含义，还是需要查看类的说明文档甚至类的代码。此外，没有任何强制性的协议来引导用户创建一致的对象，假如我们指定 hasMotor: true，意味着我们同时还需要再指定 motorCount、motorBrand 和 motorModel 参数的值。但我们无从获取此类信息（除非查看源代码）。 Builder 模式就非常适合解决上述问题。帮助用户创建一个平滑、易读、自说明的生成对象的接口，同时为创建具有一致性的对象提供指导信息。 使用 Builder 模式的 Boat 类：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455class Boat &#123; constructor(allParameters) &#123; // ... &#125;&#125;class BoatBuilder &#123; withMotors(count, brand, model) &#123; this.hasMotor = true this.motorCount = count this.motorBrand = brand this.motorModel = model return this &#125; withSails(count, material, color) &#123; this.hasSails = true this.sailsCount = count this.sailsMaterial = material this.sailsColor = color return this &#125; hullColor(color) &#123; this.hullColor = color return this &#125; withCabin() &#123; this.hasCabin = true return this &#125; build() &#123; return new Boat(&#123; hasMotor: this.hasMotor, motorCount: this.motorCount, motorBrand: this.motorBrand, motorModel: this.motorModel, hasSails: this.hasSails, sailsCount: this.sailsCount, sailsMaterial: this.sailsMaterial, sailsColor: this.sailsColor, hullColor: this.hullColor, hasCabin: this.hasCabin &#125;) &#125;&#125;const myBoat = new BoatBuilder() .withMotors(2, 'Best Motor Co. ', 'OM123') .withSails(1, 'fabric', 'white') .withCabin() .hullColor('blue') .build() BoatBuilder 类的作用就是收集 Boat 类需要的所有参数，再通过一系列 helper 方法传递给 Boat。 Builder 模式的基本规则： 将主要对象的复杂构建过程拆分为几个更为易读的、更容易管理的步骤 尝试创建 builder 方法，向需要创建的对象一组一组地传递相关联的参数 必要的情况下，在通过 builder 方法将参数传递给需要创建的对象前，尽可能地先对参数做一些处理 实例：URL builder创建并进入 url_builder 文件夹，编辑如下内容的 package.json 文件：123&#123; "type": "module"&#125; url.js：12345678910111213141516171819202122232425262728293031323334353637383940414243export class Url &#123; constructor(protocol, username, password, hostname, port, pathname, search, hash) &#123; this.protocol = protocol this.username = username this.password = password this.hostname = hostname this.port = port this.pathname = pathname this.search = search this.hash = hash this.validate() &#125; validate() &#123; if (!this.protocol || !this.hostname) &#123; throw new Error('Must specify at least a ' + 'protocol and a hostname') &#125; &#125; toString() &#123; let url = '' url += `$&#123;this.protocol&#125;://` if (this.username &amp;&amp; this.password) &#123; url += `$&#123;this.username&#125;:$&#123;this.password&#125;@` &#125; url += this.hostname if (this.port) &#123; url += this.port &#125; if (this.pathname) &#123; url += this.pathname &#125; if (this.search) &#123; url += `?$&#123;this.search&#125;` &#125; if (this.hash) &#123; url += `#$&#123;this.hash&#125;` &#125; return url &#125;&#125; urlBuilder.js：1234567891011121314151617181920212223242526272829303132333435363738394041424344import &#123; Url &#125; from './url.js'export class UrlBuilder &#123; setProtocol(protocol) &#123; this.protocol = protocol return this &#125; setAuthentication(username, password) &#123; this.username = username this.password = password return this &#125; setHostname(hostname) &#123; this.hostname = hostname return this &#125; setPort(port) &#123; this.port = port return this &#125; setPathname(pathname) &#123; this.pathname = pathname return this &#125; setSearch(search) &#123; this.search = search return this &#125; setHash(hash) &#123; this.hash = hash return this &#125; build() &#123; return new Url(this.protocol, this.username, this.password, this.hostname, this.port, this.pathname, this.search, this.hash) &#125;&#125; index.js：123456789import &#123;UrlBuilder&#125; from './urlBuilder.js'const url = new UrlBuilder() .setProtocol('https') .setAuthentication('user', 'pass') .setHostname('example.com') .build()console.log(url.toString()) 运行效果：12$ node index.jshttps://user:pass@example.com 参考资料Node.js Design Patterns: Design and implement production-grade Node.js applications using proven patterns and techniques, 3rd Edition]]></content>
      <categories>
        <category>Program</category>
      </categories>
      <tags>
        <tag>Program</tag>
        <tag>OOP</tag>
        <tag>Class</tag>
        <tag>Design</tag>
        <tag>Pattern</tag>
        <tag>JavaScript</tag>
        <tag>Node.js</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Node.js 设计模式笔记 —— 工厂模式]]></title>
    <url>%2F2022%2F04%2F28%2Fnode-js-design-patterns-factory-pattern%2F</url>
    <content type="text"><![CDATA[工厂（Factory）模式 是 Node.js 中最常见的设计模式之一。其具有以下优势： 将对象的创建过程与对象的实现细节进行解耦。由工厂创建一系列对象，某个对象继承的特征在运行时确定 工厂模式允许我们对外暴露更少的接口。一个类可以被扩展或者操控，而工厂本身仅仅是一个负责创建对象的函数，没有给用户其他选项，从而使接口更健壮和容易理解 借助闭包可以帮助强化对象的封装 解耦对象的创建和实现工厂模式封装了新对象的创建过程，给这个过程提供了更多的灵活性和控制。在工厂内部我们可以选择各种不同的方式来创建某个对象的实例，工厂的消费者对于这些细节一无所知。相反地，使用 new 关键字则会将代码绑定到一种特定的创建方式上。 比如下面的一个用于创建 Image 对象的工厂函数：1234function createImage (name) &#123; return new Image(name)&#125;const image = createImage('photo.jpeg') 上述 createImage 工厂函数看上去完全没有必要，直接使用如下一行代码就可以搞定：1const image = new Image('photo.jpeg') 按照前面所说，new 关键字会将代码绑定给一种特定类型的对象，在这里就是 Image 类型。而工厂模式则更加灵活。假设需要重构 Image 类，将其分割成几个更小的类型，对应不同的图片格式。工厂函数 createImage 作为唯一的创建新图片对象的方式，即便需要创建的图片对象添加了更多的类型，也可以很简单地只对 createImage 的内部逻辑进行重写，其对外开放的接口不会发生改变，不会破坏任何现有的代码：1234567891011function createImage(name) &#123; if (name.match(/\.jpe?g$/)) &#123; return new ImageJpeg(name) &#125; else if (name.match(/\.gif$/)) &#123; return new ImageGif(name) &#125; else if (name.match(/\.png$/)) &#123; return new ImagePng(name) &#125; else &#123; throw new Error('Unsupported format') &#125;&#125; 强化封装借助闭包，工厂模式可以成为一种强化封装性的机制。12345678910111213141516171819202122232425function createPerson (name) &#123; const privateProperties = &#123;&#125; const person = &#123; setName (name) &#123; if (!name) &#123; throw new Error('A person must have a name') &#125; privateProperties.name = name &#125;, getName () &#123; return privateProperties.name &#125; &#125; person.setName(name) return person&#125;person = createPerson('John')console.log(person.getName())// =&gt; Johnperson.setName('Michael')console.log(person.getName())// =&gt; Michael createPerson 工厂函数创建了一个 person 对象。由于闭包的存在，即便 createPerson 函数运行完毕退出了，其属性 privateProperties 仍可以被 person 对象通过其 setName 和 getName 方法访问。但与此同时，该 privateProperties 属性无法被任何外部对象（包括 person）直接访问。 完整实例：Profiler创建并进入一个新的 simple_profiler 文件夹，编辑如下内容的 package.json 文件：123&#123; "type": "module"&#125; 创建如下内容的 profiler.js 文件：12345678910111213141516171819202122232425262728class Profiler &#123; constructor (label) &#123; this.label = label this.lastTime = null &#125; start () &#123; this.lastTime = process.hrtime() &#125; end () &#123; const diff = process.hrtime(this.lastTime) console.log(`Timer "$&#123;this.label&#125;" took $&#123;diff[0]&#125; seconds ` + `and $&#123;diff[1]&#125; nanoseconds.`) &#125;&#125;const noopProfiler = &#123; start () &#123;&#125;, end () &#123;&#125;&#125;export function createProfiler (label) &#123; if (process.env.NODE_ENV === 'production') &#123; return noopProfiler &#125; return new Profiler(label)&#125; 创建如下内容的 index.js 文件：1234567891011121314151617181920212223import &#123; createProfiler &#125; from './profiler.js'function getAllFactors (intNumber) &#123; const profiler = createProfiler( `Finding all factors of $&#123;intNumber&#125;` ) profiler.start() const factors = [] for (let factor = 2; factor &lt;= intNumber; factor++) &#123; while ((intNumber % factor) === 0) &#123; factors.push(factor) intNumber = intNumber / factor &#125; &#125; profiler.end() return factors&#125;const myNumber = process.argv[2]const myFactors = getAllFactors(myNumber)console.log(`Factors of $&#123;myNumber&#125; are: `, myFactors) 运行效果：12345$ NODE_ENV=production node index.js 2201307499Factors of 2201307499 are: [ 38737, 56827 ]$ node index.js 2201307499Timer "Finding all factors of 2201307499" took 0 seconds and 9738800 nanoseconds.Factors of 2201307499 are: [ 38737, 56827 ] 简单来说，就是通过 createProfiler 工厂函数来创建不同的 Profiler 对象。若环境变量 NODE_ENV 的值为 production，则返回一个新的的 noopProfiler，不对运行的代码做任何额外的操作；若 NODE_ENV 的值不为 production，则返回一个新的 Profiler 对象，记录程序运行的时间。 参考资料Node.js Design Patterns: Design and implement production-grade Node.js applications using proven patterns and techniques, 3rd Edition]]></content>
      <categories>
        <category>Program</category>
      </categories>
      <tags>
        <tag>Program</tag>
        <tag>OOP</tag>
        <tag>Design</tag>
        <tag>JavaScript</tag>
        <tag>Node</tag>
        <tag>Node.js</tag>
        <tag>Factory</tag>
        <tag>Patterns</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kubernetes in Action 笔记 —— 通过 PersistentVolume 持久化数据]]></title>
    <url>%2F2022%2F04%2F27%2Fkubernetes-in-action-reading-notes-persistent-data-with-persistent-volume%2F</url>
    <content type="text"><![CDATA[Pods 与底层存储技术的解耦理想情况下，将应用部署到 Kubernetes 上的开发者不需要知道集群提供的存储技术，就像他们不需要知道运行 Pods 的物理服务器的属性。基础设施的细节应该由集群的维护者去掌控。 比如在 Pod 中挂载一个 NFS 共享作为持久存储，Pod 的清单文件中就需要包含 NFS 服务器的 IP 地址和共享文件的路径，从而导致该 Pod 的定义与特定的集群绑定在一起，阻止其用在其他地方。 Persistent volumes and claims为了令 Pod 清单文件面向不同的集群是可移植的，针对存储卷的环境相关的信息被移动到 PersistentVolume 对象中，再通过一个 PersistentVolumeClaim 对象将 Pod 与 PersistentVolume 连接在一起。 顾名思义，PersistentVolume 对象代表一种存储卷，用来持久化应用数据。该对象包含了底层存储的信息，从而将这些信息从 Pod 中解耦。即 Pod 的清单文件中与存储相关的部分，不必再包含基础设施相关的信息（转移到了 PersistentVolume 中），使得同样的清单文件能够部署在不同的集群上。 Pod 并不会直接引用 PersistentVolume 对象，而是指向一个 PersistentVolumeClaim 对象。PersistentVolumeClaim 代表用户对 PV 的请求或者声明，有着独立于 Pod 的生命周期，从而允许 PV 的所属权（ownership）与 Pod 解耦。用户在使用 PV 前必须先声明一个 PVC 对象。Pod 可以在任意时间删除，用户并不会因此失去对 PV 的所属权。当 PV 不再被需要时，用户可以通过删除 PVC 来释放它。 Pod 清单文件中的存储定义部分只需要包含 PVC 的名称，不需要任何基础设施相关的信息，比如 NFS 服务器的 IP 地址。PVC 会负责将其绑定的代表 NFS 存储的 PV 挂载到 Pod 中。 使用 PV 和 PVC 的优势为了让 Pod 使用某个存储卷，借助 PV 和 PVC 这两种额外的对象，肯定比直接在 Pod 清单文件中定义要复杂得多。使用 PV 和 PVC 的最大优势在于，基础设施相关的细节从 Pod 代表的应用中解脱了出来。集群管理员比任何人都更了解数据中心本身，他们负责创建 PV 对象；软件开发者则可以集中精力通过 Pod 和 PVC 来描述应用本身的需求。 应用开发人员不需要了解底层基础设施的任何细节，就可以直接创建 Pod 清单文件和 PVC 对象；同样的，集群管理员也可以在不了解应用的所有细节的前提下，创建一系列不同大小的存储卷。更进一步的，借助 PV 的动态创建功能，管理员根本不需要提前创建好存储卷。如果集群中安装了 automated volume provisioner，物理存储卷和 PV 对象会在用户创建 PVC 之后按需自动生成。 创建 PV 和 PVC创建 PV 对象测试环境使用的是 Minikube，因此这里使用工作节点的本地路径来创建 PV，而不使用网络存储。其清单文件如下：123456789101112apiVersion: v1kind: PersistentVolumemetadata: name: quiz-dataspec: capacity: storage: 1Gi accessModes: - ReadWriteOnce - ReadOnlyMany hostPath: path: /var/quiz-data 其中的 capacity 选项用来指定底层存储卷的大小。每个 PV 都必须指定其容量，以便于在 PVC 和 PV 绑定时，Kubernetes 可以判断具体哪个 PV 符合要求。每个 PV 都必须指定其支持的 accessModes 列表。依赖于底层存储的具体实现，PV 可能支持也可能不支持同时被多个工作节点以 r/w 或 r/o 模式挂载。注意 accessModes 影响的是 Nodes 而不是 Pods。一个 PV 只要能够被某个节点挂载，同时也就支持被该节点上的多个 Pods 挂载。accessModes 有 ReadWriteOnce、ReadOnlyMany、ReadWriteMany 三种模式。 创建和查看 PV创建 PV：12$ kubectl apply -f pv.quiz-data.hostpath.yamlpersistentvolume/quiz-data created 查看 PV：123$ kubectl get pvNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGEquiz-data 1Gi RWO,ROX Retain Available 2m55s 声明一个 PV创建一个 PVC 对象需要创建 PVC 对象来声明一个 PV，PVC 对象中会指定 PV 必须符合的要求，包括最小容量、访问模式等，通常是由不同应用的具体需求来决定的。因而 PVC 对象应该由应用的作者而不是集群管理员来创建。 PVC 对象的清单文件内容如下：123456789101112apiVersion: v1kind: PersistentVolumeClaimmetadata: name: quiz-dataspec: resources: requests: storage: 1Gi accessModes: - ReadWriteOnce storageClassName: "" volumeName: quiz-data 上面的 PVC 对象描述了一系列需要 PV 满足的要求。比如至少 1G 大小、能够在单节点上以读写模式挂载。storageClassName 字段用来配置 PV 的动态生成，如果需要绑定一个已经预先创建好的 PV，则该字段必须为空。因为前面创建的 PV 名字为 quiz-data，所以 PVC 中的 volumeName 字段也必须为 quiz-data；若不指定此字段，则 Kubernetes 有可能会绑定其他满足要求的 PV。如果集群管理员创建了一系列没有指定名称的 PV，用户也并不在意具体会绑定哪个 PV，则可以跳过 volumeName 字段，让 Kubernetes 随机选择满足要求的 PV。 创建和查看 PVC创建 PVC：12$ kubectl apply -f pvc.quiz-data.static.yamlpersistentvolumeclaim/quiz-data created 查看 PVC：123$ kubectl get pvcNAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGEquiz-data Bound quiz-data 1Gi RWO,ROX 31s 再次查看之前创建的 PV 的状态，可以看到此时 quiz-data PV 的 STATUS 变成了 BOUND：123$ kubectl get pvNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGEquiz-data 1Gi RWO,ROX Retain Bound default/quiz-data 92m 在 Pod 中使用 PVC 和 PV参考如下清单文件：1234567891011121314151617181920apiVersion: v1kind: Podmetadata: name: quizspec: volumes: - name: quiz-data persistentVolumeClaim: claimName: quiz-data containers: - name: quiz-api image: luksa/quiz-api:0.1 ports: - name: http containerPort: 8080 - name: mongo image: mongo volumeMounts: - name: quiz-data mountPath: /data/db 使用 kubectl apply -f pod.quiz.pvc.yaml 命令创建 Pod，待创建完成后，可以执行如下 Shell 命令向 mongo 容器中插入数据：12345678kubectl exec -it quiz -c mongo -- mongo kiada &lt;&lt;EOFdb.questions.insert(&#123; id: 1, text: "What does k8s mean?", answers: ["Kates", "Kubernetes", "Kooba Dooba Doo!"], correctAnswerIndex: 1&#125;)EOF 运行如下命令查看插入的数据：12$ kubectl exec -it quiz -c mongo -- mongo kiada --quiet --eval "db.questions.find()"&#123; "_id" : ObjectId("625fe24a095faed6c085f539"), "id" : 1, "text" : "What does k8s mean?", "answers" : [ "Kates", "Kubernetes", "Kooba Dooba Doo!" ], "correctAnswerIndex" : 1 &#125; 在 Pod 中重复使用 PVC当删除某个 Pod 中的 PVC 时，对应的底层存储卷会从工作节点解除挂载。但 PV 对象仍旧是跟 PVC 相关联的。若之后创建另一个 Pod 指向同样的 PVC，则新的 Pod 可以访问 PV 对应的存储卷和文件。 123456$ kubectl delete -f pod.quiz.pvc.yamlpod "quiz" deleted$ kubectl apply -f pod.quiz.pvc.yamlpod/quiz created$ kubectl exec -it quiz -c mongo -- mongo kiada --quiet --eval "db.questions.find()"&#123; "_id" : ObjectId("625fe24a095faed6c085f539"), "id" : 1, "text" : "What does k8s mean?", "answers" : [ "Kates", "Kubernetes", "Kooba Dooba Doo!" ], "correctAnswerIndex" : 1 &#125; 释放 PV删除 PVC 会释放对应的 PV。1234$ kubectl delete pod quizpod "quiz" deleted$ kubectl delete pvc quiz-datapersistentvolumeclaim "quiz-data" deleted 查看此时 PV 的状态：123$ kubectl get pv quiz-dataNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGEquiz-data 1Gi RWO,ROX Retain Released default/quiz-data 45h 其 STATUS 项变成了 Released 状态。 若此时重新创建删除的 PVC，对应的 PV 不会再次被绑定。原因在于，PV 已经被使用过，有可能包含一些旧的数据，需要在绑定给另一个 PVC 之前进行清理。这也是为什么 PV 已经 Released 之后，仍然显示关联的 CLAIM 是 default/quiz-data，为了方便集群管理员确认这些数据能否被安全的删除。 令释放的 PV 重新可用重新创建删除的 PVC 不会自动绑定之前的 PV，该 PV 会处于 Released 状态。为了让 PV 对应的数据再次可用，需要删除并重新创建 PV。PV 对象只是一个指向底层存储的指针，它本身并不存储任何应用数据。删除并重新创建 PV 相当于创建了一个新的指向同一个底层存储卷的指针。数据和之前是相同的。 12345678910$ kubectl delete pv quiz-datapersistentvolume "quiz-data" deleted$ kubectl apply -f pv.quiz-data.hostpath.yamlpersistentvolume/quiz-data created$ kubectl apply -f pvc.quiz-data.static.yamlpersistentvolumeclaim/quiz-data created$ kubectl apply -f pod.quiz.pvc.yamlpod/quiz created$ kubectl exec -it quiz -c mongo -- mongo kiada --quiet --eval "db.questions.find()"&#123; "_id" : ObjectId("625fe24a095faed6c085f539"), "id" : 1, "text" : "What does k8s mean?", "answers" : [ "Kates", "Kubernetes", "Kooba Dooba Doo!" ], "correctAnswerIndex" : 1 &#125; PV 的 reclaim policyPV 被释放后的动作取决于其 reclaim policy。此 policy 由 PV 对象的 .spec.persistentVolumeReclaimPolicy 条目进行配置。前面 quiz-data 的 reclaim policy 是 Retain。123$ kubectl get pvNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGEquiz-data 1Gi RWO,ROX Retain Bound default/quiz-data 5m57s 两种不同的 reclaim policy： Retain：当 PV 被释放后（即对应的 PVC 被删除后），Kubernetes 会保留该 PV。集群管理员则必须手动回收 Volume。这是手动创建 PV 的默认配置 Delete：PV 对象和对应的底层存储会在 PV 释放后自动删除。这是动态创建 PV 的默认配置 删除绑定中的 PV如果集群管理员删除了某个正在使用中的 PV（已经绑定给了某个 PVC）：123$ kubectl delete pv quiz-datapersistentvolume "quiz-data" deleted^C 上述命令会告诉 Kubernetes API 删除 PV 对象，并等待 Kubernetes 控制器完成该操作。事实上该操作并不会完成，直到待删除的 PV 最终被释放（绑定的 PVC 被删除）。可以按 Ctrl - C 取消等待，但删除动作并不会被取消：123$ kubectl get pvNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGEquiz-data 1Gi RWO,ROX Retain Terminating default/quiz-data 19m 该 PV 会一直处于 Terminating 状态，直到对应的 PVC 被删除。 删除使用中的 PVC和删除 PV 类似，删除使用中的 PVC 的动作并不会立即完成。可以强制中断删除命令的执行，但并不会取消该删除流程。123$ kubectl delete pvc quiz-datapersistentvolumeclaim "quiz-data" deleted^C 删除动作会被 Pod 阻塞。毫无疑问，删除一个正在使用中的 PVC 并不会立即对 Pod 中运行的应用产生任何影响。Kubernetes 并不会因为集群管理员需要回收一些存储空间而杀掉某个 Pod。只有删掉引用了该 PVC 的 Pod，删除该 PVC 的进度才会完成。 理解手动创建的 PV 的生命周期 在使用手动创建的 PV 时，底层存储卷的生命周期与 PV 对象的生命周期是分离的。PV 创建后的初始状态是 Available，当 PVC 出现且其要求能被某个 PV 满足时，PV 与 PVC 完成绑定。在此之前 PVC 处于 Pending 状态，绑定完成后 PV 和 PVC 都处于 Bound 状态。在这之后，一个或多个 Pod 可以通过引用 PVC 来使用对应的存储。当所有的 Pods 运行结束后，PVC 对象可以被删除。PVC 对象删除后，PV 的回收策略决定了对 PV 和底层存储的后续操作。若策略为 Delete，则 PV 和 底层存储都会被删除；若策略为 Retain，PV 对象和底层存储都会被保留，PV 的状态变为 Released，无法再次被绑定。底层存储以及其中的文件会继续存在，可以通过创建一个新的指向同样位置的 PV 来再次访问这些文件。 PV 的动态创建前面的章节中，集群管理员必须提前创建 PV 对象，每次 PV 释放后还需要管理员手动删除存储卷中的数据。为了保证集群平稳地运行，管理员需要提前创建数十甚至上百个 PV，还要持续地跟踪可用的 PV 数量，确保其没有耗尽。所有这些手动操作并没有遵循 Kubernetes 自动管理的哲学。 更好的方式是动态创建 PV。集群管理员部署一个 PV provisioner，该 provisioner 可以自动化执行实时的 PV 创建流程。 与静态创建 PV 相反，在动态创建过程中，用户先创建 PVC，然后 provisioner 再从底层存储创建对应的 PV 对象。 StorageClass 对象列出 storage classes123$ kubectl get scNAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGEstandard (default) k8s.io/minikube-hostpath Delete Immediate false 158d 进一步检查 storage class1234567891011121314151617$ kubectl get sc standard -o yamlapiVersion: storage.k8s.io/v1kind: StorageClassmetadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | &#123;"apiVersion":"storage.k8s.io/v1","kind":"StorageClass","metadata":&#123;"annotations":&#123;"storageclass.kubernetes.io/is-default-class":"true"&#125;,"labels":&#123;"addonmanager.kubernetes.io/mode":"EnsureExists"&#125;,"name":"standard"&#125;,"provisioner":"k8s.io/minikube-hostpath"&#125; storageclass.kubernetes.io/is-default-class: "true" creationTimestamp: "2021-11-15T05:36:28Z" labels: addonmanager.kubernetes.io/mode: EnsureExists name: standard resourceVersion: "298" uid: 3d1fb350-928d-4d53-a2eb-558358814839provisioner: k8s.io/minikube-hostpathreclaimPolicy: DeletevolumeBindingMode: Immediate StorageClass 对象代表某种可以被动态创建的存储类型。每一个 StorageClass 都会指定在动态创建 Volume 时需要使用的 provisioner 以及需要传递的参数。由用户来决定每一个 PVC 具体使用那种 StorageClass。 用 default storage class 动态创建 PV可以创建一个 PVC 对象并将其 storageClassName 条目设置为 standard，或者不指定任何 storageClassName，Kubernetes 会自动选择默认的 storage class。12345678910apiVersion: v1kind: PersistentVolumeClaimmetadata: name: quiz-data-defaultspec: resources: requests: storage: 1Gi accessModes: - ReadWriteOnce 创建和查看 PVC：12345kubectl apply -f pvc.quiz-data-default.yamlpersistentvolumeclaim/quiz-data-default created$ kubectl get pvc quiz-data-defaultNAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGEquiz-data-default Bound pvc-eac89776-b02b-49c1-9878-b50b9780bd3c 1Gi RWO standard 47s 对应的 PV 会在 PVC 创建后自动生成和绑定。123$ kubectl get pvNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGEpvc-eac89776-b02b-49c1-9878-b50b9780bd3c 1Gi RWO Delete Bound default/quiz-data-default standard 113s 动态生成 PV 的生命周期 参考资料Kubernetes in Action, Second Edition]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Disk</tag>
        <tag>Storage</tag>
        <tag>Docker</tag>
        <tag>Container</tag>
        <tag>Kubernetes</tag>
        <tag>Volume</tag>
        <tag>Persistent</tag>
        <tag>NFS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Fluent Python 2nd 笔记——Type hints（类型标注）介绍]]></title>
    <url>%2F2022%2F04%2F15%2Ffluent-python-2nd-reading-notes-type-hints%2F</url>
    <content type="text"><![CDATA[PEP 484—Type Hints 在 Python 中引入了显式的类型标注，可以为函数参数、返回值、变量等添加类型提示。主要目的在于帮助开发工具通过静态检查发现代码中的 Bug。 gradual typingPEP 484 引入的是一种 gradual type system（渐进式类型系统），支持同样类型系统的语言还有微软的 TypeScript、Google 的 Dart 等。该系统具有以下特征： 可选的。默认情况下，类型检查器不应该警告没有标注类型的代码。当无法确认某个对象的类型时，假设其为 Any 类型 在运行时不捕获类型错误。Type hints 主要用来帮助类型检查器、linter 和 IDE 输出警告信息，不会在运行时阻止不匹配的类型传递给某个函数 对性能没有提升。理论上讲，类型标注提供的信息能够帮助解释器对生成的字节码进行优化。目前 Python 还没有相关的实现 类型标注在任何层面上都是可选的。简单来说，用户可以选择任何一个自己感兴趣的参数或返回值进行类型标注，不用管其它的。在没有配置 IDE 进行严格检查的时候，不会有任何报错出现。即便用户错误地标注了类型，对程序的运行也不会产生任何影响。最多只是 IDE 会有报错提示。 gradual typing 示例1234567891011# messages.pydef show_count(count, word): if count == 1: return f&apos;1 &#123;word&#125;&apos; count_str = str(count) if count else &apos;no&apos; return f&apos;&#123;count_str&#125; &#123;word&#125;s&apos;print(show_count(1, &apos;dog&apos;))# =&gt; 1 dogprint(show_count(2, &apos;dog&apos;))# =&gt; 2 dogs 安装 mypy 类型检查工具：pip install mypy。 使用 mypy 命令对 messages.py 源代码进行类型检查，没有任何错误：12$ mypy messages.pySuccess: no issues found in 1 source file 只有当加上 --disallow-untyped-defs 选项的时候才会检查出错误（函数缺少类型标注）：123$ mypy --disallow-untyped-defs messages.pymessages.py:1: error: Function is missing a type annotationFound 1 error in 1 file (checked 1 source file) 修改一下检查的严格程度，使用 --disallow-incomplete-defs 选项，此时检查是通过的：12$ mypy --disallow-incomplete-defs messages.pySuccess: no issues found in 1 source file 将函数 show_count 的签名改为 show_count(count, word) -&gt; str，只为返回值添加类型标注，再次进行检查：12$ messages.py:1: error: Function is missing a type annotation for one or more argumentsFound 1 error in 1 file (checked 1 source file) 即 --disallow-incomplete-defs 不会去管完全没有类型标注的函数，而是会确保，只要某个函数添加了类型标注，则其类型标注必须完整应用到该函数的所有参数和返回值。 假如将函数 show_count 的签名改为 show_count(count: int, word: str) -&gt; int，运行类型检查则会报出其他错误（返回值类型不匹配）：1234$ mypy --disallow-incomplete-defs messages.pymessages.py:3: error: Incompatible return value type (got "str", expected "int")messages.py:5: error: Incompatible return value type (got "str", expected "int")Found 2 errors in 1 file (checked 1 source file) 但程序的运行不会受任何影响：123$ python messages.py1 dog2 dogs 即类型标注可以帮助 IDE 等工具对代码进行静态检查，在程序运行前发现可能的语法错误。但并不会对程序的运行时施加任何影响。这就是为什么称之为 Gradual。即不具备任何强制性，可以在需要的时候逐步完善任何感兴趣的变量。但加不加标注，程序该怎么跑还是怎么跑。 使用 None 作为默认值前面的 messages.py 实际上做的事情很简单，就是输出数量和名词。数量为 1 名词用单数，数量大于 1 名词就加 s 变复数。但很多名词并不是直接加 s 就能成为复数形式，比如 child -&gt; children。因此代码可以优化为如下形式：123456789101112def show_count(count: int, singular: str, plural: str = '') -&gt; str: if count == 1: return f'1 &#123;singular&#125;' count_str = str(count) if count else 'no' if not plural: plural = singular + 's' return f'&#123;count_str&#125; &#123;plural&#125;'print(show_count(2, 'dog'))# =&gt; 2 dogsprint(show_count(2, 'child', 'children'))# =&gt; 2 children 上面的代码可以很好的工作。函数中加了一个参数 plural 表示名词的复数形式，默认值是空字符串 &#39;&#39;。但从语义的角度看，默认值用 None 更符合一些。即某个名词要么有特殊的复数形式，要么没有。但这会导致 plural 参数的类型声明不适合使用 str，因为其取值可以是 None，而 None 不属于 str 类型。 把 show_count 函数的签名改为如下形式即可：12from typing import Optionaldef show_count(count: int, singular: str, plural: Optional[str] = None) -&gt; str: 其中 Optional[str] 就表示该类型可以是 str 或者 None。此外，默认值 =None 必须显式地写在声明里，否则 Python 运行时会将 plural 视为必须提供的参数。在类型声明里注明了某个参数是 Optional，并不会真的将其变为可选参数。记住对于运行时而言，类型标注总是会被忽略掉。 Types are defined by supported operations引用 PEP 483 中的定义，类型就是一组值的集合，这些值有一个共同的特点，就是一系列特定的函数能够应用到这些值上。即某种类型支持的一系列操作定义了该类型的特征。 比如下面的 double 函数：12def double(x): return x * 2 其中 x 参数的类型可以是数值类型（int、complex、Fraction、numpy.uint32 等），但也可能是某种序列类型（str、tuple、list、array 等）、N 维数组 numpy.array 甚至任何其他类型，只要该类型实现或继承了 __mul__ 方法且接收 int 作为参数。 但是对于另一个 double 函数：1234from collections import abcdef double(x: abc.Sequence): return x * 2 将 x 参数的类型声明为 abc.Sequence，此时使用 mypy 检查其类型声明会报出错误：123$ mypy double.pydouble.py:4: error: Unsupported operand types for * ("Sequence[Any]" and "int")Found 1 error in 1 file (checked 1 source file) 因为 Sequence 虚拟基类并没有实现或者继承 __mul__ 方法，类型检查器认为 x * 2 是不支持的操作。但在实际运行时，上述代码支持 x 为 str、tuple、list、array 等等实现了 Sequence 的具体类型，运行不会有任何报错。原因在于，运行时会忽略类型声明。且类型检查器只会关心显式声明的对象，比如 abc.Sequence 中有没有 __mul__。 这也是为什么在 Python 中，类型的定义就是其支持的操作。任何作为参数 x 传给 double 函数的对象，Python 运行时都会接受。它可能运行通过，也可能该对象实际并不支持 * 2 操作，报出 TypeError。 在 gradual type system 中，有两种不同的看待类型的角度： Duck typing：Smalltalk 发明的“鸭子类型”，Python、JavaScript、Ruby 等采用此方式。对象有类型，而变量（包括参数）是无类型的。在实践中，对象声明的类型是不重要的，关键在于该对象实际支持的操作。鸭子类型更加灵活，代价就是允许更多的错误出现在运行时。 Nominal typing：C++、Java、C# 等采用此方式。对象和变量都有类型。但对象只存在于运行时，而类型检查器只关心源代码中标记了类型的变量。比如 Duck 是 Bird 的子类，你可以将一个 Duck 对象绑定给标记为 birdie: Bird 的参数。但是在函数体中，类型检查器会认为 birdie.quack() 是非法的（quack() 是 Duck 类中实现的方法）。因为 Bird 类并没有提供 quack() 方法，即便实际的参数 Duck 对象已经实现了 quack()。Nominal typing 在静态检查时强制应用，类型检查器只是读取源代码，并不会执行任何一个代码片段。Nominal typing 更加严格，优势就是可以更早地发现某些 bug，比如在 build 阶段甚至代码刚输入到 IDE 中的时候。 参考下面的例子：12345678910111213141516# birds.pyclass Bird: passclass Duck(Bird): def quack(self): print('Quack')def alert(birdie): birdie.quack()def alert_duck(birdie: Duck) -&gt; None: birdie.quack()def alert_bird(birdie: Bird) -&gt; None: birdie.quack() Duck 是 Bird 的子类；alert 没有类型标注，会被类型检查器忽略；alert_duck 接收一个 Duck 类型的参数；alert_bird 接收 Bird 类型的参数。 用 mypy 检查上述代码会报出一个错误：123$ mypy birds.pybirds.py:15: error: "Bird" has no attribute "quack"Found 1 error in 1 file (checked 1 source file) Bird 类没有 quack() 方法，但函数体中却有对 quack() 方法的调用。 编写如下代码调用前面的函数：1234567# daffy.pyfrom birds import *daffy = Duck()alert(daffy)alert_duck(daffy)alert_bird(daffy) 可以成功运行：1234$ python daffy.pyQuackQuackQuack 还是那句重复了无数遍的话，在运行时，Python 并不关心声明的变量，它使用 duck typing，只关心实际传入的对象是不是支持某个操作。因而某些时候即便静态类型检查报出了错误，代码依旧能成功运行。 但是对于下面的例子，静态检查就显得很有用了。1234567# woody.pyfrom birds import *woody = Bird()alert(woody)alert_duck(woody)alert_bird(woody) 此时运行 woody.py 会报出 AttributeError: &#39;Bird&#39; object has no attribute &#39;quack&#39; 错误。因为实际传入的 woody 对象是 Bird 类的实例，它确实没有 quack() 方法。有了静态检查，就可以在程序运行前发现此类错误。 上面的几个例子表明，duck typing 更灵活更加容易上手，但同时会允许不支持的操作在运行时触发错误；Nominal typing 会在运行时之前检测错误，但有些时候会阻止本可以运行的代码。在实际的环境中，函数有可能非常臃肿，有可能 birdie 参数被传递给了更多函数，birdie 还有可能来自于很长的函数调用链，会使得运行时错误很难被精确定位到。类型检查器则会阻止很多这类错误在运行时发生。 Type hints 中用到的类型Any 类型gradual type system 的基础就是 Any 类型，也被叫做动态类型（dynamic type）。当类型检测器遇到如下未标注类型的代码：12def double(x: abc.Sequence): return x * 2 会将其视为如下形式：1234from typing import Anydef double(x: Any) -&gt; Any: return x * 2 Any 类型支持所有可能的操作，参数 n: Any 可以接受任意类型的值。 简单类型和类简单类型比如 int、float、str、bytes 可以直接用在类型标注中。来自于标准库或者第三方库，以及用户自定义的类也可以作为类型标注的关键字。虚拟基类在类型标注中也比较常用。 同时还要注意一个重要的原则：子类可以用在任何声明需要其父类的地方（Liskov Substitution Principle）。 Optional 和 Union 类型Optional[str] 实际上是 Union[str, None] 类型的简写形式，表示某个值可以是 str 或者 None。在 Python3.10 中，可以用 str | None 代替 Union[str, None]。 下面是一个有可能返回 str 或者 float 类型的函数：1234567from typing import Uniondef parse_token(token: str) -&gt; Union[str, float]: try: return float(token) except ValueError: return token Union 在相互之间不一致的类型中比较有用，比如 Union[str, float]。对于有兼容关系的类型比如 Union[int, float] 就不是很有必要，因为声明为 float 类型的参数也可以接收 int 类型的值。 通用集合类型Python 中的大多数集合类型都是不均匀的。不均匀的意思就是，比如 list 类型的变量中可以同时存放多种不同类型的值。但是，这种做法通常是不够实用的。通常用户将一系列对象保存至某个集合中，这些对象一般至少有一个共同的接口，以便用户稍后用一个函数对所有这些对象进行处理。 Generic types 可以在声明时加上一个类型参数。比如 list 可以通过参数化来控制自身存储的值的类型：12def tokenize(text: str) -&gt; list[str]: return text.upper().split() 在 Python 版本不低于 3.9 时，上述代码表示 tokenize 函数会返回一个列表，列表中的每一项都是 str 类型。 类型标注 stuff: list 和 stuf: list[Any] 是等效的，都表示 stuff 这个列表可以同时包含任意类型的元素。 元组元组作为记录比如需要保存城市、人口和国家的值 (&#39;Shanghai&#39;, 24.28, &#39;China&#39;)，其类型标注可以写作 tuple[str, float, str]。 有命名字段的元组建议使用 typing.NamedTuple：1234567891011121314from typing import NamedTupleclass Coordinate(NamedTuple): lat: float lon: floatdef display(lat_lon: tuple[float, float]) -&gt; None: lat, lon = lat_lon ns = 'N' if lat &gt;= 0 else 'S' ew = 'E' if lon &gt;= 0 else 'W' print(f'&#123;abs(lat):0.1f&#125;°&#123;ns&#125;, &#123;abs(lon):0.1f&#125;°&#123;ew&#125;')display(Coordinate(120.20, 30.26))# =&gt; 120.2°N, 30.2°E NamedTuple 与 tuple[float, float] 兼容，因而 Coordinate 对象可以直接传递给 display 函数。 元组作为不可变序列当需要将元组作为不可变列表使用时，类型标注需要指定一个单一的类型，后面跟上逗号和 ...。比如 tuple[int, ...] 表示一个元组包含未知数量的 int 类型的元素。stuff: tuple[Any, ...] 等同于 stuff: tuple，表示 stuff 对象可以包含未指定数量的任意类型的元素。 Generic mappingsGeneric mapping 类型使用 MappingType[KeyType, ValueType] 形式的标注。比如内置的 dict 和其他 collections/collections.abc 库中的 Map 类型。 Abstract Base Class理想情况下，一个函数应该接收虚拟类型的参数，不使用某个具体的类型。比如下面的函数签名：12from collections.abc import Mappingdef name2hex(name: str, color_map: Mapping[str, int]) -&gt; str: 使用 abc.Mapping 作为函数参数的类型标注，能够允许调用者传入 dict、defaultdict.ChainMap、UserDict 子类或者任意 Mapping 的子类型作为参数。 相反的，使用下面的函数签名：1def name2hex(name: str, color_map: dict[str, int]) -&gt; str: 会使得 color_map 参数必须接收 dict 或者 defaultDict、OrderedDict 等 dict 的子类型。collections.UserDict 的子类就无法通过 color_map 的类型检查。因为 UserDict 并不是 dict 类型的子类，它俩是兄弟关系，都是 abc.MutableMapping 的子类。因此，在实践中最好使用 abc.Mapping 或者 abc.MutableMapping 作为参数的类型标注。 有个法则叫做 Postel’s law，也被称为鲁棒性原则。简单来说就是对发送的内容保持谨慎，对接收的内容保持自由。 拿列表举例来说，在标注函数的返回值类型时，最好使用 list[str] 这种具体的类型；在标注函数的参数时，则使用 Sequence 或 Iterable 这类抽象的集合类型。 Iterable12345678910111213from collections.abc import IterableFromTo = tuple[str, str]def zip_replace(text: str, changes: Iterable[FromTo]) -&gt; str: for from_, to in changes: text = text.replace(from_, to) return textl33t = [('a', '4'), ('e', '3'), ('i', '1'), ('o', '0')]text = 'mad skilled noob powned leet'print(zip_replace(text, l33t))# =&gt; m4d sk1ll3d n00b p0wn3d l33t 其中 FromTo 是 type alias。 参数化通用类型与 TypeVar参数化通用类型是一种通用类型，比如 list[T] 中的 T 可以绑定任意指定类型，但是之后再次出现的 T 则会表示同样的类型。 参考下面的 sample.py 代码：123456789101112from collections.abc import Sequencefrom random import shufflefrom typing import TypeVarT = TypeVar('T')def sample(population: Sequence[T], size: int) -&gt; list[T]: if size &lt; 1: raise ValueError('size must be &gt;= 1') result = list(population) shuffle(result) return result[:size] 假如传给 sample 函数的参数类型是 tuple[int, ...]，该参数与 Sequence[int] 通用，因此类型参数 T 就代表 int，从而返回值类型变成 list[int]。假如传入的参数类型是 str，与 Sequence[str] 通用，则 T 代表 str，因而返回值类型变成 list[str]。 Restricted TypeVar12345from decimal import Decimalfrom fractions import Fractionfrom typing import TypeVarNumberT = TypeVar('NumberT', float, Decimal, Fraction) 表示类型参数 T 只能是声明中提到的有限的几个类型之一。 Bounded TypeVar1234from collections.abc import Hashablefrom typing import TypeVarHashableT = TypeVar('HashableT', bound=Hashable) 表示类型参数 T 只能是 Hashable 类型或者其子类型之一。 Static ProtocolsProtocol 类型与 Go 中的接口很相似。它的定义中会指定一个或多个方法，类型检查器则会确认对应的类型是否实现了这些方法。 比如下面的例子：123456789101112131415161718192021from collections.abc import Iterablefrom typing import TypeVar, Protocol, Anyclass SupportLessThan(Protocol): def __lt__(self, other: Any) -&gt; bool: ...LT = TypeVar('LT', bound=SupportLessThan)def top(series: Iterable[LT], length: int) -&gt; list[LT]: ordered = sorted(series, reverse=True) return ordered[:length]print(top([4, 1, 5, 2, 6, 7, 3], 3))# =&gt; [7, 6, 5]l = 'mango pear apple kiwi banana'.split()print(top(l, 3))# =&gt; ['pear', 'mango', 'kiwi']l2 = [(len(s), s) for s in l]print(top(l2, 3))# =&gt; [(6, 'banana'), (5, 'mango'), (5, 'apple')] 如果 top 函数中 series 参数的类型标注是 Iterable[T]，没有任何其他限制，意味着该类型参数 T 可以是任意类型。但将 Iterable[Any] 传给函数体中的 sorted 函数，并不总是成立，必须确保 Iterable[Any] 是可以被直接排序的类型。因而需要先创建一个 SupportLessThan protocol 指定 __lt__ 方法，再用该 protocol 来绑定类型参数 LT，从而限制 series 参数必须为可迭代对象，且其中的元素都实现了 __lt__ 方法，使得传入的 series 参数支持被 sorted 直接排序。 当类型 T 实现了 protocol P 中定义的所有方法时，则说明该类型 T 与 protocol P 通用。 CallableCallable 主要用于标注高阶函数中作为参数或者返回值的函数对象。其格式为 Callable[[ParamType1, ParamType2], ReturnType]。 参考资料Fluent Python, 2nd Edition]]></content>
      <tags>
        <tag>Python</tag>
        <tag>Program</tag>
        <tag>Class</tag>
        <tag>Advanced</tag>
        <tag>Type</tag>
        <tag>IDE</tag>
        <tag>Generic</tag>
        <tag>Tuple</tag>
        <tag>List</tag>
        <tag>Mapping</tag>
        <tag>DuckTyping</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kubernetes in Action 笔记 —— 通过 Services 对象暴露 Pod 中的服务]]></title>
    <url>%2F2022%2F04%2F12%2Fkubernetes-in-action-reading-notes-exposing-pods-with-services%2F</url>
    <content type="text"><![CDATA[不同于只运行某个提供特定服务的单一 Pod，现在人们通常会以副本的形式部署多个 Pod 实例，以便工作负载能够均匀地分发到不同的集群节点上。这也意味着同一个 Pod 的所有副本都提供相同的服务，且能够通过一个单一的地址访问。Kubernetes 中的 Services 对象就负责实现这部分功能。 Pods 间如何通信 每个 Pod 都拥有自己的网络接口和 IP 地址。集群中的所有 Pod 通过一个私有的 Flat network 相互通信，该 Flat network 实际上是一个定义在实体网络之上的虚拟网络层。Pod 中的容器可以通过这个虚拟网络层传输数据，无需进行 NAT 转换，就像是局域网中接入到同一个交换机上的计算机一样。对于应用来说，Node 之间实际的网络拓扑是不重要的。 为什么需要 Service如果某个 Pod 中的应用需要连接其他 Pod 中的另一个应用，则它需要知道目标 Pod 的访问地址，这是显而易见的。实际上实现起来要复杂的多： Pods 是有生命周期的。一个 Pod 可以在任意时间被销毁和替代（IP 地址会变） Pod 只有在分配给某个 Node 后才获取到 IP 地址，无法提前知道 在水平扩展中，多个 Pod 副本提供同样的服务，每个副本都有自己的 IP 地址。当另一个 Pod 访问所有这些副本时，就需要能够用一个单一的 IP 或 DNS 名称连接到负载均衡器，再通过负载均衡器在所有的副本间分担工作负载 Service 介绍Kubernetes Service 对象可以为一系列提供同一服务的 Pod 集合，绑定一个单一、稳定的访问点。在 Service 的生命周期里，其 IP 地址稳定不变。客户端通过该 IP 地址创建网络连接，这些请求之后再被转发给后端提供服务的 Pod。简单来说，Service 就是放置在 Pods 前面的负载均衡器。 Pod 和 Service 如何组合在一起Services 通过 label 和 label selector 机制找到对应的 Pods。 创建和更新 ServicePS：作者的示例代码可以从其 Github kubernetes-in-action-2nd-edition 处下载，Service 部分的代码位于 Chapter11。在创建 Service 之前，可以先进入到 Chapter11 路径下，运行 kubectl apply -f SETUP/ --recursive 命令，创建需要的 Pods。1234567$ kubectl get poNAME READY STATUS RESTARTS AGEquiz 2/2 Running 0 33mquote-001 2/2 Running 0 33mquote-002 2/2 Running 0 33mquote-003 2/2 Running 0 33mquote-canary 2/2 Running 0 33m Kubernetes 支持如下几种 Service 类型：ClusterIP、NodePort、LoadBalancer 和 ExternalName。ClusterIP 是默认的类型，仅用于集群内部通信。 通过 YAML 清单文件创建 Servicequote Service 最小版本的清单文件如下：12345678910111213apiVersion: v1kind: Servicemetadata: name: quotespec: type: ClusterIP selector: app: quote ports: - name: http port: 80 targetPort: 80 protocol: TCP 通过 kubectl expose 命令创建 Servicekubectl expose pod quiz --name quiz 获取 Services 列表1234$ kubectl get svc -o wideNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTORquiz ClusterIP 10.106.164.155 &lt;none&gt; 8080/TCP 45s app=quiz,rel=stablequote ClusterIP 10.102.134.27 &lt;none&gt; 80/TCP 62s app=quote 修改 Service 的 label selectorkubectl set selector service quiz app=quiz 修改 Service 暴露的端口可以运行 kubectl edit svc quiz 命令编辑清单文件，将 port 字段修改为 80，保存退出即可。 访问集群内部的 Services前面创建的 ClusterIP 类型的 Service 只支持集群内部访问，可以 ssh 到任意一个 Node 或者 Pod 上来测试其连通性。 从 Pods 连接 Services123456789101112131415161718192021222324$ kubectl get poNAME READY STATUS RESTARTS AGEquiz 2/2 Running 2 (26h ago) 27hquote-001 2/2 Running 2 (26h ago) 27hquote-002 2/2 Running 2 (26h ago) 27hquote-003 2/2 Running 2 (26h ago) 27hquote-canary 2/2 Running 2 (26h ago) 27h$ kubectl get svcNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEquiz ClusterIP 10.106.164.155 &lt;none&gt; 80/TCP 26hquote ClusterIP 10.102.134.27 &lt;none&gt; 80/TCP 26h$ kubectl exec -it quote-001 -c nginx -- sh/ # curl 10.106.164.155This is the quiz service running in pod quiz/ # curl 10.102.134.27This is the quote service running in pod quote-002 on node minikube/ # curl 10.102.134.27This is the quote service running in pod quote-003 on node minikube/ # curl 10.102.134.27This is the quote service running in pod quote-002 on node minikube/ # curl 10.102.134.27This is the quote service running in pod quote-canary on node minikube Services 的 DNS 解析Kubernetes 有一个内部的 DNS 服务器组件，供集群中所有的 Pods 使用。允许通过 Service 的名称解析其 ClusterIP 地址。12345678910/ # curl quizThis is the quiz service running in pod quiz/ # curl quoteThis is the quote service running in pod quote-003 on node minikube/ # curl quoteThis is the quote service running in pod quote-canary on node minikube/ # curl quoteThis is the quote service running in pod quote-003 on node minikube/ # curl quoteThis is the quote service running in pod quote-003 on node minikube 在 Pod 中使用 Services可以参考如下 YAML 清单文件：1234567891011121314151617181920apiVersion: v1kind: Podmetadata: name: kiada-003 labels: app: kiada rel: stablespec: containers: - name: kiada image: luksa/kiada:0.5 imagePullPolicy: Always env: - name: QUOTE_URL value: http://quote/quote - name: QUIZ_URL value: http://quiz ports: - name: http containerPort: 8080 完整的源代码参考 Chapter11 路径下的 kiada-stable-and-canary.yaml 文件。运行 kubectl apply -f kiada-stable-and-canary.yaml 命令应用该清单文件。所有容器成功运行后，运行 kubectl port-forward 命令启用本地端口转发：12345$ kubectl port-forward kiada-001 8080 8443Forwarding from 127.0.0.1:8080 -&gt; 8080Forwarding from [::1]:8080 -&gt; 8080Forwarding from 127.0.0.1:8443 -&gt; 8443Forwarding from [::1]:8443 -&gt; 8443 此时打开浏览器访问 http://localhost:8080 或 https://localhost:8443 即可进入应用页面： 向集群外部暴露服务为了令某个 Service 能够被外部世界访问，可以采取如下几种措施： 为 Node 分配一个额外的 IP，并将其设置为 Service 的 externalIP 将 Service 的类型配置为 NodePort，通过 Node 端口访问该服务 创建 LoadBalancer 类型的 Service 对象 Ingress 对象 其中第一种方式会为 Service 对象的 spec.externalIPs 字段指定一个额外的 IP，这种方式并不常用。更常见的方式是将 Service 类型设置为 NodePort。Kubernetes 会令该 Service 能够通过所有 Node 节点上的特定端口访问。通常还需要用户配置一个外部的负载均衡器负责将客户端流量转发到这些 Node 端口。不同于 NodePort 一般需要手动配置负载均衡，Kubernetes 还支持自动完成类似搭建过程，只需要用户指定 Service 的类型为 LoadBalancer。但并不是所有环境下的集群都支持这样做，因为负载均衡的创建依赖特定的云服务供应商。最后一种方式则是通过 Ingress 对象实现服务对外部的开放，其具体实现机制依赖于底层的 ingress 控制器。 NodePort Service 同 ClusterIP 类似，NodePort Service 支持通过内部的 cluster IP 访问。除此之外，它还可以通过任意一个 Node 的特定端口来访问。最终由哪一个 Node 为客户端提供连接是不重要的，因为每一个 Node 都总是会将客户端请求转发给 Service 背后的任意 Pod，不管这个 Pod 是否运行在同一个 Node 上。即 Node A 的端口接收到客户端请求，它可能会将该请求转发给 Node A 上运行的 Pod，也可能转发给 Node B 上运行的 Pod。 创建 NodePort Service1234567891011121314151617apiVersion: v1kind: Servicemetadata: name: kiadaspec: type: NodePort selector: app: kiada ports: - name: http port: 80 nodePort: 30080 targetPort: 8080 - name: https port: 443 nodePort: 30443 targetPort: 8443 上面的清单文件中共有 6 个 port，可以参考如下截图理解各个 port 的不同含义： 查看 NodePort Service12345$ kubectl get svcNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkiada NodePort 10.103.96.75 &lt;none&gt; 80:30080/TCP,443:30443/TCP 6squiz ClusterIP 10.106.164.155 &lt;none&gt; 80/TCP 28hquote ClusterIP 10.102.134.27 &lt;none&gt; 80/TCP 28h 访问 NodePort Service访问 NodePort Service 不仅仅需要知道端口号，还必须先获取到 Node 的 IP 地址。可以使用 kubectl get nodes -o wide 命令查看 Node 的 IP 地址（INTERNAL-IP 和 EXTERNAL-IP）。123$ kubectl get nodes -o wideNAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIMEminikube Ready control-plane,master 144d v1.22.3 192.168.49.2 &lt;none&gt; Ubuntu 20.04.2 LTS 5.4.72-microsoft-standard-WSL2 docker://20.10.8 此时在集群内部，则可以使用以下几种 IP 端口组合来访问 Kiada 应用： 10.103.96.75:80：cluster IP 和内部端口 192.168.49.2:30080：Node IP 和 Node 端口 因为是 Minikube 单机模拟的集群环境，只有一个 Node 可以使用。123456789101112131415$ curl 192.168.49.2:30080KUBERNETES IN ACTION DEMO APPLICATION v0.5==== TIP OF THE MINUTEYou can use the `jq` tool to print out the value of a pod’s `phase` field like this: `kubectl get po kiada -o json | jq .status.phase`.==== POP QUIZWhich of the following statements is correct?0) When the readiness probe fails, the container is restarted.1) When the liveness probe fails, the container is restarted.2) Containers without a readiness probe are never restarted.3) Containers without a liveness probe are never restarted.Submit your answer to /question/6/answers/&lt;index of answer&gt; using the POST method. LoadBalancer ServiceLoadBalancer 类型的 Service 实际上是 NodePort 类型的扩展。其基本配置如下：1234567891011121314151617apiVersion: v1kind: Servicemetadata: name: kiadaspec: type: LoadBalancer selector: app: kiada ports: - name: http port: 80 nodePort: 30080 targetPort: 8080 - name: https port: 443 nodePort: 30443 targetPort: 8443 与前面 NodePort Service 的配置几乎完全一致，只是服务类型由 NodePort 改为了 LoadBalancer。 external traffic policy任何通过 NodePort 的外部客户端连接，不管是直接访问 Node 端口还是通过 LoadBalancer 间接访问 Node 端口，客户端连接都有可能会被转发给另一个 Node 上的 Pod。即接收客户端连接的 Node 和执行任务的 Pod 所在的 Node 可能不是同一个。在这种情况下，就意味着网络路径上多了一次跳转。 此外，在上述情况下，转发连接时还需要将 source IP 替换成一开始接收客户端连接的 Node 的 IP。这会导致 Pod 中运行的应用无法看到此网络连接的初始来源，即无法在其 access log 中记录真实的客户端地址。 Local external traffic policy 的优劣为了解决上述问题，可以选择阻止 Node 将客户端连接转发给运行在其他 Node 上的 Pod。即访问 Node 的外部连接最终只会被同一个 Node 上的 Pod 接收到。具体方法是将 Service 对象 spec 字段下的 externalTrafficPolicy 字段改为 Local。 但上述配置同时会引发其他问题。第一，如果接收到外部连接的 Node 上并没有 Pods 在运行，则该连接会卡住。因此必须确保负载均衡器只会将外部连接转发给有 Pod 运行的 Node，可以通过令负载均衡器持续检测 healthCheckNodePort 来实现。第二，external traffic policy 设置为 Local 会导致 Pods 间的负载不够均衡。LoadBalancer 均匀地分发外部连接给 Nodes，Node 再将连接转发到自身运行的 Pods 上。但是每个 Node 实际上运行着不同数量的 Pods，不能跨 Node 转发就意味着，在每个 Node 接收等量连接的前提下，有些 Node 上的 Pods 较少，则这些 Pods 平均要承担的负载就更多。 externalTrafficPolicy 设置为 Cluster 与 Local 的区别可以参考下图： 参考资料Kubernetes in Action, Second Edition]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Docker</tag>
        <tag>Container</tag>
        <tag>Kubernetes</tag>
        <tag>Pod</tag>
        <tag>Service</tag>
        <tag>Cloud</tag>
        <tag>LoadBalancer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 3.10 新特性 —— 结构化模式匹配（Structural Pattern Match）详解]]></title>
    <url>%2F2022%2F04%2F02%2Fpython-3-10-new-features-structural-pattern-match%2F</url>
    <content type="text"><![CDATA[switch-case众所周知，Python 中是没有类似 switch-case 结构的语法的。但是自从 3.10 版本发布以后，这种说法就已经成为历史了。 Java 中的 switch 语句类似如下形式：1234567891011121314151617181920public class Main &#123; public static void main(String[] args) &#123; var option = 3; switch (option) &#123; case 1: System.out.println("You have chosen option 1."); break; case 2: System.out.println("You have chosen option 2."); break; case 3: System.out.println("You have chosen option 3."); break; default: System.out.println("Sorry you chose an invalid option."); break; &#125; &#125;&#125; 看起来就像是另一种形式的 if-else 语句。以某个变量值作为判断条件，根据不同的判断结果执行对应的语句，最终形成一种流程上的分支结构。这也许是 Python 不去实现它的依据（借口）之一？都已经有了 if-else 可以足够轻松地完成同样的事情。 There should be one– and preferably only one –obvious way to do it. 字典映射在模式匹配出现之前，对于分支相当多的判断语句，Python 建议通过字典映射（dictionary mapping）来实现。12345678def function_map(option): return &#123; 1: lambda : print('You have chose option 1.'), 2: lambda : print('You have chose option 2.'), 3: lambda : print('You have chose option 3.') &#125;.get(option, lambda: print('Sorry you chose an invalid option.'))function_map(3)() 借助字典这种数据结构，以匹配条件作为键值，一一对应匹配后需要执行的命令。将 switch 结构中的条件判断转化为对字典键值的搜索匹配。 Pattern Match用模式匹配实现 switch-case 语法，从形式上看就直观了很多：1234567891011option = 3match option: case 1: print("You have chosen option 1.") case 2: print("You have chosen option 2.") case 3: print("You have chosen option 3.") case _: print("You chose an invalid option.") 实际上模式匹配不只有创建流程上的分支结构这一种功能，它的作用可以比单纯的 switch-case 语法强大的多。 模式匹配可以算是一种历史悠久的编程技巧了，经常可以在函数式编程语言中见到。比较有代表性的语言比如 Haskell。相对年轻的语言比如 Rust 也引入了功能强大的模式匹配语法。模式匹配其实可以拆成两部分来理解：匹配和模式。匹配部分可以发挥类似于 if-else 和 switch 等条件判断语句的作用，生成一种分支结构；模式则定义了特定的规则即匹配的具体条件。更进一步的，还会对匹配到的对象进行解构（destructuring）或者说拆包（unpacking）。 以不同于模式匹配的正则表达式来说：12345678import resource_str = 'cats are cute'pattern = re.compile('(.*) are (.*)')matched = re.match(pattern, source_str)print(matched.groups())# =&gt; ('cats', 'cute') 正则表达式规则中的 (.*) 分别匹配到源字符串中的 cats 和 cute，与此同时，还把这两个匹配项提取了出来。 而模式匹配相对来说，则不仅仅能够匹配和提取 cats、cute 等字符串类型，还能够匹配更复杂类型的对象，同时对匹配到的对象进行拆包操作。 比如下面的代码就对类型为元组的对象进行了匹配和拆包：12345678910111213141516171819def match_person(person): match person: case (name, 'M', age): print(f'He is &#123;name&#125;, aged &#123;age&#125;.') case (name, 'F', age): print(f'She is &#123;name&#125;, aged &#123;age&#125;.') case (name,): print(f'We only know the name is &#123;name&#125;, others are secrets.')person_A = ('John', 'M', 20)person_B = ('Jenny', 'F', 18)person_C = ('Lily',)match_person(person_A)# =&gt; He is John, aged 20.match_person(person_B)# =&gt; She is Jenny, aged 18.match_person(person_C)# =&gt; We only know the name is Lily, others are secrets. match 关键字后面被匹配的对象，支持很多种复杂的类型。对应的 case 关键字后面的模式也同样灵活： 列表或元组，如 (name, 18) 字典，如 {&quot;name&quot;: name, &quot;age&quot;: 18} 使用 * 匹配列表中的剩余部分，如 [first, *rest] 使用 ** 匹配字典中的剩余部分 匹配对象和对象的属性 在模式中可以使用 | 逻辑或操作 模式匹配应用实例创建一个 Python 程序，模拟交互式命令行的行为。 匹配字符串123456789101112131415161718def run_command(command: str) -&gt; None: match command: case "quit": print("Quitting the program.") quit() case "reset": print("Resetting the system.") case other: print(f"Unknown command: &#123;other!r&#125;.")def main() -&gt; None: while True: command = input("$ ") run_command(command)if __name__ == '__main__': main() 运行效果如下：123456$ resetResetting the system.$ abcdefgUnknown command: 'abcdefg'.$ quitQuitting the program. 匹配列表123456789101112131415161718192021def run_command(command: str): match command.split(): case ["load", filename]: print(f"Loading file: &#123;filename&#125;.") case ["save", filename]: print(f"Saving to file: &#123;filename&#125;.") case ["quit" | "exit" | "bye"]: print("Quitting the program.") quit() case _: print(f"Unkown command: &#123;command!r&#125;.")def main() -&gt; None: while True: command = input("$ ") run_command(command)if __name__ == '__main__': main() 运行效果：12345678$ load input_data.txtLoading file: input_data.txt.$ save output_data.txtSaving to file: output_data.txt.$ load input_data.txt output_data.txtUnkown command: 'load input_data.txt output_data.txt'.$ byeQuitting the program. 匹配对象1234567891011121314151617181920212223242526272829303132333435from dataclasses import dataclassfrom typing import Listimport shlex@dataclassclass Command: command: str arguments: List[str]def run_command(command: Command): match command: case Command(command="load", arguments=[filename]): print(f"Loading file: &#123;filename&#125;.") case Command(command="save", arguments=[filename]): print(f"Saving to file: &#123;filename&#125;.") case Command(command="quit" | "exit" | "bye", arguments=["--force" | "-f"]): print("Sending SIGTERM and quitting the program.") quit() case Command(command="quit" | "exit" | "bye"): print("Quitting the program.") quit() case _: print(f"Unknown command: &#123;command!r&#125;.")def main() -&gt; None: while True: command, *arguments = shlex.split(input("$ ")) run_command(Command(command, arguments))if __name__ == '__main__': main() 运行效果：12345678$ not a commandUnknown command: Command(command='not', arguments=['a', 'command']).$ load input_data.txtLoading file: input_data.txt.$ save output_data.txtSaving to file: output_data.txt.$ exit -fSending SIGTERM and quitting the program. 需要注意的是，模式匹配中各条 case 语句之间的前后顺序是至关重要的。通常来说，更“具体”更“精确”一些的规则要放在相对靠前的位置。假如 case Command(command=&quot;quit&quot; | &quot;exit&quot; | &quot;bye&quot;) 为规则 1，Command(command=&quot;quit&quot; | &quot;exit&quot; | &quot;bye&quot;, arguments=[&quot;--force&quot; | &quot;-f&quot;]) 为规则 2。则更具体一些的规则 2 要放在规则 1 前面。因为模式匹配是从上到下依次检查每一个 case 语句，若遇到匹配的模式，则执行对应的命令。不再继续向下匹配。由于严格符合规则 2 的对象一定也符合规则 1，当规则 1 位于规则 2 前面时，规则 2 永远也没有被匹配的机会。 可以想象成一种逐渐“滑落”的过程。比如写一个计算成绩等级的函数，可以这样实现：1234567def grade(score): if score &gt;= 90: return 'A' elif score &gt;= 70: return 'B' elif score &gt;= 60: return 'C' 如果上面 if-else 的条件反着排，那就，所有人都是 C 了。。。 参考资料A Closer Look At Structural Pattern Matching // New In Python 3.10!]]></content>
      <tags>
        <tag>Python</tag>
        <tag>Program</tag>
        <tag>Functional</tag>
        <tag>Pattern</tag>
        <tag>Match</tag>
        <tag>Switch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kubernetes in Action 笔记 —— 使用 ConfigMaps、Secrets 和 Downward API 配置应用]]></title>
    <url>%2F2022%2F01%2F15%2Fkubernetes-in-action-reading-notes-config-maps-secrets-and-downward-api%2F</url>
    <content type="text"><![CDATA[设置命令、参数和环境变量同普通的应用一样，容器化应用也可以通过命令行参数、环境变量、文件等完成配置。比如容器启动时执行的命令可以由 Dockerfile 中的 ENTRYPOINT 选项指定；命令参数通常由 CMD 选项指定；环境变量则可以使用 ENV 选项指定；假如应用通过文件进行配置，则这些配置文件可以借助 COPY 选项复制到容器镜像中。 下面是一个名为 kiada 的示例应用。Dockerfile 配置文件：12345678FROM node:12COPY app.js /app.jsCOPY html /htmlENV INITIAL_STATUS_MESSAGE="This is the default status message"ENTRYPOINT ["node", "app.js"]CMD ["--listen-port", "8080"] 其他文件如 html、app.js 可以从作者的 Github 处获取。同时构建好的镜像也被作者放到了 Dockerhub 的 luksa/kiada:0.4 位置。 借助上述 Dockerfile，应用的监听端口可以通过 --listen-port 命令参数配置；同时应用会读取环境变量 INITIAL_STATUS_MESSAGE 来获取初始状态信息。 将配置硬编码进容器的镜像，事实上和将配置硬编码到应用的源代码中一样，都不是理想的情况。因为每当应用需要修改配置，都必须再重新 build 一遍镜像。此外，还必须避免在镜像中包含敏感的配置信息，比如认证数据或密钥等。将上述配置文件放置在 Volume 中并挂载到容器上，相对而言更安全一点。 配置命令和参数在创建镜像时，容器启动时执行的命令和参数分别由 Dockerfile 中的 ENTRYPOINT 和 CMD 选项指定。Kubernetes 提供了两个同样功能的字段：commands 和 args。假如在 Pod 的清单文件中指定了 commands 和 args，或者其中任何一个字段，则 Dockerfile 中对应的 ENTRYPOINT 和 CMD 配置会被覆盖。 设置命令假如需要在运行 Kiada 应用时启用 CPU 和 heap 优化，对于 Node.js 而言，可以在执行时传入 --cpu-prof 和 --heap-prof 参数。相对于修改 Dockerfile 重新 build 镜像，其实可以直接修改 Pod 的清单文件。1234567891011121314apiVersion: v1kind: Podspec: containers: - name: kiada image: luksa/kiada:0.4 command: - node - --cpu-prof - --heap-prof - app.js ports: - name: http containerPort: 8080 设置命令参数比如需要将 Dockerfile 中的命令参数 --listen-port 8080 改为 --listen-port 9090，可以使用如下配置的 Pod 清单文件：1234567891011121314apiVersion: v1kind: Podmetadata: name: kiadaspec: containers: - name: kiada image: luksa/kiada args: - --listen-port - "9090" ports: - containerPort: 9090 name: http 容器在创建时会自动组合 Dockerfile 中的 ENTRYPOINT 和 Pod 清单文件中的 args。 设置环境变量为环境变量设置字面量值Kiada 应用在运行时会显示 Pod 的名字，该名称由应用源代码从环境变量 POD_NAME 读取。此外还可以通过修改环境变量 INITIAL_STATUS_MESSAGE 来更改状态信息。 为了修改上述环境变量，可以向 Dockerfile 中添加 ENV 选项并重新构建镜像，但更快速的方式时向 Pod 的清单文件中添加 env 选项。示例清单文件如下：12345678910111213141516apiVersion: v1kind: Podmetadata: name: kiadaspec: containers: - name: kiada image: luksa/kiada:0.4 env: - name: POD_NAME value: kiada - name: INITIAL_STATUS_MESSAGE value: This status message is set in the pod spec. ports: - name: http containerPort: 8080 应用上述 Pod 清单文件并查看效果：123456789101112131415161718$ kubectl apply -f kiada-env.ymlpod/kiada unchanged$ kubectl exec kiada -- envPATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/binHOSTNAME=kiadaPOD_NAME=kiadaINITIAL_STATUS_MESSAGE=This status message is set in the pod spec.KUBERNETES_SERVICE_PORT=443KUBERNETES_SERVICE_PORT_HTTPS=443KUBERNETES_PORT=tcp://10.96.0.1:443KUBERNETES_PORT_443_TCP=tcp://10.96.0.1:443KUBERNETES_PORT_443_TCP_PROTO=tcpKUBERNETES_PORT_443_TCP_PORT=443KUBERNETES_PORT_443_TCP_ADDR=10.96.0.1KUBERNETES_SERVICE_HOST=10.96.0.1NODE_VERSION=16.11.1YARN_VERSION=1.22.15HOME=/root 使用变量引用除了给环境变量设置一个固定值外，还可以采用 $(VAR_NAME) 格式引用其他变量。如：12345678910111213141516apiVersion: v1kind: Podmetadata: name: kiadaspec: containers: - name: kiada image: luksa/kiada:0.4 env: - name: POD_NAME value: kiada - name: INITIAL_STATUS_MESSAGE value: My name is $(POD_NAME). I run NodeJS version $(NODE_VERSION). ports: - name: http containerPort: 8080 引用的变量中，POD_NAME 是在同一个 Pod 清单文件中定义的，NODE_VERSION 是在容器镜像中定义的。最终的状态信息会显示为 My name is kiada. I run NodeJS version $(NODE_VERSION).，因为此方式只支持引用同一个 Pod 清单文件中定义的变量，且该变量必须在引用位置之前定义。 在命令和参数中使用变量引用还可以在命令和参数中引用清单文件中定义的变量。1234567891011121314151617apiVersion: v1kind: Podmetadata: name: kiadaspec: containers: - name: kiada image: luksa/kiada:0.4 args: - --listen-port - $(LISTEN_PORT) env: - name: LISTEN_PORT value: "9090" ports: - name: http containerPort: 9090 使用 config map 将配置与 Pod 解耦在前面的章节中，可以将应用的配置硬编码进 Pod 的 yaml 清单文件中。这种方式比将配置硬编码到容器镜像中要好很多，即不需要每次修改配置后都必须重新构建镜像。但这种方式也有一定的缺陷，它意味着比如针对不同环境的部署（development、staging、production 等），可能需要多个不同版本的清单文件。为了在多个环境下重复利用同一个清单文件，最好是将配置与 Pod 的清单文件解耦。 ConfigMaps 介绍ConfigMap 是一种包含一系列键值对的 Kubernetes API 对象。其中的值可以是短字符串，也可以是一大段结构化的文本。Pod 可以引用一个或多个 ConfigMap 中的值。一个 Pod 可以引用多个 ConfigMaps，多个 Pods 可以使用同一个 ConfigMap。 如下图所示，ConfigMap 中的键值对通常作为环境变量传递给 Pod，或者通过 ConfigMap Volume 作为文件挂载到容器的文件系统中。 将配置保存在一个独立的 ConfigMap 对象中，而不是直接保存在 Pod 里。这使得在不同的环境中能够部署同一个 Pod 清单文件，与此同时应用不同的配置（引用不同的 ConfigMap 对象）。 创建 ConfigMap 对象从 YAML 文件创建 ConfigMapcm.kiada-config.yml：123456apiVersion: v1kind: ConfigMapmetadata: name: kiada-configdata: status-message: This status message is set in the kiada-config config map 运行 kubectl apply -f cm.kiada-config.yml 命令创建清单文件中定义的 ConfigMap 对象。 查看 ConfigMap可以使用 kubectl get cm 命令列出 ConfigMap 对象：1234$ kubectl get cmNAME DATA AGEkiada-config 1 6skube-root-ca.crt 1 55d 可以使用如下命令查看 ConfigMap 的详细信息：1234567891011121314$ kubectl get cm kiada-config -o yamlapiVersion: v1data: status-message: This status message is set in the kiada-config config mapkind: ConfigMapmetadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | &#123;"apiVersion":"v1","data":&#123;"status-message":"This status message is set in the kiada-config config map"&#125;,"kind":"ConfigMap","metadata":&#123;"annotations":&#123;&#125;,"name":"kiada-config","namespace":"default"&#125;&#125; creationTimestamp: "2022-01-10T05:11:35Z" name: kiada-config namespace: default resourceVersion: "99447" uid: e39e2676-2183-4582-9922-eac963b81093 若只想精确地查找 ConfigMap 中的配置项，可以使用如下命令：123456$ kubectl get cm kiada-config -o json | jq .data&#123; "status-message": "This status message is set in the kiada-config config map"&#125;$ kubectl get cm kiada-config -o json | jq '.data["status-message"]'"This status message is set in the kiada-config config map" 将 ConfigMap 中的值注入到环境变量pod.kiada.env-valueFrom.yml123456789101112131415161718apiVersion: v1kind: Podmetadata: name: kiadaspec: containers: - name: kiada image: luksa/kiada:0.4 env: - name: INITIAL_STATUS_MESSAGE valueFrom: configMapKeyRef: name: kiada-config key: status-message optional: true ports: - containerPort: 8080 name: http 创建 Pod 并查看环境变量：1234567$ kubectl apply -f pod.kiada.env-valueFrom.ymlpod/kiada created$ kubectl exec kiada -- envPATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/binHOSTNAME=kiadaINITIAL_STATUS_MESSAGE=This status message is set in the kiada-config config map... 注入整个 ConfigMap容器定义中的 env 字段接收的是由值组成的列表，因此可以同时设置多个环境变量。实际上可以使用 envFrom 注入整个 ConfigMap 中定义的多个值，而无需像 valueFrom 那样分别指定每一个键。唯一需要注意的是，ConfigMap 中的键必须与实际的环境变量名称保持一致。即之前使用的键 status-message 必须改成 INITIAL_STATUS_MESSAGE。 cm.kiada-config.yml：123456apiVersion: v1kind: ConfigMapmetadata: name: kiada-configdata: INITIAL_STATUS_MESSAGE: This status message is set in the kiada-config config map 使用 kubectl replace 命令更新 ConfigMap：12$ kubectl replace -f cm.kiada-config.ymlconfigmap/kiada-config replaced 创建 Pod 清单文件 pod.kiada.envFrom.yml：123456789101112131415apiVersion: v1kind: Podmetadata: name: kiadaspec: containers: - name: kiada image: luksa/kiada:0.4 envFrom: - configMapRef: name: kiada-config optional: true ports: - name: http containerPort: 8080 应用 Pod 清单文件并查看效果：123456789kubectl delete -f pod.kiada.env-valueFrom.ymlpod "kiada" deleted$ kubectl apply -f pod.kiada.envFrom.ymlpod/kiada created$ kubectl exec kiada -- envPATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/binHOSTNAME=kiadaINITIAL_STATUS_MESSAGE=This status message is set in the kiada-config config map... 将 ConfigMap 中的配置项作为文件嵌入到容器环境变量通常用于向应用传递较小的单行的值，而多行的数据通常作为文件传递。ConfigMap 中的配置项可以包含大块的数据，通过特殊的 ConfigMap volume 类型将这些数据以文件的形式注入到容器中。 从文件创建 ConfigMaps除了直接在集群中创建 ConfigMap 对象之外，还可以先创建一个用于描述 ConfigMap 的 YAML 清单文件，从而可以将其保存到版本控制系统中。1234$ kubectl create configmap kiada-envoy-config \--from-file=envoy.yaml \--from-file=dummy.bin \--dry-run=client -o yaml &gt; cm.kiada-envoy-config.yml 其中 dummy.bin 和 envoy.yaml 两个文件可以从作者的 Github 下载。上述命令会创建一个名为 kiada-envoy-config 的 ConfigMap 的 YAML 描述文件。当命令中使用了 --dry-run 选项时，该命令并不会在 API server 创建新对象，而是只生成对象的定义。后续可继续运行 kubectl apply -f xxx.yml 命令创建对象。 生成的 cm.kiada-envoy-config.yml 文件的具体内容如下：1234567891011121314151617apiVersion: v1binaryData: dummy.bin: n2VW39IEkyQ6Jxo+rdo5J06Vi7cz5XxZzkPOtN7MOGyXxVzWv07vUz8bxY5h4njJfZixwhPwoGepLnGZdRGw1qgpFB9HpyLsqVZ6jAwjjHD0afxxwEjb6+wofmgGmS1E3U9BZScMPZGWJK7RGzZmQOeJeDPIt/1tBvQYwzMc8wu6owky4Ri3rOY9PlFnp5VOTzvBZadW8scbqtntJeWCtJFoS0AH2y4ZFyJPJ0l0V3JpY1qunnz60CyAMO9v1DgB2PUQUG/1HH89bpzf2OyMvHUJmOTIDZOh3D7aAEuYQ/6hU0uQsQ/K7Xx/nM9TA0RzEzuh8RBoXdkAvQPP5jk7yM2DqxG/CiHVp+7CDRFWgEN2GFd76RfO+pIoOFbD1Xm4yE/JBljOA9ztwm865m3A4l2ToT2p7ZWHKmdZe8pXz4ZwIGBYDUHHNVQW5UUnf0Jbd9UR8GJ//gmbxLfVxjC/lMSWGUqCpYO4YdBPjXQGM1xdxE+YP3Pzso6Z4rw27RJu5KEc2yPMxFY9dpFyVufP81kS19glNiQq+LM4B9EFPrNW1hqi+1Tb8ni+aFkriH1YuvHepIH0Px/ifFLgn+yDgwDs4UfMru2j4t5zAftUa0i6m3sH5adKcx4aCXYN9ijvEvjRmkcB/VJU6Zbd65UZVgD1Nwt2ZCrkoEdqO3Oe1/o=data: envoy.yaml: | admin: access_log_path: /tmp/envoy.admin.log address: socket_address: protocol: TCP address: 0.0.0.0 port_value: 9901...kind: ConfigMapmetadata: creationTimestamp: null name: kiada-envoy-config 在 Pod 中使用 ConfigMap为了令 ConfigMap 能够作为文件呈现在容器的文件系统中，需要在 Pod 中定义一个 ConfigMap Volume 并将其挂载到容器里。参考下面的 pod.kiada-ssl.configmap-volume.yml：1234567891011121314151617181920212223242526apiVersion: v1kind: Podmetadata: name: kiada-sslspec: volumes: - name: envoy-config configMap: name: kiada-envoy-config containers: - name: kiada image: luksa/kiada:0.4 ports: - name: http containerPort: 8080 - name: envoy image: luksa/kiada-ssl-proxy:0.1 volumeMounts: - name: envoy-config mountPath: /etc/envoy readOnly: true ports: - name: https containerPort: 8443 - name: admin containerPort: 9901 创建 kiada-envoy-config ConfigMap：1234567$ kubectl apply -f cm.kiada-envoy-config.ymlconfigmap/kiada-envoy-config created$ kubectl get cmNAME DATA AGEkiada-config 1 46hkiada-envoy-config 2 13skube-root-ca.crt 1 57d 创建 Pod 并查看文件挂载情况：12345$ kubectl apply -f pod.kiada-ssl.configmap-volume.ymlpod/kiada-ssl created$ kubectl exec kiada-ssl -c envoy -- ls /etc/envoydummy.binenvoy.yaml 理解 ConfigMap Volume 是如何工作的挂载 Volume 会隐藏文件系统中本来就存在的文件当挂载任意 volume 到容器的文件系统中时，挂载的路径下原本存在的文件就无法被访问。比如把某个 ConfigMap volume 挂载到容器的 /etc 路径下，则本来位于 /etc 下面的配置文件都会被隐藏掉。如果你还是想将 ConfigMap volume 挂载到 /etc 目录，又不影响原来存在的文件，可以结合使用 mountPath 和 subPath。1234567spec: containers: - name: my-container volumeMounts: - name: my-volume subPath: my-app.conf mountPath: /etc/my-app.conf ConfigMap volumes 使用符号链接来实现原子化更新有些应用会监控其配置文件的变化来决定是否重新加载配置。但是，假如应用的配置文件很大或者涉及到很多文件，应用可能会在所有更新全部完成之前检查到变化并加载更新。若应用最终读取了未全部更新的文件，可能导致出现不正常的行为。为了防止上述情况发生，Kubernetes 会确保 ConfigMap volume 中所有文件的更新操作是原子的，即所有更新都会立即完成。上述机制是通过文件的符号链接实现的。123456$ kubectl exec kiada-ssl -c envoy -- ls -lA /etc/envoytotal 4drwxr-xr-x 2 root root 4096 Jan 12 03:33 ..2022_01_12_03_33_58.815959464lrwxrwxrwx 1 root root 31 Jan 12 03:33 ..data -&gt; ..2022_01_12_03_33_58.815959464lrwxrwxrwx 1 root root 16 Jan 12 03:33 dummy.bin -&gt; ..data/dummy.binlrwxrwxrwx 1 root root 17 Jan 12 03:33 envoy.yaml -&gt; ..data/envoy.yaml 每次用户修改 ConfigMap，Kubernetes 都会新建一个以当前时间戳命名的目录，将新的配置文件放到该目录中。再修改符号链接的指向，立即同时替换所有文件。 更新和删除 ConfigMaps和大多数 Kubernetes API 对象一样，ConfigMaps 也支持手动修改其清单文件，再使用 kubectl apply 命令将最新版本重新应用到集群环境。此外还可以使用下面这种更快捷的方式。 使用 kubectl edit 就地修改 API 对象kubectl edit cm kiada-config 上面的命令会使用系统默认的编辑器（比如 Vim）打开 ConfigMap kiada-config 的清单文件，允许用户直接修改该 ConfigMap 对象。当关闭编辑器时，最新的改动会自动应用到 Kubernetes API server。 修改 ConfigMap 的影响当更新某个 ConfigMap 后，若 ConfigMap 是以存储卷的形式作为配置文件挂载到容器中，则其中的文件会自动进行更新。但是不同于文件，环境变量在容器运行时不会自动进行更新。然而当容器因为某些特殊原因重启后，Kubernetes 会使用最新版本的 ConfigMap 值来初始化新容器的环境变量。 容器最重要的属性之一就是其不变性，即用户可以肯定同一个容器的多个实例一定是相同的。假如应用通过 ConfigMap 以环境变量的方式注入配置，当 ConfigMap 修改后，实际上并不会影响正在运行的应用实例。但是当应用的某一部分实例因为故障重新启动，或者需要添加新的实例副本，就会导致只有这一部分实例在使用最新的配置。类似的场景甚至发生在应用自动重新加载其配置文件的时候。Kubernetes 对于 ConfigMap volumes 采用异步的更新方式，这导致某些实例可能会比其他实例更早地看到配置的变化。上述更新操作有可能会持续几十秒的时间，则在这部分时间内，各应用实例上的配置文件之间实际上是不同步的。从而也会导致应用的各个容器实例之间正使用着不同的配置。 阻止 ConfigMap 被修改12345678apiVersion: v1kind: ConfigMapmetadata: name: my-immutable-configmapdata: mykey: myvalue another-key: another-valueimmutable: true 不可变 ConfigMap 可以阻止用户意外地修改应用配置，同时也会提高 Kubernetes 集群的性能。因为工作节点上的 Kubelets 无需再接收 ConfigMap 对象变化的通知。假如需要添加一系列使用不同配置的 Pods，就创建一个新的 ConfigMap。 删除 ConfigMapConfigMap 对象可以使用 kubectl delete 命令删除。删除某个 ConfigMap 后，引用它的正在运行的应用不会受到任何影响，直到容器重新启动。若 ConfigMap 没有被标记成可选的，则容器会在重启时失败。 使用 Secrets 向容器嵌入敏感信息Secrets 介绍Secrets 和 ConfigMaps 非常相似。它们也是包含一系列键值对且能够作为环境变量或文件注入到容器中。事实上 Secrets 比 ConfigMap 更早出现。但是 Secrets 在存储普通的明文文本时友好性较差，因而引入了 ConfigMaps。对于 Secrets 对象，Kubernetes 在处理它们时会特别注意安全性。比如 Kubernetes 会确保 Secret 中的数据在分发时只传给有需要的节点；工作节点上的 Secrets 只保存在内存中，从来不会写入到物理存储。因此，必须将敏感数据保存到 Secrets 而不是 ConfigMaps 中。 创建 Secret创建一个 TLS secret12$ kubectl create secret tls kiada-tls --cert example-com.crt --key example-com.keysecret/kiada-tls created 其中 crt 文件和 key 文件可以从作者的 Github 下载。上述命令会创建一个名为 kiada-tls 的 Secret。 从 YAML 清单文件创建 Secrets显而易见，从 YAML 文件创建 Secrets 并不是一个好主意。假如你只是需要在本地创建一个 YAML 清单文件而不是直接通过 API server 创建 Secret 对象，可以使用 kubectl create 加上 --dry-run=client -o yaml 选项。123456789101112$ kubectl create secret generic my-credentials \--from-literal user=my-username \--from-literal pass=my-password \--dry-run=client -o yamlapiVersion: v1data: pass: bXktcGFzc3dvcmQ= user: bXktdXNlcm5hbWU=kind: Secretmetadata: creationTimestamp: null name: my-credentials 使用 stringData 字段因为并不是所有敏感数据都是二进制形式，Kubernetes 允许用户通过 stringData 字段指定 Secret 中的文本数据。12345678apiVersion: v1kind: SecretstringData: user: my-username pass: my-passwordmetadata: creationTimestamp: null name: my-credentials 在容器中使用 Secrets借助 Secret Volume 注入 Secret 配置Secret volume 和前面提到的 ConfigMap volume 用法基本相同。参考如下 YAML 配置文件 pod.kiada-ssl.secret-volume.yml：1234567891011121314151617181920212223242526272829303132333435363738394041apiVersion: v1kind: Podmetadata: name: kiada-sslspec: volumes: - name: cert-and-key secret: secretName: kiada-tls items: - key: tls.crt path: example-com.crt - key: tls.key path: example-com.key mode: 0600 - name: envoy-config configMap: name: kiada-envoy-config items: - key: envoy.yaml path: envoy.yaml containers: - name: kiada image: luksa/kiada:0.4 ports: - name: http containerPort: 8080 - name: envoy image: envoyproxy/envoy:v1.14.1 volumeMounts: - name: cert-and-key mountPath: /etc/certs readOnly: true - name: envoy-config mountPath: /etc/envoy readOnly: true ports: - name: https containerPort: 8443 - name: admin containerPort: 9901 通过 Downward API 向应用传递 Pod 的元数据前面介绍了如何向容器中的应用传递配置信息。但数据并不总是静态的，假如某些数据只有在 Pod 创建之后才能被知晓，比如 Pod 的 IP 地址、工作节点的名称、容器分配到的 CPU 和内存等。这些需求可以通过 Downward API 实现。 Downward API 介绍Downward API 并不是一个需要应用去访问的 REST 端点，它实际上是一种将 Pod 的 metadata、spec 和 status 字段注入到容器中的方式。 Downward API 支持哪些元数据用户并不能通过 Downward API 注入 Pod 对象中的所有字段。实际上只有一部分数据被支持，其列表如下。 通过 fieldRef 字段注入的数据 Field Description Allowed in env Allowed in volume metadata.name Pod 的名称 Yes Yes metadata.namespace Pod 的命名空间 Yes Yes metadata.uid Pod 的 UID Yes Yes metadata.labels Pod 的所有标签 No Yes metadata.labels[&#39;key&#39;] 某个标签的值 Yes Yes metadata.annotations Pod 的所有注释 No Yes metadata.annotations[&#39;key&#39;] 某个注释的值 Yes Yes spec.nodeName Pod 运行的工作节点的名称 Yes No spec.serviceAccountName Pod 的服务账户 Yes No status.podIP Pod 的 IP 地址 Yes No status.hostIP 工作节点的 IP 地址 Yes No 通过 resourceFieldRef 字段注入的数据 Resource Field Description Allowed in env Allowed in volume requests.cpu 容器的 CPU 请求 Yes Yes requests.memory 容器的内存请求 Yes Yes requests.ephemeral-storage 容器的临时存储请求 Yes Yes limits.cpu 容器的 CPU 限制 Yes Yes limits.memory 容器的内存限制 Yes Yes limits.ephemeral-storage 容器的临时存储限制 Yes Yes 将 Pod 的元数据注入到环境变量中注入 Pod 对象中的字段kiada.yml：12345678910111213141516171819202122232425262728apiVersion: v1kind: Podmetadata: name: kiadaspec: containers: - name: kiada image: luksa/kiada:0.4 env: - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: POD_IP valueFrom: fieldRef: fieldPath: status.podIP - name: NODE_NAME valueFrom: fieldRef: fieldPath: spec.nodeName - name: NODE_IP valueFrom: fieldRef: fieldPath: status.hostIP ports: - name: http containerPort: 8080 应用 kiada.yml 清单文件并查看其日志输出：12345678910111213$ kubectl apply -f kiada.ymlpod/kiada created$ kubectl logs -f kiadaKiada - Kubernetes in Action Demo Application---------------------------------------------Kiada 0.4 starting...Pod name is kiadaLocal hostname is kiadaLocal IP is 172.17.0.6Running on node minikubeNode IP is 192.168.49.2Status message is This is the default status messageListening on port 8080 注入容器的资源数据示例配置如下：12345678910env: - name: MAX_CPU_CORES valueFrom: resourceFieldRef: resource: limits.cpu - name: MAX_MEMORY_KB valueFrom: resourceFieldRef: resource: limits.memory divisor: 1k 参考资料Kubernetes in Action, Second Edition]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Server</tag>
        <tag>Configuration</tag>
        <tag>Docker</tag>
        <tag>Container</tag>
        <tag>Kubernetes</tag>
        <tag>Administrator</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kubernetes in Action 笔记 —— 向容器挂载存储卷]]></title>
    <url>%2F2022%2F01%2F06%2Fkubernetes-in-action-reading-notes-mount-storage-volumes%2F</url>
    <content type="text"><![CDATA[Volumes 介绍Pod 就像是一个部署着某个应用的逻辑化的计算机，可以包含一个或几个容器，运行着应用的各个进程。这些进程可以共享 CPU、RAM、网络接口等计算资源。在普通的计算机中，应用的进程使用同一个文件系统。但是在 Pod 中，每个容器都有自己独有的、由容器镜像提供的隔离的文件系统。 容器构建时添加到镜像中的文件构成了容器的文件系统，容器启动后，运行在容器中的进程就可以修改这些文件或者添加新的文件。但是当容器终止和重启时，所有对于文件系统的改动都不会被保留。事实上容器并不是真正意义上的重启，而是完全地被一个新容器替换掉了。这对于一些类型的应用是 OK 的，其他应用则需要在重启时至少保留一部分文件系统。这可以通过挂载 Volume 来实现。 Volumes 与 PodVolumes 并不是 Pod 或 Node 那样的顶层资源，而是和容器一样是 Pod 中的一个组件，因而会使用 Pod 的生命周期。Volumes 与其挂载的容器的生命周期是毫无关联的，也因此能够用来在容器重启时保持数据。 Volumes 在 Pod 中定义，挂载到 Pod 下的容器中。 在容器重启时保持数据Pod 中的所有 Volumes 都会在容器启动前创建，在 Pod 关闭时销毁。应用可以向挂载到容器文件系统中的 Volume 写入数据。当容器重启时，其被替换为一个新容器，文件系统也重新从镜像创建。此时则可以再次挂载同样的 Volume，完成对之前数据的访问。 通常由应用的作者决定哪些文件需要在容器重启时被保留，一般是一些代表应用状态的数据。但不包括应用的本地缓存数据，这些数据会阻止容器完成一次“全新”的重启。而全新的重启有利于应用的“自愈”。 在一个容器中挂载多个 Volume一个 Pod 可以包含多个 Volumes，Pod 中的每个容器都可以挂载这些 Volumes 中的零个或多个。 在多个容器间共享文件一个 Volume 可以被同时挂载到多个容器中，从而这些容器中的应用可以共享文件。比如可以创建一个 Pod，包含一个 Web Server 容器和一个 content-producing agent 容器。Agent 容器负责生成静态媒体内容保存至 Volume，Web 容器则将 Volume 中的内容发布给客户端。 在 Pod 重启时保持数据Volume 与 Pod 的生命周期绑定，只会在 Pod 存在时存在。但是依靠某些特殊的 Volume 类型，其中的文件也可以在 Pod 和 Volume 消失后继续存在，后续也能够挂载到一个新的 Volume 中。 如上图所示，Pod 中的 Volume 可以映射到 Pod 外面的永久存储。代表 Volume 的文件目录并不是 Pod 内部的本地路径，而是挂载的一个脱离了 Pod 生命周期的 NAS。假如 Pod 被删除并被一个新的 Pod 所替换，同一个 NAS 可以被关联到新的 Pod 实例中，从而前一个 Pod 保持的数据可以被新的 Pod 访问。 在多个 Pod 间共享文件取决于 external storage volume 使用的具体技术，同一个外挂存储也能够同时被关联给多个 Pod，从而这些 Pod 之间能够共享数据。 Volume 类型当向某个 Pod 添加 Volume 时，必须指定 Volume 类型。主要的几种 Volume 类型如下： emptyDir：一个简单的空目录，允许 Pod 在其生命周期内向该路径下存储数据 hostPath：从工作节点的文件系统向 Pod 挂载文件 nfs：挂载到 Pod 中的 NFS 共享 gcePersistentDisk (Google Compute Engine Persistent Disk), awsElasticBlockStore (Amazon Web Services Elastic Block Store), azureFile (Microsoft Azure File Service), azureDisk (Microsoft Azure Data Disk) cephfs, cinder, fc, flexVolume, flocker, glusterfs, iscsi, portworxVolume, quobyte, rbd, scaleIO, storageos, photonPersistentDisk, vsphereVolume configMap, secret, downwardAPI, projected：特殊类型的 Volume，用来暴露 Pod 及其他 Kubernetes 对象的信息 csi：一种可插拔的通过 Container Storage Interface 添加存储的方式。任何人都可以使用这种 Volume 类型来实现自己的存储驱动 使用 Volumes使用 emptyDir 在容器重启时保留文件emptyDir 是最简单的 Volume 类型。它最开始以空目录的形式挂载到容器的文件系统，任何写入到该路径下的文件都会在 Pod 的整个生命周期中存在。这类 Volume 通常用于在容器重启后保留部分数据；或者当整个容器的文件系统都是只读时，令其部分文件系统可写；又或者在包含两个或以上容器的 Pod 中，在各容器间传递数据。 向 Pod 添加 emptyDir Volume创建如下 fortune-emptydir.yml 清单文件：1234567891011121314151617181920212223kind: Podmetadata: name: fortune-emptydirspec: volumes: - name: content emptyDir: &#123;&#125; containers: - name: nginx image: nginx:alpine volumeMounts: - name: content mountPath: /usr/share/nginx/html lifecycle: postStart: exec: command: - sh - -c - "ls /usr/share/nginx/html/quote || (apk add fortune &amp;&amp; fortune &gt; /usr/share/nginx/html/quote)" ports: - name: http containerPort: 80 当容器第一次启动时，postStart hook 会执行 apk add fortune 命令安装 fortune 软件包，并执行 fortune &gt; /usr/share/nginx/html/quote 创建 quote 文件。后续若容器因为各种原因重启，由于 quote 文件位于挂载的 Volume 中，不会随着容器一同被销毁。并且在新容器生成后仍会挂载到原来的路径下，即不管后续容器如何重启，quote 文件都会保持第一次创建时的状态。 若没有挂载 Volume，则 quote 文件会在容器重启时随着旧容器一同被销毁，每次新容器生成，都会在同样的路径下产生一个新的不同版本的 quote 文件。 运行 kubectl apply -f fortune-emptydir.yml 命令应用清单文件，检查容器中 Volume 的挂载状态：1234$ kubectl apply -f fortune-emptydir.ymlpod/fortune-emptydir created$ kubectl exec fortune-emptydir -- mount --list | grep nginx/html/dev/sdb on /usr/share/nginx/html type ext4 (rw,relatime,discard,errors=remount-ro,data=ordered) 使用 emptyDir 在容器间共享文件创建如下内容的清单文件 fortune.yml：1234567891011121314151617181920212223apiVersion: v1kind: Podmetadata: name: fortunespec: volumes: - name: content emptyDir: &#123;&#125; containers: - name: fortune image: luksa/fortune-writer:1.0 volumeMounts: - mountPath: /var/local/output name: content - name: nginx image: nginx:alpine volumeMounts: - mountPath: /usr/share/nginx/html name: content readOnly: true ports: - containerPort: 80 name: http 其中 fortune 容器是作者自己构建的，会每隔 30s 执行一次 fortune &gt; /var/local/output/quote 命令。fortune 每次执行都会随机输出一段名言警句类型的话，即 /var/local/output/quote 文件中的内容会每隔 30s 变化一次。整个 Pod 包含两个容器和一个 Volume，其中 fortune 容器挂载 content Volume 到自己的 /var/local/output 路径下，而 nginx 容器挂载同一个 content Volume 到自己的 /usr/share/nginx/html 路径下。fortune 容器会每隔 30s 生成一个新的 quote 文件，保存到挂载的 content Volume 下；而另一个 nginx 容器也挂载了 content Volume，并且将其中的 quote 文件作为 Web 服务的静态文件向外提供服务。即同一个 Pod 中的两个容器通过挂载同一个 Volume 实现文件的共享。 运行 Pod使用 kubectl apply -f fortune.yml 命令应用清单文件创建 Pod，再分别从两个容器中查看 Volume 下 quote 文件的内容：12345678$ kubectl apply -f fortune.ymlpod/fortune configured$ kubectl exec fortune -c fortune -- cat /var/local/output/quoteLife is too important to take seriously. -- Corky Siegel$ kubectl exec fortune -c nginx -- cat /usr/share/nginx/html/quoteLife is too important to take seriously. -- Corky Siegel 后两条命令输出了同样的内容。虽然两个容器查看的 quote 文件本地路径不同，它们实际上指向了同一个 Volume。 理解 external volumes 是如何挂载的如下图所示，网络存储卷首先是被宿主节点挂载，然后再授予 Pod 访问挂载点的权限。 通常情况下，底层的存储技术并不允许一个 Volume 以读写模式同时挂载到一个以上的节点，但是同一个节点上的多个 Pods 可以同时以读写模式使用 Volume。对于云环境提供的大多数存储技术，多个节点使用同一个网络存储卷的方式只有一种，即以只读模式挂载。在设计分布式应用的架构时，考虑网络存储卷的上述限制是很有必要的。同一个 Pod 的多个副本通常不能以读写模式挂载同一个网络存储卷。 访问工作节点上的文件系统绝大多数 Pods 不应该关注部署它们的宿主节点，不应该访问节点文件系统中的任何文件。除非这些 Pods 是系统级别的。可以使用 hostPath 类型的 Volume 令 Pod 能够访问宿主节点。 hostPath Volume 介绍hostPath Volume 指向宿主节点文件系统中的特定文件或目录，形成 Pod 与宿主节点之间的文件共享。 hostPath Volume 并不适合存放数据库中的数据。因为此 Volume 的内容只是保存在某个特定的工作节点上，假如数据库 Pod 被重新分配给了另一个节点，则保存在 Volume 中的数据库数据对新的 Pod 不再可见。 通常情况下，hostPath Volume 只用在当 Pod 确实是需要读写 Node 中的文件，比如 Node 上的系统日志。hostPath 是最危险的 Volume 类型之一，一般只用于具有特殊权限的 Pod。假如不对 hostPath 的使用加以限制，用户有可能对工作节点做任何事。比如用户可以使用 hostPath 挂载容器的 Docker socket 文件，然后在容器内运行 Docker 客户端，接着便可以作为 root 用户在宿主节点上执行任意命令。 使用 hostPath Volume部署如下配置的 Pod：123456789101112131415161718apiVersion: v1kind: Podmetadata: name: node-explorerspec: volumes: - name: host-root hostPath: path: / containers: - name: node-explorer image: alpine command: - "sleep" - "9999999999" volumeMounts: - mountPath: /host name: host-root Pod 部署完成后，即可运行 kubectl exec -it node-explorer -- sh 命令在 Pod 中运行一个交互式命令行窗口，在执行 cd /host 命令即可进入宿主节点的文件系统：123456$ kubectl exec -it node-explorer -- sh/ # cd /host/host # lsRelease.key boot dev etc kic.txt lib lib64 media opt root sbin sys usrbin data docker.key home kind lib32 libx32 mnt proc run srv tmp var/host # hostPath Volume 指向的是宿主节点的 / 路径，由此整个工作节点的文件系统都会向 Pod 开放。执行完上面的命令后，即可以修改工作节点上的任何文件。 参考资料Kubernetes in Action, Second Edition]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Server</tag>
        <tag>Storage</tag>
        <tag>Docker</tag>
        <tag>Container</tag>
        <tag>Kubernetes</tag>
        <tag>Volumes</tag>
        <tag>Pod</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kubernetes in Action 笔记 —— 管理 Pod 的生命周期]]></title>
    <url>%2F2022%2F01%2F05%2Fkubernetes-in-action-reading-notes-managing-pod-lifecycle%2F</url>
    <content type="text"><![CDATA[理解 Pod 的状态Pod phase在 Pod 完整的生命周期中，存在着 5 种不同的阶段： Pending：创建 Pod 对象后的初始化阶段。会一直持续到 Pod 被分配给某个工作节点，镜像被拉取到本地并启动 Running：Pod 中至少一个容器处于运行状态 Succeeded：对于不打算无限期运行的 Pod，其容器部署完成后的状态 Failed：对于不打算无限期运行的 Pod，其容器中至少有一个由于错误终止 Unknown：由于 Kubelet 与 API Server 的通信中断，Pod 的状态未知。可能是工作节点挂掉或断网 从 kubia.yml 清单文件创建一个 Pod。12345678910apiVersion: v1kind: Podmetadata: name: kubiaspec: containers: - name: kubia image: luksa/kubia:1.0 ports: - containerPort: 8080 $ kubectl apply -f kubia.yml 查看 Pod 的 Phase12$ kubectl get po kubia -o yaml | grep phase phase: Running 或者借助 jq 工具从 JSON 格式的输出中检索 phase 字段：12$ kubectl get po kubia -o json | jq .status.phase"Running" 也可以使用 kubectl describe 命令：12$ kubectl describe po kubia | grep Status:Status: Running Pod conditionsPod 的 conditions 用来表示某个 Pod 是否达到了特定的状态以及达到或未达到的原因。与 phase 相反，一个 Pod 可以同时有几个 conditions。 PodScheduled：表明 Pod 是否已经被安排给了某个工作节点 Initialized：Pod 的初始化容器已经部署完成 ContainersReady：Pod 中的所有容器都已经准备完毕 Ready：Pod 自身已经准备好对其客户端提供服务 查看 Pod 的 conditions1234567$ kubectl describe po kubia | grep Conditions: -A5Conditions: Type Status Initialized True Ready True ContainersReady True PodScheduled True kubectl describe 命令只会显示每个 condition 是 true 还是 false，不会显示更详细的信息。为了显示某个 condition 为 false 的具体原因，需要检索 Pod 的清单文件：123456789101112131415161718192021222324252627$ kubectl get po kubia -o json | jq .status.conditions[ &#123; "lastProbeTime": null, "lastTransitionTime": "2021-12-30T03:02:45Z", "status": "True", "type": "Initialized" &#125;, &#123; "lastProbeTime": null, "lastTransitionTime": "2021-12-30T03:02:46Z", "status": "True", "type": "Ready" &#125;, &#123; "lastProbeTime": null, "lastTransitionTime": "2021-12-30T03:02:46Z", "status": "True", "type": "ContainersReady" &#125;, &#123; "lastProbeTime": null, "lastTransitionTime": "2021-12-30T03:02:45Z", "status": "True", "type": "PodScheduled" &#125;] 当 condition 为 false 时，上述输出中会包含 reason 和 message 字段来显示失败的具体原因和详细信息。 容器的 status容器的 status 包含多个字段。其中 state 字段表示该容器当前的状态，restartCount 表示容器重启的频率，此外还有 containerID、image、imageID 等。 容器的 status 包含以下几种： Waiting：容器等待启动。reason 和 message 字段会记录容器处于此状态的原因 Running：容器已经创建，进程正在运行 Terminated：容器中运行的进程已经终止。exitCode 字段会记录进程的退出码 Unknown：容器的状态无法确定 比如修改 kubia.yml 清单文件中的 image 字段，故意改成 uksa/kubia:1.0 这样无效的地址，运行 kubectl apply -f kubia.yml 命令重新应用清单文件。等待几分钟直到新的配置生效，查看容器的状态。 可以使用 kubectl describe 命令查看容器的状态：1234567891011121314151617$ kubectl describe po kubia | grep Containers: -A15Containers: kubia: Container ID: docker://62fa208957d396c38f65305fd073d6b334dd8da22ab5beab196ca9bcf2f9ff91 Image: uksa/kubia:1.0 Image ID: docker-pullable://luksa/kubia@sha256:a961dc8f377916936fa963508726d77cf77dcead5c97de7e5361f0875ba3bef7 Port: 8080/TCP Host Port: 0/TCP State: Waiting Reason: ImagePullBackOff Last State: Terminated Reason: Error Exit Code: 137 Started: Fri, 31 Dec 2021 14:50:39 +0800 Finished: Fri, 31 Dec 2021 14:51:36 +0800 Ready: False Restart Count: 0 或者使用 kubectl get po kubia -o json 命令：123456789101112131415161718192021222324252627$ kubectl get po kubia -o json | jq .status.containerStatuses[ &#123; "containerID": "docker://62fa208957d396c38f65305fd073d6b334dd8da22ab5beab196ca9bcf2f9ff91", "image": "luksa/kubia:1.0", "imageID": "docker-pullable://luksa/kubia@sha256:a961dc8f377916936fa963508726d77cf77dcead5c97de7e5361f0875ba3bef7", "lastState": &#123; "terminated": &#123; "containerID": "docker://62fa208957d396c38f65305fd073d6b334dd8da22ab5beab196ca9bcf2f9ff91", "exitCode": 137, "finishedAt": "2021-12-31T06:51:36Z", "reason": "Error", "startedAt": "2021-12-31T06:50:39Z" &#125; &#125;, "name": "kubia", "ready": false, "restartCount": 0, "started": false, "state": &#123; "waiting": &#123; "message": "Back-off pulling image \"uksa/kubia:1.0\"", "reason": "ImagePullBackOff" &#125; &#125; &#125;] 上面输出中的 state 字段都表明了容器当前的状态是 waiting，还有 reason 和 message 字段表明处于此状态的原因：镜像拉取失败。 确保容器的健康状态理解容器的自动重启机制当一个 Pod 被分配给了某个工作节点，该工作节点上的 Kubelet 就会负责启动容器并保证该容器一直处于运行状态，只要该 Pod 对象一直存在没被移除。如果容器中的主进程由于某些原因终止运行，Kubernetes 就会自动重启该容器。 创建如下内容的清单文件 kubia-ssl.yml：123456789101112131415161718apiVersion: v1kind: Podmetadata: name: kubia-sslspec: containers: - name: kubia image: luksa/kubia:1.0 ports: - name: http containerPort: 8080 - name: envoy image: luksa/kubia-ssl-proxy:1.0 ports: - name: https containerPort: 8443 - name: admin containerPort: 9901 运行如下命令应用上述清单文件，并启用端口转发：123456789$ kubectl apply -f kubia-ssl.ymlpod/kubia-ssl created$ kubectl port-forward kubia-ssl 8080 8443 9901Forwarding from 127.0.0.1:8080 -&gt; 8080Forwarding from [::1]:8080 -&gt; 8080Forwarding from 127.0.0.1:8443 -&gt; 8443Forwarding from [::1]:8443 -&gt; 8443Forwarding from 127.0.0.1:9901 -&gt; 9901Forwarding from [::1]:9901 -&gt; 9901 待容器启动成功后，访问 localhost 的 8080、8443、9901 端口就等同于访问容器中 8080、8443、9901 端口上运行的服务。 打开一个新的命令行窗口运行 kubectl get pods -w 命令，实时监控容器的运行状态。打开一个新的命令行窗口运行 kubectl get events -w 命令，实时监控触发的事件。 打开一个新的命令行窗口，尝试终止 Envoy 容器中运行的主进程。Envoy 容器 9901 端口上运行的服务刚好提供了一个管理接口，能够接收 HTTP POST 请求来终止进程：12$ curl -X POST http://localhost:9901/quitquitquitOK 此时查看前两个窗口中的输出，负责监控容器状态的窗口输出如下：12345$ kubectl get pods -wNAME READY STATUS RESTARTS AGEkubia-ssl 2/2 Running 0 11mkubia-ssl 1/2 NotReady 0 12mkubia-ssl 2/2 Running 1 (2s ago) 12m 最新的输出表明，在杀掉 Envoy 容器中的主进程后，Pod 的状态是先变成 NotReady，之后就立即重启该容器，Pod 的状态稍后恢复成 Running。 负责监控最新事件的窗口输出如下：123456789101112$ kubectl get events -wLAST SEEN TYPE REASON OBJECT MESSAGE11m Normal Scheduled pod/kubia-ssl Successfully assigned default/kubia-ssl to minikube11m Normal Pulled pod/kubia-ssl Container image "luksa/kubia:1.0" already present on machine11m Normal Created pod/kubia-ssl Created container kubia11m Normal Started pod/kubia-ssl Started container kubia11m Normal Pulled pod/kubia-ssl Container image "luksa/kubia-ssl-proxy:1.0" already present on machine11m Normal Created pod/kubia-ssl Created container envoy11m Normal Started pod/kubia-ssl Started container envoy0s Normal Pulled pod/kubia-ssl Container image "luksa/kubia-ssl-proxy:1.0" already present on machine0s Normal Created pod/kubia-ssl Created container envoy0s Normal Started pod/kubia-ssl Started container envoy 最新的事件信息中包含了新的 envoy 容器启动的过程。其中有一个很重要的细节：Kubernetes 从来不会重启容器，而是直接丢弃停止的容器并创建一个新的。一般在 Kubernetes 中提及“重启”容器，实质上都指的是“重建”。任何写入到容器文件系统中的数据，在容器重新创建后都会丢失。为了持久化这些数据，需要向 Pod 添加 Volume。 配置容器的重启策略Kubernetes 支持 3 种容器重启策略： Always：默认配置。不管容器中主进程的退出码是多少，容器都会自动重启 OnFailure：容器只会在退出码非 0 时重启 Never：容器永不重启 容器重启时的延迟时间第一次容器终止时，重启会立即触发。但容器第二次重启时会先等待 10s，这个等待时间会随着重启次数依次增加到 20、40、80、160s。再之后则一直保持在 5 分钟。等待过程中容器会处于 Waiting 状态，reason 字段显示 CrashLoopBackOff，message 字段显示需要等待的时间。 liveness probesKubernetes 会在容器的进程终止时重启容器，以保证应用的健康。但应用实际上有可能在进程不终止的情况下无响应，比如一个 Java 应用报出 OutOfMemoryError 错误，而 JVM 进程仍在运行中。理想情况下，Kubernetes 需要能够检测到此类错误并重启容器。 当然，应用自身也可以捕获这类错误并令进程立即终止。但假如应用因为进入无限循环或死锁导致无响应，又或者应用本身无法检测到错误存在呢？为了确保容器能够在这些复杂情况下重启，应该提供一种从外部检查应用状态的机制。 liveness probe 介绍Kubernetes 可以通过配置 liveness probe 来检查某个应用是否能够正常响应，Pod 中的每个容器都可以分别配置 liveness probe。一旦应用无响应或有错误发生，容器就会被认为是不健康的并被终止掉。之后容器被 Kubernetes 重新启动。 Kubernetes 支持以下三种 probe 机制： HTTP GET probe：会发送 GET 请求到容器的 IP 地址、端口号和 URL 路径。如果 probe 收到正常的响应（2xx 或 3xx），该 probe 就被认定是成功的。如果服务返回了一个错误的响应码，或者没有在规定的时间内响应，则该 probe 被认定是失败的。 TCP Socket probe：会尝试打开一个 TCP 连接到容器的特定端口。若连接成功，probe 就被认定是成功的；否则失败。 Exec probe：会在容器内部执行一个命令并检查该命令的退出码。若退出码为 0，则 probe 被认定是成功的；否则失败。 HTTP GET liveness probe创建如下内容的 kubia-liveness.yml 清单文件：123456789101112131415161718192021222324252627282930apiVersion: v1kind: Podmetadata: name: kubia-livenessspec: containers: - name: kubia image: luksa/kubia:1.0 ports: - name: http containerPort: 8080 livenessProbe: httpGet: path: / port: 8080 - name: envoy image: luksa/kubia-ssl-proxy:1.0 ports: - name: https containerPort: 8443 - name: admin containerPort: 9901 livenessProbe: httpGet: path: /ready port: admin initialDelaySeconds: 10 periodSeconds: 5 timeoutSeconds: 2 failureThreshold: 3 其中 kubia 容器的 liveness probe 是最简单版本的 HTTP 应用的 probe。该 probe 只是向 8080 端口的 / 路径发送 HTTP GET，看容器是否仍然能够处理请求。当应用的响应码介于 200 到 399 之间时，该应用就被认为是健康的。由于该 probe 没有配置其他选项，默认配置生效。第一次检查请求会在容器启动 10s 后发起，之后每隔 10s 发起新的请求。若应用没有在 1s 之内响应，则该次 probe 请求被认定是失败的。连续 3 次请求失败以后，容器就被认为是不健康的并被终止掉。 Envoy 容器的管理员接口提供了一个 /ready 入口，可以返回其健康状态，因此 envoy 容器的 liveness probe 的目标可以是容器的 admin 端口即 9901。参数 initialDelaySeconds 表示容器启动后到发起第一个检测请求之间的等待时间，periodSeconds 表示两次连续的检测请求之间的时间间隔，timeoutSeconds 表示多长时间以后没有响应则认定此次检测失败，failureThreshold 则表示连续多少次检测失败以后才认定容器失效并重启。 观察 liveness probe 的效果运行 kubectl apply 命令应用上述清单文件并通过 kubectl port-forward 命令启用端口转发，启动该 Pod 并令其能够被访问：123456789$ kubectl apply -f kubia-liveness.ymlpod/kubia-liveness created$ kubectl port-forward kubia-liveness 8080 8443 9901Forwarding from 127.0.0.1:8080 -&gt; 8080Forwarding from [::1]:8080 -&gt; 8080Forwarding from 127.0.0.1:8443 -&gt; 8443Forwarding from [::1]:8443 -&gt; 8443Forwarding from 127.0.0.1:9901 -&gt; 9901Forwarding from [::1]:9901 -&gt; 9901 Pod 启动成功以后，liveness probe 会在初始等待时间过后持续向 Pod 中的容器发起检测请求，其检测结果只会在容器的 log 中看到。kubia 容器中的 Node.js 应用会在每次处理 HTTP 请求时向标准输出打印记录，这些请求也包括 liveness probe 的检测请求。因此可以打开一个新的命令行窗口，使用如下命令查看请求记录：12345678910$ kubectl logs kubia-liveness -c kubia -fKubia server starting...Local hostname is kubia-livenessListening on port 8080Received request for / from ::ffff:172.17.0.1Received request for / from ::ffff:172.17.0.1Received request for / from ::ffff:172.17.0.1Received request for / from ::ffff:172.17.0.1Received request for / from ::ffff:172.17.0.1... envoy 容器的 liveness probe 被配置成向其管理员接口发送 HTTP 请求，这些请求被记录在 /var/log/envoy.admin.log 文件中。可以使用如下命令查看：12345$ kubectl exec kubia-liveness -c envoy -- tail -f /var/log/envoy.admin.log[2022-01-02T18:34:59.818Z] "GET /ready HTTP/1.1" 200 - 0 5 0 - "172.17.0.1" "kube-probe/1.22" "-" "172.17.0.3:9901" "-"[2022-01-02T18:35:04.818Z] "GET /ready HTTP/1.1" 200 - 0 5 0 - "172.17.0.1" "kube-probe/1.22" "-" "172.17.0.3:9901" "-"[2022-01-02T18:35:09.818Z] "GET /ready HTTP/1.1" 200 - 0 5 0 - "172.17.0.1" "kube-probe/1.22" "-" "172.17.0.3:9901" "-"[2022-01-02T18:35:14.818Z] "GET /ready HTTP/1.1" 200 - 0 5 0 - "172.17.0.1" "kube-probe/1.22" "-" "172.17.0.3:9901" "-" 观察失败的 liveness probe可以尝试手动令 liveness probe 的检测请求失败。先在一个新的窗口中运行 kubectl get events -w 命令，方便后续观察检测失败时输出的事件信息。访问 Envoy 容器的管理员接口，手动配置其健康状态为 fail：12$ curl -X POST localhost:9901/healthcheck/failOK 此时转到观察事件信息的命令行窗口，发现连续输出了 3 次 probe failed 503 错误，之后 envoy 容器开始重启：12345678910kubectl get events -wLAST SEEN TYPE REASON OBJECT MESSAGE...0s Warning Unhealthy pod/kubia-liveness Liveness probe failed: HTTP probe failed with statuscode: 5030s Warning Unhealthy pod/kubia-liveness Liveness probe failed: HTTP probe failed with statuscode: 5030s Warning Unhealthy pod/kubia-liveness Liveness probe failed: HTTP probe failed with statuscode: 5030s Normal Killing pod/kubia-liveness Container envoy failed liveness probe, will be restarted0s Normal Pulled pod/kubia-liveness Container image "luksa/kubia-ssl-proxy:1.0" already present on machine0s Normal Created pod/kubia-liveness Created container envoy0s Normal Started pod/kubia-liveness Started container envoy exec &amp; tcpSocket liveness probe添加 tcpSocket liveness probe对于接收非 HTTP 请求的应用，可以配置 tcpSocket liveness probe。一个 tcpSocket liveness probe 的示例配置如下：12345livenessProbe: tcpSocket: port: 1234 periodSeconds: 2 failureThreshold: 1 该 probe 会检查容器的 1234 端口是否打开，每隔 2s 检查一次，一次检查失败则认定该容器是不健康的并重启它。 exec liveness probe不接受 TCP 连接的应用可以配置一条命令去检测其状态。下面的示例配置会每隔 2s 运行 /usr/bin/healthcheck 命令，检测容器中的应用是否仍在运行：1234567livenessProbe: exec: command: - /usr/bin/healthcheck periodSeconds: 2 timeoutSeconds: 1 failureThreshold: 1 startup probe默认的 liveness probe 配置会给应用 20 到 30s 的时间启动，如果应用需要更长的时间才能启动完成，容器会永远达不到 liveness probe 检测成功的状态，从而进入了无限重启的循环。为了防止上述情况发生，可以增大 initialDelaySeconds、periodSeconds 或 failureThreshold 的值，但也会有一定的副作用。periodSeconds * failureThreshold 的值越大，当应用不健康时重启的时间就越长。 Kubernetes 还提供了一种 startup probe。当容器配置了 startup probe 时，容器启动时只有 startup probe 会执行。startup probe 可以按照应用的启动时间配置，检测成功之后 Kubernetes 会切换到使用 liveness probe 检测。比如 Node.js 应用需要 1 分钟以上的时间启动，当启动成功以后若应用状态不正常，则在 10s 以内重启。可以这样配置：123456789101112131415161718containers:- name: kubia image: luksa/kubia:1.0 ports: - name: http containerPort: 8080 startupProbe: httpGet: path: / port: http periodSeconds: 10 failureThreshold: 12 livenessProbe: httpGet: path: / port: http periodSeconds: 5 failureThreshold: 2 上面配置的效果如下图： 应用有 120s 的时间启动。Kubernetes 一开始每隔 10s 发起 startup probe 请求，最多尝试 12 次。不同于 liveness probe，startup probe 失败是很正常的，只是说明应用还未成功启动。一旦某次 startup probe 检测返回成功状态，Kubernetes 就会立即切换到 liveness probe 模式，通常拥有更短的检测间隔，能够对未响应应用做出更快速的反应。 在容器启动或关闭时触发动作可以向容器中添加 lifecycle hooks。Kubernetes 目前支持两种类型的钩子： Post-start hooks：在容器启动后执行 Pre-stop hooks：在容器停止前执行 post-start hookspost-start lifecycle hook 会在容器创建完成之后立即触发。可以使用 exec 类型的钩子在主进程启动的同时执行一个额外的程序，或者 httpGet 类型的钩子向容器中运行的应用发送 HTTP 请求，以完成初始化或预热操作。假如你是应用的作者，类似的操作当然可以加入到应用本身的代码中。但对于一个已经存在的并非自己创建的应用，就有可能无法做到。post-start hook 提供了一种不需要修改应用或容器镜像的替代方案。 post-start hook 在容器中执行命令123456789101112131415161718apiVersion: v1kind: Podmetadata: name: fortune-poststartspec: containers: - name: nginx image: nginx:alpine lifecycle: postStart: exec: command: - sh - -c - "apk add fortune &amp;&amp; fortune &gt; /usr/share/nginx/html/quote" ports: - name: http containerPort: 80 上述清单文件定义的 Pod 名为 fortune-poststart，包含一个基于 nginx:alpine 镜像的容器，同时定义了一个 postStart 钩子。该钩子会在 Nginx 服务启动时执行以下命令：sh -c &quot;apk add fortune &amp;&amp; fortune &gt; /usr/share/nginx/html/quote&quot; postStart 这个名称其实有些误导作用，它并不是在主进程完全启动后才开始执行，而是在容器创建后，几乎和主进程同时执行。 12345$ kubectl apply -f fortune-poststart.ymlpod/fortune-poststart unchanged$ kubectl port-forward fortune-poststart 8080:80Forwarding from 127.0.0.1:8080 -&gt; 80Forwarding from [::1]:8080 -&gt; 80 打开一个新的命令行窗口使用 curl 命令测试效果：12345$ curl localhost:8080/quoteThe Official MBA Handbook on business cards: Avoid overly pretentious job titles such as "Lord of the Realm, Defender of the Faith, Emperor of India" or "Director of Corporate Planning." post-startup hook 对容器的影响虽然 post-start hook 相对于容器的主进程以异步的方式执行，它还是会对容器产生两个方面的影响。首先，在 post-start hook 的执行过程中容器会一直处于 Waiting 状态，原因显示为 ContainerCreating，直到 hook 执行完毕。其次，若 hook 绑定的命令无法执行或返回了一个非零的状态值，则整个容器会被重启。 在容器终止前执行命令前面 fortune Pod 中的 Nginx 服务在收到 TERM 信号后会立即关闭所有打开的连接并终止进程，这并不是理想的操作，不会等待正在处理的客户端请求彻底完成。可以使用 pre-stop hook 执行 nginx -s quit 命令舒缓地关闭 Nginx 服务。示例配置如下：1234567lifecycle: preStop: exec: command: - nginx - -s - quit 假如某个 pre-stop hook 执行失败，只会在 Pod 的 events 消息中显示一条 FailedPreStopHook 警告信息，并不影响容器继续被终止。 理解容器的生命周期一个 Pod 的生命周期可以被分成如下三个阶段： 初始化阶段：Pod 的 init 容器从开始运行到启动完成 运行阶段：Pod 的普通容器从开始运行到启动完成 终止阶段：Pod 的所有容器被终止运行 初始化阶段Pod 中的初始化容器会最先运行，按照 spec 的 initContainers 字段中定义的顺序。第一个初始化容器的镜像被下载到工作节点并启动，完成后继续拉取第二个初始化容器的镜像，直到所有的初始化容器都成功运行。若某个初始化容器因为某些错误启动失败，且其重启策略设置为 Always 或 OnFailure，则该失败的容器自动重启。若其重启策略设置为 Never，则 Pod 的状态显示为 Init:Error，必须删除并重新创建 Pod 对象。 运行阶段当所有的初始化容器成功运行后，Pod 的普通容器开始以并行的方式创建（需要注意的是，容器的 post-start hook 会阻塞下一个容器的创建）。 termination grace period容器中的应用都有一个固定的关闭时间做缓冲用，定义在 spec 下的 terminationGracePeriodSeconds 字段中，默认是 30s。该时间从 pre-stop hook 触发或收到 TERM 信号时开始计算，若时间过了进程仍在运行，应用会收到 KILL 信号被强制关闭。 终止阶段Pod 的容器以并行的方式终止。对每个容器来说，pre-stop hook 触发，然后主进程接收 TERM 信号，如果应用关闭的时间超过了 terminationGracePeriodSeconds，就发送 KILL 信号给容器的主进程。在所有的容器都被终止以后，Pod 对象被删除。 可以在删除一个 Pod 时手动指定一个新的时间覆盖 terminationGracePeriodSeconds 的值，如：kubectl delete po kubia-ssl --grace-period 10 Pod 完整生命周期图示 参考资料Kubernetes in Action, Second Edition]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Server</tag>
        <tag>Docker</tag>
        <tag>Container</tag>
        <tag>Kubernetes</tag>
        <tag>Administrator</tag>
        <tag>Pod</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kubernetes in Action 笔记 —— 深入了解 Pod 概念]]></title>
    <url>%2F2021%2F12%2F29%2Fkubernetes-in-action-reading-notes-understanding-pods%2F</url>
    <content type="text"><![CDATA[下图展示了 Kubernetes 如何通过 Deployment、Pod、Service 三种对象部署一个最小化应用。 其中的 Pod 对象是 Kubernetes 中最重要的一个核心概念，它代表着一个处于运行状态的应用实例。 理解 PodsPod 可以包含一个或多个有协作关系的容器，是 Kubernetes 中最基本的构造单位。 容器只能借助 Pod 进行部署，无法独立运行 一个 Pod 可以包含多个容器，但只包含一个容器的 Pod 也是很常见的 Pod 中的多个容器只允许在同一个工作节点上运行，即单个 Pod 不能跨节点部署 Pod 机制的优势为什么要在一个 Pod 中运行多个容器，而不是在一个容器中运行多个进程。 一个容器不应该包含多个进程容器就像是一台隔离的虚拟机，因此是可以同时运行多个进程的。但这样会使得容器难于管理。容器被设计成只会运行一个进程（子进程不计算在内），大多数容器管理工具也都是基于这个原则去设计的。比如容器中运行的进程有时候会向标准输出打印其日志信息，Kubernetes 中查看日志的命令就只会显示从这个输出中捕获到的内容。如果容器中同时运行多个进程，都向外输出日志信息，Kubernetes 捕获到的日志内容就会变得错综复杂。 另一个原因在于，容器运行时只会在容器的根进程挂掉时重启该容器。为了充分利用容器运行时提供的特性，应该在每个容器中只运行一个进程。 Pod 如何组合多个容器不应该在同一个容器中运行多个进程。与此同时，将分散在多个容器中相互关联的进程结合成一个单位，统一进行管理也是很有必要的。因此引入了 Pod 机制。 Pod 可以同时运行多个关系紧密的进程（容器），给它们（几乎）相同的运行环境，使得它们就像是运行在同一个容器中那样。这些进程是相互独立、隔离的，但也会共享某些资源。因而既可以使用容器提供的各种隔离特性，又能够促使多个进程相互协作。 如上图所示，Pod 中的多个容器共享同一个 Network 命名空间，因而共享该命名空间的网络接口、IP 地址、端口空间等。这些容器能够使用 loopback 设备进行通信，但不能绑定同一个网络端口。 同一个 Pod 中的多个容器还会共享相同的 UTS 命名空间，因而能看到同一个主机名；共享相同的 IPC 命名空间，因而不同容器中的多个进程之间可以通过 IPC 进行通信；还可以配置成共享同一个 PID 命名空间，使得这些容器中的进程使用同一个进程树。与前面的内容相反，每个 Pod 总是有自己的 Mount 命名空间，这意味着每个 pod 都有自己的文件系统。当然，假如 Pod 中有两个容器需要共享部分文件系统，也可以向该 Pod 添加 Volume，作为共享存储挂载到每一个容器上。 单容器 or 多容器可以将 Pod 看作一台独立的计算机。不同于虚拟机通常需要同时运行多个应用，每个 Pod 中一般只运行一个应用。 Pod 几乎没有额外的资源开销，因而在同一个 Pod 中运行多个应用并不是必须的。 假如一个系统的前端和后端运行在同一个 Pod 中，而单个 Pod 不能跨节点存在（即单个 Pod 只能部署在某一个节点上）。如果你有一个双节点的集群且只创建了一个 Pod，则该 Pod 只会运行在其中一个节点上，造成 CPU、内存、存储和带宽等资源的闲置。将前后端分别部署到不同的 Pod 中则能够提高硬件的利用率。 另一个不将多个应用部署到同一个 Pod 中的原因与横向扩展有关。Pod 不仅仅是部署的基本单位，也是扩展的基本单位。当通过修改 Deployment 对象扩展应用时，Kubernetes 并不会复制 Pod 中的容器，而是直接创建新的 Pod 实例（应用副本）。前端和后端组件通常有着不同的扩展需求，基本上都会独立地进行扩展。而同一个 Pod 中不同组件的扩展是同步的。如果一个容器中的某个组件相对于其他组件需要独立地进行扩展，该应用就必须部署在另一个 Pod 中。 将多个容器放置在同一个 Pod 中的唯一场景，就是某个应用包含一个基础进程以及一个或者多个对基础进程有补充作用的附加进程。运行附加进程称为 sidecar container。 比如某个 Pod 包含一个运行 Node.js 应用的容器，而该 Node.js 应用只支持 HTTP 协议。为了令其支持 HTTPS，可以对 JavaScript 代码做一些小的改动。其实也可以在不改动应用代码的情况下完成此需求。只需要向 Pod 中再添加一个反向代理容器，将 HTTPS 流量转发成 HTTP 传给 Node.js 容器。 另一个例子如下图所示，基础容器运行一个 Web 服务，其资源文件存放在挂载的 Volume 上。Pod 中的另一个容器则挂载了同一个 Volume，作为 Agent 定期从外部拉取新的资源文件，存储在 Volume 供 Web 服务使用。 从 YAML 文件创建 Pod可以使用 kubectl create 命令创建 Pod 对象（Kubernetes in Action 笔记 —— 部署第一个应用），但更常见的方式是创建一个 JSON 或 YAML 格式的清单文件，描述整个应用的架构，再将其发送给 Kubernetes API 来生成 Pod 等对象。 比如下面的 kubia.yml 文件：12345678910apiVersion: v1kind: Podmetadata: name: kubiaspec: containers: - name: kubia images: luksa/kubia:1.0 ports: - containerPort: 8080 创建 Pod 对象可以使用 kubectl apply -f xxx.yml 命令将清单文件发送给 API 并应用到 Kubernetes 集群：12$ kubectl apply -f kubia.ymlpod/kubia created kubectl apply 命令不仅仅用于创建对象，同时也可以对现有的对象进行修改。比如 Pod 对象创建之后需要对其做一些改动，可以直接编辑 yml 文件，再运行一遍 kubectl apply 命令即可。但有些描述 Pod 的字段是不可变的，因而更新操作有可能会失败，此时就可以先删除后再重建。 检查新创建的 Pod使用 kubectl get pod 命令获取某个 Pod 的汇总信息：123$ kubectl get pod kubiaNAME READY STATUS RESTARTS AGEkubia 1/1 Running 0 6m37s 想获取更详细一点的信息可以加上 -o wide 选项：123$ kubectl get pod kubia -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESkubia 1/1 Running 0 8m3s 172.17.0.3 minikube &lt;none&gt; &lt;none&gt; 或者使用 kubectl describe pod 命令：1234567891011121314151617181920212223242526$ kubectl describe pod kubiaName: kubiaNamespace: defaultPriority: 0Node: minikube/192.168.49.2Start Time: Fri, 24 Dec 2021 15:08:57 +0800Labels: &lt;none&gt;Annotations: &lt;none&gt;Status: RunningIP: 172.17.0.3IPs: IP: 172.17.0.3Containers: kubia: Container ID: docker://aa8778766a69122de8c8b401922d2318b15b8aaf220945d747e494de2fda9199 Image: luksa/kubia:1.0 Image ID: docker-pullable://luksa/kubia@sha256:a961dc8f377916936fa963508726d77cf77dcead5c97de7e5361f0875ba3bef7 Port: 8080/TCP...Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 9m28s default-scheduler Successfully assigned default/kubia to minikube Normal Pulled 9m27s kubelet Container image "luksa/kubia:1.0" already present on machine Normal Created 9m27s kubelet Created container kubia Normal Started 9m27s kubelet Started container kubia 与 Pod 进行交互向 Pod 中的应用发起请求可以使用 kubectl expose 命令创建一个 Service 对象，给 Pod 分配一个负载均衡器，从而可以从外部访问 Pod 中运行的应用。但是对于开发、测试和调试等目的，有可能需要直接与 Pod 中的应用进行交互。每个 Pod 都会自动绑定一个 IP 地址，从而可以被集群中的其他 Pod 访问，但该 IP 地址是仅限于集群内部的。 获取 Pod 的 IP 地址123$ kubectl get pod kubia -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESkubia 1/1 Running 0 58m 172.17.0.3 minikube &lt;none&gt; &lt;none&gt; 从工作节点连接 PodKubernetes 的网络模型支持从任意节点直接访问集群中所有节点上的任何 Pod。因而可以先登录某一个工作节点，再从该节点访问集群中的 Pod。对于 Minikube 而言，可以使用 minikube ssh 命令登录工作节点。12$ minikube sshdocker@minikube:~$ 登录成功后，即可使用内部 IP 地址访问 kubia Pod：12docker@minikube:~$ curl 172.17.0.3:8080Hey there, this is kubia. Your IP is ::ffff:172.17.0.1. One-off 客户端 Pod第二种测试应用连通性的方法是创建一个临时的 Pod 并运行 curl 命令。123$ kubectl run --image=curlimages/curl -it --restart=Never --rm client-pod curl 172.17.0.2:8080Hey there, this is kubia. Your IP is ::ffff:172.17.0.4.pod "client-pod" deleted 上面的命令会从 curlimages/curl 镜像创建一个容器，执行 curl 172.17.0.2:8080 命令。其中 -it 选项会将当前的终端与容器的标准输入输出绑定，--restart=Never 选项确保 curl 命令执行完后容器直接停掉，--rm 选项负责在最后删除 Pod。 这种方式在测试 Pod 与 Pod 之间连通性的时候非常有用。 通过 Kubectl 端口转发访问 Pod在开发过程中，最简单的访问容器中应用的方法是使用 kubectl port-forward 命令。该命令可以通过绑定到本地机器端口上的代理来访问特定的 Pod。此方式甚至不需要知道 Pod 的 IP 地址，只需要指定 Pod 的名字和端口号即可：123$ kubectl port-forward kubia 8080Forwarding from 127.0.0.1:8080 -&gt; 8080Forwarding from [::1]:8080 -&gt; 8080 上述命令会在本地机器的 8080 端口开启一个代理，指向 kubia 容器的 8080 端口。此时打开另一个命令行窗口，运行 curl localhost:8080 命令即等同于访问 kubia 容器的 8080 端口：12$ curl localhost:8080Hey there, this is kubia. Your IP is ::ffff:127.0.0.1. 查看应用日志不同于将日志写入到文件中，容器化应用通常会将日志输出到标准输出（stdout）和标准错误输出（stderr）。这使得容器运行时能够拦截应用的日志输出，将其转存在固定的位置（通常是 /var/log/containers），而不需要知道容器中日志文件的保存位置。当使用 Kubernetes 运行应用时，可以登录到工作节点通过 docker logs 命令查看应用的日志，但更简单的方式是直接在本地使用 kubectl logs 命令。 获取某个 Pod 的日志123456789$ kubectl logs kubiaKubia server starting...Local hostname is kubiaListening on port 8080Received request for / from ::ffff:172.17.0.1Received request for / from ::ffff:172.17.0.1Received request for / from ::ffff:172.17.0.4Received request for / from ::ffff:172.17.0.4Received request for / from ::ffff:127.0.0.1 加上 -f (--follow) 选项可以实时显示日志输出。 在日志输出中显示时间戳123456789$ kubectl logs kubia --timestamps=true2021-12-27T13:40:21.606420000Z Kubia server starting...2021-12-27T13:40:21.606992200Z Local hostname is kubia2021-12-27T13:40:21.607003000Z Listening on port 80802021-12-27T13:43:11.228907300Z Received request for / from ::ffff:172.17.0.12021-12-27T13:47:45.648686300Z Received request for / from ::ffff:172.17.0.12021-12-27T13:59:16.819436200Z Received request for / from ::ffff:172.17.0.42021-12-27T14:01:19.101277000Z Received request for / from ::ffff:172.17.0.42021-12-27T14:34:40.145715500Z Received request for / from ::ffff:127.0.0.1 此外还可以根据时间筛选日志输出，比如只显示最近一段时间内的日志。kubectl logs kubia --since=2h 显示最近 2 小时内输出的日志内容。 也可以通过 --since-time 选项筛选特定时间后输出的日志：kubectl logs kubia –-since-time=2020-02-01T09:50:00Z 或者直接使用 --tail=n 选项显示最近的 n 条日志。 PSKubernetes 会为每一个容器都保留一个独立的日志文件。它们通常保存在容器运行节点的 /var/log/containers 路径下，容器重启后其日志会写入到一个新的文件中。 如果某些容器应用将其日志写入到文件中而不是 stdout，理想情况下，应该配置一个中心化的日志系统定期收集这些日志。如果只是想简单地手动访问容器中的日志文件，可以参考后面的内容。 容器的文件传输有些时候需要向运行的容器中添加文件，或者从容器中获取文件。虽然修改容器中的文件的场景并不多见（尤其在生产环境中），但在开发过程中还是有一定用处的。 Kubernetes 提供了 cp 命令，能够从本地磁盘复制文件或目录到任意 Pod，或者相反方向。kubectl cp kubia:/etc/hosts /tmp/kubia-hosts 将 kubia 容器中的 /etc/hosts 文件复制到本地 /tmp 路径下。 kubectl cp /path/to/local/file kubia:path/in/container 将本地文件复制到 kubia 容器。 在运行的容器中执行命令运行单个命令可以使用 kubernetes exec 命令运行容器文件系统中的某个可执行文件。用户远程执行命令，无需登录到对应的工作节点上。1234$ kubectl exec kubia -- ps auxUSER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMANDroot 1 0.0 0.2 564436 30216 ? Ssl 13:40 0:00 node app.jsroot 13 5.0 0.0 36632 2660 ? Rs 15:54 0:00 ps aux 前面的章节中曾经通过一个 one-off 客户端容器运行 curl 命令，向应用发送请求。实际上也可以直接在应用容器中运行 curl 命令：12$ kubectl exec kubia -- curl -s localhost:8080Hey there, this is kubia. Your IP is ::ffff:127.0.0.1. 开启一个交互式 Shell如果想要交互式地在容器中运行多条命令，可以开启一个 Shell：12$ kubectl exec -it kubia -- bashroot@kubia:/# 之后就可以像在本地运行 Linux 命令一样在容器中执行命令了。 需要注意的是，为了保证容器的镜像足够小同时提高安全性，生产环境中使用的容器通常只包含运行应用所需要的可执行文件。这会大大减少潜在的攻击目标，同时也意味着用户无法使用 Shell 或其他 Linux 工具。 在 Pod 中运行多个容器前面的 kubia 容器只支持 HTTP 协议，可以为其添加 TLS 支持。其实有一种简单的不需要修改现有代码的方案，就是在现有的 Node.js 容器旁边添加一个提供反向代理功能的 sidecar 容器。 用 Envoy 代理扩展 kubia 应用简单来说，Pod 中包含两个容器，其中 Node.js 容器仍然负责处理 HTTP 请求，而新加的 Envoy 容器负责转发 HTTPS 请求。 创建如下清单文件 kubia-ssl.yml：123456789101112131415161718apiVersion: v1kind: Podmetadata: name: kubia-sslspec: containers: - name: kubia image: luksa/kubia:1.0 ports: - name: http containerPort: 8080 - name: envoy image: luksa/kubia-ssl-proxy:1.0 ports: - name: https containerPort: 8443 - name: admin containerPort: 9901 运行 kubectl apply -f kubia-ssl.yml 命令应用清单文件创建 Pod，之后等待创建完成即可。可以运行 kubectl describe pod kubia-ssl 命令查看创建的进度。 与两个容器的 Pod 进行交互使用 kubectl port-forward 命令启用端口转发：1234567$ kubectl port-forward kubia-ssl 8080 8443 9901Forwarding from 127.0.0.1:8080 -&gt; 8080Forwarding from [::1]:8080 -&gt; 8080Forwarding from 127.0.0.1:8443 -&gt; 8443Forwarding from [::1]:8443 -&gt; 8443Forwarding from 127.0.0.1:9901 -&gt; 9901Forwarding from [::1]:9901 -&gt; 9901 打开一个新的命令行窗口，使用 curl 命令测试与 kubia-ssl 应用的连通性：1234$ curl localhost:8080Hey there, this is kubia-ssl. Your IP is ::ffff:127.0.0.1.$ curl https://localhost:8443 --insecureHey there, this is kubia-ssl. Your IP is ::ffff:127.0.0.1. 查看日志由于 kubia-ssl 包含两个容器，因此查看日志时需要使用 --container 或 -c 选项指定容器的名称。12345678910$ kubectl logs kubia-ssl -c kubiaKubia server starting...Local hostname is kubia-sslListening on port 8080Received request for / from ::ffff:127.0.0.1Received request for / from ::ffff:127.0.0.1$ kubectl logs kubia-ssl -c envoy[2021-12-28 15:08:51.671][1][info][main] [source/server/server.cc:255] initializing epoch 0 (hot restart version=11.104)[2021-12-28 15:08:51.671][1][info][main] [source/server/server.cc:257] statically linked extensions:... 或者使用 kubectl logs kubia-ssl --all-containers 命令显示所有容器的日志 删除 Pod 和其他对象删除 kubia Pod：12$ kubectl delete pod kubiapod "kubia" deleted kubectl delete 命令默认会等待删除操作彻底完成后才退出，可以加上 --wait=false 异步执行此命令。 删除清单文件中定义的对象：12$ kubectl delete -f kubia-ssl.ymlpod "kubia-ssl" deleted 删除所有 Pod：kubectl delete po --all删除所有对象：kubectl delete all --all 参考资料Kubernetes in Action, Second Edition]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Server</tag>
        <tag>Docker</tag>
        <tag>Container</tag>
        <tag>Kubernetes</tag>
        <tag>Pod</tag>
        <tag>Administration</tag>
        <tag>MicroService</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kubernetes in Action 笔记 —— 部署第一个应用]]></title>
    <url>%2F2021%2F12%2F14%2Fkubernetes-in-action-reading-notes-deploying-first-application%2F</url>
    <content type="text"><![CDATA[Minikube &amp; kubectlMinikube 是一个能够在本地环境搭建 Kubernetes 集群的工具，支持 Windows、Linux 和 MacOS 等平台，由 Kubernetes 社区进行维护。它通常在 Linux 虚拟机中运行 Kubernetes。如果宿主机是基于 Linux 的系统，也可以通过 Docker 实现。即为了运行 Minikube，需要先安装 Hypervisor 比如 Virtualbox；对于 Linux 系统，也可以直接使用 Docker。 具体的安装配置步骤可以参考官方文档 Getting Started Guide。 kubectl 是一个命令行工具，能够向 Kubernetes 集群发送命令并执行，支持的功能包括部署应用、查询和管理资源、查看日志等。安装步骤可参考官方文档 Install Tools。 部署应用通常情况下，部署应用时要准备一个 JSON 或者 YAML 文件，里面包含对该应用的所有组件的描述信息，再把该描述文件应用到 Kubernetes 集群。从演示的角度来看，也可以通过单行命令的方式部署简单的应用。 创建 deployment可以使用 kubectl create deployment 命令部署应用。12$ kubectl create deployment kubia --image=luksa/kubia:1.0deployment.apps/kubia created 其中 kubia 表示创建的 deployment 对象的名称，luksa/kubia:1.0 指代需要使用的容器镜像。 kubia 对象的存在告诉 Kubernetes luksa/kubia:1.0 容器必须运行在集群中。它定义了一种用户期待的状态，而 Kubernetes 负责确保实际的状态一定会满足该期望。 kubectl get deployment 命令可以列出当前集群中存在的所有 deployment 对象及其状态。123$ kubectl get deploymentNAME READY UP-TO-DATE AVAILABLE AGEkubia 0/1 1 0 5m17s Pods容器并不是 Kubernetes 中部署的最小单位。不同于直接部署独立的容器，Kubernetes 实际上会部署一组相互关联的容器，称为 pod。pod 包含一组一个或一个以上关系密切的容器实例，同时运行在同一个工作节点上，并共享特定的 Linux 命名空间。同一个 pod 中的容器共享相同的网络和 UTS 命名空间，因而共享同样的网络接口、IP 地址、端口空间和主机名等。也可以在描述文件中定义其他需要共享的命名空间。 每个 pod 都有自己的 IP、机器名、进程、网络接口以及其他资源。同一个 pod 中的容器都会将自己看作是 pod 中唯一运行的容器，它们并不能看到其他容器中的进程。 创建 Deployment 对象后就表示已经部署了 pod，Kubernetes 会基于 Deployment 对象创建一个或多个 pod。可以使用 kubectl get pods 来列出系统中的 pod：123$ kubectl get podsNAME READY STATUS RESTARTS AGEkubia-767f9bc59d-77d2z 1/1 Running 0 41m 如果某些 issue 导致 pod 运行失败，或者单纯想查看更多 pod 相关的信息，可以使用 kubectl describe pod 命令：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253$ kubectl describe podName: kubia-767f9bc59d-77d2zNamespace: defaultPriority: 0Node: minikube/192.168.49.2Start Time: Tue, 14 Dec 2021 11:16:58 +0800Labels: app=kubia pod-template-hash=767f9bc59dAnnotations: &lt;none&gt;Status: RunningIP: 172.17.0.3IPs: IP: 172.17.0.3Controlled By: ReplicaSet/kubia-767f9bc59dContainers: kubia: Container ID: docker://e9bd5cf8f2eb15959c08bc8f154742b7194030d8ce0f9e6290cd80fc21b48692 Image: luksa/kubia:1.0 Image ID: docker-pullable://luksa/kubia@sha256:a961dc8f377916936fa963508726d77cf77dcead5c97de7e5361f0875ba3bef7 Port: &lt;none&gt; Host Port: &lt;none&gt; State: Running Started: Tue, 14 Dec 2021 11:26:25 +0800 Ready: True Restart Count: 0 Environment: &lt;none&gt; Mounts: /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-n9n9b (ro)Conditions: Type Status Initialized True Ready True ContainersReady True PodScheduled TrueVolumes: kube-api-access-n9n9b: Type: Projected (a volume that contains injected data from multiple sources) TokenExpirationSeconds: 3607 ConfigMapName: kube-root-ca.crt ConfigMapOptional: &lt;nil&gt; DownwardAPI: trueQoS Class: BestEffortNode-Selectors: &lt;none&gt;Tolerations: node.kubernetes.io/not-ready:NoExecute op=Exists for 300s node.kubernetes.io/unreachable:NoExecute op=Exists for 300sEvents: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 11m default-scheduler Successfully assigned default/kubia-767f9bc59d-77d2z to minikube Normal Pulling 11m kubelet Pulling image "luksa/kubia:1.0" Normal Pulled 115s kubelet Successfully pulled image "luksa/kubia:1.0" in 9m24.3380576s Normal Created 113s kubelet Created container kubia Normal Started 113s kubelet Started container kubia 输出的最后就包含 pod 创建和启动时触发的一系列事件（Events）。 Pods 的创建流程 运行 kubectl create deployment 命令，向 Kubernetes API Server 发送 HTTP 请求，创建一个新的 Deployment 对象 之后 Kubernetes 创建一个新的 Pod 对象，该 Pod 对象被分配给某个工作节点 工作节点上的 Kubelet agent 得知新的 Pod 对象被创建，且分配给了自己。于是 Kubelet 控制 Docker 拉取特定的镜像并创建、运行容器 向外部暴露应用应用已经成功运行了，接下来就是控制它如何被外部访问。每个 pod 都会获得一个专属的 IP 地址，但该地址是只有集群内部可见的。为了使 pod 能够从外部访问，还需要创建一个 Service 对象。 Service 对象有好几种类型，其中一种 LoadBalancer 会生成一个外部的负载均衡器，令服务能够从集群外部访问。可以使用 kubectl expose 命令创建 Service：12$ kubectl expose deployment kubia --type=LoadBalancer --port 8080service/kubia exposed 使用 kubectl get svc 命令查看当前系统中存在的 Service：1234$ kubectl get svcNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkubernetes ClusterIP 10.96.0.1 &lt;none&gt; 443/TCP 29dkubia LoadBalancer 10.106.111.39 &lt;pending&gt; 8080:30077/TCP 2m28s 创建 LoadBalancer 服务时，正常情况下 Kubernetes 会访问云服务提供商，令其创建负载均衡器并获取公共 IP。Minikube 是本地模拟的集群环境，因而无法完成上述操作。kubia Service 的 EXTERNAL-IP 会一直处于 状态。 在没有获取到外部 IP 的情况下，minikube 可以使用下面的方法获取服务的 url：123456789$ minikube service kubia --url🏃 Starting tunnel for service kubia.|-----------|-------|-------------|------------------------|| NAMESPACE | NAME | TARGET PORT | URL ||-----------|-------|-------------|------------------------|| default | kubia | | http://127.0.0.1:39529 ||-----------|-------|-------------|------------------------|http://127.0.0.1:39529❗ Because you are using a Docker driver on linux, the terminal needs to be open to run it. 打开一个新的命令行窗口，可以成功访问上面的 url：12$ curl http://127.0.0.1:39529Hey there, this is kubia-767f9bc59d-77d2z. Your IP is ::ffff:172.17.0.1. LoadBalancer 的创建流程 横向扩展应用在容器中部署应用的一个主要好处就是，横向扩展应用变得非常简单和直观。可以使用下列命令扩展 kubia 应用，令其同时运行 3 个实例副本。12345$ kubectl scale deployment kubia --replicas=3deployment.apps/kubia scaled$ kubectl get deployNAME READY UP-TO-DATE AVAILABLE AGEkubia 3/3 3 3 3h58m 此时共有 3 个 pod 实例运行：12345$ kubectl get podsNAME READY STATUS RESTARTS AGEkubia-767f9bc59d-77d2z 1/1 Running 0 3h59mkubia-767f9bc59d-rvsdq 1/1 Running 0 3m56skubia-767f9bc59d-sfn42 1/1 Running 0 3m56s 可以加上 -o wide 选项获取更详细的 pods 信息，比如 IP、运行的节点等：12345kubectl get pods -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESkubia-767f9bc59d-77d2z 1/1 Running 0 4h 172.17.0.3 minikube &lt;none&gt; &lt;none&gt;kubia-767f9bc59d-rvsdq 1/1 Running 0 5m31s 172.17.0.5 minikube &lt;none&gt; &lt;none&gt;kubia-767f9bc59d-sfn42 1/1 Running 0 5m31s 172.17.0.4 minikube &lt;none&gt; &lt;none&gt; 再次访问 Service 的 URL，可以看到多次访问返回的信息并不一样，可以证实后台提供服务的 pod 并不是同一个，而是 3 个 pod 轮流接收请求并提供服务：12345678$ curl http://127.0.0.1:39529Hey there, this is kubia-767f9bc59d-sfn42. Your IP is ::ffff:172.17.0.1.$ curl http://127.0.0.1:39529Hey there, this is kubia-767f9bc59d-77d2z. Your IP is ::ffff:172.17.0.1.$ curl http://127.0.0.1:39529Hey there, this is kubia-767f9bc59d-sfn42. Your IP is ::ffff:172.17.0.1.$ curl http://127.0.0.1:39529Hey there, this is kubia-767f9bc59d-rvsdq. Your IP is ::ffff:172.17.0.1. 负载均衡架构示意图： 总结 部署应用可以使用 kubectl create deployment 命令，暴露应用使用 kubectl expose deployment 命令，横向扩展应用使用 kubectl scale deployment 命令。 应用部署的基本单位不是容器而是 pod，一个 pod 可以包含一个或多个相互关联的容器。 Deployments、Services、Pods 和 Nodes 都是 Kubernetes 对象/资源。可以使用 kubectl get 命令获取这些对象的列表，或者使用 kubectl describe 命令获取对象的详细信息。 Deployment 对象负责部署指定数量的 pods。Services 对象则可以令这些 pods 能够通过一个单一的 IP 地址访问。 Service 在集群内部提供负载均衡。如果指定其类型为 LoadBalancer，则 Kubernetes 会请求云服务提供商令应用可以通过公共地址访问。 参考资料Kubernetes in Action, Second Edition]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Server</tag>
        <tag>Docker</tag>
        <tag>Container</tag>
        <tag>Kubernetes</tag>
        <tag>MicroService</tag>
        <tag>Deployment</tag>
        <tag>Cluster</tag>
        <tag>LoadBalance</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Uncle Bob 的 SOLID 软件设计原则——Python 实例讲解]]></title>
    <url>%2F2021%2F12%2F13%2Funcle-bob-SOLID-principles-in-python%2F</url>
    <content type="text"><![CDATA[SOLID 是 5 种软件设计原则的首字母缩写，由美国的软件工程师 Robert C. Martin（习惯上被称为 Uncle Bob）总结。可以帮助程序员写出更加灵活、容易理解、可维护性强、方便扩展的健壮代码。 S 代表 Single Responsibility Principle (SRP)，一个类应该只包含一种单一的职责，有且仅有一种原因能促使其变更。通俗点说，让一个类只做一件事。如果需要承担更多的工作，那么分解这个类。 O 代表 Open/Closed Principle (OCP)，软件实体应该对扩展是开放的，同时对修改是封闭的。如果需要添加额外的功能，应该优先扩展某个类而不是修改它。 L 代表 Liskov Substitution Principle (LSP)，程序中的对象应该能够替换为其子类型的实例，仍不影响代码的正确性。 I 代表 Interface Segregation Principle (ISP)，多个专门的基于客户端的接口要好于只有一个通用的接口。一个类对另一个类的依赖性应该建立在最小的接口上，客户端不应该被强迫实现一些他们不会使用的接口。 D 代表 Dependency Inversion Principle (DIP)，抽象不应该依赖于细节，细节应当依赖于抽象。即要针对抽象（接口）编程，而不是针对实现细节编程。 实例代码12345678910111213141516171819202122232425262728293031323334353637class Order: items = [] quantities = [] prices = [] status = "open" def add_item(self, name, quantity, price): self.items.append(name) self.quantities.append(quantity) self.prices.append(price) def total_price(self): total = 0 for i in range(len(self.prices)): total += self.quantities[i] * self.prices[i] return total def pay(self, payment_type, security_code): if payment_type == "debit": print("Processing debit payment type") print(f"Verifying security code: &#123;security_code&#125;") self.status = "paid" elif payment_type == "credit": print("Processing credit payment type") print(f"Verifying security code: &#123;security_code&#125;") self.status = "paid" else: raise Exception(f"Unknown payment type: &#123;payment_type&#125;")order = Order()order.add_item("Keyborad", 1, 50)order.add_item("SSD", 1, 150)order.add_item("USB cable", 2, 5)print(order.total_price())order.pay("debit", "0372846") 上述 Python 代码实现了一个简单的“购物车”（订单）应用。 add_item 方法可以向订单中添加新的货物 total_price 方法可以计算订单的总价 pay 方法实现了订单的支付功能，支持借记卡、信用卡等支付方式 Single Responsibility Principle单一职能原则。将支付功能从 Order 类中分离出来，在另一个 PaymentProcessor 类中实现。同时去掉 pay 方法中的 if-else 判断，分别用两个函数 pay_debit 和 pay_credit 实现。1234567891011121314151617181920212223242526272829303132333435363738class Order: items = [] quantities = [] prices = [] status = "open" def add_item(self, name, quantity, price): self.items.append(name) self.quantities.append(quantity) self.prices.append(price) def total_price(self): total = 0 for i in range(len(self.prices)): total += self.quantities[i] * self.prices[i] return totalclass PaymentProcessor: def pay_debit(self, order, security_code): print("Processing debit payment type") print(f"Verifying security code: &#123;security_code&#125;") order.status = "paid" def pay_credit(self, order, security_code): print("Processing credit payment type") print(f"Verifying security code: &#123;security_code&#125;") order.status = "paid"order = Order()order.add_item("Keyborad", 1, 50)order.add_item("SSD", 1, 150)order.add_item("USB cable", 2, 5)print(order.total_price())processor = PaymentProcessor()processor.pay_debit(order, "0372846") Open/Closed Principle在最新的支付功能的实现中，如果我们需要添加一个新的支付方法（比如 PayPal），就必须修改 PaymentProcessor 类的原始代码。这就违反了 Open/Closed 原则，额外的功能应该通过扩展而不是修改原来的类来实现。改进的方法是用一个基类（PaymentProcessor）来定义基本的支付逻辑，再通过子类（如 DebitPaymentProcessor）来实现具体的支付方法。这样每当添加一种新的支付方式，直接实现一个新的子类即可。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849from abc import ABC, abstractmethodclass Order: items = [] quantities = [] prices = [] status = "open" def add_item(self, name, quantity, price): self.items.append(name) self.quantities.append(quantity) self.prices.append(price) def total_price(self): total = 0 for i in range(len(self.prices)): total += self.quantities[i] * self.prices[i] return totalclass PaymentProcessor(ABC): @abstractmethod def pay(self, order, security_code): passclass DebitPaymentProcessor(PaymentProcessor): def pay(self, order, security_code): print("Processing debit payment type") print(f"Verifying security code: &#123;security_code&#125;") order.status = "paid"class CreditPaymentProcessor(PaymentProcessor): def pay(self, order, security_code): print("Processing credit payment type") print(f"Verifying security code: &#123;security_code&#125;") order.status = "paid"order = Order()order.add_item("Keyborad", 1, 50)order.add_item("SSD", 1, 150)order.add_item("USB cable", 2, 5)print(order.total_price())processor = DebitPaymentProcessor()processor.pay(order, "0372846") Liskov Substitution Principle假设我们现在需要添加一种新的支付方式 PayPalPaymentProcessor，它在支付时并不依赖于 security_code 而是需要 email_address 进行验证。即 pay 方法的定义是 pay(self, order, email_address)，与基类中虚拟方法的签名冲突。改进的方法是将 pay 方法依赖的参数 security_code 或 email_address 移动到支付类的 __init__ 方法中，将基类和子类的 pay 方法签名都改为 pay(self, order)。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465from abc import ABC, abstractmethodclass Order: items = [] quantities = [] prices = [] status = "open" def add_item(self, name, quantity, price): self.items.append(name) self.quantities.append(quantity) self.prices.append(price) def total_price(self): total = 0 for i in range(len(self.prices)): total += self.quantities[i] * self.prices[i] return totalclass PaymentProcessor(ABC): @abstractmethod def pay(self, order): passclass DebitPaymentProcessor(PaymentProcessor): def __init__(self, security_code): self.security_code = security_code def pay(self, order): print("Processing debit payment type") print(f"Verifying security code: &#123;self.security_code&#125;") order.status = "paid"class CreditPaymentProcessor(PaymentProcessor): def __init__(self, security_code): self.security_code = security_code def pay(self, order, security_code): print("Processing credit payment type") print(f"Verifying security code: &#123;security_code&#125;") order.status = "paid"class PaypalPaymentProcessor(PaymentProcessor): def __init__(self, email_address): self.email_address = email_address def pay(self, order): print("Processing paypal payment type") print(f"Verifying email address: &#123;self.email_address&#125;") order.status = "paid"order = Order()order.add_item("Keyborad", 1, 50)order.add_item("SSD", 1, 150)order.add_item("USB cable", 2, 5)print(order.total_price())processor = PaypalPaymentProcessor('hi@example.com')processor.pay(order) Interface Segregation Principle假设我们需要在支付组件中添加一个验证短信的功能。直观的想法是直接在 PaymentProcessor 基类中添加一个 auth_sms 虚拟方法：12345678class PaymentProcessor(ABC): @abstractmethod def pay(self, order): pass @abstractmethod def auth_sms(self, code): pass 对于需要验证短信的支付方式比如借记卡，改为如下形式：123456789101112131415class DebitPaymentProcessor(PaymentProcessor): def __init__(self, security_code): self.security_code = security_code self.verified = False def auth_sms(self, code): print(f"Verifying SMS code &#123;code&#125;") self.verified = True def pay(self, order): if not self.verified: raise Exception("Not authorized") print("Processing debit payment type") print(f"Verifying security code: &#123;self.security_code&#125;") order.status = "paid" 对于不需要短信验证的支付方式比如信用卡，就改为如下形式：123456789101112class CreditPaymentProcessor(PaymentProcessor): def __init__(self, security_code): self.security_code = security_code def auth_sms(self, code): raise Exception( "Credit card payments don't support SMS code authorization.") def pay(self, order, security_code): print("Processing credit payment type") print(f"Verifying security code: &#123;security_code&#125;") order.status = "paid" 上述实现的问题在于，我们定义了一个通用的支付接口（PaymentProcessor），包含 pay 和 auth_sms 两种验证逻辑。但这两种逻辑并不总是被具体的支付方式（比如 CreditPaymentProcessor）所需要。这违反了接口分离原则。即接口的实现应该依赖于具体的客户端（子类）需求，而不能不管客户端是否需要，就将所有的功能都放在一个胖接口中。可以额外再实现一个 PaymentProcessor_SMS 基类来定义短信验证的逻辑，让不需要短信验证的支付方式继承 PaymentProcessor 基类，需要短信验证的支付方式继承 PaymentProcessor_SMS 基类。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586from abc import ABC, abstractmethodclass Order: items = [] quantities = [] prices = [] status = "open" def add_item(self, name, quantity, price): self.items.append(name) self.quantities.append(quantity) self.prices.append(price) def total_price(self): total = 0 for i in range(len(self.prices)): total += self.quantities[i] * self.prices[i] return totalclass PaymentProcessor(ABC): @abstractmethod def pay(self, order): passclass PaymentProcessor_SMS(PaymentProcessor): @abstractmethod def auth_sms(self, code): passclass DebitPaymentProcessor(PaymentProcessor_SMS): def __init__(self, security_code): self.security_code = security_code self.verified = False def auth_sms(self, code): print(f"Verifying SMS code &#123;code&#125;") self.verified = True def pay(self, order): if not self.verified: raise Exception("Not authorized") print("Processing debit payment type") print(f"Verifying security code: &#123;self.security_code&#125;") order.status = "paid"class CreditPaymentProcessor(PaymentProcessor): def __init__(self, security_code): self.security_code = security_code def pay(self, order, security_code): print("Processing credit payment type") print(f"Verifying security code: &#123;security_code&#125;") order.status = "paid"class PaypalPaymentProcessor(PaymentProcessor_SMS): def __init__(self, email_address): self.email_address = email_address self.verified = False def auth_sms(self, code): print(f"Verifying SMS code &#123;code&#125;") self.verified = True def pay(self, order): if not self.verified: raise Exception("Not authorized") print("Processing paypal payment type") print(f"Verifying email address: &#123;self.email_address&#125;") order.status = "paid"order = Order()order.add_item("Keyborad", 1, 50)order.add_item("SSD", 1, 150)order.add_item("USB cable", 2, 5)print(order.total_price())processor = PaypalPaymentProcessor('hi@example.com')processor.auth_sms(123456)processor.pay(order) Composition over Inheritance在软件设计的大部分场景中，组合要优于继承。因为继承总是意味着更紧密的耦合性。实际上短信认证并不一定通过继承来实现（PaymentProcessor_SMS），还可以通过组合来实现。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485from abc import ABC, abstractmethodclass Order: items = [] quantities = [] prices = [] status = "open" def add_item(self, name, quantity, price): self.items.append(name) self.quantities.append(quantity) self.prices.append(price) def total_price(self): total = 0 for i in range(len(self.prices)): total += self.quantities[i] * self.prices[i] return totalclass SMSAuth: authorized = False def verify_code(self, code): print(f"Verifying code &#123;code&#125;") self.authorized = True def is_authorized(self) -&gt; bool: return self.authorizedclass PaymentProcessor(ABC): @abstractmethod def pay(self, order): passclass DebitPaymentProcessor(PaymentProcessor): def __init__(self, security_code, authorizer: SMSAuth): self.security_code = security_code self.authorizer = authorizer self.verified = False def pay(self, order): if not self.authorizer.is_authorized(): raise Exception("Not authorized") print("Processing debit payment type") print(f"Verifying security code: &#123;self.security_code&#125;") order.status = "paid"class CreditPaymentProcessor(PaymentProcessor): def __init__(self, security_code): self.security_code = security_code def pay(self, order, security_code): print("Processing credit payment type") print(f"Verifying security code: &#123;security_code&#125;") order.status = "paid"class PaypalPaymentProcessor(PaymentProcessor): def __init__(self, email_address, authorizer: SMSAuth): self.email_address = email_address self.authorizer = authorizer def pay(self, order): if not self.authorizer.is_authorized(): raise Exception("Not authorized") print("Processing paypal payment type") print(f"Verifying email address: &#123;self.email_address&#125;") order.status = "paid"order = Order()order.add_item("Keyborad", 1, 50)order.add_item("SSD", 1, 150)order.add_item("USB cable", 2, 5)print(order.total_price())authorizer = SMSAuth()processor = DebitPaymentProcessor('0372846', authorizer)authorizer.verify_code(123456)processor.pay(order) 定义一个 SMS_Auth 类来实现短信验证的逻辑，再通过组合的方式将其实例添加到具体的需要短信验证的支付方式中（比如 DebitPaymentProcessor）。 Dependency Inversion Principle细节应该依赖于抽象，而不是抽象依赖于细节。上述实现中就违反了这个原则。比如借记卡支付方式（DebitPaymentProcessor）的 __init__ 方法，签名是 __init__(self, security_code, authorizer: SMSAuth)。其中的 SMSAuth 是一个具体的短信验证类型，而不是一个通用的代表某种验证类型的抽象。这样当支付方式需要的是另外一种验证方法（比如 NotARobot），这里的签名就需要修改。 可以创建一个 Authorizer 基类来代表通用的验证方式，具体的验证方式比如 SMSAuth、NotARobot 则作为 Authorizer 的子类来实现。在支付方式的实现中，则使用 Authorizer 作为验证方式的类型定义。这样在使用支付类的实例时，就可以灵活地传入 Authorizer 的子类 SMSAuth 或者 NotARobot 进行组合。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102from abc import ABC, abstractmethodclass Order: items = [] quantities = [] prices = [] status = "open" def add_item(self, name, quantity, price): self.items.append(name) self.quantities.append(quantity) self.prices.append(price) def total_price(self): total = 0 for i in range(len(self.prices)): total += self.quantities[i] * self.prices[i] return totalclass Authorizer(ABC): @abstractmethod def is_authorized(self) -&gt; bool: passclass SMSAuth(Authorizer): authorized = False def verify_code(self, code): print(f"Verifying code &#123;code&#125;") self.authorized = True def is_authorized(self) -&gt; bool: return self.authorizedclass NotARobot(Authorizer): authorized = False def not_a_robot(self): print("Are you a robot? Naa") self.authorized = True def is_authorized(self) -&gt; bool: return self.authorizedclass PaymentProcessor(ABC): @abstractmethod def pay(self, order): passclass DebitPaymentProcessor(PaymentProcessor): def __init__(self, security_code, authorizer: Authorizer): self.security_code = security_code self.authorizer = authorizer self.verified = False def pay(self, order): if not self.authorizer.is_authorized(): raise Exception("Not authorized") print("Processing debit payment type") print(f"Verifying security code: &#123;self.security_code&#125;") order.status = "paid"class CreditPaymentProcessor(PaymentProcessor): def __init__(self, security_code): self.security_code = security_code def pay(self, order, security_code): print("Processing credit payment type") print(f"Verifying security code: &#123;security_code&#125;") order.status = "paid"class PaypalPaymentProcessor(PaymentProcessor): def __init__(self, email_address, authorizer: Authorizer): self.email_address = email_address self.authorizer = authorizer def pay(self, order): if not self.authorizer.is_authorized(): raise Exception("Not authorized") print("Processing paypal payment type") print(f"Verifying email address: &#123;self.email_address&#125;") order.status = "paid"order = Order()order.add_item("Keyborad", 1, 50)order.add_item("SSD", 1, 150)order.add_item("USB cable", 2, 5)print(order.total_price())authorizer = NotARobot()processor = DebitPaymentProcessor('0372846', authorizer)authorizer.not_a_robot()processor.pay(order) 参考资料Uncle Bob’s SOLID principles made easy 🍀 - in Python!]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>OOP</tag>
        <tag>Class</tag>
        <tag>Design</tag>
        <tag>Advanced</tag>
        <tag>Composition</tag>
        <tag>Inheritance</tag>
        <tag>Interface</tag>
        <tag>Principle</tag>
        <tag>SOLID</tag>
        <tag>Dependency</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux xargs 命令解析]]></title>
    <url>%2F2021%2F12%2F09%2Flinux-xargs-usage-and-examples%2F</url>
    <content type="text"><![CDATA[基本使用xargs 是 Linux 系统中的一个命令行工具，它可以读取标准输入并将其作为参数构建新的命令并执行。xargs 可以帮助 echo、rm、mkdir 等命令接收标准输入作为参数。 1234$ xargs mkdirtest1 test2$ lstest1 test2 比如执行 xargs mkdir 命令，输入 test1 test2 后回车，再按 CTRL-D 结束输入，等效于直接执行 mkdir test1 test2 命令。即 xarg 读取了标准输入中的 test1 test2，并将它们作为参数传递给 mkdir 命令，组合成一个完整的 mkdir test1 test2 命令并执行。 实际上 xargs 很少用在上述交互式场景中，更多的是搭配管道符 |，通过前一个命令的输出构建新的命令。 123$ echo test3 test4 test5 | xargs mkdir$ lstest1 test2 test3 test4 test5 | 和 | xargs 的区别： 管道符 | 是将左边命令的输出结果作为右边命令的输入值 | xargs 则可以将左边命令的输出结果作为右边命令的参数选项 find 与 xargsxargs 最常见的搭配应该就是 find 命令了。即通过 find 查找特定的文件或目录，再将结果传递给 xargs，对找到的结果执行特定的操作。 如删除 /tmp 路径下最近两周内未做改动的文件：find /tmp -type f -mtime +14 | xargs rm 有一点特别需要注意：默认情况下，xargs 从标准输入读取命令参数时，会以空格作为分隔符来识别多个选项。而文件和目录的名字有时候也会包含空格，导致一个文件名被 xargs 识别为两个参数。 12345678$ mkdir test\ 6$ ls'test 6' test1 test2 test3 test4 test5$ find . -type d | xargs rmdirrmdir: failed to remove './test': No such file or directoryrmdir: failed to remove '6': No such file or directory$ ls'test 6' 更安全一点的做法是使用 -0 选项。-0 选项可以指定 xargs 在读取标准输入时使用 null 作为分隔符。而 find 命令的 -print0 选项同样可以将输出指定为使用 null 进行分割。 即将前面的命令替换为 find . -type d -print0 | xargs -0 rmdir。 find 搭配 -exec 选项和搭配 xargs 命令的区别find 命令可以使用其 -exec 选项对查找到的结果执行特定的操作。同样的需求 xargs 也可以实现。 比如需要删除当前目录下所有的 TXT 文件： 使用 -exec：find . -type f -name &quot;*.txt&quot; -exec rm {} \; 使用 xargs：find . -type f -name &quot;*.txt&quot; | xargs rm 实际上两者的执行效率存在着很大的差距。比如使用 for i in {1..100}; do touch $i.txt; done 命令创建 100 个 TXT 文件，再分别使用上述两个命令删除这些文件（用 time 命令计时），具体的效率如下：123456789101112131415161718$ for i in &#123;1..100&#125;; do touch $i.txt; done$ ls1.txt 18.txt 27.txt 36.txt 45.txt 54.txt 63.txt 72.txt 81.txt 90.txt10.txt 19.txt 28.txt 37.txt 46.txt 55.txt 64.txt 73.txt 82.txt 91.txt100.txt 2.txt 29.txt 38.txt 47.txt 56.txt 65.txt 74.txt 83.txt 92.txt11.txt 20.txt 3.txt 39.txt 48.txt 57.txt 66.txt 75.txt 84.txt 93.txt12.txt 21.txt 30.txt 4.txt 49.txt 58.txt 67.txt 76.txt 85.txt 94.txt13.txt 22.txt 31.txt 40.txt 5.txt 59.txt 68.txt 77.txt 86.txt 95.txt14.txt 23.txt 32.txt 41.txt 50.txt 6.txt 69.txt 78.txt 87.txt 96.txt15.txt 24.txt 33.txt 42.txt 51.txt 60.txt 7.txt 79.txt 88.txt 97.txt16.txt 25.txt 34.txt 43.txt 52.txt 61.txt 70.txt 8.txt 89.txt 98.txt17.txt 26.txt 35.txt 44.txt 53.txt 62.txt 71.txt 80.txt 9.txt 99.txt$ time find . -type f -name "*.txt" -exec rm &#123;&#125; \;find . -type f -name "*.txt" -exec rm &#123;&#125; \; 0.05s user 0.02s system 104% cpu 0.060 total$ for i in &#123;1..100&#125;; do touch $i.txt; done$ time find . -type f -name "*.txt" | xargs rmfind . -type f -name "*.txt" 0.00s user 0.00s system 80% cpu 0.001 totalxargs rm 0.00s user 0.00s system 94% cpu 0.003 total 前者是 0.06，后者是 0.004。使用 xargs 的执行效率更高。 输出执行的命令-t 选项可以把 xargs 拼接后执行的命令打印到终端窗口中。方便对脚本进行调试。12$ echo test1 test2 test3 | xargs -t mkdirmkdir test1 test2 test3 输出命令并提示用户确认-p 选项可以把 xargs 拼接后执行的命令打印出来，并等待用户进行确认。12$ ls | xargs -p rmdirrmdir test1 test2 test3 ?...y 执行多个命令借助 -I 选项可以令 xargs 同时执行多个命令。12345678910$ cat foo.txtonetwothree$ cat foo.txt | xargs -I % sh -c 'echo %; mkdir %'onetwothree$ lsfoo.txt one three two 常用实例查找当前路径下所有的 PNG 图片并将它们归档到 images.tar.gz 压缩包中：$ find . -name &quot;*.png&quot; -type f -print0 | xargs -0 tar -cvzf images.tar.gz 获取当前系统中所有用户的用户名，以单行列表的形式输出：12$ cut -d: -f1 &lt; /etc/passwd | sort | xargs echo_apt backup bin daemon games gnats irc landscape list lp mail man messagebus news nobody pollinate postgres proxy root sshd starky sync sys syslog systemd-network systemd-resolve systemd-timesync tcpdump tss uucp uuidd www-data 删除当前路径下名为 no_use 的文件：$ find . -name &quot;no_use&quot; -type f -print0 | xargs -0 rm -v -f 复制一个文件到多个路径下：12345$ lsdest1 dest2 test_file$ echo ./dest1/ ./dest2/ | xargs -n 1 cp -v ./test_file'./test_file' -&gt; './dest1/test_file''./test_file' -&gt; './dest2/test_file' 上面例子中 xargs 的 -n 1 选项非常重要。-n 选项用于指定 xargs 在将标准输入作为参数与命令拼接在一起时，参数的最大长度。 简单来说，当 -n 为 1 时，标准输入中以空格分割的每一项都与命令进行拼接，最终形成多条命令；当不存在 -n 1 时，标准输入中以空格分割的所有参数项直接与命令进行拼接，形成一条命令并执行。123456$ echo ./dest1/ ./dest2/ | xargs -n 1 -t cp ./test_filecp ./test_file ./dest1/cp ./test_file ./dest2/$ echo ./dest1/ ./dest2/ | xargs -t cp ./test_filecp ./test_file ./dest1/ ./dest2/cp: -r not specified; omitting directory './dest1/' echo ./dest1/ ./dest2/ | xargs -n 1 cp ./test_file 等效于 cp ./test_file ./dest1/ 和 cp ./test_file ./dest2/ 两条命令 echo ./dest1/ ./dest2/ | xargs cp ./test_file 等效于 cp ./test_file ./dest1/ ./dest2/ 一条命令（语法是错的） 参考资料Linux and Unix xargs command tutorial with examples12 Practical Examples of Linux Xargs Command for Beginners]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Shell</tag>
        <tag>Script</tag>
        <tag>Administration</tag>
        <tag>Xargs</tag>
        <tag>Find</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python code smells 实例讲解]]></title>
    <url>%2F2021%2F12%2F06%2F7-python-code-smells-by-practical-example%2F</url>
    <content type="text"><![CDATA[code smells 可以理解为代码中让人感觉到不舒服的地方。可能是代码规范问题，也可能是设计上的缺陷。很多时候一段代码符合基本逻辑，能够正常运行，并不代表它是不“丑”的。代码中可能会存在诸如可读性差、结构混乱、重复代码太多、不够健壮等问题。 示例代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113"""Very advanced Employee management system."""from dataclasses import dataclassfrom typing import List# The fixed number of vacation days that can be paid out.FIXED_VACATION_DAYS_PAYOUT = 5@dataclassclass Employee: """Basic representation of an employee at the company""" name: str role: str vacation_days: int = 25 def take_a_holiday(self, payout: bool) -&gt; None: """Let the employee take a single holiday, or pay out 5 holidays.""" if payout: if self.vacation_days &lt; FIXED_VACATION_DAYS_PAYOUT: raise ValueError( f"You don't have enough holidays left over for a payout. \ Remaining holidays: &#123;self.vacation_days&#125;" ) try: self.vacation_days -= FIXED_VACATION_DAYS_PAYOUT print( f"Paying out a holiday. Holidays left: &#123;self.vacation_days&#125;") except Exception: pass else: if self.vacation_days &lt; 1: raise ValueError( "You don't have any holidays left. Now back to work, you!" ) self.vacation_days -= 1 print("Have fun on your holiday. Don't forget to check your emails!")@dataclassclass HourlyEmployee(Employee): """ Employee that's paid based on number of worked hours.""" hourly_rate: float = 50 amount: int = 10@dataclassclass SalariedEmployee(Employee): """Employee that's paid based on a fixed monthly salary.""" monthly_salary: float = 5000class Company: """Represents a company with employees.""" def __init__(self) -&gt; None: self.employees: List[Employee] = [] def add_employee(self, employee: Employee) -&gt; None: self.employees.append(employee) def find_managers(self) -&gt; List[Employee]: managers = [] for employee in self.employees: if employee.role == "manager": managers.append(employee) return managers def find_vice_presidents(self) -&gt; List[Employee]: vice_presidents = [] for employee in self.employees: if employee.role == "president": vice_presidents.append(employee) return vice_presidents def find_interns(self) -&gt; List[Employee]: interns = [] for employee in self.employees: if employee.role == "intern": interns.append(employee) return interns def pay_employee(self, employee: Employee) -&gt; None: if isinstance(employee, SalariedEmployee): print( f"Paying employee &#123;employee.name&#125; a monthly salary of $&#123;employee.monthly_salary&#125;" ) elif isinstance(employee, HourlyEmployee): print( f"Paying employee &#123;employee.name&#125; a hourly rate of \ $&#123;employee.hourly_rate&#125; for &#123;employee.amount&#125; hours." )def main() -&gt; None: company = Company() company.add_employee(SalariedEmployee(name="Louis", role="manager")) company.add_employee(HourlyEmployee(name="Brenda", role="president")) company.add_employee(HourlyEmployee(name="Tim", role="intern")) print(company.find_vice_presidents()) print(company.find_managers()) print(company.find_interns()) company.pay_employee(company.employees[0]) company.employees[0].take_a_holiday(False)if __name__ == '__main__': main() 上述代码实现了一个简单的“员工管理系统”。 Employee 类代表公司里的员工，有姓名、角色、假期等属性。可以请假（take_a_holiday），或者单独请一天，或者以 5 天为单位将假期兑换为报酬 HourlyEmployee 和 MonthlyEmployee 分别代表以时薪或者月薪来计算工资的员工 Company 类代表公司，可以招收员工（add_employee）、返回特定角色的员工列表（如 find_managers）、发放薪资等（pay_employee） code smells上面的代码中存在着很多可以改进的地方。 用 Enum 类型替代 str 作为员工的 role 属性上面的 Employee 类使用了 str 类型来存储 role 属性的值，比如用 &quot;manager&quot; 代表经理，用 &quot;intern&quot; 代表实习生。123company.add_employee(SalariedEmployee(name="Louis", role="manager"))company.add_employee(HourlyEmployee(name="Brenda", role="president"))company.add_employee(HourlyEmployee(name="Tim", role="intern")) 实际上 String 过于灵活，可以拥有任何含义，用来表示角色属性时不具有足够清晰的指向性。不同的拼写规则和大小写习惯都会导致出现错误的指向，比如 &quot;Manager&quot; 和 &quot;manager&quot;，&quot;vice-president&quot; 和 &quot;vice_president&quot;。可以使用 Enum 替代 str。1234567891011from enum import Enum, autoclass Role(Enum): """Employee roles.""" PRESIDENT = auto() VICEPRESIDENT = auto() MANAGER = auto() LEAD = auto() WORKER = auto() INTERN = auto() 修改 Employee 类中 role 属性的定义：123456@dataclassclass Employee: name: str role: Role vacation_days: int = 25 Company 类中 find_managers 等方法也做相应的修改：123456def find_managers(self) -&gt; List[Employee]: managers = [] for employee in self.employees: if employee.role == Role.MANAGER: managers.append(employee) return managers main 方法中使用新的 role 创建员工对象：123company.add_employee(SalariedEmployee(name="Louis", role=Role.MANAGER))company.add_employee(HourlyEmployee(name="Brenda", role=Role.VICEPRESIDENT))company.add_employee(HourlyEmployee(name="Tim", role=Role.INTERN)) 消除重复代码Company 类中有一个功能是返回特定角色的员工列表，即 find_managers、find_vice_presidents、find_interns 三个方法。这三个方法实际上有着同样的逻辑，却分散在了三个不同的函数里。可以合并成一个方法来消除重复代码。1234567def find_employees(self, role: Role) -&gt; List[Employee]: """Find all employees with a particular role.""" employees = [] for employee in self.employees: if employee.role == role: employees.append(employee) return employees 同时将 main 函数中的 find_managers、find_vice_presidents、find_interns 都改为如下形式：123print(company.find_employees(Role.VICEPRESIDENT))print(company.find_employees(Role.MANAGER))print(company.find_employees(Role.INTERN)) 尽量使用内置函数上面版本中的 find_employees 方法，包含了一个 for 循环。实际上该部分逻辑可以使用 Python 内置的列表推导来实现。合理的使用 Python 内置函数可以使代码更短、更直观，同时内置函数针对很多场景在性能上也做了一定的优化。1234def find_employees(self, role: Role) -&gt; List[Employee]: """Find all employees with a particular role.""" return [employee for employee in self.employees if employee.role is role] 更清晰明确的变量名旧版本：123456@dataclassclass HourlyEmployee(Employee): """ Employee that's paid based on number of worked hours.""" hourly_rate: float = 50 amount: int = 10 新版本：123456@dataclassclass HourlyEmployee(Employee): """ Employee that's paid based on number of worked hours.""" hourly_rate_dollars: float = 50 hours_worked: int = 10 isinstance当你在代码的任何地方看到 isinstance 这个函数时，都需要特别地加以关注。它意味着代码中有可能存在某些有待提升的设计。 比如代码中的 pay_employee 函数：12345678910def pay_employee(self, employee: Employee) -&gt; None: if isinstance(employee, SalariedEmployee): print( f"Paying employee &#123;employee.name&#125; a monthly salary of $&#123;employee.monthly_salary&#125;" ) elif isinstance(employee, HourlyEmployee): print( f"Paying employee &#123;employee.name&#125; a hourly rate of \ $&#123;employee.hourly_rate_dollars&#125; for &#123;employee.hours_worked&#125; hours." ) 这里 isinstance 的使用，实际上在 pay_employee 函数中引入了对 Employee 的子类的依赖。这种依赖导致各部分代码之间的职责划分不够清晰，耦合性变强。pay_employee 方法需要与 Employee 的子类的具体实现保持同步。每新增一个新的员工类型（Employee 的子类），此方法中的 if-else 也就必须再新增一个分支。即需要同时改动不同位置的两部分代码。 可以将 pay_employee 的实现从 Company 类转移到具体的 Employee 子类中。即特定类型的员工拥有对应的报酬支付方法，公司在发薪时只需要调用对应员工的 pay 方法，无需实现自己的pay_employee 方法。由 isinstance 引入的依赖关系从而被移除。123456789101112131415161718192021222324@dataclassclass HourlyEmployee(Employee): &quot;&quot;&quot; Employee that&apos;s paid based on number of worked hours.&quot;&quot;&quot; hourly_rate_dollars: float = 50 hours_worked: int = 10 def pay(self): print( f&quot;Paying employee &#123;self.name&#125; a hourly rate of \ $&#123;self.hourly_rate_dollars&#125; for &#123;self.hours_worked&#125; hours.&quot; )@dataclassclass SalariedEmployee(Employee): &quot;&quot;&quot;Employee that&apos;s paid based on a fixed monthly salary.&quot;&quot;&quot; monthly_salary: float = 5000 def pay(self): print( f&quot;Paying employee &#123;self.name&#125; a monthly salary of $&#123;self.monthly_salary&#125;&quot; ) 再把 main 函数中的 company.pay_employee(company.employees[0]) 改为 company.employees[0].pay()。 由于每一个特定的 Employee 子类都需要实现 pay 方法，更好的方式是将 Employee 实现为虚拟基类，pay 成为子类必须实现的虚拟方法。12345678from abc import ABC, abstractmethodclass Employee(ABC): @abstractmethod def pay() -&gt; None: """Method to call when paying an employee""" Bool flagEmployee 类中的 take_a_holiday 方法有一个名为 payout 的参数。它是布尔类型，作为一个开关，来决定某个员工是请一天假，还是以 5 天为单位将假期兑换为报酬。这个开关实际上导致了 take_a_holiday 方法包含了两种不同的职责，只通过一个布尔值来决定具体执行哪一个。 函数原本的目的就是职责的分离。使得同一个代码块中不会包含过多不同类型的任务。因此 take_a_holiday 方法最好分割成两个不同的方法，分别应对不同的休假方式。1234567891011121314151617181920212223242526class Employee(ABC): def take_a_holiday(self) -&gt; None: """Let the employee take a single holiday.""" if self.vacation_days &lt; 1: raise ValueError( "You don't have any holidays left. Now back to work, you!" ) self.vacation_days -= 1 print("Have fun on your holiday. Don't forget to check your emails!") def payout_a_holiday(self) -&gt; None: """Let the employee get paid for unused holidays.""" if self.vacation_days &lt; FIXED_VACATION_DAYS_PAYOUT: raise ValueError( f"You don't have enough holidays left over for a payout. \ Remaining holidays: &#123;self.vacation_days&#125;" ) try: self.vacation_days -= FIXED_VACATION_DAYS_PAYOUT print( f"Paying out a holiday. Holidays left: &#123;self.vacation_days&#125;") except Exception: pass Exceptionspayout_a_holiday 方法中有一步 try-except 代码。但该部分代码实际上对 Exception 没有做任何事。对于 Exception 而言： 如果需要 catch Exception，就 catch 特定类型的某个 Exception，并对其进行处理；如果不会对该 Exception 做任何处理，就不要 catch 它。 在此处使用 try-except 会阻止异常向外抛出，导致外部代码在调用 payout_a_holiday 时获取不到异常信息。此外，使用 Exception 而不是某个特定类型的异常，会导致所有的异常信息都被屏蔽掉，包括语法错误、键盘中断等。因此，去掉上述代码中的 try-except。 使用自定义 Exception 替代 ValueErrorValueError 是 Python 内置的在内部出现值错误时抛出的异常，并不适合用在自定义的场景中。最好在代码中定义自己的异常类型。12345678class VacationDaysShortageError(Exception): """Custom error that is raised when not enough vacation days available.""" def __init__(self, requested_days: int, remaining_days: int, message: str) -&gt; None: self.requested_days = requested_days self.remaining_days = remaining_days self.message = message super().__init__(message) 12345678910def payout_a_holiday(self) -&gt; None: """Let the employee get paid for unused holidays.""" if self.vacation_days &lt; FIXED_VACATION_DAYS_PAYOUT: raise VacationDaysShortageError( requested_days=FIXED_VACATION_DAYS_PAYOUT, remaining_days=self.vacation_days, message="You don't have enough holidays left over for a payout.") self.vacation_days -= FIXED_VACATION_DAYS_PAYOUT print(f"Paying out a holiday. Holidays left: &#123;self.vacation_days&#125;") 最终版本123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125"""Very advanced Employee management system."""from dataclasses import dataclassfrom typing import Listfrom enum import Enum, autofrom abc import ABC, abstractmethod# The fixed number of vacation days that can be paid out.FIXED_VACATION_DAYS_PAYOUT = 5class VacationDaysShortageError(Exception): """Custom error that is raised when not enough vacation days available.""" def __init__(self, requested_days: int, remaining_days: int, message: str) -&gt; None: self.requested_days = requested_days self.remaining_days = remaining_days self.message = message super().__init__(message)class Role(Enum): """Employee roles.""" PRESIDENT = auto() VICEPRESIDENT = auto() MANAGER = auto() LEAD = auto() WORKER = auto() INTERN = auto()@dataclassclass Employee(ABC): """Basic representation of an employee at the company""" name: str role: Role vacation_days: int = 25 def take_a_holiday(self) -&gt; None: """Let the employee take a single holiday.""" if self.vacation_days &lt; 1: raise VacationDaysShortageError( requested_days=1, remaining_days=self.vacation_days, message="You don't have any holidays left. Now back to work, you!") self.vacation_days -= 1 print("Have fun on your holiday. Don't forget to check your emails!") def payout_a_holiday(self) -&gt; None: """Let the employee get paid for unused holidays.""" if self.vacation_days &lt; FIXED_VACATION_DAYS_PAYOUT: raise VacationDaysShortageError( requested_days=FIXED_VACATION_DAYS_PAYOUT, remaining_days=self.vacation_days, message="You don't have enough holidays left over for a payout." ) self.vacation_days -= FIXED_VACATION_DAYS_PAYOUT print(f"Paying out a holiday. Holidays left: &#123;self.vacation_days&#125;") @abstractmethod def pay() -&gt; None: """Method to call when paying an employee"""@dataclassclass HourlyEmployee(Employee): """ Employee that's paid based on number of worked hours.""" hourly_rate_dollars: float = 50 hours_worked: int = 10 def pay(self): print( f"Paying employee &#123;self.name&#125; a hourly rate of \ $&#123;self.hourly_rate_dollars&#125; for &#123;self.hours_worked&#125; hours." )@dataclassclass SalariedEmployee(Employee): """Employee that's paid based on a fixed monthly salary.""" monthly_salary: float = 5000 def pay(self): print( f"Paying employee &#123;self.name&#125; a monthly salary of $&#123;self.monthly_salary&#125;" )class Company: """Represents a company with employees.""" def __init__(self) -&gt; None: self.employees: List[Employee] = [] def add_employee(self, employee: Employee) -&gt; None: self.employees.append(employee) def find_employees(self, role: Role) -&gt; List[Employee]: """Find all employees with a particular role.""" return [employee for employee in self.employees if employee.role is role]def main() -&gt; None: company = Company() company.add_employee(SalariedEmployee(name="Louis", role=Role.MANAGER)) company.add_employee(HourlyEmployee( name="Brenda", role=Role.VICEPRESIDENT)) company.add_employee(HourlyEmployee(name="Tim", role=Role.INTERN)) print(company.find_employees(Role.VICEPRESIDENT)) print(company.find_employees(Role.MANAGER)) print(company.find_employees(Role.INTERN)) company.employees[0].pay() company.employees[0].take_a_holiday()if __name__ == '__main__': main() 参考资料7 Python Code Smells: Olfactory Offenses To Avoid At All Costs]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Program</tag>
        <tag>OOP</tag>
        <tag>Refactoring</tag>
        <tag>Class</tag>
        <tag>Design</tag>
        <tag>Advanced</tag>
        <tag>Exception</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kubernetes in Action 笔记 —— Kubernetes 介绍]]></title>
    <url>%2F2021%2F11%2F30%2Fkubernetes-in-action-reading-notes-introducing-kubernetes%2F</url>
    <content type="text"><![CDATA[Kubernetes 这个名字来自于希腊语，意思是舵手。还是很符合这个平台的作用的。Kubernetes 负责管理部署的应用并报告它们的情况，而用户就像是船长，只需要决定想要整个系统达到怎样的状态。 Kubernetes 是一个负责自动化部署和管理应用的软件系统，主要针对由容器构成的复杂的大型应用系统。 Kubernetes 的基本特性抽象化基础设施 Kubernetes 为用户和应用在底层的硬件之上提供了一个抽象层，底层的基础设施如计算机、网络及其他组件等对应用都是不可见的。用户通过这个抽象层部署和管理应用，不需要再面对每一台特定的机器。因此配置起来更加方便。 标准化部署由于底层基础设施的具体细节不会再影响到应用的部署，本地数据中心和云环境都可以使用同样的部署方式。任何底层基础设施的差异都交给 Kubernetes 去处理，用户可以只关注产品及其内部逻辑。 声明式部署Kubernetes 使用声明式的模型来定义具体的应用。用户只需要完成对应用中各组件的描述，Kubernetes 就会将这些描述转化成运行的应用。并在之后保证该应用的健康运行，在必要的时候重启或重新创建某个组件。 当用户修改了某些描述，Kubernetes 会根据改动自动采取必要的步骤重新配置应用，令应用满足最新的描述。 接管应用的日常管理一旦用户通过 Kubernetes 部署了某个应用，该应用的日常管理就会被 Kubernetes 接管。假如服务停止运行，Kubernetes 会自动重启该应用；或者由于硬件失效、基础设施架构调整导致该应用需要移动到其他机器上，Kubernetes 也会自行处理。 就像之前提到的，用户类似于船长负责更高层级的决策，而 Kubernetes 则类似于舵手负责执行具体的底层任务。 Kubernetes 与微服务之前的年头，绝大多数应用都是单体应用。应用里的各个组件是强耦合的，全部运行在同一个进程里。当应用的容量需要提升时，水平扩展单体应用是非常困难的。只能不断升级服务器的硬件，即垂直扩展。 微服务范式是后来才出现的。单体应用被分割成数十甚至数百个独立的进程（微服务）。每一个微服务都拥有自己所独有的开发和部署周期，不同微服务的依赖会随着时间的推移差距越来越大。这使得在同一个操作系统内部运行两个微服务应用变得非常困难。容器正好方便解决这个问题。但每个微服务都是一个独立的应用，需要单独进行管理。随着应用数量的上升这将会越来越困难。整个应用系统的各个部分不需要部署到同一台机器上，这使得扩展起来更加方便。但同时也意味着各组件之间需要配置成能够相互通信的状态。同样增加了维护成本。因此当微服务的规模变得很大时，自动化管理就显得尤为必要。Kubernetes 则正好提供了这种自动化功能。 Kubernetes 的架构Kubernetes 可以看作是一个面向服务器集群的操作系统。操作系统用来支撑计算机的基本功能比如 CPU 调度，作为应用和计算机硬件之间沟通的接口。类似的，Kubernetes 负责在服务器集群的各台机器上调度安排分布式应用的各个组件，作为应用和集群之间的接口。 一个 Kubernetes 集群包含两组节点： 主节点：负责运行 Control Plane 组件，是系统的大脑，控制整个集群 工作节点：组成 Workload Plane，承接应用和负载 控制平台Control Plane 负责控制整个集群。它运行在一台主节点上，或者以副本的方式运行在多个主节点上。包含 Scheduler、Controllers、Kubernetes API Server、etcd 等几个组件。 Kubernetes API Server 负责暴露 RESTful API 接口，用户可以通过此接口创建对象 etcd 分布式数据仓库负责持久化通过 API 创建的对象，因为 API Server 本身是无状态的。Server 是唯一一个与 etcd 交互的组件 Scheduler 负责决定每个应用实例具体运行在哪个工作节点上 Controllers 负责具体化由 API 创建的对象。它们中的大部分实际上就是负责创建其他对象，有一些也会与外部系统进行交互（比如云提供商） 负载平台工作节点就是实际上运行应用的节点，它们构成了 Workload Plane。负责运行、监控各个应用，并在各应用之间提供连通性。 其中各组件的功能如下： Kubelet：一个与 API Server 进行交互的 agent，负责管理当前节点上运行的应用。会通过 API 向主节点报告应用的状态 Container Runtime：可以是 Docker 或其他与 Kubernetes 兼容的容器运行时，受 kubelet 指挥，负责运行应用 Kubernetes Service Proxy：在各应用的网络流量之间提供负载均衡 Kubernetes 的工作流程Kubernetes 中的所有元素都由对象表示，可以通过 API 创建和获取这些对象。用户需要几种不同的对象来定义自己的应用，通常在 YAML 或 JSON 格式的清单文件中定义。 向 Kubernetes 部署应用的具体步骤为： 向 Kubernetes API 提交应用的 manifest 文件。API Server 将文件中定义的对象写入 etcd controller 接到已经创建了新对象的通知，继续创建几个新的对象，对应不同的应用实例 Scheduler 为每一个应用实例分配工作节点 工作节点上的 Kubelet 接到通知，借助容器运行时运行应用实例 应用实例准备好接收客户端请求后，Kube Proxy 收到通知，为这些应用实例配置负载均衡 工作节点上的 Kubelets 和 Controllers 负责之后的监控工作，保证应用健康运行 参考资料Kubernetes in Action, Second Edition]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Server</tag>
        <tag>Docker</tag>
        <tag>Container</tag>
        <tag>Kubernetes</tag>
        <tag>Virtualization</tag>
        <tag>Cloud</tag>
        <tag>Microservice</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kubernetes in Action 笔记 —— 理解容器技术]]></title>
    <url>%2F2021%2F11%2F29%2Fkubernetes-in-action-reading-notes-understanding-containers%2F</url>
    <content type="text"><![CDATA[一、容器 vs 虚拟机系统开销每一台虚拟机都需要安装独属于自己的操作系统，包含一系列系统进程；而同一台宿主机上的多个容器会共享宿主机的操作系统，它们的环境仍然是独立的。即对于容器而言，不需要装一个独立的操作系统。不会像虚拟机那样，存在很多套重复的系统进程。因而容器更加轻量。容器只包含一套隔离的进程，运行在已有的宿主机操作系统上，只会消耗这套隔离进程运行所需的系统资源。 由于虚拟机高额的开销，通常需要将多个关联的应用部署在同一台虚拟机上。对于开销较低的容器而言，则可以为每一个应用都创建一个独立的容器。实际上也应该确保每一个容器都只包含一个应用，这样方便管理，同时 Kubernetes 等容器管理平台也是默认这个原则的。 启动时间容器拥有更快的启动时间。因为容器只需要启动自身包含的应用进程，不需要像启动一台新的虚拟机那样，先启动一些额外的系统进程。 隔离性无疑虚拟机的隔离性更好。当使用虚拟机部署应用时，每台虚拟机都拥有一套独立的操作系统和内核。这些虚拟机的底层是 hypervisor，将物理硬件划分成一系列更小的虚拟资源，供给不同的虚拟机使用。 当虚拟机中运行的应用向虚拟机内核发起系统调用时，内核会先在虚拟的 CPU 上执行机器指令，再通过 hypervisor 转发给宿主机的物理 CPU 执行。容器发起的系统调用则都可以直接传递给宿主机上运行的系统内核，再转化为机器指令传递给宿主机的 CPU。宿主机 CPU 不需要处理任何形式的虚拟化。 同一台机器上的多个容器共享同一个宿主机内核，但它们之间仍然是隔离的，相互之间并不清楚其他人的存在，也只能看到一部分物理硬件。这种隔离是由宿主机内核提供的。 由隔离性引发的安全性容器使用同一个内核。如果内核出现 bug，某个容器中的应用有可能会利用这个 bug 读取其他容器中其他应用的内存。此外，容器会共享内存空间。如果不限制某个容器能够使用的内存总量，有可能会导致其他容器没有足够的内存使用。 二、Docker 容器平台介绍Docker 是一个帮助用户打包、发布和运行容器应用的平台。用户可以使用 Docker 将应用及其运行环境（可以是一些动态库等依赖，甚至操作系统提供的所有文件）打包，并可以将打包后的镜像发布到一个公共的镜像源，再部署到其他安装了 Docker 的机器上。 三个 Docker 中的基本概念： Images（镜像）：类似于一个 zip 压缩包，包含了应用及其运行环境 Registries（源）：一个方便用户和机器分发、共享镜像的站点。可以将打包好的镜像 push 到源，这样另一台机器就可以从镜像源 pull 该镜像到本地 Containers（容器）：相当于实例化的镜像。一个运行的容器相当于宿主机中一个普通的进程，只不过它的环境是隔离的 Image Layers不同于虚拟机镜像是由整个文件系统构成的一个大的文件块，容器镜像通常是由更小的层构成。这些层可以被多个镜像所共享。如果某个镜像需要的部分镜像层已经被下载到了宿主机上（在 pull 其他镜像的时候），则 pull 该镜像时只需要下载之前未 pull 的层即可。镜像层使得发布镜像变得更加高效，同时也提升了宿主机的存储空间。 如上图中的三个容器，它们可以共享访问一部分共有的文件。但它们是如何同时做到隔离的呢？其中某个容器若修改了共享的文件，如何做到不对其他容器可见？文件系统的隔离由 Copy-on-Write (CoW) 机制实现。容器中的文件系统由从镜像而来的只读层和加在只读层上面的一个读写层构成。当某个运行的容器修改了只读层中的文件，该文件会被整个复制到容器的读写层。每个容器都拥有自己所独有的读写层，因此对共享文件的修改并不会对其他容器可见。当删除某个文件，该文件只是在读写层中被标记为已删除，实际上该文件仍然存在于只读层中。因此删除文件并不会减少镜像的大小。 镜像层的潜在限制理论上讲，基于 Docker 的镜像可以运行在任意一台启用了 Docker 的机器上。但是由于容器并没有自己的内核，如果一个容器需要特定版本的内核才能运行，它有可能不会运行在每一台机器上。 此外，容器化的应用只能运行在特定的硬件架构上。比如不能把一个构建在 x86 CPU 架构上的应用，部署在 ARM 平台的 Docker 上。 三、容器背后的技术NamespaceLinux 命名空间可以确保每个进程都只能看到它自己视角的系统。即容器中的进程只能看到部分文件、进程、网络接口和机器名，就好像它运行在一个独立的虚拟机上。内核可以创建额外的命名空间，然后将部分资源移动到该命名空间，并令其只对某一个或一组进程可见。 命名空间的类型： Mount 命名空间用来隔离挂载点（文件系统） Process ID 命名空间用来隔离进程 ID Network 命名空间用来隔离网络设备、端口等 ipc 命名空间用来隔离进程间的通信（包括管理消息队列、共享内存等） UTS (UNIX Time-sharing System) 命名空间用来隔离系统的主机名和 NIS (Network Information Service) 域名 User ID 命名空间用来隔离用户和组 ID Cgroup 命名空间用来隔离 Control Groups 根目录 有时候并不会想要将某个容器与另一个容器完全隔离，相互关联的容器之间有可能会共享特定的资源。 比如上图中的两个容器。它们可以看见并使用相同的两个网络设备（eth0 和 lo），因为它们拥有相同的网络命名空间。它们也因此可以绑定相同的 IP 地址并通过 loopback 设备相互通信。这两个容器还使用同一个 UTS 命名空间，因此它们可以见到相同的主机名。但它们的 Mount 命名空间是不同的，即拥有不同的文件系统。 容器中运行的进程只是一个绑定了 7 类命名空间的普通进程。 Linux Control GroupsLinux 命名空间可以控制进程只能访问一部分系统资源，但它们不能限制每个进程消耗的资源总量。Linux Control Groups (cgroups) 则可以限制进程只能使用预先分配好的固定额度的 CPU 时间、内存和网络带宽等。避免某些进程吃掉所有的系统资源。 sys-callsLinux 命名空间和 Cgroups 能够隔离容器的环境并防止某个容器消耗掉所有的计算资源。但这些容器中的进程仍然使用同一个系统内核，一个非法容器仍然可以通过一些恶意的系统调用来影响其他容器。 内核提供了一系列 sys-calls 可以被程序用来与操作系统及底层的硬件交互，包括创建进程、操作文件和设备、创建应用间的通信通道等。其中有些 sys-calls 是安全的，可以被任意进程使用。其他一些则只允许具有更高权限的进程使用。比如容器中的应用应该允许访问它们的本地文件，但不能修改系统时钟或者以破坏其他容器的方式修改内核。 Linux 内核把这些权限分成了名为 capabilities 的单位。如： CAP_NET_ADMIN：允许进程执行网络相关的操作 CAP_NET_BIND_SERVICE：允许进程绑定小于 1024 的端口号 CAP_SYS_TIME：允许进程修改系统时钟 Capabilities 能够在容器创建时添加或移除，每个 Capability 都代表一系列特殊权限。此外，还可以使用 seccomp (Secure Computing Mode) 。创建一个 包含 seccomp 配置的 JSON 文件，在构建容器时提供给 Docker。 AppArmor &amp; SELinux容器还可以依靠两种 MAC（强制访问控制）机制 SELinux 和 AppArmor 来获得更高的安全性。 参考资料Kubernetes in Action, Second Edition]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Docker</tag>
        <tag>Container</tag>
        <tag>Kubernetes</tag>
        <tag>Virtualization</tag>
        <tag>Isolation</tag>
        <tag>VM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 语言实现循环的最快方式（for、while 等速度对比）]]></title>
    <url>%2F2021%2F11%2F23%2Fthe-fastest-way-to-loop-in-python%2F</url>
    <content type="text"><![CDATA[众所周知，Python 不是一种执行效率较高的语言。此外在任何语言中，循环都是一种非常消耗时间的操作。假如任意一种简单的单步操作耗费的时间为 1 个单位，将此操作重复执行上万次，最终耗费的时间也将增长上万倍。 while 和 for 是 Python 中常用的两种实现循环的关键字，它们的运行效率实际上是有差距的。比如下面的测试代码：12345678910111213141516171819202122232425262728import timeitdef while_loop(n=100_000_000): i = 0 s = 0 while i &lt; n: s += i i += 1 return sdef for_loop(n=100_000_000): s = 0 for i in range(n): s += i return sdef main(): print('while loop\t\t', timeit.timeit(while_loop, number=1)) print('for loop\t\t', timeit.timeit(for_loop, number=1))if __name__ == '__main__': main()# =&gt; while loop 4.718853999860585# =&gt; for loop 3.211570399813354 这是一个简单的求和操作，计算从 1 到 n 之间所有自然数的总和。可以看到 for 循环相比 while 要快 1.5 秒。其中的差距主要在于两者的机制不同。在每次循环中，while 实际上比 for 多执行了两步操作：边界检查和变量 i 的自增。即每进行一次循环，while 都会做一次边界检查 (while i &lt; n）和自增计算（i +=1）。这两步操作都是显式的纯 Python 代码。for 循环不需要执行边界检查和自增操作，没有增加显式的 Python 代码（纯 Python 代码效率低于底层的 C 代码）。当循环的次数足够多，就出现了明显的效率差距。 可以再增加两个函数，在 for 循环中加上不必要的边界检查和自增计算：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950import timeitdef while_loop(n=100_000_000): i = 0 s = 0 while i &lt; n: s += i i += 1 return sdef for_loop(n=100_000_000): s = 0 for i in range(n): s += i return sdef for_loop_with_inc(n=100_000_000): s = 0 for i in range(n): s += i i += 1 return sdef for_loop_with_test(n=100_000_000): s = 0 for i in range(n): if i &lt; n: pass s += i return sdef main(): print('while loop\t\t', timeit.timeit(while_loop, number=1)) print('for loop\t\t', timeit.timeit(for_loop, number=1)) print('for loop with increment\t\t', timeit.timeit(for_loop_with_inc, number=1)) print('for loop with test\t\t', timeit.timeit(for_loop_with_test, number=1))if __name__ == '__main__': main()# =&gt; while loop 4.718853999860585# =&gt; for loop 3.211570399813354# =&gt; for loop with increment 4.602369500091299# =&gt; for loop with test 4.18337869993411 可以看出，增加的边界检查和自增操作确实大大影响了 for 循环的执行效率。 前面提到过，Python 底层的解释器和内置函数是用 C 语言实现的。而 C 语言的执行效率远大于 Python。对于上面的求等差数列之和的操作，借助于 Python 内置的 sum 函数，可以获得远大于 for 或 while 循环的执行效率。12345678910111213141516171819202122232425262728293031323334import timeitdef while_loop(n=100_000_000): i = 0 s = 0 while i &lt; n: s += i i += 1 return sdef for_loop(n=100_000_000): s = 0 for i in range(n): s += i return sdef sum_range(n=100_000_000): return sum(range(n))def main(): print('while loop\t\t', timeit.timeit(while_loop, number=1)) print('for loop\t\t', timeit.timeit(for_loop, number=1)) print('sum range\t\t', timeit.timeit(sum_range, number=1))if __name__ == '__main__': main()# =&gt; while loop 4.718853999860585# =&gt; for loop 3.211570399813354# =&gt; sum range 0.8658821999561042 可以看到，使用内置函数 sum 替代循环之后，代码的执行效率实现了成倍的增长。内置函数 sum 的累加操作实际上也是一种循环，但它由 C 语言实现，而 for 循环中的求和操作是由纯 Python 代码 s += i 实现的。C &gt; Python。 再拓展一下思维。小时候都听说过童年高斯巧妙地计算 1 到 100 之和的故事。1…100 之和等于 (1 + 100) * 50。这个计算方法同样可以应用到上面的求和操作中。12345678910111213141516171819202122232425262728293031323334353637383940import timeitdef while_loop(n=100_000_000): i = 0 s = 0 while i &lt; n: s += i i += 1 return sdef for_loop(n=100_000_000): s = 0 for i in range(n): s += i return sdef sum_range(n=100_000_000): return sum(range(n))def math_sum(n=100_000_000): return (n * (n - 1)) // 2def main(): print('while loop\t\t', timeit.timeit(while_loop, number=1)) print('for loop\t\t', timeit.timeit(for_loop, number=1)) print('sum range\t\t', timeit.timeit(sum_range, number=1)) print('math sum\t\t', timeit.timeit(math_sum, number=1))if __name__ == '__main__': main()# =&gt; while loop 4.718853999860585# =&gt; for loop 3.211570399813354# =&gt; sum range 0.8658821999561042# =&gt; math sum 2.400018274784088e-06 最终 math sum 的执行时间约为 2.4e-6，缩短了上百万倍。这里的思路就是，既然循环的效率低，一段代码要重复执行上亿次。索性直接不要循环，通过数学公式，把上亿次的循环操作变成只有一步操作。效率自然得到了空前的加强。 最后的结论（有点谜语人）： 实现循环的最快方式——————就是不用循环 对于 Python 而言，则尽可能地使用内置函数，将循环中的纯 Python 代码降到最低。 参考资料The Fastest Way to Loop in Python - mCoding]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Advanced</tag>
        <tag>Efficiency</tag>
        <tag>loop</tag>
        <tag>C</tag>
        <tag>big-O</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ansible 使用 lineinfile 模块修改配置文件]]></title>
    <url>%2F2021%2F11%2F16%2Fansible-lineinfile-module-for-modifying-configuration%2F</url>
    <content type="text"><![CDATA[需要用 Ansible 修改配置文件，其实就是在某个文件末尾添加几行内容。直观地想，直接用 shell 模块，echo 加 &gt;&gt; 就完事了。但仔细一琢磨，很可能会引发一些意想不到的问题，比如： 如果需要添加的配置已经存在，echo 仍会向配置文件底部添加同样的内容 如果添加配置的任务重复执行多次，则配置文件中也会多次出现重复的内容。无法做到幂等 如何做到，当对应的配置已经存在，则将该配置改为期望的值；当对应的配置不存在，不做任何操作（有就修改，没有就不动。好像可以用 sed） 如何安全地移除指定的配置项 诸如此类。运维工作常常要关系到生产环境。任何无法预期的效果都可能产生严重的影响。而单纯使用 echo 和 &gt;&gt; 向配置文件中添加内容，具有很大的不确定性。当然可以形成一个 Shell 脚本，对各种边界进行足够的检查和判定，但这会导致代码量变大，结构复杂难以标准化；同时也容易出现遗漏的情况。 实际上 Ansible 内置的 lineinfile 就是专门用来处理上述任务的模块。 比如针对如下内容的配置文件 test_conf.ini：12FIRST=trueSECOND=2 需要添加一行配置 THIRD=3。 可以运行如下内容的 playbook change_config.yml：12345678910- name: change configuration gather_facts: false hosts: localhost tasks: - name: change content in test_conf.ini lineinfile: path: /home/starky/projects/ansible/practice/test_conf.ini regexp: '^THIRD' line: THIRD=3 其中 lineinfile 模块的 path 参数用于指定目标配置文件的路径；regexp 参数则用于指定对文件内容进行匹配时使用的正则表达式；最后的 line 参数表示希望在目标文件中出现的内容。 具体的步骤为： 检查 line 对应的内容是否存在于 path 对应的目标文件中 若已经存在。则目标文件符合要求，不对该文件做任何操作 若不存在。通过 regexp 指定的正则表达式对目标文件进行匹配 若 regexp 匹配到文本行，则将该行内容修改为 line 指定的内容 若 regexp 未匹配到文本行，则将 line 对应的内容作为新的一行添加到目标文件末尾 运行效果：123456789$ ansible-playbook change_config.ymlPLAY [change configuration] *********************************************************************************************************TASK [change content in test_conf.ini] **********************************************************************************************changed: [localhost]PLAY RECAP **************************************************************************************************************************localhost : ok=1 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 此时 test_conf.ini 配置文件的内容被修改为：123FIRST=trueSECOND=2THIRD=3 若再次运行 ansible-playbook change_config.yml 命令：123456789$ ansible-playbook change_config.ymlPLAY [change configuration] *********************************************************************************************************TASK [change content in test_conf.ini] **********************************************************************************************ok: [localhost]PLAY RECAP **************************************************************************************************************************localhost : ok=1 changed=0 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 可以看到修改配置文件的任务执行结果为 ok，而不同于上一次的 changed。这表示 lineinfile 模块对配置文件的内容进行了检查，发现需要添加的配置行已经存在，因此未做任何改动。符合幂等的原则。 假如将配置文件中的 THIRD=3 改为 THIRD=false，再次运行 playbook：123456789$ ansible-playbook change_config.ymlPLAY [change configuration] *********************************************************************************************************TASK [change content in test_conf.ini] **********************************************************************************************changed: [localhost]PLAY RECAP **************************************************************************************************************************localhost : ok=1 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 正则表达式 ^THIRD 会匹配到配置文件的第三行 THIRD=false，再将该行内容替换为 THIRD=3。 最终仍可以得到我们想要的内容：123FIRST=trueSECOND=2THIRD=3 对于 Ansible playbook 而言，我们只需要关注期望达到的状态，而不用纠结为了达到该状态需要执行哪些步骤。如 lineinfile 模块，line 指定的内容即为我们期望目标文件达到的状态。即该文件最终一定会包含一行与 line 相同的文本。不管该行内容是本就已经存在的，还是通过修改 regexp 匹配到的文本行得到的，还是直接在目标文件末尾新增的。而我们只需要定义 path、regexp、line 三个参数即可。 其他用法backrefslineinfile 默认的行为是若 line 指定的内容未存在，regexp 正则表达式也没有任何匹配，就在文件末尾添加一行 line 指定的内容。backrefs 参数可以修改此行为。当 backrefs 设定为 true 时，若 line 指定的内容不存在，正则表达式也没有匹配。则不做任何操作。 比如如下 playbook：1234567891011- name: change configuration gather_facts: false hosts: localhost tasks: - name: change content in test_conf.ini lineinfile: path: /home/starky/projects/ansible/practice/test_conf.ini regexp: '^THIRD' line: 'THIRD=3' backrefs: true 当目标文件的内容如下：12FIRST=trueSECOND=2 playbook 实际不会对其做任何修改，不会在文件末尾添加 THIRD=3。只有当文件中存在如 THIRD=false 这类内容时，playbook 才会完成匹配并替换对应的行。 没有 backrefs 表示匹配就替换，不匹配就在文件末尾添加；有 backrefs 表示匹配就替换，不匹配就不动。 删除一行内容12345678910- name: change configuration gather_facts: false hosts: localhost tasks: - name: change content in test_conf.ini lineinfile: path: /home/starky/projects/ansible/practice/test_conf.ini regexp: '^THIRD' state: absent 在匹配行前/后添加12345678910- name: change configuration gather_facts: false hosts: localhost tasks: - name: change content in test_conf.ini lineinfile: path: /home/starky/projects/ansible/practice/test_conf.ini insertbefore: '^FIRST' line: 'ZERO=false' 12345678910- name: change configuration gather_facts: false hosts: localhost tasks: - name: change content in test_conf.ini lineinfile: path: /home/starky/projects/ansible/practice/test_conf.ini insertafter: '^THIRD' line: 'FOURTH=4' 需要注意以下两点： 当 line 指定的内容已经存在于目标文件中时，不管其具体在什么位置，目标文件都不会做任何修改 当 insertbefore 或 insertafter 指定的正则表达式没有任何匹配时，都会在文件末尾添加 line 指定的内容 官网示例12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182# NOTE: Before 2.3, option 'dest', 'destfile' or 'name' was used instead of 'path'- name: Ensure SELinux is set to enforcing mode ansible.builtin.lineinfile: path: /etc/selinux/config regexp: '^SELINUX=' line: SELINUX=enforcing- name: Make sure group wheel is not in the sudoers configuration ansible.builtin.lineinfile: path: /etc/sudoers state: absent regexp: '^%wheel'- name: Replace a localhost entry with our own ansible.builtin.lineinfile: path: /etc/hosts regexp: '^127\.0\.0\.1' line: 127.0.0.1 localhost owner: root group: root mode: '0644'- name: Replace a localhost entry searching for a literal string to avoid escaping lineinfile: path: /etc/hosts search_string: '127.0.0.1' line: 127.0.0.1 localhost owner: root group: root mode: '0644'- name: Ensure the default Apache port is 8080 ansible.builtin.lineinfile: path: /etc/httpd/conf/httpd.conf regexp: '^Listen ' insertafter: '^#Listen ' line: Listen 8080- name: Ensure php extension matches new pattern lineinfile: path: /etc/httpd/conf/httpd.conf search_string: '&lt;FilesMatch ".php[45]?$"&gt;' insertafter: '^\t&lt;Location \/&gt;\n' line: ' &lt;FilesMatch ".php[34]?$"&gt;'- name: Ensure we have our own comment added to /etc/services ansible.builtin.lineinfile: path: /etc/services regexp: '^# port for http' insertbefore: '^www.*80/tcp' line: '# port for http by default'- name: Add a line to a file if the file does not exist, without passing regexp ansible.builtin.lineinfile: path: /tmp/testfile line: 192.168.1.99 foo.lab.net foo create: yes# NOTE: Yaml requires escaping backslashes in double quotes but not in single quotes- name: Ensure the JBoss memory settings are exactly as needed ansible.builtin.lineinfile: path: /opt/jboss-as/bin/standalone.conf regexp: '^(.*)Xms(\d+)m(.*)$' line: '\1Xms$&#123;xms&#125;m\3' backrefs: yes# NOTE: Fully quoted because of the ': ' on the line. See the Gotchas in the YAML docs.- name: Validate the sudoers file before saving ansible.builtin.lineinfile: path: /etc/sudoers state: present regexp: '^%ADMIN ALL=' line: '%ADMIN ALL=(ALL) NOPASSWD: ALL' validate: /usr/sbin/visudo -cf %s# See https://docs.python.org/3/library/re.html for further details on syntax- name: Use backrefs with alternative group syntax to avoid conflicts with variable values ansible.builtin.lineinfile: path: /tmp/config regexp: ^(host=).* line: \g&lt;1&gt;&#123;&#123; hostname &#125;&#125; backrefs: yes 参考资料ansible.builtin.lineinfile – Manage lines in text files]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Configuration</tag>
        <tag>Shell</tag>
        <tag>Ansible</tag>
        <tag>Devops</tag>
        <tag>Automation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Vim 技巧 —— 实现多行注释的几种方法]]></title>
    <url>%2F2021%2F10%2F08%2Fvim-tricks-some-ways-to-comment-multiple-lines%2F</url>
    <content type="text"><![CDATA[习惯问题，一直在用 Vim。之前装的几个插件，基本上覆盖了日常使用中的绝大部分场景。只是遇到需要多行注释的时候，一直没有一套比较直接的方法。于是上网查了些资料，也实际测试了下效果。整理出一部分方法（不借助插件），以作备忘。 字符串替换命令模式下可以使用 :s/old/new 替换当前文本行中的指定字符串。前面还可以加上数字等用来指定范围。如： :n1,n2 s/old/new：从 n1 行开始，到 n2 行结束，将每一行中符合 old 模式的内容替换为 new :% s/old/new：将整个文件中的 old 替换为 new 类似于 Vim 中内置了一个 sed 工具。 对于 Python 代码，注释代码块当然可以用双 &#39;&#39;&#39;，临时的注释我个人比较偏向行首加 #。借助字符串替换可以很方便的实现。命令如下：:n1,n2 s/^/#将 n1 到 n2 行中的行首空白字符替换为 #，等同于在每一行行首插入 # 对于如下 Python 代码：1234567891011from datetime import datetimeimport calendarimport oscurrent_time = datetime.now()total_days = calendar.monthrange(current_time.year, current_time.month)total_days = total_days[1]for i in range(total_days): os.mkdir(str(i + 1)) 在 Vim 中运行 :1,3 s/^/#，后，效果如下：1234567891011# from datetime import datetime# import calendar# import oscurrent_time = datetime.now()total_days = calendar.monthrange(current_time.year, current_time.month)total_days = total_days[1]for i in range(total_days): os.mkdir(str(i + 1)) 如果要取消注释，再运行如下命令即可：:1,3 s/# /将 1 - 3 行行首的 # 替换为空字符串（即移除行首的 #) visual 模式下指定文本范围如果不喜欢用数字行号指定施行替换的文本行，也可以在 Visual 模式下手动选择生效的文本范围。 Normal 模式下按键盘上的 v 键进入 visual 模式，然后就可以通过光标移动命令（如 h、j、k、l、{、} 等）选中多行文本。 接着在命令模式下运行 :&#39;&lt;,&#39;&gt; s/^/# 即可在选中的文本上执行文本替换。 其中 &#39;&lt;,&#39;&gt; 即代表 visual 模式下选中的文本行。一般情况下，选中文本以后再按 : 进入命令模式，&#39;&lt;,&#39;&gt; 部分内容会自动补全。 多行编辑多行编辑指同时在多行文本中每一行的相同位置插入相同的内容。只需要编辑某一行文本，其他文本行就会自动进行同样的修改。 比如需要注释 Python 代码的前 3 行，就可以同时编辑这 3 行文本，在第一行行首插入 #，则前 3 行会同时被注释掉。 首先光标定位到第一行行首，按下键盘上的 ctrl + v 组合键（Windows 下可以用 ctrl + q）进入 Visual Block 模式，按两次 j 键下移光标，选中前三行的首字符。再按下键盘上 I（大写）键进入插入模式，在第一行行首插入 # 字符。按下 Esc 退出插入模式，则后两行行首也会自动插入 # 字符。 录制 MacroVim 中的宏（Macro）即一系列编辑操作的合集。在 Vim 中，可以把需要重复执行的多步编辑操作录制下来，绑定到某个按键上。之后就可以按下 @绑定的按键 重复执行录制好的步骤。有点像定义和调用函数。 使用 q某按键 开始录制宏，执行某些编辑操作后，再按下 q 结束录制。随后按下 @某按键 调用录制好的宏。 如需要注释前 3 行 Python 代码，则可以录制包含如下步骤的宏： 在当前行的行首开始插入文本（I） 输入注释符号（#） 退出插入模式（Esc） 移动到下一行（j) 上述宏中的操作步骤可以首尾相接，无限循环。即注释当前行，移动到下一行；注释当前行，移动到下一行。。。 具体的操作步骤如下： 在 Normal 模式下，光标移动到首行，按下 qq 开始录制宏（录制结束后会绑定给 q） 按下 I 键（大写），进入行首插入模式 输入 # 按下 Esc 退出插入模式 按下 j 移动光标到下一行文本 按下 q 结束宏的录制 宏录制结束后，即可连按两次 @q 连续调用两次宏，分别注释第二行和第三行内容。 . 命令. 命令相当于一种简化了的宏。它表示重复应用上一步中对内容的更改。. 命令无需录制，但只会重复对内容的编辑（如插入、替换等），不会重复其他操作。比如前面的需要注释 Python 代码的前三行，可以执行如下操作： 光标定位到首行，按 I（大写）在行首插入内容 输入注释符号 # 按下 Esc 退出插入模式 按下 j 跳转到下一行，按 . 重复执行对上一行的编辑操作（行首插入 #） 重复上一步操作（j.），注释第三行 其他在查阅资料的时候，还发现一个 norm 命令。比如还是需要注释前 3 行，可以执行如下命令：:1,3 norm I # 其中 1,3 表示只对 1 - 3 行应用后面的命令；norm 表示从 Normal 模式下开始；I # 表示进入插入模式（同时光标移动到行首），插入文本 #。 参考资料Vim: insert the same characters across multiple lines]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Tools</tag>
        <tag>Tricks</tag>
        <tag>Vim</tag>
        <tag>Editor</tag>
        <tag>Efficiency</tag>
        <tag>Macro</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Effective Python 笔记 —— 并发与并行（Queue）]]></title>
    <url>%2F2021%2F10%2F08%2Feffective-python-notes-concurrency-and-queue%2F</url>
    <content type="text"><![CDATA[借助 Queue 实现多线程间的协同Pipeline并行地执行多个任务的 Python 程序通常都需要一种协作机制，使得多个线程负责的各部分之间的工作能够相互协同。其中一种协作机制称为管线（pipeline）。pipeline 的工作方式类似于工厂里的流水线，分为串行排列的多道工序（phase）。每道工序都由特定的函数处理，函数之间可以并行地执行。 比如需要创建这样一个系统，可以从相机接收持续的图片流，再将收到的图片更改尺寸，最后上传到线上的图片库中。这样的系统就可以分为三道工序，分别用 download、resize、upload 三个函数去处理。此外还需要一个在各道工序间传递任务对象的媒介，这个可以通过线程安全的 producer-consumer 队列去实现。 具体的示例代码如下：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879import timefrom threading import Threadfrom collections import dequefrom threading import Lockdef upload(item): passdef download(item): passdef resize(item): passclass MyQueue: def __init__(self) -&gt; None: self.items = deque() self.lock = Lock() def put(self, item): with self.lock: self.items.append(item) def get(self): with self.lock: return self.items.popleft()class Worker(Thread): def __init__(self, func, in_queue, out_queue): super().__init__() self.func = func self.in_queue = in_queue self.out_queue = out_queue self.polled_count = 0 # self.work_done = 0 def run(self): while True: self.polled_count += 1 try: item = self.in_queue.get() except IndexError: time.sleep(0.01) else: result = self.func(item) self.out_queue.put(result) # self.work_done += 1download_queue = MyQueue()resize_queue = MyQueue()upload_queue = MyQueue()done_queue = MyQueue()threads = [ Worker(download, download_queue, resize_queue), Worker(resize, resize_queue, upload_queue), Worker(upload, upload_queue, done_queue),]for thread in threads: thread.start()for i in range(100): download_queue.put(object())while len(done_queue.items) &lt; 100: passprocessed = len(done_queue.items)polled = sum(t.polled_count for t in threads)print(f'Processed &#123;processed&#125; items after ' f'polling &#123;polled&#125; times')# Processed 100 items after polling 308 times 上述实现虽然能够处理完成输入的所有任务，但仍存在很多问题。首先是 polled_count 值远大于任务的数量。即工作线程的 run 方法中定义的从队列中取项目的动作执行了太多次。各个工作函数的执行速度其实是不一致的，前置位的工作函数（比如 download）运行缓慢，会导致后一道工序（比如 resize）上的函数持续不断的向其队列请求新的任务，然而队列为空导致不断地触发 IndexError 错误，最终导致 CPU 时间的浪费。 其次，确认所有任务是否全部完成，需要一个 while 循环不断地检查 done_queue 队列中元素的数量。 再次，工作线程中的 run 方法会一直处于 while True 的循环当中，没有一种明显的方法可以向该工作线程发送任务完成可以退出的消息。 最后，当第一道工序执行很快而第二道工序执行很慢时，处于两道工序之间的队列中的元素数量会持续增长。如果有足够多的任务和足够长的时间，程序最终会耗尽内存并崩溃。 Queue内置的 queue 模块中的 Queue 类可以解决上述问题。 Queue 类中的 get 方法是阻塞的，即在有新的项目放置到队列中以前，get 会一直处于等待状态，直到获取到某个项目。12345678910111213141516171819202122232425import timefrom queue import Queuefrom threading import Threadmy_queue = Queue()def consumer(): print('Consumer waiting') my_queue.get() print('Consumer done')thread = Thread(target=consumer)thread.start()time.sleep(1)print('Producer putting')my_queue.put(object())print('Producer done')thread.join()# Consumer waiting# Producer putting# Producer done# Consumer done 即便线程先于主程序运行，它也会先处于等待状态，直到一个新的项目被放置到队列中，能够被 get 获取到。这可以解决前面的程序中 polled_count 值过大的问题。 Queue 类可以指定 buffer size，从而限制了两道工序间 pending 的任务的最大数量。即队列中的元素数量达到最大值后，向队列中放入新元素的 put 方法会阻塞，等待队列中某个元素被消耗从而为新元素腾出空间。12345678910111213141516171819202122232425262728293031import timefrom threading import Threadfrom queue import Queuemy_queue = Queue(1)def consumer(): time.sleep(1) my_queue.get() print('Consumer got 1') my_queue.get() print('Consumer got 2') print('Consumer done')thread = Thread(target=consumer)thread.start()my_queue.put(object())print('Producer put 1')my_queue.put(object())print('Producer put 2')print('Producer done')thread.join()# Producer put 1# Consumer got 1# Producer put 2# Producer done# Consumer got 2# Consumer done Consumer 线程中的 sleep 应该使得主程序有足够的时间将两个对象都放置到队列中。但队列的大小是 1，就导致队列中先放入的元素必须通过 get 方法取出之后，才能继续使用 put 方法放置新的元素进去。即 Producer 会等待 Consumer 线程把放置到队列中的旧元素消耗掉，才能继续向队列中添加新的元素。 task_doneQueue 类可以使用其 task_done 方法来追踪任务的进度，使得程序可以确保在某个特定的时间点，队列中的所有任务都已经被处理完成。12345678910111213141516171819202122232425262728293031from queue import Queuefrom threading import Threadimport timein_queue = Queue()def consumer(): print('Consumer waiting') work = in_queue.get() print('Consumer working') print('Consumer done') in_queue.task_done()thread = Thread(target=consumer)thread.start()print('Producer putting')in_queue.put(object())print('Producer waiting')in_queue.join()print('Producer done')thread.join()# Consumer waiting# Producer putting# Producer waiting# Consumer working# Consumer done# Producer done 在代码中调用 in_queue.join() 后，只有队列 in_queue 中的所有元素都执行了一遍 task_done（即有几个元素就需要几条 task_done），in_queue.join() 之后的代码才会执行。否则就继续等待，直到 Consumer 调用了足够次数的 task_done。 结合前面提到的特性，可以创建一个新的 Queue 类，它能够告知工作线程什么时候该停止执行。123456789101112131415class ClosableQueue(Queue): SENTINEL = object() def close(self): self.put(self.SENTINEL) def __iter__(self): while True: item = self.get() try: if item is self.SENTINEL: return # Cause the thread to exit yield item finally: self.task_done() 更新后的完整代码如下：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374import timefrom queue import Queuefrom threading import Threadfrom collections import dequefrom threading import Lockdef upload(item): passdef download(item): passdef resize(item): passclass ClosableQueue(Queue): SENTINEL = object() def close(self): self.put(self.SENTINEL) def __iter__(self): while True: item = self.get() try: if item is self.SENTINEL: return # Cause the thread to exit yield item finally: self.task_done()class StoppableWorker(Thread): def __init__(self, func, in_queue, out_queue): super().__init__() self.func = func self.in_queue = in_queue self.out_queue = out_queue def run(self): for item in self.in_queue: result = self.func(item) self.out_queue.put(result)download_queue = ClosableQueue()resize_queue = ClosableQueue()upload_queue = ClosableQueue()done_queue = ClosableQueue()threads = [ StoppableWorker(download, download_queue, resize_queue), StoppableWorker(resize, resize_queue, upload_queue), StoppableWorker(upload, upload_queue, done_queue),]for thread in threads: thread.start()for _ in range(1000): download_queue.put(object())download_queue.close()download_queue.join()resize_queue.close()resize_queue.join()upload_queue.close()upload_queue.join()print(done_queue.qsize(), 'items finished')# 1000 items finished 逻辑上就是给 Queue 类加了一个 SENTINEL 对象，用来作为队列结束的标志。工作线程通过循环读取输入队列中的任务，这些任务对象经过特定函数处理后放置到输出队列中。若读取到的任务是 SENTINEL 对象，则线程结束运行。 task_done 方法和主程序中的 xxx_queue.join 用于确保某个队列中的所有任务都已经处理完成，转移到了下一个队列中。后面再调用下一个队列的 close 方法在该队列尾部添加一个 SENTINEL 对象，作为队列的结束标志。 上述实现的好处在于，工作线程会在读取到 SENTINEL 对象时自动结束运行；主程序中 upload_queue.join() 执行结束后就能保证三个阶段的所有任务都被处理完了，而不再需要频繁地去检查 done_queue 中的元素数量。 最终实现当需要对不同的阶段（download、resize、upload）都分别绑定多个线程去处理时，只稍微修改下代码就可以了。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687import timefrom queue import Queuefrom threading import Threadfrom collections import dequefrom threading import Lockdef upload(item): passdef download(item): passdef resize(item): passclass ClosableQueue(Queue): SENTINEL = object() def close(self): self.put(self.SENTINEL) def __iter__(self): while True: item = self.get() try: if item is self.SENTINEL: return # Cause the thread to exit yield item finally: self.task_done()class StoppableWorker(Thread): def __init__(self, func, in_queue, out_queue): super().__init__() self.func = func self.in_queue = in_queue self.out_queue = out_queue def run(self): for item in self.in_queue: result = self.func(item) self.out_queue.put(result)def start_threads(count, *args): threads = [StoppableWorker(*args) for _ in range(count)] for thread in threads: thread.start() return threadsdef stop_threads(closable_queue, threads): for _ in threads: closable_queue.close() closable_queue.join() for thread in threads: thread.join()download_queue = ClosableQueue()resize_queue = ClosableQueue()upload_queue = ClosableQueue()done_queue = ClosableQueue()download_threads = start_threads( 3, download, download_queue, resize_queue)resize_threads = start_threads( 4, resize, resize_queue, upload_queue)upload_threads = start_threads( 5, upload, upload_queue, done_queue)for _ in range(1000): download_queue.put(object())stop_threads(download_queue, download_threads)stop_threads(resize_queue, resize_threads)stop_threads(upload_queue, upload_threads)print(done_queue.qsize(), 'items finished')# 1000 items finished 要点 Pipeline 可以很好地组织流水线类型的工作，尤其是 IO 相关的 Python 多线程程序 需要特别注意构建 pipeline 时的隐藏问题：怎样告诉工作线程终止运行、busy waiting 以及潜在的内存爆炸等 Queue 类具备构建健壮的 pipeline 所需的特性，如阻塞式操作、buffer size 和 joining 等。 参考资料Effective PYTHON Second Edition]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Program</tag>
        <tag>Advanced</tag>
        <tag>Concurrency</tag>
        <tag>Queue</tag>
        <tag>Pipeline</tag>
        <tag>Producer</tag>
        <tag>Consumer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Effective Python 笔记 —— 并发与并行（subprocess、Thread、Lock）]]></title>
    <url>%2F2021%2F09%2F23%2Feffective-python-notes-concurrency-subprocess-thread-and-lock%2F</url>
    <content type="text"><![CDATA[使用 subprocess 管理子进程由 Python 启动的子进程能够以并行的方式运行，从而最大化地利用 CPU 的多个核心。 可以借助 subprocess 内置模块调用子进程。1234567891011import subprocessresult = subprocess.run( ['echo', 'Hello from the child!'], capture_output=True, encoding='utf-8')result.check_returncode()print(result.stdout)# =&gt; Hello from the child! 子进程相对于其父进程是独立地运行的。如果使用 Popen 类创建一个子进程处理某个任务，则主程序能够在处理其他任务的同时，通过轮询的方式定期查看子进程的状态，确认其是否已经终止运行。Popen 中的 poll 方法可以实时地检查子进程的运行状态。若子进程还在运行中，则返回 None；若子进程执行完毕，则返回一个 returncode 值。 1234567891011import subprocessproc = subprocess.Popen(['sleep', '1'])while proc.poll() is None: print('Working...')print('Exit status', proc.poll())# Working...# Working...# Working...# ...# Exit status 0 解耦子进程与父进程使得父进程可以同时调用多个并行执行的子程序。123456789101112131415import timeimport subprocessstart = time.time()sleep_procs = []for _ in range(10): proc = subprocess.Popen(['sleep', '1']) sleep_procs.append(proc)for proc in sleep_procs: proc.communicate()end = time.time()print(f'Finished in &#123;(end - start):.3&#125; seconds')# =&gt; Finished in 1.01 seconds 代码中的 communicate 方法可以用来与子进程通信并等待其终止，此处用于等待所有的子进程执行完毕。如果上述代码中的子进程以顺序的方式执行，最终整体的延迟会达到 10s 以上。而实际的延迟只略大于 1s，即多个子进程之间是并行的关系。 可以通过管道从 Python 程序向调用的子进程传递数据，并获取子进程的输出内容。比如调用如下形式的 Shell 测试脚本：1234#!/bin/bashecho input your nameread nameecho your name is $name 123456789101112import subprocessproc = subprocess.Popen('bash name.sh', stdin=subprocess.PIPE, stdout=subprocess.PIPE, shell=True)proc.stdin.write(b'john')proc.stdin.flush()stdout, stderr = proc.communicate()print(stdout)# b'input your name\nyour name is john\n' 其中在初始化 Popen 对象时，传入了 stdin=subprocess.PIPE 和 stdout=subprocess.PIPE 两个参数，目的是将子进程的标准输入 STDIN 绑定到 proc 实例的 stdin 属性上，将标准输出 STDOUT 绑定到 proc 实例的 stdout 属性上。从而可以使用 proc.stdin.write() 方法向子进程传入数据。proc 实例的 communicate 方法会等待子进程终止，并返回 stdout 和 stderr，即子进程的标准输出和标准错误输出。若初始化 Popen 时未传入 stdout=subprocess.PIPE 参数，则上面返回的 stdout 为 None。 如果担心子程序永远不会终止或者长时间阻塞了输入和输出，可以向 communicate 方法传入 timeout 参数来指定等待的最长时间。1234567891011import subprocessproc = subprocess.Popen(['sleep', '10'])try: proc.communicate(timeout=0.1)except subprocess.TimeoutExpired: proc.terminate() proc.wait()print('Exit status', proc.poll())# Exit status -15 知识点 subprocess 模块可以调用子进程，且能够管理子进程的输入流和输出流，达到源程序与子进程交互的目的 子进程和 Python 解释器之间是并行运行的，因而可以最大化地利用 CPU 的多个核心 subprocess 模块提供的 run 函数可以完成简单的调用操作，而 Popen 类提供了类似 Unix 管线的高级功能 communicate 方法的 timeout 参数可以避免死锁及卡住的子进程 使用线程处理阻塞式 IOPython 的标准实现叫做 CPython。CPython 在运行 Python 程序时，会首先解析源代码并将其编译为字节码，再通过一个基于栈的解释器来运行字节码。CPython 通过一种称为 GIL 的机制来管理解释器自身的状态信息，强化其一致性。GIL 是一种可以阻止 CPython 解释器受抢占式多线程影响的互斥锁（mutex），从而使控制程序的线程不会被另一个线程意外中断，导致解释器的状态发生混乱。 但 GIL 有一个非常严重的负面影响。不像 C++ 或 Java 等语言可以利用多线程最大化多核心 CPU 的计算能力，Python 虽然支持多线程，但 GIL 会导致任一时刻实际上都只能有一个线程在推进。简单来说，Python 中的多线程不是并行计算，无法同时利用 CPU 的多个核心来提升计算密集型多任务的效率。 单线程处理计算密集型任务：123456789101112131415161718import timedef factorize(number): for i in range(1, number + 1): if number % i == 0: yield inumbers = [21390799, 12147599, 15166379, 18522859, 12345678, 87654321]start = time.time()for number in numbers: list(factorize(number))end = time.time()print(f'Took &#123;(end - start):.3&#125; seconds')# Took 6.19 seconds 多线程处理计算密集型任务：12345678910111213141516171819202122232425262728293031323334import timefrom threading import Threaddef factorize(number): for i in range(1, number + 1): if number % i == 0: yield iclass FactorizeThread(Thread): def __init__(self, number): super().__init__() self.number = number def run(self): self.factors = list(factorize(self.number))numbers = [21390799, 12147599, 15166379, 18522859, 12345678, 87654321]start = time.time()threads = []for number in numbers: thread = FactorizeThread(number) thread.start() threads.append(thread)for thread in threads: thread.join()end = time.time()print(f'Took &#123;(end -start):.3&#125; seconds')# Took 6.3 seconds 可以看出，Python 中的单线程和多线程在应对计算密集型任务时，两者的处理时间没有相差多少。 但是对于 IO 密集 型的任务，比如从磁盘读写文件、网络传输等阻塞式 IO 操作，使用 Python 中的多线程对于效率的提升就会非常显著。多线程使得 CPU 不必去等待缓慢的文件读写等 IO 操作。 单线程处理 IO 密集型任务：1234567891011121314import timefrom urllib.request import urlopendef get_example_page(): urlopen('https://example.org')start = time.time()for i in range(10): get_example_page()print(f'Took &#123;time.time() - start&#125; seconds')# Took 6.853585243225098 seconds 多线程处理 IO 密集型任务：123456789101112131415161718192021import timefrom urllib.request import urlopenfrom threading import Threaddef get_example_page(): urlopen('https://example.org')start = time.time()threads = []for _ in range(10): thread = Thread(target=get_example_page) thread.start() threads.append(thread)for thread in threads: thread.join()print(f'Took &#123;time.time() - start&#125; seconds')# Took 0.8039891719818115 seconds 知识点 由于 GIL 的存在，Python 中的线程无法并行地在多个 CPU 核心上执行 Python 中的多线程能够并行地发起多个系统调用，因而可以同时处理计算任务和阻塞式 IO 使用 Lock 避免数据竞争GIL 总是会阻止 Python 代码在多个 CPU 核心上并行执行，任意时刻都只能有一个 Python 线程处于活跃状态。但 GIL 并不会保护代码不受数据竞争的影响。一个线程对于数据结构的操作仍有可能被 Python 解释器中邻近的字节码破坏，尤其是在通过多线程同步地去访问同一个对象的时候。123456789101112131415161718192021222324252627282930313233from threading import Threadclass Counter: def __init__(self): self.count = 0 def increment(self, offset): self.count += offsetdef worker(sensor_index, how_many, counter): for _ in range(how_many): counter.increment(1)how_many = 10 ** 5counter = Counter()threads = []for i in range(5): thread = Thread(target=worker, args=(i, how_many, counter)) threads.append(thread) thread.start()for thread in threads: thread.join()expected = how_many * 5found = counter.countprint(f'Counter should be &#123;expected&#125;, got &#123;found&#125;')# Counter should be 500000, got 252472 上述代码模拟了一个从传感器网络并行地读取数据并计数的过程。对任意一个传感器，其数据的读取都属于阻塞式 IO，由独立的工作线程去处理，数据读取完成后该工作线程会调用一个计数器对象来累计结果。 但程序运行后，实际得到的计数结果与预期差距很大。Python 解释器在执行多个线程时会确保这些线程之间的“平等关系”，令它们获得几乎相等的处理时间。这因此需要 Python 时不时地在线程间进行切换，暂时挂起一个正在运行的线程，转而去恢复执行另一个线程。一个线程甚至有可能在看似符合原子性的操作中间被暂停。 比如 += 操作符在作用到实例的属性上时，类似这样的代码：1counter.count += 1 实际上等同于 Python 做出如下所示的三个分开的步骤：123value = getattr(counter, 'count')result = value + 1setattr(counter, 'count', result) 再加上线程切换，就有可能导致出现下面这种情况：123456789# Running in Thread Avalue_a = getattr(counter, 'count')# Context switch to Thread Bvalue_b = getattr(counter, 'count')result_b = value_b + 1setattr(counter, 'count', result_b)# Context switch back to Thread Aresult_a = value_a + 1setattr(counter, 'count', result_a) 即原本应该计算两次的累加操作实际上只有一次生效了，最终导致出现错误的结果。 为避免上述情形中的数据竞争或者其他形式的数据结构损坏现象，可以借助 Lock 类保护特定的值不被多个线程同步访问。即任一时刻都只能有一个线程可以获得该数据的锁。1234567891011121314151617181920212223242526272829303132333435from threading import Thread, Lockclass LockingCounter: def __init__(self): self.lock = Lock() self.count = 0 def increment(self, offset): with self.lock: self.count += offsetdef worker(sensor_index, how_many, counter): for _ in range(how_many): counter.increment(1)how_many = 10 ** 5counter = LockingCounter()threads = []for i in range(5): thread = Thread(target=worker, args=(i, how_many, counter)) threads.append(thread) thread.start()for thread in threads: thread.join()expected = how_many * 5found = counter.countprint(f'Counter should be &#123;expected&#125;, got &#123;found&#125;')# Counter should be 500000, got 500000 知识点 Python 有 GIL，但在编写代码时仍需关注多线程中的数据竞争 允许多个线程修改同一个不加锁的对象，有可能会损坏数据结构 Lock 类可以保护多线程中数据的一致性 参考资料Effective PYTHON Second Edition]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Program</tag>
        <tag>Advanced</tag>
        <tag>Concurrency</tag>
        <tag>Thread</tag>
        <tag>Process</tag>
        <tag>Lock</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[The Rust programming language 读书笔记——不安全 Rust]]></title>
    <url>%2F2021%2F08%2F11%2Fthe-rust-programming-language-reading-notes-unsafe-rust%2F</url>
    <content type="text"><![CDATA[Rust 内部隐藏了一种不会强制实施内存安全保障的语言：不安全 Rust。其之所以存在，是因为静态分析从本质上讲是保守的，它宁可错杀一些合法程序也不会接受可能非法的代码。使用不安全代码的缺点在于程序员需要对自己的行为负责。若错误地使用了不安全代码，就可能引发不安全的内存问题，如空指针解引用等。 另一个原因在于底层计算机硬件固有的不安全性。若 Rust 不允许进行不安全的操作，则某些底层任务可能根本就完成不了。 不安全 Rust 允许你执行 4 种在安全 Rust 中不被允许的操作： 解引用裸指针 调用不安全的函数或方法 访问或修改可变的静态变量 实现不安全 trait 可以在代码块前使用关键字 unsafe 来切换到不安全模式。unsafe 关键字并不会关闭借用检查器或禁用任何其他 Rust 安全检查。unsafe 仅仅令你可以访问上述 4 种不会被编译器检查的特性。因此即便处于不安全的代码块中，也仍然可以获得一定程度的安全性。 unsafe 并不意味着块中的代码一定就是危险的或一定会导致内存安全问题，它仅仅是将责任转移到了程序员的肩上。通过对 4 种不安全操作标记上 unsafe，可以在出现内存相关的错误时快速地将问题定位到 unsafe 代码块中。应当尽量避免使用 unsafe 代码块。 为了尽可能地隔离不安全代码，可以将其封装在一个安全的抽象中并提供一套安全的 API。实际上某些标准库功能同样使用了不安全代码，并以此为基础提供了安全的抽象接口。 解引用裸指针不安全 Rust 拥有两种类似于引用的新指针类型，都被叫做裸指针（raw pointer）。与引用类似，裸指针要么是可变的，要么是不可变的，分别写作 *const T 和 *mut T。这里的星号 * 是类型名的一部分而不代表解引用操作。 裸指针与引用、智能指针的区别： 允许忽略借用规则，可以同时拥有指向同一个内存地址的可变和不可变指针，或者拥有指向同一个地址的多个可变指针 不能保证自己总是指向了有效的内存地址 允许为空 没有实现任何自动清理机制 在避免 Rust 强制执行某些保障后，就能够以放弃安全保障为代价换取更好的性能，或者与其他语言、硬件进行交互的能力。 通过引用创建裸指针12345fn main() &#123; let mut num = 5; let r1 = &amp;num as *const i32; let r2 = &amp;mut num as *mut i32;&#125; 上述代码中并没有使用 unsafe 关键字。你可以在安全代码内合法地创建裸指针，但不能在不安全代码块外解引用裸指针。 创建一个指向任意内存地址的裸指针，这个地址可能有数据，也可能没有数据，因此无法确定其有效性。12let address = 0x012345usize;let r = address as *const i32; 为了使用 * 解引用裸指针，需要添加一个 unsafe 块：12345678910fn main() &#123; let mut num = 5; let r1 = &amp;num as *const i32; let r2 = &amp;mut num as *mut i32; unsafe &#123; println!("r1 is: &#123;&#125;", *r1); println!("r2 is: &#123;&#125;", *r2); &#125;&#125; 在使用裸指针时，我们可以创建同时指向同一地址的可变指针和不可变指针，并通过可变指针来修改数据。这样的修改操作会导致潜在的数据竞争。裸指针主要用来与 C 代码接口进行交互，或者构造一些借用检查器无法理解的安全抽象。 调用不安全函数或方法除了在定义前面要标记 unsafe，不安全函数或方法看上去与正常的函数或方法几乎一模一样。这里的 unsafe 关键字意味着我们需要在调用该函数时手动满足一些先决条件，因为 Rust 无法对这些条件进行验证。通过在 unsafe 代码块中调用不安全函数，我们向 Rust 表明自己确实理解并实现了相关约定。12345unsafe fn dangerous() &#123;&#125;unsafe &#123; dangerous();&#125; 因为不安全函数的函数体也是 unsafe 代码块，你可以直接在一个不安全函数中执行其他不安全操作而无需添加额外的 unsafe 代码块。 创建不安全代码的安全抽象函数中包含不安全代码并不意味着我们需要将整个函数都标记为不安全的。实际上，将不安全代码封装在安全函数中是一种十分常见的抽象。 比如标准库中使用了不安全代码的 split_at_mut 函数。这个安全方法被定义在可变切片上，它接收一个切片并从给定的索引参数处将其分割为两个切片。12345678fn main() &#123; let mut v = vec![1, 2, 3, 4, 5, 6]; let r = &amp;mut v[..]; let (a, b) = r.split_at_mut(3); assert_eq!(a, &amp;mut [1, 2, 3]); assert_eq!(b, &amp;mut [4, 5, 6]);&#125; 我们无法仅仅使用安全 Rust 来实现这个函数。比如尝试用安全代码将 split_at_mut 实现为函数，并只处理 i32 类型的切片：1234567fn split_at_mut(slice: &amp;mut [i32], mid: usize) -&gt; (&amp;mut [i32], &amp;mut [i32]) &#123; let len = slice.len(); assert!(mid &lt;= len); (&amp;mut slice[..mid], &amp;mut slice[mid..])&#125; 这个函数会首先取得整个切片的长度，并通过断言检查给定的参数是否小于或等于当前切片的长度。若大于则会在尝试使用该索引前触发 panic。我们会返回一个包含两个可变切片的元组，一个从原切片的起始位置到 mid 索引的位置，另一个则从 mid 索引的位置到原切片的末尾。 尝试编译上述代码会触发 error[E0499]: cannot borrow `*slice` as mutable more than once at a time 错误。Rust 的借用检查器无法理解我们正在借用一个切片的不同部分，它只知道我们借用了两次同一个切片。借用一个切片的不同部分从原理上来讲是没有任何问题的，因为没有交叉的地方。但 Rust 没有足够智能到理解这些信息。此类场景即适用于不安全代码。 使用 unsafe、裸指针及一些不安全函数实现 split_at_mut：123456789101112131415161718192021222324use std::slice;fn split_at_mut(slice: &amp;mut [i32], mid: usize) -&gt; (&amp;mut [i32], &amp;mut [i32]) &#123; let len = slice.len(); let ptr = slice.as_mut_ptr(); assert!(mid &lt;= len); unsafe &#123; ( slice::from_raw_parts_mut(ptr, mid), slice::from_raw_parts_mut(ptr.offset(mid as isize), len - mid), ) &#125;&#125;fn main() &#123; let mut v = vec![1, 2, 3, 4, 5, 6]; let r = &amp;mut v[..]; let (a, b) = split_at_mut(r, 3); assert_eq!(a, &amp;mut [1, 2, 3]); assert_eq!(b, &amp;mut [4, 5, 6]);&#125; 在 unsafe 代码中，slice::from_raw_parts_mut 函数接收一个裸指针和长度来创建一个切片。这里使用该函数从 ptr 处创建了一个拥有 mid 个元素的切片，接着又在 ptr 上使用 mid 作为偏移量参数调用 offset 方法得到了一个从 mid 处开始的裸指针，并基于它创建了另外一个起始于 mid 处且拥有剩余所有元素的切片。 函数 slice::from_raw_parts_mut 接收一个裸指针作为参数并默认该参数的合法性，所以它是不安全的。裸指针的 offset 方法默认此地址的偏移量也是一个有效的指针，它也是不安全的。因此我们必须在 unsafe 代码块中调用上述两个函数。通过审查代码并添加断言，我们可以确定 unsafe 中的裸指针都会指向有效的切片数据且不会产生数据竞争。这就是一个恰当的 unsafe 使用场景。 代码没有将 split_at_mut 函数标记为 unsafe，因此我们可以在安全 Rust 中调用该函数。这就是对不安全代码的安全抽象。 与上述代码相反，下面对 slice::from_raw_parts_mut 函数的调用就很有可能导致崩溃。其试图用一个随意的内存地址来创建拥有 10000 个元素的切片。123456789use std::slice;fn main() &#123; let address = 0x01234usize; let r = address as *mut i32; let slice: &amp;[i32] = unsafe &#123; slice::from_raw_parts_mut(r, 10000) &#125;; println!("&#123;&#125;", slice);&#125; 使用 extern 函数调用外部代码Rust 代码可能需要与另外一种语言编写的代码进行交互。Rust 为此提供了 extern 关键字来简化创建和使用外部函数接口（FFI）的过程。任何 extern 块中声明的函数都是不安全的。因为其他语言不会强制执行 Rust 遵守的规则，Rust 又无法对它们进行检查。因此保证安全的责任就落到了开发者身上。 下面的代码集成了 C 标准库中的 abs 函数。123456789extern "C" &#123; fn abs(input: i32) -&gt; i32;&#125;fn main() &#123; unsafe &#123; println!("Absolute value of -3: &#123;&#125;", abs(-3)); &#125;&#125; 访问或修改静态变量Rust 支持全局变量，但在使用的过程中可能因为所有权机制而产生某些问题。如果两个线程同时访问同一个可变的全局变量，就会产生数据竞争。全局变量也被称为静态（static）变量。12345static HELLO_WORLD: &amp;str = "Hello World";fn main() &#123; println!("name is: &#123;&#125;", HELLO_WORLD);&#125; 静态变量必须要标注类型，访问一个不可变的静态变量是安全的。静态变量的值在内存中拥有固定的地址，使用它的值总会访问到同样的数据。而常量则允许在任何被使用到的时候复制其数据。与常量不同的是，静态变量是可变的。但访问和修改可变的静态变量是不安全的。123456789101112131415static mut COUNTER: u32 = 0;fn add_to_count(inc: u32) &#123; unsafe &#123; COUNTER += inc; &#125;&#125;fn main() &#123; add_to_count(3); unsafe &#123; println!("COUNTER: &#123;&#125;", COUNTER); &#125;&#125; 在上述代码中，任何读写静态变量 COUNTER 的代码都必须位于 unsafe 代码块中。 实现不安全 trait当某个 trait 中存在至少一个方法拥有编译器无法校验的不安全因素时，我们就称这个 trait 是不安全的。可以在 trait 定义的前面加上 unsafe 关键字来声明一个不安全 trait，同时该 trait 也只能在 unsafe 代码块中实现。1234567unsafe trait Foo &#123; // 某些方法&#125;unsafe impl Foo for i32 &#123; // 对应的方法实现&#125; 通过使用 unsafe impl，我们向 Rust 保证我们会手动维护好那些编译器无法验证的不安全因素。 参考资料The Rust Programming Language]]></content>
      <categories>
        <category>Program</category>
      </categories>
      <tags>
        <tag>Program</tag>
        <tag>Advanced</tag>
        <tag>Rust</tag>
        <tag>Pointer</tag>
        <tag>Memory</tag>
        <tag>Unsafe</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[The Rust programming language 读书笔记——面向对象编程]]></title>
    <url>%2F2021%2F08%2F11%2Fthe-rust-programming-language-reading-notes-oop%2F</url>
    <content type="text"><![CDATA[面向对象编程（OOP）是一种程序建模的方法。通常认为面向对象的语言需要包含命名对象、封装、继承等特性。 对象包含数据和行为 面向对象的程序由对象构成。对象包装了数据和操作这些数据的流程（称作方法)。基于这个定义，Rust 是面向对象的。比如结构体和枚举都可以包含数据，而 impl 块则提供了可用于结构体和枚举的方法。 封装实现细节封装使得调用对象的外部代码无法直接访问对象内部的实现细节，而唯一可以与对象进行交互的方法便是通过它公开的接口。使用对象的代码不应当深入对象的内部去改变数据或行为，封装使得开发者在修改或重构对象的内部实现时无需改变调用这个对象的外部代码。 在 Rust 中，我们可以使用 pub 关键字来决定代码中哪些模块、类型、函数和方法是公开的，而默认情况下所有内容都是私有的。 如下面计算移动平均值的代码：1234567891011121314151617181920212223242526272829303132333435363738394041424344pub struct AveragedCollection &#123; list: Vec&lt;i32&gt;, average: f64,&#125;impl AveragedCollection &#123; pub fn add(&amp;mut self, value: i32) &#123; self.list.push(value); self.update_average(); &#125; pub fn remove(&amp;mut self) -&gt; Option&lt;i32&gt; &#123; let result = self.list.pop(); match result &#123; Some(value) =&gt; &#123; self.update_average(); Some(value) &#125; None =&gt; None, &#125; &#125; pub fn average(&amp;self) -&gt; f64 &#123; self.average &#125; fn update_average(&amp;mut self) &#123; let total: i32 = self.list.iter().sum(); self.average = total as f64 / self.list.len() as f64; &#125;&#125;fn main() &#123; let mut collection = AveragedCollection &#123; list: vec![], average: 0.0, &#125;; collection.add(1); collection.add(2); collection.add(3); println!("The average of the collection is: &#123;&#125;", collection.average());&#125; 先是定义了一个名为 AveragedCollection 的结构体，其 list 字段包含了一个存储 i32 元素的动态数组；为了避免每次取元素平均值的时候重复计算，又添加了一个用于存储平均值的 average 字段。结构体本身被标记为 pub 使得其他代码可以使用它，但其内部字段仍然保持私有。公共方法 add、remove、average 是仅有的几个可以访问或修改 AveragedCollection 实例中数据的方法。当用户调用 add 方法向 list 中添加元素，或者调用 remove 从 list 中删除元素时，方法内部的实现都会再调用私有方法 update_average 来更新 average 字段。 由于 list 和 average 字段是私有的，外部代码无法直接读取 list 字段来增加或删除其中的元素。一旦缺少了这样的封装，average 字段便无法在用户私自更新 list 字段时同步保持更新。 因为结构体 AveragedCollection 封装了内部的实现细节，我们能够在未来轻松地改变数据结构等内部实现。比如可以在 list 字段上使用 HashSet&lt;i32&gt; 替代 Vec&lt;i32&gt;。只要 add、remove、average 这几个公共方法的签名保持不变，正在使用 AveragedCollection 的外部代码就无需进行任何修改。假如将 list 字段声明为 pub，就必然会失去上面这一优势。HashSet&lt;i32&gt; 和 Vec&lt;i32&gt; 在增加或删除元素时使用的具体方法是不同的，因此若直接修改 list，外部代码将不得不随之发生变化。 作为类型系统和代码共享机制的继承继承机制使得对象可以沿用另一个对象的数据与行为，而无需重复定义代码。Rust 中无法定义一个继承父结构体字段和方法的子结构体。 选择继承主要有两个原因。其一是代码复用，作为替代方案，可以使用 Rust 中的默认 trait 方法来进行代码共享。它与继承十分相似，父类中实现的方法可以被继承它的子类所拥有；子类也可以选择覆盖父类中的方法。 另一个使用继承的原因与类型系统有关，希望子类型能够被应用到一个需要父类型的地方。即多态：如果一些对象具有某些共同的特征，则这些对象就可以在运行时相互替换使用。可以在 Rust 中使用泛型来构建不同类型的抽象，并使用 trait 约束来决定类型必须提供的具体特性。这一技术被称为限定参数化多态。 许多较为新潮的语言已经不太喜欢将继承作为内置的程序设计方案，因为使用继承意味着你会无意间共享出比所需内容更多的代码。子类并不应该总是共享父类的所有特性，但使用继承机制却会始终产生这样的结果，进而使程序设计缺乏灵活性。而某些语言强制要求子类只能继承自单个父类，进一步限制了程序设计的灵活性。 使用 trait 对象来存储不同类型的值动态数组有一个限制，即只能存储同一类型的元素。有些时候的变通方案可以使用枚举。比如定义一个 SpreadsheetCell 枚举同时包含了可以持有整数、浮点数和文本的变体。这样我们就可以在每个表格中存储不同的数据类型，且依然能够用一个动态数组来表示一整行单元格。但是总有某些时候，我们希望用户能够在特定的场景下为类型的集合进行扩展。 比如需要创建一个含有 GUI 库架构的 gui 包，并在包中提供一些可供用户使用的具体类型，如 Button 或 TextField 等，这些类型都实现了 draw 方法用于支持将其绘制到屏幕中。此外，gui 的用户也应当能够创建支持绘制的自定义类型，如某些开发者可能会添加 Image，另一些可能会添加 SelectBox 等。在那些支持继承的语言中，我们可以定义出一个拥有 draw 方法的 Component 类。其他如 Button、Image、SelectBox 等则都需要继承 Component 类来获得 draw 方法。当然也可以选择覆盖 draw 方法来实现自定义行为，但框架会在处理过程中将它们全部视作 Component 类型的实例，并以此调用 draw 方法。 为共有行为定义一个 traitRust 没有继承功能。为了在 gui 中实现预期的功能，需要定义一个拥有 draw 方法的 Draw trait。trait 对象可以被用在泛型或具体类型所处的位置，无论我们在哪里使用 trait 对象，Rust 类型系统都会在编译时确保出现在相应位置上的值实现了 trait 对象中的指定方法。 Rust 有意避免将结构体和枚举称为对象，以便于与其他语言中的对象区别开。对于结构体和枚举而言，其字段中的数据与 impl 块中的行为是分开的；而在其他语言中，数据和行为往往被组合在名为对象的概念中。trait 对象则有些类似于其他语言中的对象，它也在某种程度上组合了数据和行为。但 trait 对象被专门用于抽象某些共有行为，没有其他语言中的对象那么通用。 创建一个名为 gui 的 Rust 项目：cargo new gui 在 lib.rs 中定义一个拥有 draw 方法的 Draw trait：1234// src/lib.rspub trait Draw &#123; fn draw(&amp;self);&#125; 定义一个持有 components 动态数组的 Screen 结构体，代码中的 Box&lt;dyn Draw&gt; 代表所有被放置在 Box 中且实现了 Draw trait 的具体类型。1234// src/lib.rspub struct Screen &#123; pub components: Vec&lt;Box&lt;dyn Draw&gt;&gt;,&#125; Screen 结构体定义了一个名为 run 的方法，会逐一调用 components 中每个元素的 draw 方法：12345678// src/lib.rsimpl Screen &#123; pub fn run(&amp;self) &#123; for component in self.components.iter() &#123; component.draw(); &#125; &#125;&#125; 实现 trait在代码中添加一些实现了 Draw trait 的具体类型。需要注意的是，draw 方法不会包含任何有意义的内容，仅作为演示：12345678910111213// src/lib.rspub struct Button &#123; pub width: u32, pub height: u32, pub label: String, &#125;impl Draw for Button &#123; fn draw(&amp;self) &#123; println!("Drawing a button, the button's label is &#123;&#125;", self.label); // 实际绘制一个按钮的代码 &#125;&#125; Button 中持有的 width、height 和 label 字段也许会不同于其他组件中的字段，比如 TextField 类型就可能在这些字段外额外持有一个 placeholder 字段。每一个希望绘制在屏幕上的类型都应当实现 Draw trait，并在 draw 方法中使用不同的代码来自定义具体的绘制行为。除了实现 Draw trait，Button 类型也许会在另外的 impl 块中实现响应用户点击按钮时的行为，这些方法并不适用于 TextField 等其他类型。 用户也可以在 main.rs 中为SelectBox 这种自定义类型实现 Draw trait：123456789101112131415// src/main.rsuse gui::Draw;struct SelectBox &#123; width: u32, height: u32, options: Vec&lt;String&gt;,&#125;impl Draw for SelectBox &#123; fn draw(&amp;self) &#123; println!("Drawing a selectbox, the options are &#123;:?&#125;", self.options); // 实际绘制一个选择框的代码 &#125;&#125; 此时就可以在编写 main 函数的时候创建 Screen 实例了。使用 Box&lt;T&gt; 生成 SelectBox 或 Button 的 trait 对象，再将它们添加到 Screen 实例中。便可以运行 Screen 实例的 run 方法来依次调用所有组件的 draw 实现：1234567891011121314151617181920212223242526// src/main.rsuse gui::&#123;Screen, Button&#125;;fn main() &#123; let scrren = Screen &#123; components: vec![ Box::new(SelectBox &#123; width: 75, height: 10, options: vec![ String::from("Yes"), String::from("Maybe"), String::from("No"), ], &#125;), Box::new(Button &#123; width: 50, height: 10, label: String::from("OK"), &#125;), ], &#125;; scrren.run()&#125;// =&gt; Drawing a selectbox, the options are ["Yes", "Maybe", "No"]// =&gt; Drawing a button, the button's label is OK 我们在编写库的时候无法得知用户是否会添加自定义的 SelectBox 类型，但我们的 Screen 实现依然能够接收新的类型并完成绘制工作。因为 SelectBox 实现了 Draw trait 及其 draw 方法。 run 方法只关心值对行为的响应，而不在意值的具体类型。这一概念与动态类型中的 duck typing 十分相似。通过在定义动态数组 components 时指定 Box&lt;dyn Draw&gt; 元素类型，Screen 实例只会接收那些能够调用 draw 方法的值，而不会去检查该值究竟是 Button 实例还是 SelectBox 实例。 使用 trait 对象与类型系统实现 duck typing 的优势在于，不需要在运行时检查某个值是否实现了指定的方法，或者担心出现调用未定义方法等运行时错误。Rust 会在编译时发现这类错误。 参考资料The Rust Programming Language]]></content>
      <categories>
        <category>Program</category>
      </categories>
      <tags>
        <tag>Program</tag>
        <tag>OOP</tag>
        <tag>Advanced</tag>
        <tag>Inheritance</tag>
        <tag>Encapsulation</tag>
        <tag>Rust</tag>
        <tag>Trait</tag>
        <tag>Polymorphism</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[The Rust programming language 读书笔记——并发]]></title>
    <url>%2F2021%2F07%2F29%2Fthe-rust-programming-language-reading-notes-concurrency%2F</url>
    <content type="text"><![CDATA[并发编程（concurrent programming）允许程序中的不同部分相互独立地运行，而并行编程（parallel programming）则允许程序中的不同部分同时执行。Rust 中的所有权和类型系统能够同时帮助开发者管理内存安全及并发问题。 高级语言往往通过放弃部分控制能力来获得有益于用户的抽象，因此只支持全部解决方案的一部分是可以理解的设计策略。比如 Erlang 提供了一套优雅的消息传递并发特性，但没有提供可以在线程间共享状态的简单方法。底层语言被期望在任意场景下都可以提供一套性能最佳的解决方案，并对硬件建立尽可能少的抽象，因此 Rust 提供了多种建模并发问题的工具。 使用线程同时运行代码多个线程可以同时运行，因此将程序中的计算操作拆分至多个线程可以提高性能。但也增加了程序的复杂度，因为不同线程在执行过程中的具体顺序是无法确定的。可能导致下列问题： 当多个线程以不一致的顺序访问数据或资源时产生的竞争状态（race condition） 当两个线程同时尝试获取对方持有的资源时产生的死锁（deadlock），会导致这两个线程都无法继续运行 只会出现在特定情形下且难以稳定复现和修复的 bug 使用 spawn 创建新线程可以调用 thread::spawn 函数来创建线程，它接收一个闭包作为参数，该闭包会包含我们想要在新线程中运行的代码：123456789101112131415use std::thread;use std::time::Duration;fn main() &#123; thread::spawn(|| &#123; for i in 1..10 &#123; println!("hi number &#123;&#125; from the spawned thread!", i); thread::sleep(Duration::from_millis(1)); &#125; &#125;); for i in 1..5 &#123; println!("hi number &#123;&#125; from the main thread!", i); thread::sleep(Duration::from_millis(1)); &#125;&#125; 程序输出如下：123456789hi number 1 from the main thread!hi number 1 from the spawned thread!hi number 2 from the main thread!hi number 2 from the spawned thread!hi number 3 from the main thread!hi number 3 from the spawned thread!hi number 4 from the main thread!hi number 4 from the spawned thread!hi number 5 from the spawned thread! 主线程首先打印出了文本，即便新线程的打印语句出现得更早一些。这些线程可能会交替执行，执行顺序由操作系统的线程调度策略决定。需要注意的是，只要上述程序中的主线程运行结束，创建出的新线程也会停止，不管其打印任务是否完成。虽然我们要求新线程不停打印文本直到 i 迭代到 9，但它在主线程停止前仅迭代到了 5。 使用 join 句柄等待所有线程结束thread::spawn 的返回类型是一个自持有所有权的 joinHandle，调用它的 join 方法可以阻塞当前线程直到对应的新线程运行结束。 调用 join 方法保证新线程能够在 main 函数退出前执行完毕：12345678910111213141516use std::thread;use std::time::Duration;fn main() &#123; let handle = thread::spawn(|| &#123; for i in 1..10 &#123; println!("hi number &#123;&#125; from the spawned thread!", i); thread::sleep(Duration::from_millis(1)); &#125; &#125;); for i in 1..5 &#123; println!("hi number &#123;&#125; from the main thread!", i); thread::sleep(Duration::from_millis(1)); &#125; handle.join().unwrap();&#125; 输出结果如下：12345678910111213hi number 1 from the main thread!hi number 1 from the spawned thread!hi number 2 from the main thread!hi number 2 from the spawned thread!hi number 3 from the main thread!hi number 3 from the spawned thread!hi number 4 from the main thread!hi number 4 from the spawned thread!hi number 5 from the spawned thread!hi number 6 from the spawned thread!hi number 7 from the spawned thread!hi number 8 from the spawned thread!hi number 9 from the spawned thread! 假如将 handle.join() 放置到 main 函数的 for 循环之前，即：12345678910111213141516use std::thread;use std::time::Duration;fn main() &#123; let handle = thread::spawn(|| &#123; for i in 1..10 &#123; println!("hi number &#123;&#125; from the spawned thread!", i); thread::sleep(Duration::from_millis(1)); &#125; &#125;); handle.join().unwrap(); for i in 1..5 &#123; println!("hi number &#123;&#125; from the main thread!", i); thread::sleep(Duration::from_millis(1)); &#125;&#125; 在上述代码中，由于主线程会等待新线程执行完毕后才开始执行自己的 for 循环，程序的输出将不再出现交替的情形：12345678910111213hi number 1 from the spawned thread!hi number 2 from the spawned thread!hi number 3 from the spawned thread!hi number 4 from the spawned thread!hi number 5 from the spawned thread!hi number 6 from the spawned thread!hi number 7 from the spawned thread!hi number 8 from the spawned thread!hi number 9 from the spawned thread!hi number 1 from the main thread!hi number 2 from the main thread!hi number 3 from the main thread!hi number 4 from the main thread! 在并发编程中，诸如在哪里调用 join 等微小细节也会影响到多个线程是否能够同时运行。 在线程中使用 move 闭包move 闭包常被用来与 thread::spawn 函数配合使用，允许在某个线程中使用来自另一个线程的数据。1234567891011use std::thread;fn main() &#123; let v = vec![1, 2, 3]; let handle = thread::spawn(|| &#123; println!("Here's a vector: &#123;:?&#125;", v); &#125;); handle.join().unwrap();&#125; 在编译上述代码时会报出 error[E0373]: closure may outlive the current function, but it borrows `v`, which is owned by the current function 错误。Rust 在推导出如何捕获 v 后决定让闭包借用 v，因为闭包中的 println! 只需要使用 v 的引用。但 Rust 不知道新线程会运行多久，因此它无法确定 v 的引用是否一直有效。 通过在闭包前添加 move 关键字，会强制闭包获得它所需值的所有权，而不仅仅是基于 Rust 的推导来获得值的借用。1234567891011use std::thread;fn main() &#123; let v = vec![1, 2, 3]; let handle = thread::spawn(move || &#123; println!("Here's a vector: &#123;:?&#125;", v); &#125;); handle.join().unwrap();&#125; 使用消息传递在线程间转移数据使用消息传递（message passing）机制来保证并发安全正变得越来越流行。Go 语言文档中的口号正体现了这样的思路：不要通过共享内存来通信，而是通过通信来共享内存。 Rust 的标准库中实现了一个名为通道（channel）的编程概念，可以被用来实现基于消息传递的并发机制。通道由发送者（transmitter）和接收者（receiver）两部分组成。发送者位于通道的上游，接收者位于下游。某一处的代码可以通过调用发送者的方法来传送数据，另一处代码则可以通过检查接收者来获取数据。当发送者或接收者的任何一端被丢弃，则相应的通道被关闭。 12345678910111213141516use std::sync::mpsc;use std::thread;fn main() &#123; let (tx, rx) = mpsc::channel(); thread::spawn(move || &#123; let val = String::from("hi"); tx.send(val).unwrap(); &#125;); let received = rx.recv().unwrap(); println!("Got: &#123;&#125;", received);&#125;// =&gt; Got: hi 上述代码使用 mpsc::channel 函数创建了一个新的通道。mpsc 是英文 multiple producer, single consum（多个生产者，单个消费者）的缩写。mpsc::channel 会返回一个含有发送端与接收端的元组。再使用 thread::spawn 生成一个新线程。为了令新线程拥有发送端 tx 的所有权，使用 move 关键字将 tx 移动到了闭包的环境中。新线程必须拥有通道发送端的所有权才能通过通道来发送消息。发送端提供了 send 方法来处理我们想要发送的值，该方法会返回 Result&lt;T, E&gt; 类型作为结果。当接收端已经被丢弃而无法继续传递内容时，执行发送操作会返回一个错误。 通道的接收端有两个可用于获取消息的方法。其中 recv 会阻塞主线程的执行直到有值被传入通道。一旦有值传入通道，recv 就会将其包裹在 Result&lt;T, E&gt; 中返回。若通道的发送端全部关闭了，recv 会返回一个错误来表明当前通道再也没有可接收的值。try_recv 方法不会阻塞线程，它会立即返回 Result&lt;T, E&gt;。当通道中存在消息时，返回包含该消息的 Ok 变体；否则返回 Err 变体。可以编写一个不断调用 try_recv 方法的循环，并在有消息时对其进行处理，没有消息时执行其他指令。 发送多个值123456789101112131415161718192021222324252627282930use std::sync::mpsc;use std::thread;use std::time::Duration;fn main() &#123; let (tx, rx) = mpsc::channel(); thread::spawn(move || &#123; let vals = vec![ String::from("hi"), String::from("from"), String::from("the"), String::from("thread"), ]; for val in vals &#123; tx.send(val).unwrap(); thread::sleep(Duration::from_secs(1)); &#125; &#125;); for received in rx &#123; println!("Got: &#123;&#125;", received); &#125;&#125;// =&gt; Got: hi// =&gt; Got: from// =&gt; Got: the// =&gt; Got: thread 上述代码会迭代新线程中的动态数组来逐个发送其中的字符串，并在每次发送后调用 thread::sleep 函数来稍作暂停。在主线程中，我们会将 rx 视作迭代器，不再显式地调用 recv 函数。迭代中的代码会打印出每个接收到的值，并在通道关闭时退出循环。代码执行时每次打印后都会出现 1 秒的时间间隔。但我们并没有在主线程的 for 循环中执行延迟指令，表明主线程确实在等待接收新线程中传递过来的值。 通过克隆发送者创建多个生产者通过克隆通道的发送端来创建出多个能够发送值到同一个接收端的线程：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849use std::sync::mpsc;use std::thread;use std::time::Duration;fn main() &#123; let (tx, rx) = mpsc::channel(); let tx1 = mpsc::Sender::clone(&amp;tx); thread::spawn(move || &#123; let vals = vec![ String::from("hi"), String::from("from"), String::from("the"), String::from("thread"), ]; for val in vals &#123; tx1.send(val).unwrap(); thread::sleep(Duration::from_secs(1)); &#125; &#125;); thread::spawn(move || &#123; let vals = vec![ String::from("more"), String::from("messages"), String::from("for"), String::from("you"), ]; for val in vals &#123; tx.send(val).unwrap(); thread::sleep(Duration::from_secs(1)); &#125; &#125;); for received in rx &#123; println!("Got: &#123;&#125;", received); &#125;&#125;// =&gt; Got: hi// =&gt; Got: more// =&gt; Got: from// =&gt; Got: messages// =&gt; Got: for// =&gt; Got: the// =&gt; Got: you// =&gt; Got: thread 共享状态的并发从某种程度上来说，任何编程语言中的通道都有些类似于单一所有权的概念。因为用户不应该在值传递给通道后再次使用它。基于共享内存的并发通信机制更类似于多重所有权概念，多个线程可以同时访问相同的内存地址。我们可以通过智能指针实现多重所有权，但由于需要同时管理多个所有者，会为系统增加额外的复杂性。当然，Rust 的类型系统和所有权规则有助于正确地管理这些所有权。 互斥体（mutex）互斥体在任意时刻只允许一个线程访问数据。为了访问互斥体中的数据，线程必须首先发出信号来获取互斥体的锁（lock）。锁是互斥体的一部分，这种数据结构被用来记录当前谁拥有数据的唯一访问权。 关于互斥体必须牢记以下两条规则： 必须在使用数据前尝试获取锁 必须在使用完互斥体守护的数据后释放锁，这样其他线程才能继续执行获取锁的操作 在单线程环境中使用互斥体：12345678910111213use std::sync::Mutex;fn main() &#123; let m = Mutex::new(5); &#123; let mut num = m.lock().unwrap(); *num = 6; &#125; println!("m = &#123;:?&#125;", m);&#125;// =&gt; m = Mutex &#123; data: 6 &#125; 为了访问 Mutex&lt;T&gt; 实例中的数据，我们首先需要调用其 lock 方法来获取锁。此调用会阻塞当前线程直到我们取得锁为止。当前线程对于 lock 函数的调用会在其他某个持有锁的线程发生 panic 时失败，因此上述代码选择使用 unwrap 在意外发生时触发当前线程的 panic。 一旦获取了锁，便可以将它的返回值 num 视作一个指向内部数据的可变引用。Rust 的类型系统会确保我们在使用 m 的值之前执行加锁操作。因为 Mutex&lt;i32&gt; 并不是 i32 类型，我们必须获取锁才能使用其内部的 i32 值。 实际上对 lock 的调用会返回一个名为 MutexGuard 的智能指针。该智能指针通过实现 Deref 来指向存储在内部的数据，通过实现 Drop 完成自己离开作用域时的自动解锁操作。这种释放过程会发生在内部作用域的结尾处，因此我们不会因为忘记释放锁而导致其他线程无法继续使用该互斥体。 多个线程间共享 Mutex&lt;T&gt;123456789101112131415161718192021use std::sync::Mutex;use std::thread;fn main() &#123; let counter = Mutex::new(0); let mut handles = vec![]; for _ in 0..10 &#123; let handle = thread::spawn(move || &#123; let mut num = counter.lock().unwrap(); *num += 1; &#125;); handles.push(handle); &#125; for handle in handles &#123; handle.join().unwrap(); &#125; println!("Result: &#123;&#125;", *counter.lock().unwrap());&#125; 上述代码会依次启动 10 个线程，并在每个线程中分别为共享的计数器的值加 1。但代码目前无法通过编译，会报出 error[E0382]: use of moved value: counter 错误。原因是变量 counter 被移动进了 handle 指代的线程中，这一移动行为阻止我们在另一个线程中调用 lock 来再次捕获 counter。 多线程与多重所有权智能指针 Rc&lt;T&gt; 提供的引用计数能够为单个值赋予多个所有者。现在尝试使用 Rc&lt;T&gt; 来包裹 Mutex&lt;T&gt;，并在每次需要移动所有权至线程时克隆 Rc&lt;T&gt;。看改进后的程序能否编译通过。1234567891011121314151617181920212223use std::rc::Rc;use std::sync::Mutex;use std::thread;fn main() &#123; let counter = Rc::new(Mutex::new(0)); let mut handles = vec![]; for _ in 0..10 &#123; let counter = Rc::clone(&amp;counter); let handle = thread::spawn(move || &#123; let mut num = counter.lock().unwrap(); *num += 1; &#125;); handles.push(handle); &#125; for handle in handles &#123; handle.join().unwrap(); &#125; println!("Result: &#123;&#125;", *counter.lock().unwrap());&#125; 再次尝试编译代码，报出另外一个错误：error[E0277]: `Rc&lt;Mutex&lt;i32&gt;&gt;` cannot be sent between threads safely。原因是 Rc&lt;T&gt; 在跨线程使用时并不安全。Rc&lt;T&gt; 会在每次调用 clone 的过程中增加引用计数，在克隆出的实例被丢弃后减少引用计数。但它并没有使用任何并发原语来保证修改计数的过程中不会被另一个线程所打断。这极有可能导致计数错误并产生诡异的 bug。 原子引用计数 Arc&lt;T&gt;Arc&lt;T&gt; 类型既拥有类似于 Rc&lt;T&gt; 的行为，又保证自己可以被安全地用于并发场景。123456789101112131415161718192021222324use std::sync::&#123;Arc, Mutex&#125;;use std::thread;fn main() &#123; let counter = Arc::new(Mutex::new(0)); let mut handles = vec![]; for _ in 0..10 &#123; let counter = Arc::clone(&amp;counter); let handle = thread::spawn(move || &#123; let mut num = counter.lock().unwrap(); *num += 1; &#125;); handles.push(handle); &#125; for handle in handles &#123; handle.join().unwrap(); &#125; println!("Result: &#123;&#125;", *counter.lock().unwrap());&#125;// =&gt; Result: 10 使用 Arc&lt;T&gt; 替换掉代码中的 Rc&lt;T&gt; 后，代码可以编译通过。 需要注意的是，Rust 并不能使你完全避免使用 Mutex&lt;T&gt; 过程中所有的逻辑错误。使用 Mutex&lt;T&gt; 也会有产生死锁（deadlock）的风险。当某个操作需要同时锁住两个资源，而两个线程分别持有其中一个锁并相互请求另外一个锁时，这两个线程就会陷入无穷尽的等待过程。 参考资料The Rust Programming Language]]></content>
      <categories>
        <category>Program</category>
      </categories>
      <tags>
        <tag>Program</tag>
        <tag>Advanced</tag>
        <tag>Concurrency</tag>
        <tag>Thread</tag>
        <tag>Rust</tag>
        <tag>Deadlock</tag>
        <tag>Channel</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[The Rust programming language 读书笔记——智能指针]]></title>
    <url>%2F2021%2F07%2F26%2Fthe-rust-programming-language-reading-notes-smart-pointer%2F</url>
    <content type="text"><![CDATA[指针（pointer）是一个通用概念，用来指代那些包含内存地址的变量。这些地址“指向”内存中的其他数据。Rust 中最常见的指针是引用，会借用它所指向的数据。除此之外，引用没有任何其他功能和额外的开销。智能指针（smart pointer）是一种数据结构，它的行为类似于指针但拥有额外的元数据和附加功能。 在拥有所有权和借用概念的 Rust 中：引用只是用来借用数据的指针；而大多数智能指针本身就拥有它们指向的值。比如 String 和 Vec&lt;T&gt; 类型就都可以被算作智能指针。它们都拥有一片内存区域并允许用户对其进行操作，拥有元数据（如容量等），能提供额外的功能或保障（如 String 会保证其中的数据必是合法的 UTF-8 编码）。 通常使用结构体来实现智能指针。区别于普通的结构体，智能指针会实现 Deref 与 Drop 这两个 trait。Deref trait 使得智能指针的实例拥有与引用一致的行为；Drop trait 使得用户可以自定义智能指针离开作用域时运行的代码。 标准库中最为常见的智能指针如下： Box&lt;T&gt;：可用于在堆上分配数据 Rc&lt;T&gt;：具备多重所有权的引用计数类型 Ref&lt;T&gt; 和 ReMut&lt;T&gt;：可以通过 RefCell&lt;T&gt; 访问，是一种可以在运行时而不是编译时执行借用规则的类型 使用 Box&lt;T&gt; 在堆上分配数据装箱（box）是最简单的一种智能指针，其类型为 Box&lt;T&gt;。它使我们可以将数据存储在堆上，并在栈中保留一个指向堆数据的指针。 装箱常被用于以下场景： 拥有一个无法在编译时确定大小的类型，但又想在一个要求固定尺寸的上下文环境中使用这个类型 需要传递大量数据的所有权，但又不希望产生大量数据的复制行为 希望拥有一个实现了指定 trait 的类型值，但又不关心具体的类型 使用装箱在堆上存储一个 i32 值：1234fn main() &#123; let b = Box::new(5); println!("b = &#123;&#125;", b);&#125; 将单一值存放在堆上没有太大用处，大部分情况下都可以将类似的单个 i32 值默认放置在栈上。 使用装箱定义递归类型Rust 必须在编译时知道每一种类型占据的空间大小，但有一种被称为递归的类型无法在编译时确定具体大小。递归类型的值可以在自身中存储另一个相同类型的值，这种嵌套理论上可以无穷无尽地进行下去，根本无法计算出具体的空间大小。 链接列表（cons list）是一种在函数式语言中非常常见的数据类型。cons 函数会将两个参数组成一个二元组，而这个元组通常由一个值和另一个二元组组成，通过这种不断嵌套元组的形式最终组成一个列表。使用枚举来定义一个链接列表：12345678910enum List &#123; Cons(i32, List), Nil,&#125;use crate::List::&#123;Cons, Nil&#125;;fn main() &#123; let list = Cons(1, Cons(2, Cons(3, Nil)));&#125; 上述代码在编译时会报出 error[E0072]: recursive type `List` has infinite size 错误。 Rust 在计算枚举类型需要的空间大小时，会遍历枚举中的每一个成员来找到需要最大空间的那个变体。因为在每个时间点，只会有一个变体存在。对于递归类型大小的计算，以前面的 List 为例，编译器会先检查 Cons 变体，它持有一个 i32 类型的值及另外一个 List 类型；为了确定此处List 的大小，编译器又会从 Cons 开始遍历其下的所有变体，这样的检查将永远进行下去。 可以使用 Box&lt;T&gt; 将递归类型的大小固定下来。Box&lt;T&gt; 是一个指针，其大小总是恒定的，不会因为指向数据的大小而发生变化。我们可以在 Cons 变体中存放一个 Box&lt;T&gt; 指针，Box&lt;T&gt; 指向下一个存储在堆上的 List。即嵌套的 List 并没有直接存放在 Cons 变体中，而是放置在堆上，打破了无限递归的过程。此时任意的 List 值都只需要占用一个 i32 值加上一个装箱指针的大小。 12345678910enum List &#123; Cons(i32, Box&lt;List&gt;), Nil,&#125;use crate::List::&#123;Cons, Nil&#125;;fn main() &#123; let list = Cons(1, Box::new(Cons(2, Box::new(Cons(3, Box::new(Nil))))));&#125; 装箱除了间接访问内存和堆分配，没有提供任何其他的特殊功能，也没有这些特殊功能附带的性能开销。因此正好能被用在类似于链接列表这类只是需要间接访问的场景中。 通过 Deref trait 将智能指针视作常规引用实现 Deref trait 使我们可以自定义解引用运算符 * 的行为，这意味着原本用于处理引用的代码可以不加修改地用于处理智能指针。 使用解引用跳转到指针指向的值指针可以被理解为一种箭头，会指向存储在别处的值。1234567fn main() &#123; let x = 5; let y = &amp;x; assert_eq!(5, x); assert_eq!(5, *y);&#125; 数字和引用是两种不同的类型，所以不能直接比较 5 和 y。必须使用 *y 来跳转到引用指向的值。 把 Box&lt;T&gt; 当成引用来操作1234567fn main() &#123; let x = 5; let y = Box::new(x); assert_eq!(5, x); assert_eq!(5, *y);&#125; 自定义智能指针12345678910111213141516171819202122232425struct MyBox&lt;T&gt;(T);impl&lt;T&gt; MyBox&lt;T&gt; &#123; fn new(x: T) -&gt; MyBox&lt;T&gt; &#123; MyBox(x) &#125;&#125;use std::ops::Deref;impl&lt;T&gt; Deref for MyBox&lt;T&gt; &#123; type Target = T; fn deref(&amp;self) -&gt; &amp;T &#123; &amp;self.0 &#125;&#125;fn main() &#123; let x = 5; let y = MyBox::new(x); assert_eq!(5, x); assert_eq!(5, *y);&#125; 上述代码中的 MyBox 是一个拥有 T 类型的元组结构体，其关联函数 MyBox::new 接收一个 T 类型的参数，并返回一个存储有传入值的 MyBox 实例作为结果。 为了为 MyBox&lt;T&gt; 类型实现解引用功能，代码中实现了 Deref trait。标准库中的 Deref trait 要求我们实现一个 deref 方法，该方法会借用 self 并返回一个指向内部数据的引用。deref 方法体中的 &amp;self.0 意味着 deref 会返回一个指向值的引用，进而允许调用者通过 * 运算符访问值。代码中的 *y 会被 Rust 隐式地展开为 *(y.deref())。使得我们可以用完全相同的方式编写代码来处理常规引用及实现了 Deref trait 的类型。 函数和方法的隐式解引用转换解引用转换是 Rust 为函数和方法的参数提供的一种编程特性。当某个类型 T 实现了 Deref trait 时，它能够将 T 的引用转换为 T 经过 Deref 操作后生成的引用。当我们将某个类型的值引用作为参数传递给类型或方法，但传入的类型与参数类型不一致时，解引用转换就会自动发生。编译器会插入一系列的 deref 方法来将我们提供的类型转换为参数所需的类型。 解引用转换使程序员在调用函数或方法时无需多次显式地使用 &amp; 和 * 操作符来进行引用和解引用操作。我们因而可以更多地编写出能够同时作用于常规引用和智能指针的代码。 1234567891011121314151617181920212223242526272829struct MyBox&lt;T&gt;(T);impl&lt;T&gt; MyBox&lt;T&gt; &#123; fn new(x: T) -&gt; MyBox&lt;T&gt; &#123; MyBox(x) &#125;&#125;use std::ops::Deref;impl&lt;T&gt; Deref for MyBox&lt;T&gt; &#123; type Target = T; fn deref(&amp;self) -&gt; &amp;T &#123; &amp;self.0 &#125;&#125;fn hello(name: &amp;str) &#123; println!("Hello, &#123;&#125;!", name);&#125;fn main() &#123; let m = MyBox::new(String::from("Rust")); hello(&amp;m); // =&gt; Hello, Rust! hello(&amp;(*m)[..]); // =&gt; Hello, Rust!&#125; &amp;m 是一个指向 MyBox&lt;String&gt; 值的引用。因为 MyBox&lt;T&gt; 实现了 Deref trait，Rust 可以通过调用 deref 将 &amp;MyBox&lt;String&gt; 转换为 String；因为标准库为 String 提供的 Deref 实现会返回字符串切片，所以 Rust 可以继续调用 deref 将 &amp;String 转换为 &amp;str，最终与 hello 函数的定义匹配。 解引用转换与可变性使用 Deref trait 能够重载不可变引用的 * 运算符，使用 DerefMut trait 能够重载可变引用的 * 运算符。 Rust 会在类型与 trait 满足下面 3 种情况时执行解引用转换： 当 T: Deref&lt;Target=U&gt; 时，允许 &amp;T 转换为 &amp;U 当 T: DerefMut&lt;Target=U&gt; 时，允许 &amp;mut T 转换为 &amp;mut U 当 T: Deref&lt;Target=U&gt; 时，允许 &amp;mut T 转换为 &amp;U 借助 Drop trait 在清理时运行代码Drop trait 允许我们在变量离开作用域时执行某些自定义操作。可以为任意类型实现一个 Drop trait，它常常被用来释放诸如文件、网络连接等资源。几乎每一种智能指针的实现都会用到这一 trait，比如 Box&lt;T&gt; 通过自定义 Drop 来释放装箱指针指向的堆内存空间。 1234567891011121314151617181920212223struct CustomSmartPointer &#123; data: String,&#125;impl Drop for CustomSmartPointer &#123; fn drop(&amp;mut self) &#123; println!("Dropping CustomSmartPointer with data `&#123;&#125;`!", self.data) &#125;&#125;fn main() &#123; let c = CustomSmartPointer &#123; data: String::from("my stuff"), &#125;; let d = CustomSmartPointer &#123; data: String::from("other stuff"), &#125;; println!("CustomSmartPointer created.")&#125;// =&gt; CustomSmartPointer created.// =&gt; Dropping CustomSmartPointer with data `other stuff`!// =&gt; Dropping CustomSmartPointer with data `my stuff`! 使用 std::mem::drop 提前丢弃值12345678910111213141516171819202122struct CustomSmartPointer &#123; data: String,&#125;impl Drop for CustomSmartPointer &#123; fn drop(&amp;mut self) &#123; println!("Dropping CustomSmartPointer with data `&#123;&#125;`!", self.data) &#125;&#125;fn main() &#123; let c = CustomSmartPointer &#123; data: String::from("some data"), &#125;; println!("CustomSmartPointer created."); drop(c); println!("CustomSmartPointer dropped before the end of main");&#125;// =&gt; CustomSmartPointer created.// =&gt; Dropping CustomSmartPointer with data `some data`!// =&gt; CustomSmartPointer dropped before the end of main 基于引用计数的智能指针 Rc所有权在大多数情况下都是清晰的，对于一个给定的值，可以准确地判断出哪个变量拥有它。但在某些场景中，单个值也可能同时被多个所有者持有。比如在图数据结构中，多个边可能会指向相同的节点，这个节点同时属于所有指向它的边。一个节点只要在任意指向它的边还存在时就不应该被清理掉。Rust 提供了一种名为 Rc&lt;T&gt; 的类型来支持多重所有权。Rc&lt;T&gt; 类型的实例会在内部维护一个用于记录值引用次数的计数器，从而确认这个值是否仍在使用。若对一个值的引用次数为零，就意味着这个值可以被安全地清理掉。 当你希望将堆上的一些数据分享给程序的多个部分同时使用，而又无法在编译期确定哪个部分会最后释放这些数据时，就可以使用 Rc&lt;T&gt; 类型。相反地，若我们能够在编译期确定哪一部分最后会释放数据，那么就只需要让这部分代码成为数据的所有者即可。Rc&lt;T&gt; 只能被用于单线程场景中。 使用 Rc&lt;T&gt; 共享数据创建两个链接列表，并让它们同时持有第三个列表的所有权。 123456789101112enum List &#123; Cons(i32, Box&lt;List&gt;), Nil,&#125;use crate::List::&#123;Cons, Nil&#125;;fn main() &#123; let a = Cons(5, Box::new(Cons(10, Box::new(Nil)))); let b = Cons(3, Box::new(a)); let c = Cons(4, Box::new(a));&#125; 尝试编译上述代码会报出 error[E0382]: use of moved value: a 错误。原因是 a 列表会在创建 b 列表时被移动至 b 中。即 b 列表持有了 a 列表的所有权。随后再次尝试使用 a 来创建 c 列表时就会出现编译错误，因为 a 已经被移走了。 12345678910111213enum List &#123; Cons(i32, Rc&lt;List&gt;), Nil,&#125;use crate::List::&#123;Cons, Nil&#125;;use std::rc::Rc;fn main() &#123; let a = Rc::new(Cons(5, Rc::new(Cons(10, Rc::new(Nil))))); let b = Cons(3, Rc::clone(&amp;a)); let c = Cons(4, Rc::clone(&amp;a));&#125; 上述代码中，每个 Cons 变体都持有一个值及一个指向 List 的 Rc&lt;T&gt;。我们只需要在创建 b 的过程中克隆 a 的 Rc&lt;List&gt; 智能指针即可，无需获取 a 的所有权。这使得 a 和 b 可以共享 Rc&lt;List&gt; 数据的所有权，并使智能指针中的引用计数从 1 增加到 2。随后创建 c 时也会同样克隆 a 并将引用计数从 2 增加到 3。每次调用 Rc::clone 都会使引用计数增加，而 Rc&lt;List&gt; 中的数据只有在引用计数器减少到 0 时才会被真正清理掉。 克隆 Rc&lt;T&gt; 会增加引用计数123456789101112131415161718192021222324enum List &#123; Cons(i32, Rc&lt;List&gt;), Nil,&#125;use crate::List::&#123;Cons, Nil&#125;;use std::rc::Rc;fn main() &#123; let a = Rc::new(Cons(5, Rc::new(Cons(10, Rc::new(Nil))))); println!("count after creating a = &#123;&#125;", Rc::strong_count(&amp;a)); let b = Cons(4, Rc::clone(&amp;a)); println!("count after creating b = &#123;&#125;", Rc::strong_count(&amp;a)); &#123; let c = Cons(4, Rc::clone(&amp;a)); println!("count after creating c = &#123;&#125;", Rc::strong_count(&amp;a)); &#125; println!("count after c goes out of scope = &#123;&#125;", Rc::strong_count(&amp;a));&#125;// =&gt; count after creating a = 1// =&gt; count after creating b = 2// =&gt; count after creating c = 3// =&gt; count after c goes out of scope = 2 在上面的代码中，a 存储的 Rc&lt;List&gt; 拥有初始引用计数 1，并在随后每次调用 clone 时增加 1。当 c 离开作用域被丢弃时，引用计数减少 1。Rc&lt;T&gt; 的 Drop 实现会在 Rc&lt;T&gt; 离开作用域时自动将引用计数减 1。 使用 Rc&lt;T&gt; 可以使单个值拥有多个所有者，而引用计数机制则保证了这个值会在其所有者存活时一直有效，并在所有者全部离开作用域时被自动清理。Rc&lt;T&gt; 通过不可变引用使你可以在程序的不同部分之间共享只读数据。但如果 Rc&lt;T&gt; 也允许持有多个可变引用的话，就会违反一个借用原则：多个指向同一区域的可变借用会导致数据竞争及数据不一致。但在实际开发中，允许数据可变是非常有用的。实际上可以通过 RefCell&lt;T&gt; 与 Rc&lt;T&gt; 联合使用来绕开不可变的限制。 RefCell 和内部可变性模式内部可变性 是 Rust 的设计模式之一，它允许你在只持有不可变引用的前提下对数据进行修改。为了能够改变数据，此模式在它的数据结构中使用了 unsafe 代码来绕过 Rust 正常的可变性和借用规则。假如我们能够保证自己的代码在运行时符合借用规则，就可以在即使编译器无法在编译阶段保证符合借用规则的前提下，使用那些采用了内部可变性模式的类型。实现过程中涉及的那些不安全代码会被妥善地封装在安全的 API 内，而类型本身从外部看依然是不可变的。 使用 RefCell&lt;T&gt; 在运行时检查借用规则Rust 中的借用规则如下： 在任何给定的时间内，只能拥有一个可变引用或者任意数量的不可变引用 引用总是有效的 对于一般引用和 Box&lt;T&gt; 的代码，Rust 会在编译阶段强制代码遵守这些借用规则。而对于使用 RefCell&lt;T&gt; 的代码，Rust 只会在运行时检查这些规则，并在违反的时候触发 panic 来提前终止程序。 借用规则的检查放在编译阶段不仅会帮助我们在开发阶段尽早暴露问题，并且不会带来任何运行时开销。对于大多数场景都是最佳的选择。在运行时检查借用规则可以使我们实现某些特定的内存安全场景，即便这些场景无法通过编译时检查。因为某些静态分析是根本无法完成的。这类编译器无法理解代码，但开发者可以保证借用规则能够满足的场景，就适用于 RefCell&lt;T&gt;。 可变地借用一个不可变的值内部可变性模式允许用户更改一个不可变值的内部数据。借用规则限制用户可变地借用一个不可变的值，如下面的代码无法通过编译：1234fn main() &#123; let x = 5; let y = &amp;mut x;&#125; 但是在某些情况下，我们也需要一个值对外保持不可变性的同时能够在方法内部修改自身。除了这个值本身的方法，其余的代码仍不能修改这个值。使用 RefCell&lt;T&gt; 可以获得这种内部可变性。这种内部可变性的机制把借用规则的检查从编译期延后到了运行阶段，若违反了借用规则，只会在运行时触发 panic!。 内部可变性的应用场景：模拟对象测试替代是一种通用的编程概念，代表了那些在测试工作中被用作其他类型替代品的类型。而模拟对象则指代了测试替代中某些特定的类型，会承担起记录测试过程的工作。 Rust 没有和其他语言中类似的对象概念，也没有在标准库中提供模拟对象的测试功能。但是可以自定义一个结构体来实现与模拟对象相同的效果。比如我们希望开发一个库，会基于当前值与最大值之间的接近程度向外传递信息。比如可以记录用户调用不同 API 的次数，并与设置的调用限额作比较。使用这个库的应用程序需要自行实现发送消息的功能，例如在应用程序中打印信息、发送邮件、发送文字短信等。源代码如下：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071pub trait Messenger &#123; fn send(&amp;self, msg: &amp;str);&#125;pub struct LimitTracker&lt;'a, T: 'a + Messenger&gt; &#123; messenger: &amp;'a T, value: usize, max: usize,&#125;impl&lt;'a, T&gt; LimitTracker&lt;'a, T&gt;where T: Messenger,&#123; pub fn new(messenger: &amp;T, max: usize) -&gt; LimitTracker&lt;T&gt; &#123; LimitTracker &#123; messenger, value: 0, max, &#125; &#125; pub fn set_value(&amp;mut self, value: usize) &#123; self.value = value; let percentage_of_max = self.value as f64 / self.max as f64; if percentage_of_max &gt;= 1.0 &#123; self.messenger.send("Error: You're over your quota!"); &#125; else if percentage_of_max &gt;= 0.9 &#123; self.messenger .send("Urgent warning: You've used 90% of your quota!"); &#125; else if percentage_of_max &gt;= 0.75 &#123; self.messenger .send("Warning: You've used 75% of your quota!"); &#125; &#125;&#125;#[cfg(test)]mod tests &#123; use super::*; struct MockMessenger &#123; sent_messages: Vec&lt;String&gt;, &#125; impl MockMessenger &#123; fn new() -&gt; MockMessenger &#123; MockMessenger &#123; sent_messages: vec![], &#125; &#125; &#125; impl Messenger for MockMessenger &#123; fn send(&amp;self, message: &amp;str) &#123; self.sent_messages.push(String::from(message)); &#125; &#125; #[test] fn it_sends_an_over_75_percent_warning_message() &#123; let mock_messenger = MockMessenger::new(); let mut limit_tracker = LimitTracker::new(&amp;mock_messenger, 100); limit_tracker.set_value(80); assert_eq!(mock_messenger.sent_messages.len(), 1); &#125;&#125; 其中 Messenger trait 的 send 方法可以接收 self 的不可变引用及一条文本消息作为参数。我们需要在测试中确定的是，当某段程序使用一个实现了 Messenger trait 的模拟对象与一个 max 值来创建 LimitTracker 实例时，传入的不同 value 值能够触发 messenger 发送不同的信息。此处的模拟对象 MockMessenger 在调用 send 时只需要将收到的消息存档记录即可，不需要真的去发送邮件或短信。使用模拟对象创建 LimitTracker 实例后，就可以通过调用 set_value 方法检查模拟对象中是否存储了我们希望见到的消息。 尝试编译（cargo test）上述代码会报出如下错误：error[E0596]: cannot borrow `self.sent_messages` as mutable, as it is behind a `&amp;` reference send 方法接收了 self 的不可变引用，因此我们无法通过修改 MockMessenger 的内容来记录消息。我们也不能将函数签名修改为 &amp;mut self，因为修改后的签名与 Messenger trait 定义的 send 签名不符。此时就是一个非常适合内部可变性的场景。只要在 RefCell&lt;T&gt; 中存入 sent_messages，send 方法就能够修改 sent_messages。 修改后的代码如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172pub trait Messenger &#123; fn send(&amp;self, msg: &amp;str);&#125;pub struct LimitTracker&lt;'a, T: 'a + Messenger&gt; &#123; messenger: &amp;'a T, value: usize, max: usize,&#125;impl&lt;'a, T&gt; LimitTracker&lt;'a, T&gt;where T: Messenger,&#123; pub fn new(messenger: &amp;T, max: usize) -&gt; LimitTracker&lt;T&gt; &#123; LimitTracker &#123; messenger, value: 0, max, &#125; &#125; pub fn set_value(&amp;mut self, value: usize) &#123; self.value = value; let percentage_of_max = self.value as f64 / self.max as f64; if percentage_of_max &gt;= 1.0 &#123; self.messenger.send("Error: You're over your quota!"); &#125; else if percentage_of_max &gt;= 0.9 &#123; self.messenger .send("Urgent warning: You've used 90% of your quota!"); &#125; else if percentage_of_max &gt;= 0.75 &#123; self.messenger .send("Warning: You've used 75% of your quota!"); &#125; &#125;&#125;#[cfg(test)]mod tests &#123; use super::*; use std::cell::RefCell; struct MockMessenger &#123; sent_messages: RefCell&lt;Vec&lt;String&gt;&gt;, &#125; impl MockMessenger &#123; fn new() -&gt; MockMessenger &#123; MockMessenger &#123; sent_messages: RefCell::new(vec![]), &#125; &#125; &#125; impl Messenger for MockMessenger &#123; fn send(&amp;self, message: &amp;str) &#123; self.sent_messages.borrow_mut().push(String::from(message)); &#125; &#125; #[test] fn it_sends_an_over_75_percent_warning_message() &#123; let mock_messenger = MockMessenger::new(); let mut limit_tracker = LimitTracker::new(&amp;mock_messenger, 100); limit_tracker.set_value(80); assert_eq!(mock_messenger.sent_messages.borrow().len(), 1); &#125;&#125; 其中 sent_messages 字段的类型变为了 RefCell&lt;Vec&lt;String&gt;&gt;。对于 send 方法的实现，其第一个参数依然是 self 的不可变借用，与 trait 的定义保持一致。后面的代码调用 self.messages.borrow_mut 方法获取 RefCell&lt;Vec&lt;String&gt;&gt; 内部值（也就是动态数组）的可变引用，再调用其 push 方法存入数据。在断言语句中，调用了 RefCell&lt;Vec&lt;String&gt;&gt; 的 borrow 方法来取得动态数组的不可变引用。 对于 RefCell&lt;T&gt; 而言，我们可以使用 borrow 与 borrow_mut 分别创建不可变和可变引用。这两个方法会分别返回 Ref&lt;T&gt; 与 RefMut&lt;T&gt; 两种智能指针，可以被当作一般的引用来对待。RefCell&lt;T&gt; 会记录当前存在多少个活跃的 Ref&lt;T&gt; 和 RefMut&lt;T&gt;，维护和编译器同样的借用规则：任何给定的时间都只允许拥有多个不可变借用或一个可变借用。当借用规则被违背时，RefCell&lt;T&gt; 会在运行时触发 panic。 Rc&lt;T&gt; 和 RefCell&lt;T&gt; 实现一个拥有多重所有权的可变数据Rc&lt;T&gt; 允许多个所有者持有同一数据，但只能提供针对数据的不可变访问。若在 Rc&lt;T&gt; 内存储了 RefCell&lt;T&gt;，就可以定义出拥有多个所有者且能够进行修改的值了。 在 Cons 定义中使用 RefCell&lt;T&gt; 来实现修改现有列表内数值的功能：12345678910111213141516171819202122#[derive(Debug)]enum List &#123; Cons(Rc&lt;RefCell&lt;i32&gt;&gt;, Rc&lt;List&gt;), Nil,&#125;use crate::List::&#123;Cons, Nil&#125;;use std::cell::RefCell;use std::rc::Rc;fn main() &#123; let value = Rc::new(RefCell::new(5)); let a = Rc::new(Cons(Rc::clone(&amp;value), Rc::new(Nil))); let b = Cons(Rc::new(RefCell::new(6)), Rc::clone(&amp;a)); let c = Cons(Rc::new(RefCell::new(10)), Rc::clone(&amp;a)); *value.borrow_mut() += 10; println!("a after = &#123;:?&#125;", a); println!("b after = &#123;:?&#125;", b); println!("c after = &#123;:?&#125;", c);&#125; 循环引用与内存泄漏与数据竞争不同，在编译期彻底防止内存泄漏并不是 Rust 做出的保证。这意味着内存泄漏在 Rust 中是一种内存安全行为。可以通过使用 Rc&lt;T&gt; 和 RefCell&lt;T&gt; 创建出相互引用成环状的实例。由于环中每一个指针的引用计数都不可能减少到 0，对应的值也不会被释放丢弃，最终造成内存泄漏。 代码中先创建了一个 Rc&lt;List&gt; 实例并存储至变量 a，其中的 List 初始值为 5, Nil。之后又创建了一个 Rc&lt;List&gt; 实例并存储至变量 b，其中的 List 包含数值 10 及指向列表 a 的指针。接着将 a 指向的下一个元素 Nil 修改为 b，此时即创建出了循环引用。 在完成 a 指向 b 的操作后，这两个 Rc&lt;List&gt; 实例的引用计数都变为了 2。而在 main 函数结尾处，Rust 会首先释放 b，并使 b 存储的 Rc&lt;List&gt; 实例的引用计数减少 1。但由于 a 仍然有一个指向 b 中 Rc&lt;List&gt; 的引用，这个 Rc&lt;List&gt; 的引用计数仍然是 1 而不是 0。因此该 Rc&lt;List&gt; 在堆上的内存不会被释放，这块内存会永远以引用计数为 1 的状态保留在堆上。 假如去除最后一行 println! 的注释并再次运行程序，Rust 会在尝试将循环引用打印出来的过程中反复地从 a 跳转到 b，再从 b 跳转至 a，直到发生栈溢出为止。 如果程序中存在 RefCell&lt;T&gt; 包含 Rc&lt;T&gt; 或其他联用了内部可变性与引用计数指针的情形，就需要自行确保不会在代码中创建出循环引用。创建出循环引用意味着代码逻辑有 bug，可以通过自动化测试及其他软件开发手段来尽可能地避免。 使用 Weak&lt;T&gt; 替代 Rc&lt;T&gt; 来避免循环引用调用 Rc::clone 会增加 Rc&lt;T&gt; 实例的 strong_count 引用计数，而 Rc&lt;T&gt; 实例只有在 strong_count 为 0 时才会被清理。除此之外，我们还可以调用 Rc::downgrade 函数来创建 Rc&lt;T&gt; 实例中值的弱引用。使用 Rc&lt;T&gt; 的引用来调用 Rc::downgrade 会返回一个类型为 Weak&lt;T&gt; 的智能指针，这一操作会让 Rc&lt;T&gt; 的 weak_count 计数增加 1。Rc&lt;T&gt; 类型使用 weak_count 来记录当前存在多少个 Weak&lt;T&gt; 引用，但不会在执行清理操作前要求 weak_count 必须为 0。强引用可以被用来共享一个 Rc&lt;T&gt; 实例的所有权，而弱引用则不会表达所有权关系。一旦强引用计数为 0，任何由弱引用组成的循环就会被打破。弱引用不会造成循环引用。我们无法确定 Weak&lt;T&gt; 引用的值是否已经被释放，因此需要在使用 Weak&lt;T&gt; 指向的值之前确保它依然存在。可以调用 Weak&lt;T&gt; 实例的 upgrade 方法来完成这一验证。此函数返回的 Option&lt;Rc&lt;T&gt;&gt; 会在 Rc&lt;T&gt; 值依然存在时表达为 Some，在 Rc&lt;T&gt; 值被释放时表达为 None。Rust 能够保证 Some 和 None 两个分支都得到妥善的处理，不会产生无效指针之类的问题。 创建树状结构体：带有子节点的 Node源代码如下：1234567891011121314151617181920use std::cell::RefCell;use std::rc::Rc;#[derive(Debug)]struct Node &#123; value: i32, children: RefCell&lt;Vec&lt;Rc&lt;Node&gt;&gt;&gt;,&#125;fn main() &#123; let leaf = Rc::new(Node &#123; value: 3, children: RefCell::new(vec![]), &#125;); let branch = Rc::new(Node &#123; value: 5, children: RefCell::new(vec![Rc::clone(&amp;leaf)]), &#125;);&#125; 我们希望 Node 持有自身所有子节点并通过变量来共享它们的所有权，从而可以直接访问树中的每个 Node。因此将 Vec&lt;T&gt; 的元素定义为 Rc&lt;Node&gt; 类型的值。我们还希望在 children 字段中使用 RefCell&lt;T&gt; 包裹 Vec&lt;Rc&lt;Node&gt;&gt; 来实现内部可变性。我们使用上述结构体定义一个值为 3 且没有子节点的 Node 实例，并将其作为叶子节点存入 leaf 变量。再定义一个值为 5 且将 leaf 作为子节点的 branch 实例。接着克隆 leaf 的 Rc&lt;Node&gt; 实例，并将其存入 branch。此时我们可以使用 branch.children 来从 branch 访问 leaf，但是反之则不行。因为 leaf 并不持有 branch 的引用，它甚至对两个节点之间存在父子关系一无所知。 增加子节点指向父节点的引用为了让子节点意识到父节点的存在，可以为 Node 结构体添加一个 parent 字段。但 parent 的类型不能是 Rc&lt;T&gt;，会创建循环引用。branch.children 指向 leaf 的同时使 leaf.parent 指向 branch 会导致两者的 strong_count 都无法清零。 父节点自然拥有子节点的所有权，因为父节点被丢弃时，子节点也应该随之被丢弃；但子节点却不应该拥有父节点的所有权，即父节点的存在不会因为丢弃子节点而受到影响。这恰好是使用弱引用的场景。12345678910111213141516171819202122232425262728293031use std::cell::RefCell;use std::rc::&#123;Rc, Weak&#125;;#[derive(Debug)]struct Node &#123; value: i32, parent: RefCell&lt;Weak&lt;Node&gt;&gt;, children: RefCell&lt;Vec&lt;Rc&lt;Node&gt;&gt;&gt;,&#125;fn main() &#123; let leaf = Rc::new(Node &#123; value: 3, parent: RefCell::new(Weak::new()), children: RefCell::new(vec![]), &#125;); println!("leaf parent = &#123;:?&#125;", leaf.parent.borrow().upgrade()); let branch = Rc::new(Node &#123; value: 5, parent: RefCell::new(Weak::new()), children: RefCell::new(vec![Rc::clone(&amp;leaf)]), &#125;); *leaf.parent.borrow_mut() = Rc::downgrade(&amp;branch); println!("leaf parent = &#123;:?&#125;", leaf.parent.borrow().upgrade());&#125;// =&gt; leaf parent = None// =&gt; leaf parent = Some(Node &#123; value: 5, parent: RefCell &#123; value: (Weak) &#125;, children: RefCell &#123; value: [Node &#123; value: 3, parent: RefCell &#123; value: (Weak) &#125;, children: RefCell &#123; value: [] &#125; &#125;] &#125; &#125;) 在上面的代码中，branch 创建完毕后，我们通过 RefCell&lt;Weak&lt;Node&gt;&gt; 的 borrow_mut 方法取出 leaf 中 parent 字段的可变借用，再使用 Rc::downgrade 函数获取 branch 中 Rc&lt;Node&gt; 的 Weak&lt;Node&gt; 引用，将其存入 leaf 的 parent 字段中。最后在打印 leaf 的父节点时，便可以看到一个包含了 branch 实际内容的 Some 变体，即表明 leaf 可以访问其父节点。另外，此时打印 leaf 还可以避免之前因循环引用导致的栈溢出故障，因为 Weak&lt;Node&gt; 引用会被直接打印为 (Weak)。 参考资料The Rust Programming Language]]></content>
      <categories>
        <category>Program</category>
      </categories>
      <tags>
        <tag>Program</tag>
        <tag>DataStructure</tag>
        <tag>Rust</tag>
        <tag>Ownership</tag>
        <tag>Pointer</tag>
        <tag>Memory</tag>
        <tag>Heap</tag>
        <tag>Stack</tag>
        <tag>Borrow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[The Rust programming language 读书笔记——枚举类型]]></title>
    <url>%2F2021%2F07%2F13%2Fthe-rust-programming-language-reading-notes-enum-and-match%2F</url>
    <content type="text"><![CDATA[枚举类型（enum），通常也被简称为枚举，它允许我们列举所有可能的值来定义一个类型。枚举搭配 match 表达式使用模式匹配，可以根据不同的枚举值来执行不同的代码。Rust 中的枚举更类似于 Haskell 这类函数式编程语言中的代数数据类型（ADT）。 定义枚举假设我们需要对 IP 地址进行处理。目前只有两种广泛被使用的 IP 地址标准：IPv4 和 IPv6。我们只需要处理这两种情形，且一个地址要么是 IPv4，要么是 IPv6，因此可以使用枚举将所有可能的值（IPv4 和 IPv6）列举出来，作为一种新的数据类型。 1234enum IpAddrKind &#123; V4, V6,&#125; 现在，IpAddrKind 就是一个可以在代码中随处使用的自定义数据类型了。 枚举值可以参照下面的代码使用 IpAddrKind 中的两个变体（V4 和 V6）创建实例：12let four = IpAddrKind::V4;let six = IpAddrKind::V6; 由于 IpAddrKind:V4 和 IpAddrKind:V6 拥有相同的类型（都是 IpAddrKind），我们可以定义一个接收 IpAddrKind 类型参数的函数来统一处理它们：1fn route(ip_type: IpAddrKind) &#123; &#125; 现在，我们可以使用任意一个变体来调用这个函数了：12route(IpAddrKind::V4);route(IpAddrKind::V6); 当前定义的枚举类型 IpAddrKind，还只能区分 IP 地址的种类，没有办法去存储实际的 IP 地址数据。可以使用结构体来解决这个问题：12345678910111213141516171819enum IpAddrKind &#123; V4, V6,&#125;struct IpAddr &#123; kind: IpAddrKind, address: String,&#125;let home = IpAddr &#123; kind: IpAddrKind::V4, address: String::from("127.0.0.1"),&#125;;let loopback = IpAddr &#123; kind: IpAddrKind::V6, address: String::from("::1"),&#125;; 实际上，我们可以直接将枚举关联的数据嵌入其变体内，而不用像上面那样将枚举集成至结构体中。 下面的代码直接定义了 IpAddr 枚举，V4 和 V6 两个变体都被关联上了一个 String 值：12345678enum IpAddr &#123; V4(String), V6(String),&#125;let home = IpAddr::V4(String::from("127.0.0.1"));let loopback = IpAddr::V6(String::from("::1")); 我们直接将数据附加到枚举的每个变体中，就不需要额外地使用结构体了。 另外一个枚举替代结构体的优势在于，每个变体可以拥有不同类型和数量的关联数据，同时所有变体仍属于同一个枚举类型。12345678enum IpAddr &#123; V4(u8, u8, u8, u8), V6(String),&#125;let home = IpAddr::V4(127, 0, 0, 1);let loopback = IpAddr::V6(String::from("::1")); 参考下面代码中定义的一个 Message 枚举：123456enum Message &#123; Quit, Move &#123; x: i32, y: i32 &#125;, Write(String), ChangeColor(i32, i32, i32),&#125; 该枚举拥有 4 个内嵌了不同类型数据的变体： Quit 没有关联任何数据 Move 包含了一个匿名结构体 Write 包含了一个 String ChangeColor 包含了 3 个 i32 值 枚举有些类似于定义多个不同类型的结构体。但枚举除了不会使用 struct 关键字，还将变体们组合到了同一个 Message 类型中。下面代码中的结构体可以存储与这些变体完全一样的数据：1234567struct QuitMessage; // 空结构体struct MoveMessage &#123; x: i32, y: i32,&#125;struct WriteMessage(String); // 元组结构体struct ChangeColorMessage(i32, i32, i32); // 元组结构体 两种实现方式的差别在于，如果使用了不同的结构体，则每个结构体都会拥有自己的类型，无法轻易定义一个统一处理这些类型的函数。而前面的 Message 枚举是单独的一个类型。 正如我们可以用 impl 关键字定义结构体的方法一样，我们同样可以为 Message 定义自己的方法：12345678910impl Message &#123; fn call(&amp;self) &#123; // 方法在这里定义 &#125;&#125;fn main() &#123; let m = Message::Write(String::from("hello")); m.call();&#125; Option 枚举及空值处理Option 是一种定义于标准库中的枚举类型，它描述了一种值可能不存在的情形。借助类型系统，编译器可以自动检查我们是否妥善地处理了所有应该被处理的情况。 Rust 没有像其他语言一样支持空值（Null）。空值本身是一个值，但它的含义却是没有值。空值的问题在于，当你尝试像使用非空值那样使用空值时，就会触发某种程度上的错误。由于空或非空的属性广泛散布在程序中，因此很难避免引起此类问题。但空值本身所尝试表达的概念仍是有意义的，它代表了因为某种原因而变得无效或缺失的值。 Rust 中虽然没有空值，但提供了一个拥有类似概念的枚举 Option&lt;T&gt;，它可以用来标识一个值无效或缺失。Option&lt;T&gt; 在标准库中的定义如下：1234enum Option&lt;T&gt; &#123; Some(T), None,&#125; Option&lt;T&gt; 是一个普通的枚举类型，Some&lt;T&gt; 和 None 是该类型的变体。1234let some_number = Some(5);let some_string = Some("a string");let absent_number: Option&lt;i32&gt; = None; 若使用 None 而不是 Some 变体来进行赋值，则需要明确声明这个 Option&lt;T&gt; 的具体类型，否则编译器无法进行类型推导。 当我们有了一个 Some 值时，就可以确定值是存在的，并且被 Some 所持有；当我们有了一个 None 值时，就知道当前并不存在一个有效的值。Option&lt;T&gt; 的设计相对于空值的优势在于，Option&lt;T&gt; 和 T 是不同的类型，编译器不会允许我们像使用普通值一样直接去使用 Option&lt;T&gt; 的值。如：1234let x: i8 = 5;let y: Option&lt;i8&gt; = Some(5);let sum = x + y; 运行上述代码会导致编译器报错，因为 i8 和 Option&lt;i8&gt; 是不同的类型。当我们持有的类型是 i8 时，编译器可以确保该值是有效的。但是当我们持有的类型是 Option&lt;i8&gt; 时，我们必须要考虑值不存在的情况，编译器会迫使我们在使用值之前正确地做出处理操作。 为了持有一个可能为空的值，我们总是需要将其显式地放入对应类型的 Option&lt;T&gt; 值当中。当我们随后使用这个值时，也必须显式地处理它可能为空的情况。即在处理 Option&lt;T&gt; 时，必须编写应对每个变体的代码。某些代码只会在持有 Some(T) 值时运行，它们可以使用变体中存储的 T；另外一些代码则只会在持有 None 值时运行，这些代码没有可用的 T 值。 match 表达式就是一种可以用来处理 Option&lt;T&gt; 这类枚举的控制流结构。它允许我们基于枚举拥有的变体来决定运行的代码分支，并允许代码通过模式匹配来获取变体内的数据。 控制流运算符 matchmatch 是 Rust 中一个强大的控制流运算符，它允许将一个值与一系列模式相比较，并根据匹配的模式执行相应的代码。这些模式可以由字面量、变量名、通配符及许多其他东西组成。 下面的代码会接收一个美国的硬币作为输入，确定硬币的类型并返回其分值：1234567891011121314151617181920enum Coin &#123; Penny, Nickel, Dime, Quarter,&#125;fn value_in_cents(coin: Coin) -&gt; u32 &#123; match coin &#123; Coin::Penny =&gt; 1, Coin::Nickel =&gt; 5, Coin::Dime =&gt; 10, Coin::Quarter =&gt; 25, &#125;&#125;fn main() &#123; let coin = Coin::Dime; println!("&#123;&#125;", value_in_cents(coin));&#125; 每个 match 分支所关联的代码同时也是一个表达式，这个表达式运行的结果同时也会作为整个 match 表达式的结果返回。 绑定值的模式匹配分支还可以绑定匹配对象的部分值，这使得我们能够从枚举变体中提取特定的值。 比如美国的 25 美分硬币 50 个州采用了不同的设计。现在将这些信息添加至枚举中：1234567891011121314151617181920212223242526272829303132#[derive(Debug)] // 方便打印输出默认不支持打印的类型enum UsState &#123; Alabama, Alaska, // ...&#125;enum Coin &#123; Penny, Nickel, Dime, Quarter(UsState),&#125;fn value_in_cents(coin: Coin) -&gt; u32 &#123; match coin &#123; Coin::Penny =&gt; 1, Coin::Nickel =&gt; 5, Coin::Dime =&gt; 10, Coin::Quarter(state) =&gt; &#123; println!("State quarter from &#123;:?&#125;.", state); 25 &#125; &#125;&#125;fn main() &#123; let alaska = UsState::Alaska; let coin = Coin::Quarter(alaska); value_in_cents(coin); // =&gt; State quarter from Alaska.&#125; 上面的代码中，我们在模式中加入了一个名为 state 的变量用于匹配变体 Coin::Quarter 中的值。当匹配到 Coin::Quarter 时，变量 state 就会绑定到 25 美分所包含的值上。比如代码中 Coin::Quarter(UsState::Alaska) 作为 coin 的值传入 value_in_cents 函数，最终值 UsState::Alaska 被绑定到变量 state 上。 匹配 Option可以使用 match 表达式来处理 Option&lt;T&gt;，从 Some 中取出内部的 T 值。比如编写一个接收 Option&lt;i32&gt; 的函数，若其中有值存在，则将这个值加 1；若其中不存在值，则直接返回 None。123456789101112131415161718fn plus_one(x: Option&lt;i32&gt;) -&gt; Option&lt;i32&gt; &#123; match x &#123; None =&gt; &#123; println!("The result is None"); None &#125; Some(i) =&gt; &#123; println!("The result is &#123;&#125;", i + 1); Some(i + 1) &#125; &#125;&#125;fn main() &#123; let five = Some(5); let six = plus_one(five); let none = plus_one(None);&#125; 需要注意的是，匹配必须穷举所有的可能。尤其是 Option&lt;T&gt; 这个例子中，Rust 会强迫我们明确地处理值为 None 的情形。 简单控制流 if letif let 能让我们通过一种不那么繁琐的语法结合使用 if 与 let，处理那些只关心某一种匹配而忽略其他匹配的情况。下面的代码会匹配一个 Option&lt;u32&gt; 的值，并只在值为 3 时执行代码：1234567fn main() &#123; let some_number = Some(3); match some_number &#123; Some(3) =&gt; println!("three"), _ =&gt; (), &#125;&#125; 为了满足 match 表达式穷尽性的要求，我们不得不在处理完 Some(3) 变体后额外加上一句 _ =&gt; ()。可以使用 if let 以一种更简单的方式实现上述代码：123if let Some(3) = some_number &#123; println!("three");&#125; 还可以在 if let 中搭配使用 else：12345678fn main() &#123; let some_number = Some(8); if let Some(3) = some_number &#123; println!("three"); &#125; else &#123; println!("other number"); &#125;&#125; 参考资料The Rust Programming Language]]></content>
      <categories>
        <category>Program</category>
      </categories>
      <tags>
        <tag>Program</tag>
        <tag>Functional</tag>
        <tag>Pattern</tag>
        <tag>Match</tag>
        <tag>ADT</tag>
        <tag>Enum</tag>
        <tag>Rust</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[The Rust programming language 读书笔记——包（package）、单元包（crate）与模块系统]]></title>
    <url>%2F2021%2F07%2F09%2Fthe-rust-programming-language-reading-notes-package-crate-and-module-system%2F</url>
    <content type="text"><![CDATA[模块系统在编写较为复杂的项目时，合理地对代码进行组织与管理非常重要。只有按照不同的特性来组织或分割相关功能的代码，才能够清晰地找到实现指定功能的代码片段，确定哪些地方需要修改。 除了对功能进行分组，对实现的细节进行封装可以使开发者在更高的层次上复用代码：一旦实现了某个功能，其他代码就可以通过公共接口调用这个操作，而无需了解具体的实现细节。 Rust 提供了一系列的功能来管理代码，包括决定哪些细节是暴露的，那些细节是私有的，以及不同的作用域内存在哪些名称。这些功能被统称为模块系统： 包（package）：一个用于构建、测试并分享单元包的 Cargo 特性 单元包（crate）：一个用于生成库或可执行文件的树形模块结构 模块（module）及 use 关键字：用于控制文件结构、作用域及路径的私有性 路径（path）：一种用于命名条目的方法，这些条目包括结构体、函数和模块等 包与单元包当我们使用 cargo new 命令创建新项目时，如：cargo new restaurant Cargo 会自动创建如下结构的 Rust 项目：1234restaurant├── Cargo.toml└── src └── main.rs Cargo 默认会将自动生成的 src/main.rs 源文件视作一个二进制单元包（crate）的根节点，与包（package）拥有相同的名称（即 restaurant）。假设包的目录中包含文件 src/lib.rs，Cargo 也会自动将其视作与包同名的库单元包的根节点。可以在路径 src/bin 下添加源文件来创建更多的二进制单元包，这些源文件都会被视作独立的二进制单元包。 自动生成的 src/main.rs 源文件内容如下：123fn main() &#123; println!("Hello, world!");&#125; 我们可以创建一个 src/lib.rs 源文件，把上面的打印输出的操作作为公共函数定义在 lib.rs 中，再在 main.rs 中调用该公共函数，效果与之前是一致的。 src/lib.rs 代码：123pub fn greeting() &#123; println!("Hello, World!");&#125; src/main.rs 代码：123fn main() &#123; restaurant::greeting();&#125; 因为 lib.rs 默认会作为一个与包同名（都叫 restaurant）的库单元包（crate）存在，且其中的 greeting 函数已被声明为公开的（pub），因此可以直接在 main.rs 中使用 restaurant::greeting() 调用 lib.rs 中定义的 greeting 函数。 使用 cargo run 命令运行项目后，target/debug 路径下除了像之前一样生成 restaurant 可执行文件外，还会额外生成 librestaurant.rlib 库文件。 单元包可以将相关的功能分组，并放到同一作用域下，这样便可以使这些功能轻松地在多个项目中共享。将单元包的功能保留在它们自己的作用域中有助于指明某个特定功能来源于哪个单元包，并避免可能的命名冲突。比如 rand 包提供了一个名为 Rng 的 trait，我们同样也可以在自己的单元包中定义一个名为 Rng 的结构体。正是由于这些功能被放置在了各自的作用域中，我们能够使用 rng::Rng 访问 rand 包中提供的 Rng trait，而 Rng 则指向刚刚创建的 Rng 结构体。 通过定义模块来控制作用域及私有性假设我们需要编写一个提供就餐服务的库单元包。一个现实的店面常常会划分为前厅与后厨两个部分，前厅负责点单和结账等，后厨则负责制作料理。 为了按照餐厅的实际工作方式来组织单元包，可以将函数放置在嵌套的模块中。修改 src/lib.rs 源代码文件，内容如下：123456789101112131415mod front_of_house &#123; mod hosting &#123; fn seat_at_table() &#123; println!("Seat at table."); &#125; &#125; mod serving &#123; fn take_order() &#123; println!("Taking order."); &#125; fn take_payment() &#123; println!("Taking payment."); &#125; &#125;&#125; 我们可以使用 mod 关键字来定义一个模块（如本例中的 front_of_house），模块内还可以继续定义其他模块（如本例中的 hosting 和 serving）。模块内同样也可以包含其他条目的定义，如结构体、枚举、常量、trait 或函数等。 src/main.rs 与 src/lib.rs 被称作单元包（crate）的根节点，它们的内容各自组成了一个名为 crate 的模块。这个模块的结构也被称为模块树。上面 src/lib.rs 形成的树状模块结构如下：1234567crate └── front_of_house ├── hosting │ └── seat_at_table └── serving ├── take_order └── take_payment 路径类似于在文件系统中使用路径进行导航，在 Rust 的模块树中定位某个条目同样需要使用路径。 路径有两种形式： 使用单元包名或字面量 crate 从根节点开始的绝对路径 使用 self、super 或内部标识符从当前模块开始的相对路径 绝对路径与相对路径都至少由一个标识符组成，标识符之间使用双冒号（::）分隔。 现在尝试在模块外部调用模块中定义的函数。在 src/lib.rs 末尾添加一个公共函数 eat_at_restaurant，调用模块 front_of_house 中定义的函数：123456pub fn eat_at_restaurant() &#123; // 绝对路径 crate::front_of_house::hosting::seat_at_table(); // 相对路径 front_of_house::serving::take_order();&#125; 修改 src/main.rs，在 main 函数中调用上一步中定义的公共函数：123fn main() &#123; restaurant::eat_at_restaurant();&#125; 尝试编译项目，会报出如下错误：12error[E0603]: module `hosting` is privateerror[E0603]: module `serving` is private 即模块 hosting 和 serving 是私有的，Rust 不允许我们访问。Rust 中的模块不仅仅用于组织代码，同时也定义了私有边界：外部代码无法知晓、调用或依赖那些由私有边界封装了的实现细节。Rust 中的所有条目（函数、方法、结构体、枚举、模块及常量）默认都是私有的，处于父级模块中的条目无法使用子模块中的私有条目，但子模块中的条目可以使用其祖先模块中的条目。Rust 希望默认隐藏内部的实现细节，这样用户就能明确地知道修改哪些内容不会破坏外部代码。 使用 pub 关键字暴露路径可以使用 pub 关键字将某些条目标记为公共的，从而使子模块中的这些部分可以被暴露到祖先模块中。接上面的例子，为了使父模块中的 eat_at_restaurant 函数能够正常访问子模块中定义的函数，可以使用 pub 关键字来标记 hosting 和 serving 模块。需要注意的是，模块被 pub 标记，其效果仅限于模块本身，并不会影响到它内部条目的状态，模块中的内容依旧是私有的。为了使前面的代码正常工作，还必须在需要公开的函数前面添加 pub 关键字。 编辑 src/lib.rs 中，内容改动如下：12345678910111213141516171819202122mod front_of_house &#123; pub mod hosting &#123; pub fn seat_at_table() &#123; println!("Seat at table."); &#125; &#125; pub mod serving &#123; pub fn take_order() &#123; println!("Taking order."); &#125; pub fn take_payment() &#123; println!("Taking payment."); &#125; &#125;&#125;pub fn eat_at_restaurant() &#123; // 绝对路径 crate::front_of_house::hosting::seat_at_table(); // 相对路径 front_of_house::serving::take_order();&#125; 此时程序即可以正常运行。 将结构体声明为公共的当我们在结构体定义前使用 pub 关键字时，结构体本身就成为了公共结构体，但是它的字段依旧保持私有状态。我们可以逐一决定是否将某个字段公开。 下面的代码定义了一个公共的 back_of_house::Breakfast 结构体，并令其 toast 字段公开，而 seasonal_fruit 字段保持私有。使得客户可以自行选择想要的面包，而只有厨师才能根据季节与存货决定配餐水果。 编辑 src/lib.rs 源文件，添加如下 back_of_house 模块：1234567891011121314mod back_of_house &#123; pub struct Breakfast &#123; pub toast: String, seasonal_fruit: String, &#125; impl Breakfast &#123; pub fn summer(toast: &amp;str) -&gt; Breakfast &#123; Breakfast &#123; toast: String::from(toast), seasonal_fruit: String::from("peaches"), &#125; &#125; &#125;&#125; 为了测试新添加的代码能否正常工作，修改 src/lib.rs 中的 eat_at_restaurant 函数如下：12345678910pub fn eat_at_restaurant() &#123; // 选择黑麦面包作为夏季早餐 let mut meal = back_of_house::Breakfast::summer("Rye"); // 修改我们想要的面包类型 meal.toast = String::from("Wheat"); println!("I'd like &#123;&#125; toast please", meal.toast); // 接下来的这一行无法通过编译，我们不能看到或更换附带的季节性水果 // meal.seasonal_fruit = String::from("blueberries");&#125; back_of_house::Breakfast 结构体中的 toast 字段是公共的，我们因此能够在 eat_at_restaurant 中使用点号读写 toast 字段。同样由于 seasonal_fruit 字段是私有的，我们不能在 eat_at_restaurant 中访问它。另外，由于 back_of_house::Breakfast 拥有一个私有字段，这个结构体必须提供一个公共的关联函数来构造 Breakfast 实例（本例中的 summer），否则我们将无法在结构体外部创建任何的 Breakfast 实例。 用 use 关键字将路径导入作用域基于路径来调用函数的写法看上去会有些重复与冗长。无论我们使用绝对路径还是相对路径来指定 seat_at_table 函数，都必须在每次调用时指定路径上的 front_of_house 和 hosting 节点。可以借助 use 关键字将路径引入作用域，简化上述步骤。如：1234567891011// src/lib.rs// ...// 绝对路径use crate::front_of_house::hosting;// 相对路径use self::front_of_house::serving;pub fn eat_at_restaurant() &#123; hosting::seat_at_table(); serving::take_order();&#125; 在作用域中使用 use 引入路径有点类似于在文件系统中创建符号链接。通过在单元包的根节点下添加上述两条 use 语句，hosting 和 serving 成了该作用域下的一个有效名称，就如同这两个模块被定义在根节点下一样。 这里使用了 use crate::front_of_house::hosting 并接着调用 hosting::seat_at_table，而没有使用 use crate::front_of_house::hosting::seat_at_table 来直接引入 seat_at_table 函数。相对而言，前者的方式更常用一些。使用 use 将函数的父模块引入作用域，意味着我们必须在调用函数时指定这个父模块，从而更清晰地表明当前函数没有被定义在当前作用域中。 不同于函数，使用 use 将结构体、枚举或其他条目引入作用域时，我们习惯于通过指定完整路径的方式引入。 使用 as 提供新的名称使用 use 将多个同名类型引入作用域时，还可以在路径后使用 as 关键字为类型指定一个新的本地名称，也就是别名。如：12use std::fmt::Result;use std::io::Result as IoResult; 使用 pub use 重导出名称当我们使用 use 关键字将名称引入作用域时，这个名称会以私有的方式在新的作用域中生效。为了让外部代码能够访问到这些名称，可以通过组合使用 pub 和 use 修饰其路径。这项技术也被称作重导出。 比如使用 pub 修饰前面 src/lib.rs 中的某条 use 语句：1pub use crate::front_of_house::hosting; 于是在另一个文件 src/main.rs 中也就可以使用 restaurant::hosting::seat_at_table() 形式的代码调用 hosting 模块中的函数了。通过使用 pub use，我们可以在编写代码时使用一种结构，在对外暴露时使用另外一种不同的结构。这一方法可以让我们的代码库对编写者和调用者同时保持良好的组织结构。 将模块拆分为不同的文件当模块规模逐渐增大时，我们可以将它们的定义移动到新的文件中。比如我们需要将 src/lib.rs 中定义的 front_of_house 模块移动到它自己的文件 src/front_of_house.rs 中。首先将根节点文件 lib.rs 中的代码改为如下版本：123456789mod front_of_house;pub use self::front_of_house::serving;pub use crate::front_of_house::hosting;pub fn eat_at_restaurant() &#123; hosting::seat_at_table(); serving::take_order();&#125; 在 mod front_of_house 后使用分号而不是代码块，会让 Rust 前往与当前模块同名的文件中加载模块内容。因此可以将 front_of_house 模块的具体定义转移到 src/front_of_house.rs 文件中，效果是一样的。1234567891011121314// src/front_of_house.rspub mod hosting &#123; pub fn seat_at_table() &#123; println!("Seat at table."); &#125;&#125;pub mod serving &#123; pub fn take_order() &#123; println!("Taking order."); &#125; pub fn take_payment() &#123; println!("Taking payment."); &#125;&#125; 事实上还可以更进一步，继续拆解 front_of_house 模块到其他文件中。首先将 src/front_of_house.rs 文件的内容改为如下版本：12pub mod hosting;pub mod serving; 接着创建一个 src/front_of_house 目录，以及一个 src/front_of_house/hosting.rs 文件用来存放 hosting 模块的定义，一个 src/front_of_house/serving.rs 文件存放 serving 模块的定义：1234// src/front_of_house/hosting.rspub fn seat_at_table() &#123; println!("Seat at table.");&#125; 1234567// src/front_of_house/serving.rspub fn take_order() &#123; println!("Taking order.");&#125;pub fn take_payment() &#123; println!("Taking payment.");&#125; 最终效果与前两种版本也是一致的。此时 restaurant 项目的目录结构如下：12345678910restaurant├── Cargo.lock├── Cargo.toml└── src ├── front_of_house │ ├── hosting.rs │ └── serving.rs ├── front_of_house.rs ├── lib.rs └── main.rs 所有的修改都没有改变原有的模块树结构，尽管这些定义被放置到了不同的文件中，eat_at_restaurant 中的函数调用依旧有效。 参考资料The Rust Programming Language]]></content>
      <categories>
        <category>Program</category>
      </categories>
      <tags>
        <tag>Program</tag>
        <tag>Package</tag>
        <tag>Project</tag>
        <tag>Rust</tag>
        <tag>Module</tag>
        <tag>Crate</tag>
        <tag>Scope</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Haskell 基本语法（五）自定义 Types]]></title>
    <url>%2F2021%2F07%2F06%2Fbasic-haskell-user-defined-types%2F</url>
    <content type="text"><![CDATA[ADT (Algebraic data types)类似 Bool、Int、Char 这些都是内置的数据类型，我们可以使用 data 关键字创建自己的类型。 标准库中的 Bool 类型实际上是这样定义的：1data Bool = False | True 其中 = 左边部分指定类型的名称，右边部分叫做 value constructors，用来指定当前类型能够拥有的不同数值。整个语句可以读作“Bool 类型可以使用 True 或者 False 作为它的值”。 现在思考下应该用怎样的形式表示一种形状。可以使用元组，比如圆圈可以表示为 (43.1, 55.0, 10.4)。前两项表示圆心的坐标，最后一项表示半径。但这种形式的元组也同样可以表示一个三维向量或者其他对象。更好一点的方法是创建自定义的数据类型。 假设一个形状对象可以是圆或者矩形，则可以定义如下形式的 Shape 类型：1data Shape = Circle Float Float Float | Rectangle Float Float Float Float 可以这样理解，Circle value constructor 包含三个 Float 类型的字段，前两个字段是圆心的坐标，最后一个字段表示半径；Rectangle value constructor 包含四个 Float 类型的字段，前两个字段表示左上角顶点的坐标，后两个字段表示右下角的坐标。 Value constructor 实际上是一种函数，所谓的“字段”是函数的参数，最终返回特定的数据类型。1234Prelude&gt; :t CircleCircle :: Float -&gt; Float -&gt; Float -&gt; ShapePrelude&gt; :t RectangleRectangle :: Float -&gt; Float -&gt; Float -&gt; Float -&gt; Shape 接下来就可以针对 Shape 类型定义一个 surface 函数，用来计算某个 Shape 的面积：123surface :: Shape -&gt; Float surface (Circle _ _ r) = pi * r ^ 2 surface (Rectangle x1 y1 x2 y2) = (abs (x2 - x1)) * (abs (y2 - y1)) 12345678910Prelude&gt; :&#123;Prelude| surface :: Shape -&gt; FloatPrelude| surface (Circle _ _ r) = pi * r ^ 2Prelude| surface (Rectangle x1 y1 x2 y2) = (abs (x2 - x1)) * (abs (y2 - y1))Prelude| :&#125;Prelude&gt;Prelude&gt; surface (Circle 10 20 10)314.15927Prelude&gt; surface (Rectangle 0 0 100 100)10000.0 但是当我们在 ghci 中像调用函数那样直接执行如 Circle 10 20 5 这类命令时，会报出如下错误：12345Prelude&gt; (Circle 10 20 5)&lt;interactive&gt;:14:1: error: • No instance for (Show Shape) arising from a use of ‘print’ • In a stmt of an interactive GHCi command: print it 原因是 Haskell 不清楚如何将此处的自定义类型表示为字符串。当在 ghci 中打印一个值时，实际上 Haskell 调用了 show 函数用来获取对应值的字符串形式，并输出到命令行。为了使我们的 Shape 类型支持打印输出，需要令其实现 Show typeclass。语法如下：1data Shape = Circle Float Float Float | Rectangle Float Float Float Float deriving (Show) 此时 Shape 类型即可支持打印输出操作：12345Prelude&gt; data Shape = Circle Float Float Float | Rectangle Float Float Float Float deriving (Show)Prelude&gt; Circle 10 20 5Circle 10.0 20.0 5.0Prelude&gt; Rectangle 50 230 60 90Rectangle 50.0 230.0 60.0 90.0 Value constructor 实际上就是函数，也因此支持 map、partially apply 等操作。比如可以使用如下代码创建一系列半径不同的同心圆：12Prelude&gt; map (Circle 10 20) [4,5,6,6][Circle 10.0 20.0 4.0,Circle 10.0 20.0 5.0,Circle 10.0 20.0 6.0,Circle 10.0 20.0 6.0] 让我们再定义一个 Point 类型，并令其成为 Shape 类型的一部分，从而更加容易理解：12data Point = Point Float Float deriving (Show)data Shape = Circle Point Float | Rectangle Point Point deriving (Show) 此时的 Circle 类型拥有两个字段，用 Point 类型表示圆圈的圆心，再加一个 Float 类型表示半径。 重新实现下之前的 surface 函数：123surface :: Shape -&gt; Floatsurface (Circle _ r) = pi * r ^ 2surface (Rectangle (Point x1 y1) (Point x2 y2)) = (abs (x2 - x1)) * (abs (y2 - y1)) 只需要重新定义模式匹配的部分即可。123Prelude&gt; surface (Rectangle (Point 0 0) (Point 100 100))10000.0Prelude&gt; surface (Circle (Point 0 0) 24) 还可以创建一个 nudge 函数用来移动某个形状对象的位置。它接收一个形状及其在 x 轴和 y 轴上的偏移量作为参数，返回一个同样大小、不同位置的形状对象。123nudge :: Shape -&gt; Float -&gt; Float -&gt; Shapenudge (Circle (Point x y) r) a b = Circle (Point (x+a) (y+b)) rnudge (Rectangle (Point x1 y1) (Point x2 y2)) a b = Rectangle (Point (x1+a) (y1+b)) (Point (x2+a) (y2+b)) 12Prelude&gt; nudge (Circle (Point 34 34) 10) 5 10Circle (Point 39.0 44.0) 10.0 Record 语法假设创建一个名为 Person 的自定义数据类型。它需要包含名字、姓氏、年龄、身高、手机号和最喜欢的冰淇淋种类等字段。可以使用如下代码实现：1234Prelude&gt; data Person = Person String String Int Float String String deriving (Show)Prelude&gt; let guy = Person "Buddy" "Finklestein" 43 184.2 "526-2928" "Chocolate"Prelude&gt; guyPerson "Buddy" "Finklestein" 43 184.2 "526-2928" "Chocolate" 上述代码是可以运行的，但可读性却很差。当我们需要创建函数来获取 Person 对象中的某个字段的值时，可能就需要借助如下形式的代码：1234567891011121314151617firstName :: Person -&gt; StringfirstName (Person firstname _ _ _ _ _) = firstname lastName :: Person -&gt; StringlastName (Person _ lastname _ _ _ _) = lastname age :: Person -&gt; Intage (Person _ _ age _ _ _) = age height :: Person -&gt; Floatheight (Person _ _ _ height _ _) = height phoneNumber :: Person -&gt; StringphoneNumber (Person _ _ _ _ number _) = number flavor :: Person -&gt; Stringflavor (Person _ _ _ _ _ flavor) = flavor 鉴于上述场景中的代码实现有诸多不方便的地方，Haskell 提供了另外一种创建数据类型的方法，即 Record 语法。1234567data Person = Person &#123; firstName :: String , lastName :: String , age :: Int , height :: Float , phoneNumber :: String , flavor :: String &#125; deriving (Show) 通过上述语法，Haskell 会自动创建 firstName、lastName、age、height、phoneNumber、flavor 等函数，用于访问该类型对象中的对应字段。1234Prelude&gt; :t flavorflavor :: Person -&gt; StringPrelude&gt; :t firstNamefirstName :: Person -&gt; String Record 语法的另一个好处在于，Value constructor 中涉及到的所有字段都可以拥有一个有意义的名称，方便各字段之间的相互区分。1234Prelude&gt; data Car = Car &#123;company :: String, model :: String, year :: Int&#125; deriving (Show)Prelude&gt; let car = Car &#123;company="Ford", model="Mustang", year=1967&#125;Prelude&gt; carCar &#123;company = "Ford", model = "Mustang", year = 1967&#125; 比如创建一个用于表示三维向量的数据类型，可以使用 data Vector = Vector Int Int Int 语句。但这样的语法对于前面的 Person 和 Car 来讲，其含义就不如使用 Record 语法来得清晰。 参数化类型Value constructor 可以接收特定数量的参数来生成一个特定类型的值。比如前面的 Car 接收 3 个参数生成一个新的 car。Type constructor 则可以接收 type 作为参数来生成一个新的类型。 比如内置的 Maybe 的实现：data Maybe a = Nothing | Just a 其中 a 表示类型参数，Maybe 即为 type constructor。我们可以向 Maybe 传入一个 Char 作为类型参数，就可以得到一个新的 Maybe Char 类型。比如值 Just &#39;a&#39; 就属于 Maybe Char 类型。同样的方式可以得到类型 Maybe Int、Maybe String 等等。 Maybe 实际上表示一种可选项，它可以是任意某种特定类型的值，也可以什么值都不包含（Nothing）。比如 Maybe Int 类型就表示该类型的值可能包含 Int（值 Just 5），也可能不包含任意类型（Nothing）。 12345678Prelude&gt; :t Just "Haha"Just "Haha" :: Maybe [Char]Prelude&gt; :t Just 84Just 84 :: Num a =&gt; Maybe aPrelude&gt; :t NothingNothing :: Maybe aPrelude&gt; Just 10 :: Maybe DoubleJust 10.0 实际上还有一种类型涉及到了类型参数，只不过借助了语法糖，其形式稍有不同。该类型就是 list。list 类型可以接收一个类型参数生成更具体的类型。比如 [Int]、[Char] 甚至 [[String]] 等等。但是没有任何值的类型可以是 []。空列表实际上可以表现得像任意类型的列表，其类型是 [a]，也因此可以使用如下形式的表达式：[1,2,3] ++ []、[&quot;ha&quot;,&quot;ha&quot;,&quot;ha&quot;] ++ []。 下面的代码实现了一种三维的向量类型：12345678910data Vector a = Vector a a a deriving (Show) vplus :: (Num t) =&gt; Vector t -&gt; Vector t -&gt; Vector t (Vector i j k) `vplus` (Vector l m n) = Vector (i+l) (j+m) (k+n) vectMult :: (Num t) =&gt; Vector t -&gt; t -&gt; Vector t (Vector i j k) `vectMult` m = Vector (i*m) (j*m) (k*m) scalarMult :: (Num t) =&gt; Vector t -&gt; Vector t -&gt; t (Vector i j k) `scalarMult` (Vector l m n) = i*l + j*m + k*n 上述函数可以作用在 Vector Int、Vector Integer、Vector Float 类型上，只要类型 Vector a 中的 a 属于 Num typeclass。123456Prelude&gt; Vector 3 5 8 `vplus` Vector 9 2 8Vector 12 7 16Prelude&gt; Vector 3 9 7 `vectMult` 10.0Vector 30.0 90.0 70.0Prelude&gt; Vector 4 9 5 `scalarMult` Vector 9.0 2.0 4.074.0 类型参数通常用在当 type constructor 中包含的类型对该类型的正常工作并不产生影响时。即我们的自定义类型表现得像某种“盒子”，里面可以放任意的特定类型的值。 派生实例typeclass 是一种定义了某种行为的接口。若某个类型支持 typeclass 定义的行为，则该类型成为 typeclass 的实例。比如 Eq typeclass 定义了可以被测试是否相等的行为，而整数之间可以比较是否相等，因此 Int 类型是 Eq typeclass 的实例。与此同时，作为 Eq 接口的函数如 == 和 /=，则可以直接调用 Int 类型的值，测试它们是否相等（或不相等）。 typeclass 经常会与其他语言如 Java 中的类相混淆。实际上在其他语言中，类可以看作创建对象（包含自身状态和行为）的蓝图；而 typeclass 则更像是接口。在 Haskell 中，我们先创建某个数据类型，然后考虑该类型有怎样的行为。若该类型可以被排序，则令其成为 Ord typeclass 的实例。这之后该类型的值就可以被 &gt;、&lt;、compare 等比较大小的函数调用了。 现在假设两个人可以有相同的姓氏、名字和年龄，则这两个人就是“相等”的。由此创建一个可以比较是否相等的 Person 类型：1234data Person = Person &#123; firstName :: String , lastName :: String , age :: Int &#125; deriving (Eq) 当我们使用 == 比较两个实现了 Eq typeclass 的类型实例时，Haskell 会先用 == 比较两个类型实例的 value constructor 是否相等，再比较类型实例中包含的所有字段的值是否都相等。1234567891011Prelude&gt; let mikeD = Person &#123;firstName = "Michael", lastName = "Diamond", age = 43&#125;Prelude&gt; let adRock = Person &#123;firstName = "Adam", lastName = "Horovitz", age = 41&#125;Prelude&gt; let mca = Person &#123;firstName = "Adam", lastName = "Yauch", age = 44&#125;Prelude&gt; mca == adRockFalsePrelude&gt; mikeD == adRockFalsePrelude&gt; mikeD == mikeDTruePrelude&gt; mikeD == Person &#123;firstName = "Michael", lastName = "Diamond", age = 43&#125;True 由于 Person 类型现在是 Eq typeclass 的实例，因此我们可以将其传给类型约束是 Eq a 的函数，比如 elem：123Prelude&gt; let beastieBoys = [mca, adRock, mikeD]Prelude&gt; mikeD `elem` beastieBoysTrue Show 和 Read typeclass 与类型值的字符串转换有关。Show 表示将类型值转换为 String，Read 则表示将 String 转换为特定类型的值。12345678910111213Prelude&gt; :&#123;Prelude| data Person = Person &#123; firstName :: StringPrelude| , lastName :: StringPrelude| , age :: IntPrelude| &#125; deriving (Eq, Show, Read)Prelude| :&#125;Prelude&gt; let mikeD = Person &#123;firstName = "Michael", lastName = "Diamond", age = 43&#125;Prelude&gt; mikeDPerson &#123;firstName = "Michael", lastName = "Diamond", age = 43&#125;Prelude&gt; "mikeD is: " ++ show mikeD"mikeD is: Person &#123;firstName = \"Michael\", lastName = \"Diamond\", age = 43&#125;"Prelude&gt; read "Person &#123;firstName =\"Michael\", lastName =\"Diamond\", age = 43&#125;" :: PersonPerson &#123;firstName = "Michael", lastName = "Diamond", age = 43&#125; 对于实现了 Ord typeclass 的类型，我们可以根据 value constructor 中值出现的顺序比较同一类型不同值的大小。value constructor 中左侧的值总小于右侧的值。内置的 Bool 类型可以大概视作有如下实现：data Bool = False | True deriving (Ord)则在比较 False 和 True 时，False 总小于 True。12Prelude&gt; False &lt; TrueTrue 借助 Enum 和 Bounded typeclass，可以很轻松地实现枚举类型的 ADT。比如：12data Day = Monday | Tuesday | Wednesday | Thursday | Friday | Saturday | Sunday deriving (Eq, Ord, Show, Read, Bounded, Enum) 由于 Day 类型实现了 Show 和 Read typeclass，则可以在此类型与字符串之间进行转换：123456Prelude&gt; WednesdayWednesdayPrelude&gt; show Wednesday"Wednesday"Prelude&gt; read "Saturday" :: DaySaturday 又由于 Day 类型实现了 Eq 和 Ord typeclass，则可以在 Day 类型的值之间进行比较：12345678Prelude&gt; Saturday == SundayFalsePrelude&gt; Saturday == SaturdayTruePrelude&gt; Saturday &gt; FridayTruePrelude&gt; Monday `compare` WednesdayLT 又由于 Day 类型实现了 Bounded typeclass，我们可以获取“最低”和最高的 Day 类型值：1234Prelude&gt; minBound :: DayMondayPrelude&gt; maxBound :: DaySunday 又由于 Day 类型实现了 Enum，因此我们可以对其进行序列类型的操作：12345678Prelude&gt; succ MondayTuesdayPrelude&gt; pred SaturdayFridayPrelude&gt; [Thursday .. Sunday][Thursday,Friday,Saturday,Sunday]Prelude&gt; [minBound .. maxBound] :: [Day][Monday,Tuesday,Wednesday,Thursday,Friday,Saturday,Sunday] 参考资料Learn You a Haskell for Great Good!]]></content>
      <categories>
        <category>Program</category>
      </categories>
      <tags>
        <tag>Program</tag>
        <tag>Haskell</tag>
        <tag>Functional</tag>
        <tag>Type</tag>
        <tag>Typeclass</tag>
        <tag>ADT</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Haskell 基本语法（四）高阶函数]]></title>
    <url>%2F2021%2F06%2F30%2Fbasic-haskell-high-order-function%2F</url>
    <content type="text"><![CDATA[Haskell 中的函数可以作为另一个函数的参数或返回值，这类函数叫做高阶函数（high order functions）。想要通过定义是什么而不是定义一系列可以改变程序状态的步骤来完成计算过程，高阶函数是必不可少的。 Curried functionsHaskell 中的函数实际上都只接收一个参数。前面遇到的接收多个参数的函数是一种 Curried functions，可以看作某种特殊形式。比如 max 4 5，看上去是向函数 max 传入两个参数 4 和 5，返回数值较大的 5。实际的计算过程是 (max 4) 5。1234Prelude&gt; max 4 55Prelude&gt; (max 4) 55 首先将参数 4 传递给函数 max，会返回另一个函数，该函数接收任意一个参数，将该参数与数字 4 比较，返回较大的数。所以后面将 5 传给函数 (max 4) 后，得到最终的结果 5。123Prelude&gt; let maxWithFour = max 4Prelude&gt; maxWithFour 55 那么这种形式的函数究竟有什么好处呢？ 当我们使用部分参数调用某个函数的时候，并不会直接得到结果，而是返回一个部分应用（partially applied）的函数。这个部分应用的函数可以继续接收剩余的参数，最终得到计算结果。 partially applied 机制可以方便我们简单地实现动态地创建函数、将函数作为参数传入、用特定数据初始化函数等需求。 对于函数 multThree：let multThree x y z = x * y * z它可以接收三个数字作为参数，并计算这三个参数的乘积作为返回值。 如 multThree 3 5 9，实际上的执行流程为 ((multThree 3) 5) 9。 将数字 3 传递给 multThree，它会返回一个函数 (multThree 3)。该函数接收任意两个数字，并计算它们和 3 的乘积 将数字 5 传递给 (multThree 3)，返回另一个函数 ((multThree 3) 5)。该函数接收任意一个数字，并计算它和 15 的乘积 将数字 9 传递给 ((multThree 3) 5)，返回 9 和 15 的乘积作为结果 123456789Prelude&gt; let multThreeNums x y z = x * y * zPrelude&gt; multThreeNums 2 3 424Prelude&gt; let multTwoNumsWithNine = multThreeNums 9Prelude&gt; multTwoNumsWithNine 2 354Prelude&gt; let multOneNumWithEighteen = multTwoNumsWithNine 2Prelude&gt; multOneNumWithEighteen 10180 中缀函数如 + - * / 等也可以 partially applied。 比如可以将数字 5 传递给中缀函数 + 生成一个新的函数 (5+)，而新函数 (5+) 可以接收一个数字作为参数，返回该参数与 5 的和。即函数 (5+) 其实是中缀函数 + 固定一个参数 5 之后生成的新函数，这个新函数接收任何一个数字作为另一个加数并求和。 使用 / 固定除数生成新函数 divideByTen：1234567Prelude&gt; let divideByTen = (/10)Prelude&gt; divideByTen 20020.0Prelude&gt; (/10) 20020.0Prelude&gt; 200 / 1020.0 divideByTen 200 等同于 (/10) 200 等同于 200 / 10。 同样的方式还可以定义 divideTen 固定被除数：1234567Prelude&gt; let divideTen = (10/)Prelude&gt; divideTen 25.0Prelude&gt; (10/) 25.0Prelude&gt; 10 / 25.0 检查输入的字符是否是大写字母：12345Prelude&gt; let isUpperAlphanum = (`elem` ['A'..'Z'])Prelude&gt; isUpperAlphanum 'D'TruePrelude&gt; isUpperAlphanum 'a'False 函数作为参数123456789Prelude&gt; let applyTwice f x = f (f x)Prelude&gt; applyTwice (+3) 1016Prelude&gt; applyTwice (++ " HAHA") "HEY""HEY HAHA HAHA"Prelude&gt; applyTwice ("HAHA " ++) "HEY""HAHA HAHA HEY"Prelude&gt; applyTwice (3:) [1][3,3,1] zipWith 的自定义实现1234zipWith' :: (a -&gt; b -&gt; c) -&gt; [a] -&gt; [b] -&gt; [c]zipWith' _ [] _ = []zipWith' _ _ [] = []zipWith' f (x:xs) (y:ys) = f x y : zipWith' f xs ys 12345678910111213Prelude&gt; :&#123;Prelude| zipWith' :: (a -&gt; b -&gt; c) -&gt; [a] -&gt; [b] -&gt; [c]ith' f Prelude| zipWith' _ [] _ = []Prelude| zipWith' _ _ [] = []Prelude| zipWith' f (x:xs) (y:ys) = f x y : zipWith' f xs ysPrelude| :&#125;Prelude&gt;Prelude&gt; zipWith' (+) [4,2,5,6] [2,6,2,3][6,8,7,9]Prelude&gt; zipWith' (++) ["foo ", "bar ", "baz "] ["fighters", "hoppers", "aldrin"]["foo fighters","bar hoppers","baz aldrin"]Prelude&gt; zipWith' (*) (replicate 5 2) [1..][2,4,6,8,10] flip 的自定义实现12flip' :: (a -&gt; b -&gt; c) -&gt; b -&gt; a -&gt; cflip' f y x = f x y 123456789Prelude&gt; :&#123;Prelude| flip' :: (a -&gt; b -&gt; c) -&gt; b -&gt; a -&gt; cPrelude| flip' f y x = f x yPrelude| :&#125;Prelude&gt;Prelude&gt; flip' zip [1,2,3,4,5] "hello"[('h',1),('e',2),('l',3),('l',4),('o',5)]Prelude&gt; zipWith (flip' div) [2,2..] [10,8,6,4,2][5,4,3,2,1] Maps &amp; Filtersmap 接收一个函数和一个列表作为参数，可以将函数应用到列表的每一项元素上。 map 函数的定义如下：123map :: (a -&gt; b) -&gt; [a] -&gt; [b] map _ [] = [] map f (x:xs) = f x : map f xs 12345678910Prelude&gt; map (+3) [1,5,3,1,6][4,8,6,4,9]Prelude&gt; map (++ "!") ["BIFF", "BANG", "POW"]["BIFF!","BANG!","POW!"]Prelude&gt; map (replicate 3) [3..6][[3,3,3],[4,4,4],[5,5,5],[6,6,6]]Prelude&gt; map (map (^2)) [[1,2],[3,4,5,6],[7,8]][[1,4],[9,16,25,36],[49,64]]Prelude&gt; map fst [(1,2),(3,5),(6,3),(2,6),(2,5)][1,3,6,2,2] filter 接收一个判断函数和一个列表作为参数，返回列表中所有使判断函数为真的元素。 filter 函数的定义如下：12345filter :: (a -&gt; Bool) -&gt; [a] -&gt; [a]filter _ [] = []filter p (x:xs) | p x = x : filter p xs | otherwise = filter p xs 123456789101112Prelude&gt; filter (&gt;3) [1,5,3,2,1,6,4,3,2,1][5,6,4]Prelude&gt; filter (==3) [1,2,3,4,5][3]Prelude&gt; filter even [1..10][2,4,6,8,10]Prelude&gt; let notNull x = not (null x) in filter notNull [[1,2,3],[],[3,4,5],[2,2],[],[],[]][[1,2,3],[3,4,5],[2,2]]Prelude&gt; filter (`elem` ['a'..'z']) "u LaUgH aT mE BeCaUsE I aM diFfeRent""uagameasadifeent"Prelude&gt; filter (`elem` ['A'..'Z']) "i lauGh At You BecAuse u r aLL the Same""GAYBALLS" 借助 filter 实现 quicksort：123456quicksort :: (Ord a) =&gt; [a] -&gt; [a]quicksort [] = []quicksort (x:xs) = let smallerSorted = quicksort (filter (&lt;=x) xs) biggerSorted = quicksort (filter (&gt;x) xs) in smallerSorted ++ [x] ++ biggerSorted 找出 100000 以内能够被 3829 整除的最大的数：123largestDivisible :: (Integral a) =&gt; alargestDivisible = head (filter p [100000,99999..]) where p x = x `mod` 3829 == 0 其中 p x = x `mod` 3829 == 0 定义了函数 p 作为 filter 的判断函数，而列表 [100000, 99999..] 实际上是一个逆序的无穷列表。借助 Haskell 的惰性计算机制，函数获取的是最大的可被整除的数，获得该值后就不会再继续计算下去。 实际上还可以这样使用 map 函数：map (*) [0..]将函数 * 映射到列表 [0..]，会返回一个包含一系列函数的新列表。新列表的形式类似 [(0*),(1*),(2*),(3*),(4*),(5*)..]。其中的任何一个函数如 (4*)，都是接收两个参数的函数 * 固定了一个参数后的形式，再向其传入一个参数即可完成乘法运算。 123Prelude&gt; let listOfFuns = map (*) [0..]Prelude&gt; (listOfFuns !! 4) 520 !! 函数可以从指定列表中根据索引值获取特定的元素。(listOfFuns !! 4) 即为 (4*)。 LambdasLambda 基本上是代码中只使用一次的匿名函数。由 \ 反斜杠符号指定参数，-&gt; 符号指定函数体。 12Prelude&gt; zipWith (\a b -&gt; a * b - 1) [5,4,3,2,1] [1,2,3,4,5][4,7,8,7,4] 和通常的函数类似，lambda 中也可以应用模式匹配：12Prelude&gt; map (\(a,b) -&gt; a + b) [(1,2),(3,5),(6,3),(2,6),(2,5)][3,8,9,8,7] 不同的是，lambda 不支持对同一个参数定义多个模式。 FoldFold 有点类似于 map 函数，只不过 fold 操作最终会将列表中的元素归并（reduce）到单个值。 Fold 函数接收三个参数： binary function：接收两个参数的函数 初始值：称作累加器（accumulator） 需要被折叠的列表 首先是 binary function 接收 accumulator 和列表的第一个元素作为参数，执行特定的计算后返回一个新的 accumulator；binary function 继续接收刚返回的新 accumulator 和列表中剩余元素中的第一个作为参数，执行计算并返回新的 accumulator；若干次循环过后，列表中的最后一个元素被传入 binary function，返回的 accumulator 即为整个列表归并（折叠）后的最终结果。 左折叠 foldl运用左折叠（从左侧开始折叠）实现自定义的 sum 函数：123Prelude&gt; let sum' xs = foldl (\acc x -&gt; acc + x) 0 xsPrelude&gt; sum' [3, 5, 2, 1]11 对应到前面提到的概念，lambda 函数 (\acc x -&gt; acc + x) 即为 binary function，0 是初始值（accumulator），xs 为传入的待折叠列表。 借助 curried function，甚至可以写出更简单的形式：1let sum' = foldl (+) 0 lambda 函数 (\acc x -&gt; acc + x) 实际上等效于 (+)。对于 xs 参数的化简，原因是通常情况下，若函数具有 foo a = bar b a 这样的形式，则该函数可以简化为 foo = bar b。 运用左折叠实现自定义的 elem 函数：123Prelude&gt; let elem' y ys = foldl (\acc x -&gt; if x == y then True else acc) False ysPrelude&gt; elem' 2 [1, 2, 3]True 右折叠 foldr运用右折叠（从右侧开始折叠）实现自定义的 map 函数：123Prelude&gt; let map' f xs = foldr (\x acc -&gt; f x : acc) [] xsPrelude&gt; map' (+3) [1, 2, 3][4,5,6] 此外还有两个折叠函数 foldl1 和 foldr1。它们与 foldl 和 foldr 的功能基本相同，只不过不需要显式地提供初始值。而是会自动地将列表的第一个值（不管从左起还是从右起）作为初始值。 以下是几个通过 fold 操作实现的标准库函数：1234567891011121314151617maximum' :: (Ord a) =&gt; [a] -&gt; amaximum' = foldr1 (\x acc -&gt; if x &gt; acc then x else acc) reverse' :: [a] -&gt; [a]reverse' = foldl (\acc x -&gt; x : acc) [] product' :: (Num a) =&gt; [a] -&gt; aproduct' = foldr1 (*) filter' :: (a -&gt; Bool) -&gt; [a] -&gt; [a]filter' p = foldr (\x acc -&gt; if p x then x : acc else acc) [] head' :: [a] -&gt; ahead' = foldr1 (\x _ -&gt; x) last' :: [a] -&gt; alast' = foldl1 (\_ x -&gt; x) 参考资料Learn You a Haskell for Great Good!]]></content>
      <categories>
        <category>Program</category>
      </categories>
      <tags>
        <tag>Program</tag>
        <tag>Haskell</tag>
        <tag>Functional</tag>
        <tag>Function</tag>
        <tag>Map</tag>
        <tag>Reduce</tag>
        <tag>Filter</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Haskell 基本语法（三）递归]]></title>
    <url>%2F2021%2F06%2F29%2Fbasic-haskell-recursion%2F</url>
    <content type="text"><![CDATA[递归是一种定义函数的方式，在该方式下，函数的定义中调用了该函数本身。有点像俄罗斯套娃。 数学中的定义很多时候都会用到递归，比如 fibonacci 数列： F(0) = 1 F(1) = 1 F(n) = F(n - 1) + F(n - 2) 于是有 F(3) = F(2) + F(1) = (F(1) + F(0)) + F(1) = 2。 递归函数的定义中，并不只是包含调用自身的代码，常常还需要非递归形式的定义，如上面的 F(0) = 1 和 F(1) = 1。这样的代码称作边缘条件（edge condition）。边缘条件对于递归函数的终止至关重要。假如上面的 F(0) 和 F(1) 未定义，则任何一个输入都会导致函数无限调用自身，永远不会终止。 递归是 Haskell 中很重要的概念。不同于命令式的语言，在 Haskell 中需要定义计算本身是什么，而不是定义怎样一步步得出结果。 求最大值1234567maximum' :: (Ord a) =&gt; [a] -&gt; amaximum' [] = error "maximum of empty list"maximum' [x] = xmaximum' (x:xs) | x &gt; maxTail = x | otherwise = maxTail where maxTail = maximum' xs 使用 max 函数（返回两个输入值中较大的那个）编写更短的形式：1234maximum' :: (Ord a) =&gt; [a] -&gt; amaximum' [] = error "maximum of empty list"maximum' [x] = xmaximum' (x:xs) = max x (maximum' xs) 当输入为 [2, 5, 1] 时，计算过程如下：maximum&#39; [2, 5, 1] -&gt; max 2 (maximum&#39; [5, 1]) -&gt; max 2 (max 5 (maximum&#39; [1])) -&gt; max 2 (max 5 1) -&gt; max 2 5 -&gt; 5。 生成由固定数量的同一元素构成的列表1234replicate' :: (Num i, Ord i) =&gt; i -&gt; a -&gt; [a]replicate' n x | n &lt;= 0 = [] | otherwise = x:replicate' (n-1) x 如 replicate&#39; 3 5 -&gt; 5:(replicate&#39; 2 5) -&gt; 5:(5:(replicate&#39; 1 5)) -&gt; 5:(5:(5:(replicate&#39; 0 5))) -&gt; 5:(5:(5:[])) -&gt; [5, 5, 5]。 取出列表中的前几个元素12345take' :: (Num i, Ord i) =&gt; i -&gt; [a] -&gt; [a] take' n _ | n &lt;= 0 = []take' _ [] = []take' n (x:xs) = x : take' (n-1) xs 其中 take&#39; n _ 和 take&#39; _ [] 分别作为两种不同情况下的终止条件。第一个模式 take&#39; n _ 表示当 n 小于等于 0 时，不管输入的是什么样的列表都返回空列表 []。可以作为如 take&#39; 2 [1, 2, 3] 的终止条件。即前两个元素被取出并拼接成 [1, 2] 后 n 等于 0，满足第一个模式，递归终止。第二个模式 take _ [] 表示当输入的列表是空列表时，不管 n 是多少都返回空列表。可以作为如 take&#39; 3 [1, 2] 的终止条件。即前两个元素被取出并拼接成 [1, 2] 后，n 为 1，但列表成为空列表，满足第二个模式，递归终止。第三个模式 take&#39; n (x:xs) 则用来定义从输入的列表头部逐个取出 n 个元素并拼接成新列表的递归逻辑。 reverse 的自定义实现123reverse' :: [a] -&gt; [a]reverse' [] = []reverse' (x:xs) = reverse' xs ++ [x] zip 的自定义实现1234zip' :: [a] -&gt; [b] -&gt; [(a,b)]zip' _ [] = []zip' [] _ = []zip' (x:xs) (y:ys) = (x,y):zip' xs ys elem 的自定义实现（判断某个元素是否属于某个列表）12345elem' :: (Eq a) =&gt; a -&gt; [a] -&gt; Boolelem' a [] = Falseelem' a (x:xs) | a == x = True | otherwise = a `elem'` xs 快速排序123456quicksort :: (Ord a) =&gt; [a] -&gt; [a]quicksort [] = []quicksort (x:xs) = let smallerSorted = quicksort [a | a &lt;- xs, a &lt;= x] biggerSorted = quicksort [a | a &lt;- xs, a &gt; x] in smallerSorted ++ [x] ++ biggerSorted 递归思维递归函数的定义通常遵循如下模式： 定义边缘条件（edge conditon）用于在特定条件下终止递归的执行 取出部分元素执行特定操作，再调用递归函数本身处理剩余的元素 某个列表中所有元素之和等于该列表的第一个元素加上剩余的所有元素之和；某个列表的长度等于尾部（去除头部第一个元素）所有元素的长度加 1。 通常情况下，edge condition 就是令递归函数无实际意义的条件。对于列表来说，最常见的 edge condition 就是空列表。 参考资料Learn You a Haskell for Great Good!]]></content>
      <categories>
        <category>Program</category>
      </categories>
      <tags>
        <tag>Program</tag>
        <tag>Haskell</tag>
        <tag>Functional</tag>
        <tag>Recursion</tag>
        <tag>Function</tag>
        <tag>Computation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Haskell 基本语法（二）模式匹配]]></title>
    <url>%2F2021%2F06%2F25%2Fbasic-haskell-pattern-match%2F</url>
    <content type="text"><![CDATA[式匹配包含一系列特定的模式，用来判断数据是否符合规则，且能够通过这些模式把符合要求的数据解构出来。Haskell 中的模式匹配可以应用到任意的数据类型上（数字、字符、列表、元组等等）。 函数中的模式匹配可以在函数体的定义中，用不同的代码行分别指定不同的模式：123456789101112Prelude&gt; :&#123;Prelude| lucky :: (Integral a) =&gt; a -&gt; StringPrelude| lucky 7 = "LUCKY NUMBER SEVEN!"Prelude| lucky x = "Sorry, you're out of luck!"Prelude| :&#125;Prelude&gt;Prelude&gt; lucky 1"Sorry, you're out of luck!"Prelude&gt; lucky 10"Sorry, you're out of luck!"Prelude&gt; lucky 7"LUCKY NUMBER SEVEN!" PS：上述代码是在 Haskell 的交互式解释器（REPL) ghci 中定义和执行函数的效果。像 lucky 这种包含多行代码的函数，在 ghci 解释器中直接定义时，需要把整个函数体用 :{ 和 :} 括起来（否则解释器会报错）。实际的 lucky 函数代码应为：123lucky :: (Integral a) =&gt; a -&gt; String lucky 7 = "LUCKY NUMBER SEVEN!" lucky x = "Sorry, you're out of luck, pal!" 即 :{ 和 :} 从代码的角度讲是多余的，只是 ghci 解释器的缘故，导致必须加上这两个分隔符。若在文件中编写代码，则应该使用第二种形式。 代码的第一行 lucky :: (Integral a) =&gt; a -&gt; String 是函数的类型签名，也可以省略，解释器会自行推导。lucky 7 和 lucky x 两行代码则指定了具体的两个模式：当函数输入为数字 7 时匹配第一个模式，任何其他的数字输入则匹配第二个模式并将该输入值绑定给变量 x。 一个包含更多个模式的函数：1234567sayMe :: (Integral a) =&gt; a -&gt; String sayMe 1 = "One!" sayMe 2 = "Two!" sayMe 3 = "Three!" sayMe 4 = "Four!" sayMe 5 = "Five!" sayMe x = "Not between 1 and 5" 需要注意的是，最后一行代码 sayMe x 必须作为最后一个模式。函数体中的模式会按照自顶而下的顺序检查是否匹配，若当前的模式已完成匹配，则忽略后面的检查；若当前模式不匹配，则继续向下逐个进行检查。若 sayMe x 作为顶部的第一个模式（它实际上会匹配所有合法值），则任何输入值都会在第一步就完成匹配，进而忽略后面的 sayMe 1、sayMe 2 等模式，不再进行判断。即输入任何数字都会先匹配 x 并输出 Not between 1 and 5。 使用模式匹配和递归实现阶乘函数123factorial :: (Integral a) =&gt; a -&gt; a factorial 0 = 1 factorial n = n * factorial (n - 1) 比如当输入为 3 时，factorial 函数会匹配第二个模式，结果为 3 * (factorial 2)。继续迭代，进一步计算结果中的 factorial 2，得到 3 * (2 * (factorial 1))、3 * (2 * (1 * (factorial 0)))。而 factorial 0 会匹配第一个模式得到结果 1，迭代终止，再和前面的数字相乘后得到最终结果。 假如将 factorial n = n * factorial (n - 1) 作为第一个模式，则 factorial n 会匹配包含数字 0 在内的所有数字，另一个模式 factorial 0 = 1 就永远不会触发。从而导致迭代没有终止条件，一直进行下去。因此，在模式匹配中，更精确更有指向性的模式总是放在相对通用和宽泛的模式前面。 在使用模式匹配时，应该总是包含一个 catch-all 模式，这样就不会出现所有模式都不匹配的情况。若程序的输入与所有模式都不匹配，程序会崩溃掉。12345678910111213Prelude&gt; :&#123;Prelude| charName :: Char -&gt; StringPrelude| charName 'a' = "Albert"Prelude| charName 'b' = "Broseph"Prelude| charName 'c' = "Cecil"Prelude| :&#125;Prelude&gt;Prelude&gt; charName 'a'"Albert"Prelude&gt; charName 'b'"Broseph"Prelude&gt; charName 'h'*** Exception: &lt;interactive&gt;:(3,1)-(5,22): Non-exhaustive patterns in function charName 元组中的模式匹配在不使用模式匹配的情况下，实现一个计算两个向量之和的函数：12addVectors :: (Num a) =&gt; (a, a) -&gt; (a, a) -&gt; (a, a)addVectors a b = (fst a + fst b, snd a + snd b) 通过模式匹配实现上述功能：1234567Prelude&gt; :&#123;Prelude| addVectors :: (Num a) =&gt; (a, a) -&gt; (a, a) -&gt; (a, a)Prelude| addVectors (x1, y1) (x2, y2) = (x1 + x2, y1 + y2)Prelude| :&#125;Prelude&gt;Prelude&gt; addVectors (1, 2) (3, 4)(4,6) fst 和 snd 函数可以分别用来获取元组中的第一个和第二个元素（但是只针对包含两个元素的元组）。对于有 3 个元素的元组，实际上可以借助模式匹配自己实现：12345678first :: (a, b, c) -&gt; afirst (x, _, _) = x second :: (a, b, c) -&gt; bsecond (_, y, _) = y third :: (a, b, c) -&gt; cthird (_, _, z) = z 可以在列表推导中使用模式匹配：123Prelude&gt; let xs = [(1,3), (4,3), (2,4), (5,3), (5,6), (3,1)]Prelude&gt; [a+b | (a,b) &lt;- xs][4,7,6,8,11,4] 甚至列表本身也可以用于模式匹配。如模式 x:xs 会将列表的第一个元素绑定给变量 x，把其余元素绑定给 xs。此模式的应用非常普遍，尤其是在递归函数中。如果想提取列表的前 3 个元素并将它们绑定给指定变量，可以使用 x:y:z:zs 形式的模式。 利用对列表的模式匹配实现自定义的 head 函数（获取列表中的第一个元素）：12345678910Prelude&gt; :&#123;Prelude| head' :: [a] -&gt; aPrelude| head' [] = error "Can't call head on an empty list!"Prelude| head' (x:_) = xPrelude| :&#125;Prelude&gt;Prelude&gt; head' [4, 5, 6]4Prelude&gt; head' "Hello"'H' 借助递归和模式匹配实现自定义的 length 函数（获取列表的长度）：123length' :: (Num b) =&gt; [a] -&gt; blength' [] = 0length' (_:xs) = 1 + length' xs 对于任何一个合法的输入如 &quot;ham&quot;，length&#39; 函数的计算过程如下：length&#39; &quot;ham&quot; =&gt; 1 + length&#39; &quot;am&quot; =&gt; 1 + (1 + length&#39; &quot;m&quot;) =&gt; 1 + (1 + (1 + length&#39; [])) =&gt; 1 + (1 + (1 + 0)) 实现自定义的 sum 函数（求列表中各元素之和）：123sum' :: (Num a) =&gt; [a] -&gt; asum' [] = 0sum' (x:xs) = x + sum' xs 守卫（guards）守卫一般用来测试某个（些）值的特定属性是否为真，很像 if 语句。守卫和模式整合得非常好。 以下是一个求 BMI（体重指数）的函数定义：123456bmiTell :: (RealFloat a) =&gt; a -&gt; StringbmiTell bmi | bmi &lt;= 18.5 = "You're underweight, you emo, you!" | bmi &lt;= 25.0 = "You're supposedly normal. Pffft, I bet you're ugly!" | bmi &lt;= 30.0 = "You're fat! Lose some weight, fatty!" | otherwise = "You're a whale, congratulations!" 管道符（|）后面的布尔表达式即为守卫的定义。若该表达式计算结果为 True，则对应的代码被执行。若该表达式计算结果为 False，则继续测试下一个守卫。 通常情况下，最后一个守卫是 otherwise。它其实是 otherwise = True 的简写形式，会捕获所有剩余的情况。 守卫可以配合有多个参数的函数使用：123456bmiTell :: (RealFloat a) =&gt; a -&gt; a -&gt; StringbmiTell weight height | weight / height ^ 2 &lt;= 18.5 = "You're underweight, you emo, you!" | weight / height ^ 2 &lt;= 25.0 = "You're supposedly normal. Pffft, I bet you're ugly!" | weight / height ^ 2 &lt;= 30.0 = "You're fat! Lose some weight, fatty!" | otherwise = "You're a whale, congratulations!" 12Prelude&gt; bmiTell 65 1.75"You're supposedly normal. Pffft, I bet you're ugly!" 通过守卫自定义 max 函数：1234max' :: (Ord a) =&gt; a -&gt; a -&gt; amax' a b | a &gt; b = a | otherwise = b where可以通过 where 关键字优化上面的 bmiTell 函数：1234567bmiTell :: (RealFloat a) =&gt; a -&gt; a -&gt; StringbmiTell weight height | bmi &lt;= 18.5 = "You're underweight, you emo, you!" | bmi &lt;= 25.0 = "You're supposedly normal. Pffft, I bet you're ugly!" | bmi &lt;= 30.0 = "You're fat! Lose some weight, fatty!" | otherwise = "You're a whale, congratulations!" where bmi = weight / height ^ 2 变量 bmi 在这里只计算了一次，不同于之前的 weight / height ^ 2 有可能会被重复计算 3 次。 更进一步，bmiTell 函数还可以改为如下形式：12345678910bmiTell :: (RealFloat a) =&gt; a -&gt; a -&gt; StringbmiTell weight height | bmi &lt;= skinny = "You're underweight, you emo, you!" | bmi &lt;= normal = "You're supposedly normal. Pffft, I bet you're ugly!" | bmi &lt;= fat = "You're fat! Lose some weight, fatty!" | otherwise = "You're a whale, congratulations!" where bmi = weight / height ^ 2 skinny = 18.5 normal = 25.0 fat = 30.0 where 语句中也可以定义函数，比如通过由多个包含身高体重的元组组成的列表，计算一系列 BMI 值：123calcBmis :: (RealFloat a) =&gt; [(a, a)] -&gt; [a]calcBmis xs = [bmi w h | (w, h) &lt;- xs] where bmi weight height = weight / height ^ 2 参考资料Learn You a Haskell for Great Good!]]></content>
      <categories>
        <category>Program</category>
      </categories>
      <tags>
        <tag>Haskell</tag>
        <tag>Functional</tag>
        <tag>Recursion</tag>
        <tag>GHC</tag>
        <tag>Pattern</tag>
        <tag>Match</tag>
        <tag>Deconstruction</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[The Rust programming language 读书笔记——模式匹配]]></title>
    <url>%2F2021%2F06%2F23%2Fthe-rust-programming-language-reading-notes-pattern-match%2F</url>
    <content type="text"><![CDATA[模式是 Rust 中一种用来匹配类型结构的特殊语法，将其与 match 表达式或其他工具配合使用可以更好地控制程序流程。模式被用来与某个特定的值进行匹配，若匹配成功，则可以继续使用这个值的某些部分；若匹配失败，模式对应的代码就被简单地略过。 模式的应用场景match 分支模式可以被应用在 match 表达式的分支中。match 表达式由 match 关键字、待匹配的值以及至少一个匹配分支组成。匹配分支则由某个模式及模式匹配成功后应当执行的表达式组成。12345match 值 &#123; 模式 =&gt; 表达式, 模式 =&gt; 表达式, 模式 =&gt; 表达式,&#125; match 表达式必须穷尽匹配值的所有可能性。为了确保代码满足要求，可以在最后的分支处使用全匹配模式。例如变量名可以被用来覆盖所有剩余的可能性。还有一个特殊的 _ 模式可以被用来匹配所有可能的值，且不将它们绑定到任何一个变量上。即忽略所有未被指定的值。12345678910fn main() &#123; let some_value = 3; match some_value &#123; 1 =&gt; println!("one"), 3 =&gt; println!("three"), 5 =&gt; println!("five"), 7 =&gt; println!("seven"), _ =&gt; (), &#125;&#125; if let 表达式12345678910111213141516fn main() &#123; let favorite_color: Option&lt;&amp;str&gt; = None; let age: Result&lt;u8, _&gt; = "34".parse(); if let Some(color) = favorite_color &#123; println!("Using your favorite color &#123;&#125; as the background", color); &#125; else if let Ok(age) = age &#123; if age &gt; 30 &#123; println!("Using purple as the background color"); &#125; else &#123; println!("Using orange as the background color"); &#125; &#125; else &#123; println!("Using blue as the background color"); &#125;&#125; 上述代码通过执行一系列的条件检查来确定使用的背景颜色。其中的变量已经被赋予了硬编码值，但现实中应当通过用户输入来获取这些值。 和 match 分支类似，if let 分支能够以同样的方式对变量进行覆盖。if let Ok(age) = age 这句代码中引入了新的变量 age 来存储 Ok 变体中的值，并覆盖了右侧的同名变量。 while let 循环while let 会反复执行同一个模式匹配直到出现失败的情形。1234567891011fn main() &#123; let mut stack = Vec::new(); stack.push(1); stack.push(2); stack.push(3); while let Some(top) = stack.pop() &#123; println!("&#123;&#125;", top); &#125;&#125; 上面的代码会依次打印 3、2、1。其中的 pop 方法会尝试取出动态数组的最后一个元素并将它包裹在 Some(value) 中返回。若动态数组为空，则 pop 返回 None。while 循环会在 pop 返回 Some 时执行循环体中的代码，pop 返回 None 时结束循环。 for 循环for 语句中紧随关键字 for 之后的值就是一个模式。比如 for x in y 中的 x 就是一个模式。 在 for 循环中使用模式来解构元组：1234567fn main() &#123; let v = vec!['a', 'b', 'c']; for (index, value) in v.iter().enumerate() &#123; println!("&#123;&#125; is at index &#123;&#125;", value, index); &#125;&#125; 上述代码使用 enumerate 方法作为迭代器的适配器，会在每次迭代过程中生成一个包含值本身及其索引的元组。如首次调用 enumerate 会产生元组 (0, &#39;a&#39;)。当这个值与模式 (index, value) 进行匹配时，index 就会被赋值为 0，value 就会被赋值为 ‘a’。 let 语句最基本的 let 赋值语句中也同样用到了模式。更正式的 let 语句的定义如下：let PATTERN = EXPRESSION; 在类似于 let x = 5; 这样的语句中，单独的变量名成为最朴素的模式。其中 x 作为模式表达的含义是，将此处匹配到的所有内容绑定至变量 x，因为 x 就是整个模式本身。 用 let 模式匹配来解构元组：let (x, y, z) = (1, 2, 3); 如果模式中元素的数量与元组中元素的数量不同，则整个类型会匹配失败，导致编译错误。 函数的参数函数的参数同样也是模式。12345678fn print_coordinates(&amp;(x, y): &amp;(i32, i32)) &#123; println!("Current location: (&#123;&#125;, &#123;&#125;)", x, y);&#125;fn main() &#123; let point = (3, 5); print_coordinates(&amp;point);&#125; 模式 &amp;(x, y) 能够和值 &amp;(3, 5) 匹配，因此 x 的值为 3，y 的值为 5。 可失败性模式可以被分为不可失败（irrefutable）和可失败（refutable）两种类型。不可失败的模式能够匹配任何传入的值。如语句 let x = 5; 中的 x，因为 x 能够匹配右侧表达式所有可能的返回值。可失败模式则可能因为某些特定的值而匹配失败。如表达式 if let Some(x) = a_value 中的 Some(x)。若 a_value 变量的值是 None 而不是 Some，则左边的 Some(x) 模式就会出现不匹配的情况。 函数参数、let 语句及 for 循环只接收不可失败模式。因为这些场合下，程序无法在值不匹配时执行任何有意义的行为。if let 和 while let 表达式则只接收可失败模式。因为它们在被设计时就将匹配失败的情形考虑在内了，条件表达式的功能就是根据条件的成功与否执行不同的操作。 模式语法匹配字面量12345678910fn main() &#123; let x = 1; match x &#123; 1 =&gt; println!("one"), 2 =&gt; println!("two"), 3 =&gt; println!("three"), _ =&gt; println!("anything"), &#125;&#125; 匹配命名变量命名变量是一种可以匹配任何值的不可失败模式。需要注意的是，当我们在 match 表达式中使用命名变量时，由于 match 开启了一个新的作用域，所以被定义在 match 表达式内作为模式一部分的变量会覆盖掉 match 结构外的同名变量。1234567891011121314fn main() &#123; let x = Some(5); let y = 10; match x &#123; Some(50) =&gt; println!("Got 50"), Some(y) =&gt; println!("Matched, y = &#123;:?&#125;", y), _ =&gt; println!("Default case, x = &#123;:?&#125;", x), &#125; // =&gt; Matched, y = 5 println!("at the end: x = &#123;:?&#125;, y = &#123;:?&#125;", x, y); // =&gt; at the end: x = Some(5), y = 10&#125; 在上述代码中，第二个匹配分支的模式引入了新的变量 y，它会匹配 Some 变体中携带的任何值。因为处在 match 表达式创建的新作用域中，这里的 y 是一个新的变量，而不是程序起始处声明的那个存储了 10 的 y。新的 y 绑定能够匹配 Some 中的任意值，即匹配 x 变量中 Some 内部的值 5。 match 表达式创建的作用域会随着当前表达式的结束而结束，其内部的 y 也无法幸免。因此代码最后的 println! 会输出 at the end: x = Some(5), y = 10。 多重模式可以在 match 表达式的分支匹配中使用 | 来表示或的意思，从而一次性地匹配多个模式。123456789fn main() &#123; let x = 1; match x &#123; 1 | 2 =&gt; println!("one or two"), 3 =&gt; println!("three"), _ =&gt; println!("anything"), &#125;&#125; 使用 ..= 来匹配区间12345678fn main() &#123; let x = 5; match x &#123; 1..=5 =&gt; println!("one through five"), _ =&gt; println!("something else"), &#125;&#125; 使用解构来分解值可以使用模式来分解结构体、枚举、元组或引用，从而使用这些值中的不同部分。 解构结构体123456789101112struct Point &#123; x: i32, y: i32,&#125;fn main() &#123; let p = Point &#123; x: 0, y: 7 &#125;; let Point &#123; x: a, y: b &#125; = p; assert_eq!(0, a); assert_eq!(7, b);&#125; 上述代码创建了 a 和 b 两个变量，分别匹配了 p 结构体中字段 x 和 y 的值。采用与字段名相同的变量名在实践中非常常见，为了避免写出类似于 let Point { x: x, y: y } = p 这样冗余的代码，Rust 允许采用如下形式的代码解构结构体：123456789101112struct Point &#123; x: i32, y: i32,&#125;fn main() &#123; let p = Point &#123; x: 0, y: 7 &#125;; let Point &#123; x, y &#125; = p; assert_eq!(0, x); assert_eq!(7, y);&#125; 除了为所有字段创建变量，还可以在结构体模式中使用字面量来进行解构。这一技术使我们可以在某些特定字段符合要求的前提下再对其他字段进行解构。1234567891011121314struct Point &#123; x: i32, y: i32,&#125;fn main() &#123; let p = Point &#123; x: 0, y: 7 &#125;; match p &#123; Point &#123; x, y: 0 &#125; =&gt; println!("On the x axis at &#123;&#125;", x), Point &#123; x: 0, y &#125; =&gt; println!("On the y axis at &#123;&#125;", y), Point &#123; x, y &#125; =&gt; println!("On neither axis: (&#123;&#125;, &#123;&#125;)", x, y), &#125;&#125; 通过在第一个分支中要求 y 字段匹配字面量 0，从而匹配到所有位于 x 轴上的点，同时创建了一个可以在随后代码块中使用的 x 变量。类似的第二个分支匹配 y 轴上的点，第三个分支匹配所有剩余的点。 甚至可以按照某种更为复杂的方式来将模式混合、匹配或嵌套在一起。let ((feet, inches), Point {x, y}) = ((3, 10), Point { x: 3, y: -10 }); 忽略模式中的值使用 _ 忽略整个值可以使用下划线 _ 作为通配符来匹配任意可能的值而不绑定值本身。虽然 _ 模式最常被用在 match 表达式的最后一个分支中，实际上我们可以把它用于包括函数参数在内的一切模式中。1234567fn foo(_: i32, y: i32) &#123; println!("This code only uses the y parameter: &#123;&#125;", y);&#125;fn main() &#123; foo(3, 4);&#125; 上述代码会忽略传给第一个参数的值 3。忽略函数参数在某些情况下会变得有用。比如正在实现一个 trait，而这个 trait 的方法包含了你不需要的某些参数。此时就可以借助忽略模式避免编译器产生未使用变量的警告。 使用 .. 忽略值的剩余部分12345678910111213struct Point &#123; x: i32, y: i32, z: i32,&#125;fn main() &#123; let origin = Point &#123; x: 0, y: 0, z: 0 &#125;; match origin &#123; Point &#123; x, .. &#125; =&gt; println!("x is &#123;&#125;", x), &#125;&#125; .. 语法会自动展开并填充任意多个所需的值。123456789fn main() &#123; let numbers = (2, 4, 8, 16, 32); match numbers &#123; (first, .., last) =&gt; &#123; println!("Some numbers: &#123;&#125;, &#123;&#125;", first, last); &#125; &#125;&#125; 上述代码使用 first 和 last 分别匹配了元组中的第一个值和最后一个值，而它们之间的 .. 模式则会匹配并忽略中间的值。 使用匹配守卫添加额外条件匹配守卫（match guard）是附加在 match 分支模式后的 if 条件语句，分支中的模式只有在该条件被同时满足时才能匹配成功。匹配守卫的条件可以使用模式中创建的变量。123456789fn main() &#123; let num = Some(4); match num &#123; Some(x) if x &lt; 5 =&gt; println!("less than five: &#123;&#125;", x), Some(x) =&gt; println!("&#123;&#125;", x), None =&gt; (), &#125;&#125; 上述代码中，num 能够与第一个分支中的模式匹配成功，随后的匹配守卫则会检查模式中创建的变量 x 是否小于 5。由于 num 同样满足这一条件，最终执行了第一个分支中的代码。假设 num 的值是 Some(10)，则第一个匹配分支中的匹配守卫无法成立，Rust 会进入第二个分支继续比较最终匹配成功。 我们无法通过模式表达类似于 if x &lt; 5 这样的条件，匹配守卫增强了语句中表达相关逻辑的能力。 参考资料The Rust Programming Language]]></content>
      <categories>
        <category>Rust</category>
      </categories>
      <tags>
        <tag>Program</tag>
        <tag>Advanced</tag>
        <tag>Functional</tag>
        <tag>Pattern</tag>
        <tag>Match</tag>
        <tag>Rust</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[The Rust programming language 读书笔记——泛型与 trait（特征）]]></title>
    <url>%2F2021%2F06%2F20%2Fthe-rust-programming-language-reading-notes-generics-and-trait%2F</url>
    <content type="text"><![CDATA[所有的编程语言都会致力于高效地处理重复概念，Rust 中的泛型（generics）就是这样一种工具。泛型是具体类型或其他属性的抽象替代。比如 Option&lt;T&gt;、Vec&lt;T&gt;、Hash&lt;K, V&gt; 等。 将代码提取为函数以减少重复工作下面的代码可以用来在数字列表中找到最大值：1234567891011fn main() &#123; let number_list = vec![34, 50, 25, 100, 65]; let mut largest = number_list[0]; for number in number_list &#123; if number &gt; largest &#123; largest = number; &#125; &#125; println!("The largest number is &#123;&#125;", largest);&#125; 为了消除重复代码，可以通过定义函数来创建抽象，令该函数可以接收任意整数列表作为参数并进行求值。12345678910111213141516fn largest(list: &amp;[i32]) -&gt; i32 &#123; let mut largest = list[0]; for &amp;item in list.iter() &#123; if item &gt; largest &#123; largest = item; &#125; &#125; largest&#125;fn main() &#123; let number_list = vec![34, 50, 25, 100, 65]; let result = largest(&amp;number_list); println!("The largest number is &#123;&#125;", result);&#125; 假设我们拥有两个不同的函数：一个用于在 i32 切片中搜索最大值；另一个用于在 char 切片中搜索最大值。代码可能是下面这个样子：12345678910111213141516171819202122232425262728293031fn largest_i32(list: &amp;[i32]) -&gt; i32 &#123; let mut largest = list[0]; for &amp;item in list.iter() &#123; if item &gt; largest &#123; largest = item; &#125; &#125; largest&#125;fn largest_char(list: &amp;[char]) -&gt; char &#123; let mut largest = list[0]; for &amp;item in list.iter() &#123; if item &gt; largest &#123; largest = item; &#125; &#125; largest&#125;fn main() &#123; let number_list = vec![34, 50, 25, 100, 65]; let result = largest_i32(&amp;number_list); println!("The largest number is &#123;&#125;", result); let char_list = vec!['y', 'm', 'a', 'q']; let result = largest_char(&amp;char_list); println!("The largest char is &#123;&#125;", result);&#125; 泛型数据类型在函数定义中使用当使用泛型来定义一个函数时，我们需要将泛型放置在函数签名中用于指定参数和返回值类型的地方。以这种方式编写的代码更加灵活，可以在不引入重复代码的同时向函数调用者提供更多的功能。 上面代码中的 largest_i32 和 largest_char 是两个只在名称和签名上有所区别的函数。largest_i32 作用于 i32 类型的切片，而 largest_char 作用于 char 类型的切片。这两个函数拥有完全相同的代码，因此可以通过在一个函数中使用泛型来消除重复代码。 在函数签名中使用泛型合并不同的 largest 函数：1234567891011121314151617181920fn largest&lt;T: PartialOrd + Copy&gt;(list: &amp;[T]) -&gt; T &#123; let mut largest = list[0]; for &amp;item in list.iter() &#123; if item &gt; largest &#123; largest = item; &#125; &#125; largest&#125;fn main() &#123; let number_list = vec![34, 50, 25, 100, 65]; let result = largest(&amp;number_list); println!("The largest number is &#123;&#125;", result); let char_list = vec!['y', 'm', 'a', 'q']; let result = largest(&amp;char_list); println!("The largest char is &#123;&#125;", result);&#125; 其中 largest&lt;T: PartialOrd + Copy&gt; 部分的 PartialOrd 和 Copy 是为类型 T 指定的两个 trait 约束（后面会提到）。 在结构体定义中使用同样地，也可以使用 &lt;&gt; 语法来定义在一个或多个字段中使用泛型的结构体。123456789struct Point&lt;T&gt; &#123; x: T, y: T,&#125;fn main() &#123; let integer = Point &#123; x: 5, y: 1 &#125;; let float = Point &#123; x: 1.0, y: 4.0 &#125;;&#125; 如上面的代码，在结构名后的一对尖括号中声明泛型参数后，就可以在结构体定义中用于指定具体数据类型的位置使用泛型了。 在定义 Point&lt;T&gt; 结构体时仅使用了一个泛型参数，表明该结构体对某个类型 T 是通用的。但无论 T 具体的类型是什么，字段 x 和 y 都同时属于这个类型。即 x 和 y 只能是同一类型。 为了使结构体 Point 中的 x 和 y 能够被实例化为不同的类型，可以使用多个泛型参数。12345678910struct Point&lt;T, U&gt; &#123; x: T, y: U,&#125;fn main() &#123; let both_integer = Point &#123; x: 5, y: 1 &#125;; let both_float = Point &#123; x: 1.0, y: 4.0 &#125;; let integer_and_float = Point &#123; x: 5, y: 4.0 &#125;;&#125; 在方法定义中使用方法也可以在自己的定义中使用泛型：123456789101112131415struct Point&lt;T&gt; &#123; x: T, y: T,&#125;impl&lt;T&gt; Point&lt;T&gt; &#123; fn x(&amp;self) -&gt; &amp;T &#123; &amp;self.x &#125;&#125;fn main() &#123; let p = Point &#123; x: 5, y: 10 &#125;; println!("p.x = &#123;&#125;", p.x());&#125; 上面的代码为结构体 Point&lt;T&gt; 实现了名为 x 的方法，返回一个指向 x 字段中 T 类型值的引用。 紧跟着 impl 关键字声明 T 是必须的。通过在 impl 之后将 T 声明为泛型，Rust 能够识别出 Point&lt;T&gt; 中尖括号内的类型是泛型而不是具体的类型。 实际上，可以单独为 Point&lt;f32&gt; 实例而不是所有的 Point&lt;T&gt; 泛型实例来实现特定的方法。当在 Point&lt;32&gt; 声明中使用了明确的类型 f32，也意味着无需在 impl 之后附带任何类型声明了。12345impl Point&lt;f32&gt; &#123; fn distance_from_origin(&amp;self) -&gt; f32 &#123; (self.x.powi(2) + self.y.powi(2)).sqrt() &#125;&#125; 上面的代码意味着，类型 Point&lt;f32&gt; 将会拥有一个名为 distance_from_origin 的方法，而其他的 Point&lt;T&gt; 实例则没有该方法的定义。 结构体定义中的泛型参数并不总是与方法签名中使用的类型参数一致。12345678910111213141516171819202122struct Point&lt;T, U&gt; &#123; x: T, y: U,&#125;impl&lt;T, U&gt; Point&lt;T, U&gt; &#123; fn mixup&lt;V, W&gt;(self, other: Point&lt;V, W&gt;) -&gt; Point&lt;T, W&gt; &#123; Point &#123; x: self.x, y: other.y, &#125; &#125;&#125;fn main() &#123; let p1 = Point &#123; x: 5, y: 10.4 &#125;; let p2 = Point &#123; x: "Hello", y: 'c' &#125;; let p3 = p1.mixup(p2); println!("p3.x = &#123;&#125;, p3.y = &#123;&#125;", p3.x, p3.y); // =&gt; p3.x = 5, p3.y = c&#125; trait：定义共享行为trait 用来向 Rust 编译器描述某些特定类型拥有的且能够被其他类型共享的功能，使我们可以以一种抽象的方式来定义共享行为。 trait 与其他语言中的接口（interface）功能类似，但也不尽相同。类型的行为由该类型本身可供调用的方法组成。当我们可以在不同的类型上调用相同的方法时，就称这些类型共享了相同的行为。trait 提供了一种将特定方法组合起来的途径，定义了为达成某种目的所必须的方法（行为）集合。 定义 trait假如我们拥有多个结构体（struct），分别持有不同类型、不同数量的文本字段。其中 NewsArticle 结构体存放新闻故事，Tweet 结构体存放推文。我们还想要方便地获取存储在 NewsArticle 和 Tweet 实例中的数据摘要。因此需要为每个结构体类型都实现摘要行为，从而可以在这些实例上统一地调用 summarize 方法来请求摘要内容。 可以定义如下形式的 Summary trait：123trait Summary &#123; fn summarize(&amp;self) -&gt; String;&#125; 在大括号中声明了用于定义类型行为的方法签名，即 fn summarize(&amp;self) -&gt; String;。方法签名后省略了大括号及方法的具体实现。任何想要实现这个 trait 的类型都需要为上述方法提供自定义行为。编译器会确保每一个实现了 Summary trait 的类型都定义了与这个签名完全一致的 summarize 方法。一个 trait 可以包含多个方法，每个方法签名占据单独一行并以分号结尾。 为类型实现 trait 完整代码：1234567891011121314151617181920212223242526272829303132333435363738394041trait Summary &#123; fn summarize(&amp;self) -&gt; String;&#125;struct NewsArticle &#123; headline: String, location: String, author: String, content: String,&#125;impl Summary for NewsArticle &#123; fn summarize(&amp;self) -&gt; String &#123; format!("&#123;&#125;, by &#123;&#125; (&#123;&#125;)", self.headline, self.author, self.location) &#125;&#125;struct Tweet &#123; username: String, content: String, reply: bool, retweet: bool,&#125;impl Summary for Tweet &#123; fn summarize(&amp;self) -&gt; String &#123; format!("&#123;&#125;: &#123;&#125;", self.username, self.content) &#125;&#125;fn main() &#123; let tweet = Tweet &#123; username: String::from("horse_ebooks"), content: String::from("of course, as you probably already know, people"), reply: false, retweet: false, &#125;; println!("1 new tweet: &#123;&#125;", tweet.summarize()); // =&gt; 1 new tweet: horse_ebooks: of course, as you probably already know, people&#125; 其中 impl Summary for NewsArticle 和 impl Summary for Tweet 部分负责为 NewsArticle 和 Tweet 两个结构体类型定义 Summary trait 中指定的 summarize 方法，并为该方法实现具体的行为。 默认实现某些时候，为 trait 中的某些或所有方法都提供默认行为非常有用，使我们无需为每一个类型的 trait 实现都提供自定义行为。当我们为某个特定类型实现 trait 时，可以选择保留或重载每个方法的默认行为。 如为 Summary trait 中的 summarize 方法指定一个默认的字符串返回值：12345trait Summary &#123; fn summarize(&amp;self) -&gt; String &#123; String::from("(Read More...)") &#125;&#125; 假如需要在 NewsArticle 的实例中使用上述默认实现，而不是自定义实现，可以指定一个空的 impl 代码块：impl Summary for NewsArticle {} 此时虽然没有直接为 NewsArticle 定义 summarize 方法，依然可以在 NewsArticle 实例上调用 summarize 方法。1234567891011121314fn main() &#123; let article = NewsArticle &#123; headline: String::from("Penguins win the Stanley Cup Championship!"), location: String::from("Pittsburgh, PA, USA"), author: String::from("Iceburgh"), content: String::from( "The Pittsburgh Penguins once again are the best hockey team in the NHL.", ), &#125;; println!("New article available! &#123;&#125;", article.summarize()); // =&gt; New article available! (Read More...)&#125; 可以在默认实现中调用同一 trait 中的其他方法，哪怕这些被调用的方法没有默认实现。例如，可以为 Summary trait 定义一个需要被实现的方法 summarize_author（即 trait 中没有该方法的默认实现，需要在后续的类型中实现），再通过调用 summarize_author 为 summarize 方法提供一个默认实现：1234567trait Summary &#123; fn summarize_author(&amp;self) -&gt; String; fn summarize(&amp;self) -&gt; String &#123; format!("(Read more from &#123;&#125;...)", self.summarize_author()) &#125;&#125; 为了使用这个版本的 Summary，只需要在后续类型实现这一 trait 时定义 summarize_author 方法：12345impl Summary for Tweet &#123; fn summarize_author(&amp;self) -&gt; String &#123; format!("@&#123;&#125;", self.username) &#125;&#125; 定义了 summarize_author 之后，就可以在 Tweet 实例上调用 summarize 了。summarize 的默认实现会进一步调用我们提供的 summarize_author 的定义。1234567891011fn main() &#123; let tweet = Tweet &#123; username: String::from("horse_ebooks"), content: String::from("of course, as you probably already know, people"), reply: false, retweet: false, &#125;; println!("1 new tweet: &#123;&#125;", tweet.summarize()); // =&gt; 1 new tweet: (Read more from @horse_ebooks...)&#125; trait 作为参数前面的代码中为 NewsArticle 和 Tweet 类型实现了 Summary trait，我们还可以定义一个 notify 函数来调用这些类型的 summarize 方法。语法如下：123fn notify(item: impl Summary) &#123; println!("Breaking news! &#123;&#125;", item.summarize());&#125; 上述代码没有为 item 参数指定具体的类型，而是使用了 impl 关键字及对应的 trait 名称。这意味着 item 参数可以接收任何实现了指定 trait 的类型。在 notify 函数体内，则可以调用来自 Summary trait 的任何方法。尝试使用其他类型（如 String 或 i32）来调用 notify 函数则无法通过编译，因为这些类型没有实现 Summary trait。 上述代码其实只是 trait 约束的一种语法糖，完整形式如下：12fn notify&lt;T: Summary&gt;(item: T) &#123; println!("Breaking news! &#123;&#125;", item.summarize()); 通过 + 语法来指定多个 trait 约束如果 notify 函数需要在调用 summarize 方法的同时显示格式化后的 item，则此处的 item 就必须实现两个不同的 trait：Summary 和 Display。fn notify(item: impl Summary + Display) { 这一语法在泛型的 trait 约束中同样有效：fn notify&lt;T: Summary + Display&gt;(item: T) { where 从句简化 trait 约束因为每个泛型都拥有自己的 trait 约束，定义多个类型参数的函数可能会有大量的 trait 约束信息需要被填写在函数名与参数列表之间。Rust 提供了一种替代语法。 如 fn some_function&lt;T: Display + Clone, U: Clone + Debug&gt;(t: T, u: U) -&gt; i32 { 可以改写成如下形式：1234fn some_function&lt;T, U&gt;(t: T, u: U) -&gt; i32 where T: Display + Clone, U: Clone + Debug&#123; 返回实现了 trait 的类型同样可以在返回值中使用 impl Trait 语法，用于返回某种实现了特定 trait 的类型。12345678fn returns_summarizable() -&gt; impl Summary &#123; Tweet &#123; username: String::from("horse_ebooks"), content: String::from("of course, as you probably already know, people"), reply: false, retweet: false, &#125;&#125; 之前在介绍泛型时编写的 largest 函数就通过 trait 约束来限定泛型参数的具体类型。在 largest 函数中，我们想要使用大于号运算符来比较两个 T 类型的值。这一运算符被定义为标准库 std::cmp::PartialOrd 的一个默认方法，因此需要在 T 的 trait 约束中指定 PartialOrd，才能够使 largest 函数用于任何可比较类型的切片上。 我们在编写 largest 函数的非泛型版本时，只尝试过搜索 i32 和 char 类型的最大值。这两种都是拥有确定大小并存储在栈上的类型，实现了 Copy trait。但当我们尝试将 largest 函数泛型化时，list 参数中的类型有可能是没有实现 Copy trait 的。为了确保这个函数只会被那些实现了 Copy trait 的类型所调用，还需要把 Copy 加入到 T 的 trait 约束中。 所以最终的 largest 函数采用如下声明：fn largest&lt;T: PartialOrd + Copy&gt;(list: &amp;[T]) -&gt; T { 使用 trait 约束有条件地实现方法通过在带有泛型参数的 impl 代码块中使用 trait 约束，我们可以单独为实现了指定 trait 的类型编写方法。123456789101112131415161718192021222324252627use std::fmt::Display;struct Pair&lt;T&gt; &#123; x: T, y: T,&#125;impl&lt;T&gt; Pair&lt;T&gt; &#123; fn new(x: T, y: T) -&gt; Self &#123; Self &#123; x, y &#125; &#125;&#125;impl&lt;T: Display + PartialOrd&gt; Pair&lt;T&gt; &#123; fn cmp_display(&amp;self) &#123; if self.x &gt;= self.y &#123; println!("The largest member is x = &#123;&#125;", self.x); &#125; else &#123; println!("The largest member is y = &#123;&#125;", self.y); &#125; &#125;&#125;fn main() &#123; let pair = Pair::new(3, 4); pair.cmp_display()&#125; 上面的代码中，所有的 Pair&lt;T&gt; 类型都会实现 new 方法，但只有在内部类型 T 实现了 PartialOrd（用于比较）和 Display（用于打印）这两个 trait 的前提下，才会实现 cmd_display 方法。 总结借助于 trait 和 trait 约束，我们可以在使用泛型参数消除重复代码的同时，向编译器指明自己希望泛型拥有的功能。而编译器则可以利用这些 trait 约束信息来确保代码中使用的具体类型提供了正确的行为。在动态语言中，尝试调用类型没有实现的方法会导致在运行时出现错误。Rust 将这些错误出现的时机转移到了编译期，我们无需编写那些用于在运行时检查类型的代码，这一机制在保留泛型灵活性的同时提升了代码性能。 参考资料The Rust Programming Language]]></content>
      <categories>
        <category>Rust</category>
      </categories>
      <tags>
        <tag>Program</tag>
        <tag>OOP</tag>
        <tag>Type</tag>
        <tag>Rust</tag>
        <tag>Generics</tag>
        <tag>Interface</tag>
        <tag>Trait</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[The Rust programming language 读书笔记——通用集合类型]]></title>
    <url>%2F2021%2F06%2F14%2Fthe-rust-programming-language-reading-notes-collections%2F</url>
    <content type="text"><![CDATA[Rust 标准库包含了一系列被称为集合的数据结构。与内置的数组和元组不同，集合将自己持有的数据存储在堆上。这使得数据的大小不需要在编译时确定，且可以随着程序的运行按需扩大或缩小数据占用的空间。 Rust 中有 3 种最常用的集合类型： 动态数组（vector） 字符串（string） 哈希映射（hash map） 使用动态数组存储多个值动态数组（Vec&lt;T&gt;）支持在单个数据结构中存储多个相同类型的值，这些值会彼此相邻地排布在内存中。 可以调用函数 Vec::new 来创建一个空的动态数组：let v: Vec&lt;i32&gt; = Vec::new(); 上述代码会创建一个用来存储 i32 数据地的空的动态数组。由于并未在这个动态数组中插入任何值，因此需要显式地添加类型标记（Vec&lt;i32&gt;）。 用初始值去创建动态数组的场景也十分常见，因此 Rust 特意提供了一个用于简化代码的 vec! 宏。let v = vec![1, 2, 3] 更新动态数组：123456let mut v = Vec::new();v.push(5);v.push(6);v.push(7);v.push(8); 销毁动态数组时也会销毁其中的元素123456&#123; let v = vec![1, 2, 3, 4]; // 执行与 v 相关的操作&#125; // &lt;- v 在这里离开作用域并随之被销毁 可以使用索引和 get 两种方法读取动态数组中的元素。12345678910fn main() &#123; let v = vec![1, 2, 3, 4]; let third: &amp;i32 = &amp;v[2]; println!("The third element is &#123;&#125;", third); match v.get(2) &#123; Some(third) =&gt; println!("The third number is &#123;&#125;", third), None =&gt; println!("There is no third element"), &#125;&#125; 需要注意的是： 动态数组使用数字进行索引，索引值从 0 开始 使用 &amp; 与 [] 会直接返回元素的引用 接收索引作为参数的 get 方法会返回一个 Option&lt;T&gt; 类型 当尝试使用不存在的索引值去访问动态数组时，上述两种引用方法会导致程序触发不同的响应方式。比如对于某个持有 5 个元素的动态数组，尝试访问其索引为 100 的元素。 [] 方法会因为索引指向了不存在的元素而触发 panic，假如希望在尝试越界访问元素时令程序直接崩溃，此方法就很适用。get 方法会在检测到索引越界时直接返回 None，而不会导致程序崩溃。当偶尔越界访问动态数组的元素是一种正常行为时，可以使用此方法。此外，代码应该合乎逻辑地处理 Some(&amp;element) 与 None 两种不同的情形。 在存在指向动态数组元素的引用时，尝试向动态数组中添加元素会导致编译器报错。比如下面的代码：123456fn main() &#123; let mut v = vec![1, 2, 3, 4, 5]; let first = &amp;v[0]; v.push(6); print!("The first element is &#123;&#125;", first);&#125; 对第一个元素的引用需要关心动态数组结尾处的变化，这与动态数组的机制有关。动态数组中的元素是连续地存储在堆中的，插入新的元素也许会没有足够多的空间将所有元素依次相邻地放下。这就需要分配新的内存空间，再将旧的元素移动到新的空间上，旧的空间被释放。也就是说，动态数组末尾插入数据有可能导致上面代码中第一个元素的引用指向了被释放的内存。 遍历动态数组123456fn main() &#123; let v = vec![100, 32, 57]; for i in &amp;v &#123; print!("&#123;&#125;", i); &#125;&#125; 也可以遍历可变的动态数组，获得元素的可变引用，并修改其中的值。1234567fn main() &#123; let mut v = vec![100, 32, 57]; for i in &amp;mut v &#123; *i += 50; print!("&#123;&#125;", i) &#125;&#125; 为了使 += 运算符修改可变引用的值，需要使用解引用运算符 * 来获取 i 绑定的值。 使用字符串存储 UTF-8 文本Rust 在语言核心部分只有一种字符串类型，即字符串切片 str，通常以借用的形式（&amp;str）出现。字符串切片是一些指向存储在别处的 UTF-8 编码的字符串的引用。 String 类型被定义在 Rust 标准库中，没有内置在语言的核心部分。它也采用了 UTF-8 编码。 创建字符串许多对于 Vec&lt;T&gt; 的操作同样可用于 String，比如可以从 new 函数创建一个新的空字符串：let mut s = String::new(); 可以对那些实现了 Display trait 的类型调用 to_string 方法，创建有初始数据的字符串：let s = &quot;initial contents&quot;.to_string(); 也可以使用 String::from 函数基于字符串字面量生成 String：let s = String::from(&quot;initial contents&quot;); 字符串是基于 UTF-8 编码的，因此可以将任何合法的数据编码进字符串：let hello = String::from(&quot;你好&quot;); 更新字符串可以使用 push_str 方法来向 String 中添加一段字符串切片。12let mut s = String::from("foo");s.push_str("bar"); push 方法接收单个字符作为参数，并将它添加到 String 中。12let mut s = String::from("lo");s.push('l'); 使用 + 运算符将两个 String 合并到一个新的 String 中：123456fn main() &#123; let s1 = String::from("Hello, "); let s2 = String::from("world!"); let s3 = s1 + &amp;s2; // 这里的 s1 已经被移动且再也不能被使用 println!("&#123;&#125;", s3);&#125; 需要注意的是，上面的加法操作中只对变量 s2 采用了引用，而 s1 由于所有权的移动在加法操作之后不再有效。这里的 + 运算符会调用一个 add 方法，其签名类似于：fn add(self, s: &amp;str) -&gt; String { 由于函数签名中的 self 并没有使用 &amp; 标记，因此 add 函数会取得 self 的所有权，导致 s1 被移动至 add 函数调用中，在调用后失效。这种实现要比单纯的复制更加高效。 对于复杂一些的比如多个字符串的合并，可以使用 format! 宏：12345let s1 = String::from("tic");let s2 = String::from("tac");let s3 = String::from("toe");let s = format!("&#123;&#125;-&#123;&#125;-&#123;&#125;", s1, s2, s3); format! 宏与 println! 宏的工作原理完全相同，只不过 format! 会将结果包含在一个 String 中返回。这使得用 format! 的代码更加易读，且不会夺取任何参数的所有权。 字符串索引Rust 中的字符串不支持索引。比如下面的代码会导致编译器报错：12let s1 = String::from("hello");let h = s1[0]; String 实际上是一个基于 Vec&lt;u8&gt; 的封装类型。let len = String::from(&quot;Hola&quot;).len(); 中，变量 len 的值为 4，意味着动态数组所存储的字符串 Hola 占用了 4 个字节。而 let len = String::from(&quot;你好&quot;).len(); 中，Rust 返回的结果却并不是 2，而是 6。这就是使用 UTF-8 编码来存储“你好”所需要的字节数。因此对于字符串中字节的索引并不总是能对应到一个有效的 Unicode 标量值。 还有一个原因，索引操作的复杂度往往会被预期为常数时间 O(1)，但在 String 中，Rust 必须要从头遍历至索引位置来确定究竟有多少合法字符存在，这无法保障常数时间的性能。 字符串切片字符串切片是指向 String 对象中某个连续部分的引用：123let s = String::from("hello world");let hello = &amp;s[0..5];let world = &amp;s[6..11]; 向函数传入字符串切片并不会导致切片指向的原始 String 因为所有权的移动而失效。 字符串字面量就是切片。let s = &quot;Hello, world!&quot;;变量 s 的类型其实是 &amp;str，是一个指向二进制程序特定位置的切片。正是由于 &amp;str 是一个不可变引用，字符串字面量才是不可变的。 尝试通过索引引用字符串通常是一个坏主意，因为该操作应当返回的类型是不明确的：究竟应该是字节、字符、字形簇还是切片呢？Rust 要求程序员做出更加明确的标记，在索引的 [] 中填写范围来指定所需的字节内容，即明确其类型为字符串切片。123456fn main() &#123; let hello = String::from("你好"); let s = &amp;hello[0..3]; println!("&#123;&#125;", s); // =&gt; 你&#125; 在上面的代码中，s 将会是一个包含了字符串前 3 个字节的 &amp;str，即 你。若尝试在代码中使用 &amp;hello[0..2]，则程序运行时会发生 panic：thread &#39;main&#39; panicked at &#39;byte index 2 is not a char boundary; it is inside &#39;你&#39; (bytes 0..3) of `你好`&#39; 切记要小心谨慎地使用范围语法创建字符串切片。 假如确实需要对每一个 Unicode 标量值都进行处理，最好的办法是使用 chars 方法：12345678fn main() &#123; let hello = String::from("你好"); for c in hello.chars() &#123; println!("&#123;&#125;", c); // =&gt; 你 // =&gt; 好 &#125;&#125; 在映射中存储键值对哈希映射 HashMap&lt;K, V&gt; 存储了从 K 类型键关联到 V 类型值之间的映射关系。 创建哈希映射123456use std::collections::HashMap;let mut scores = HashMap::new();scores.insert(String::from("Blue"), 10);scores.insert(String::from("Yellow"), 50); 和动态数组一样，哈希映射也将其数据存储在堆上。它同样也是同质的，即所有键必须拥有相同的类型，所有的值也必须拥有相同的类型。 另一种构建哈希映射的方法：123456use std::collections::HashMap;let teams = vec![String::from("Blue"), String::from("Yellow")];let initial_scores = vec![10, 50];let scores: HashMap&lt;_, _&gt; = teams.iter().zip(initial_scores.iter()).collect(); 哈希映射与所有权对于那些实现了 Copy trait 的类型如 i32，它们的值会被简单地复制到哈希映射中。而对于 String 这种持有所有权的值，其所有权会转移给哈希映射：12345678use std::collections::HashMap;let field_name = String::from("Favorite color");let field_value = String::from("Blue");let mut map = HashMap::new();map.insert(field_name, field_value);// filed_name 和 field_value 从这一刻开始失效，若尝试使用它们则会导致编译错误！ 在调用 insert 方法后，field_name 和 field_value 变量会被移动到哈希映射中，之后就无法再使用这两个变量了。 访问哈希映射中的值123456789use std::collections::HashMap;let mut scores = HashMap::new();scores.insert(String::from("Blue"), 10);scores.insert(String::from("Yellow"), 50);let team_name = String::from("Blue");let score = scores.get(&amp;team_name); get 返回的是一个 Option&lt;&amp;V&gt; 类型。因此上面代码中的 score 将会是与蓝队相关联的值，即 Some(&amp;10)。若哈希映射中没有指定键所对应的值，get 方法就会返回 None。 可以使用 for 循环遍历哈希映射：12345678910use std::collections::HashMap;let mut scores = HashMap::new();scores.insert(String::from("Blue"), 10);scores.insert(String::from("Yellow"), 50);for (key, value) in &amp;scores &#123; println!("&#123;&#125;: &#123;&#125;", key, value);&#125; 更新哈希映射替换旧值12345678use std::collections::HashMap;let mut scores = HashMap::new();scores.insert(String::from("Blue"), 10);scores.insert(String::from("Blue"), 25);println!("&#123;:?&#125;", scores); 原来的值 10 会被新值 25 替换掉。 只在某个键没有对应值时才插入数据即若某个键对应的值存在，保持原状；若该值不存在，将参数作为新值插入。 1234567891011fn main() &#123; use std::collections::HashMap; let mut scores = HashMap::new(); scores.insert(String::from("Blue"), 10); scores.entry(String::from("Yellow")).or_insert(50); scores.entry(String::from("Blue")).or_insert(50); println!("&#123;:?&#125;", scores); // =&gt; &#123;"Blue": 10, "Yellow": 50&#125;&#125; 基于旧值更新值1234567891011121314fn main() &#123; use std::collections::HashMap; let text = "hello world wonderful world"; let mut map = HashMap::new(); for word in text.split_whitespace() &#123; let count = map.entry(word).or_insert(0); *count += 1; &#125; println!("&#123;:?&#125;", map); // =&gt; &#123;"hello": 1, "world": 2, "wonderful": 1&#125;&#125; 参考资料The Rust Programming Language]]></content>
      <categories>
        <category>Rust</category>
      </categories>
      <tags>
        <tag>Program</tag>
        <tag>DataStructure</tag>
        <tag>List</tag>
        <tag>Rust</tag>
        <tag>Collection</tag>
        <tag>Vector</tag>
        <tag>String</tag>
        <tag>HashMap</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[The Rust programming language 读书笔记——结构体（Struct）]]></title>
    <url>%2F2021%2F06%2F09%2Fthe-rust-programming-language-reading-notes-struct%2F</url>
    <content type="text"><![CDATA[结构（Struct）是一种自定义数据类型。允许我们命名多个相关的值并将它们组成一个有机的结合体。 定义与实例化关键字 struct 被用来定义并命名结构体，一个良好的结构体名称需反映出自身数据组合的意义。123456struct User &#123; username: String, email: String, sign_in_count: u64, active: bool,&#125; 结构体就像是类型的通用模板，将具体的数据填入模板时就创建了新的实例。123456let user1 = User &#123; email: String::from("someone@example.com"), username: String::from("someone@example.com"), active: true, sign_in_count: 1,&#125;; 在创建了结构体实例后，可以通过点号来访问实例中的特定字段。假如这个实例是可变的，还可以通过点号来修改字段的值。12345678let mut user1 = User &#123; email: String::from("someone@example.com"), username: String::from("someusername123"), active: true, sign_in_count: 1,&#125;;user1.email = String::from("anotheremail@example.com"); 需要注意的是，一旦结构体实例定义为可变，那么实例中的所有字段都将是可变的。 可以在函数体的最后一个表达式中构建结构体实例，来隐式的将这个实例作为结果返回。12345678fn build_user(email: String, username: String) -&gt; User &#123; User &#123; email: email, username: username, active: true, sign_in_count: 1, &#125;&#125; 在变量名与字段名相同时，可以使用简化版的字段初始化方法重构上面的 build_user 函数。12345678fn build_user(email: String, username: String) -&gt; User &#123; User &#123; email, username, active: true, sign_in_count: 1, &#125;&#125; 在许多情况下，新创建的实例中，除了需要修改的小部分字段以外，其余字段的值与旧实例完全相同。可以使用结构体更新语法快速实现此类新实例的创建。 使用结构体更新语法来为一个 User 实例设置新的 email 和 username 字段的值，并从 user1 实例中获取剩余字段的值：12345let user2 = User &#123; email: String::from("another@example.com"), username: String::from("anotherusername567"), ..user1&#125;; .. 表示剩下的那些还未被显式赋值的字段都与给定实例拥有相同的值。 元组结构体可以使用一种类似元组的方式定义结构体，这种结构体也被称作元组结构体。元组结构体同样拥有表明自身含义的名称，但无需在声明时对其字段进行命名，只标注类型即可。12345struct Color(i32, i32, i32);struct Point(i32, i32, i32);let black = Color(0, 0, 0);let origin = Point(0, 0, 0); 这里的 black 和 origin 是不同的类型，因为它们两个分别是不同元组结构体的实例。每一个结构体都拥有自己的类型。 示例程序使用 cargo 命令创建一个名为 rectangles 的项目：cargo new rectangles这个程序会接收以像素为单位的宽度和高度作为输入，并计算出对应的长方形面积。 编辑项目中的 src/main.rs 源代码文件：12345678910fn main() &#123; let width1 = 30; let height1 = 50; print!("The area of the rectangle is &#123;&#125;", area(width1, height1));&#125;fn area(width: u32, height: u32) -&gt; u32 &#123; width * height&#125; 运行 cargo run 命令查看输出：1234$ cargo run Finished dev [unoptimized + debuginfo] target(s) in 2.53s Running `target/debug/rectangle`The area of the rectangle is 1500 area 函数用来计算长方形的面积，接收宽和高两个参数。这两个参数是相互关联的，但程序中没有任何地方可以体现这一点。将宽和高放在一起能够使代码更加易懂和易于维护。 使用元组关联长方形的宽和高12345678fn main() &#123; let rect1 = (30, 50); print!("The area of the rectangle is &#123;&#125;", area(rect1));&#125;fn area(dimensions: (u32, u32)) -&gt; u32 &#123; dimensions.0 * dimensions.1&#125; 在上面的代码中，元组使输入的参数结构化了，现在只需要传递一个参数就可以调用函数 area。但元组不会给出自身元素的名称，只能通过索引访问。这使得程序变得难以阅读。比如当需要将该长方形绘制到屏幕上时，混淆宽度和高度就容易出现问题。 使用结构体增加有意义的描述信息12345678910111213141516struct Rectangle &#123; width: u32, height: u32,&#125;fn main() &#123; let rect1 = Rectangle &#123; width: 30, height: 50, &#125;; print!("The area of the rectangle is &#123;&#125;", area(&amp;rect1));&#125;fn area(rectangle: &amp;Rectangle) -&gt; u32 &#123; rectangle.width * rectangle.height&#125; Rectangle 结构体表明了宽度和高度是相互关联的两个值，并为这些值提供了描述性的名字。因此代码看起来会更加清晰。 方法方法与函数十分相似，它们都使用 fn 关键字及一个名称进行声明；它们都可以拥有参数和返回值；它们都包含了一段在调用时执行的代码。但方法总是被定义在某个结构体（或者枚举类型、trait 对象）的上下文中，且它们的第一个参数都是 self，用于指代调用该方法的结构体实例。 将 area 函数定义为 Rectangle 结构体中的方法：123456789101112131415161718struct Rectangle &#123; width: u32, height: u32,&#125;impl Rectangle &#123; fn area(&amp;self) -&gt; u32 &#123; self.width * self.height &#125;&#125;fn main() &#123; let rect1 = Rectangle &#123; width: 30, height: 50, &#125;; print!("The area of the rectangle is &#123;&#125;", rect1.area());&#125; 由于方法的声明被放置在 impl Rectangle 块中，因此 Rust 能够将 self 的类型推导为 Rectangle，我们才可以在 area 的签名中使用 &amp;self 来替代 &amp;Rectangle。使用方法替代函数不仅能够避免在每个方法的签名中重复编写 self 的类型，还有助于程序员组织代码的结构。可以将某个类型的实例需要的功能放置在同一个 impl 块中，避免用户在代码库中盲目地搜索它们。 添加 can_hold 方法检测当前的 Rectangle 实例能否完整地包含传入的另一个 Rectangle 实例：12345678910111213141516171819202122232425262728293031struct Rectangle &#123; width: u32, height: u32,&#125;impl Rectangle &#123; fn area(&amp;self) -&gt; u32 &#123; self.width * self.height &#125; fn can_hold(&amp;self, other: &amp;Rectangle) -&gt; bool &#123; self.width &gt; other.width &amp;&amp; self.height &gt; other.height &#125;&#125;fn main() &#123; let rect1 = Rectangle &#123; width: 30, height: 50, &#125;; let rect2 = Rectangle &#123; width: 10, height: 40, &#125;; let rect3 = Rectangle &#123; width: 60, height: 45, &#125;; print!("Can rect1 hold rect2? &#123;&#125;", rect1.can_hold(&amp;rect2)); print!("Can rect1 hold rect3? &#123;&#125;", rect1.can_hold(&amp;rect3));&#125; 关联函数除了方法，impl 块还允许我们定义不用接收 self 作为参数的函数。这类函数与结构体（而不是实例）相互关联，因此也被称为关联函数。它们不会作用于某个具体的结构体实例。之前用到的 String::from 就是关联函数的一种。 关联函数常被用作构造器来返回一个结构体的新实例。例如可以编写一个 square 关联函数，只接收一个参数，该参数同时用作宽度与高度来构造正方形实例。 12345impl Rectangle &#123; fn square(size: u32) -&gt; Rectangle &#123; Rectangle &#123; width: size, height: size &#125; &#125;&#125; 这样就可以使用 let sq = Rectangle::square(3); 类似的语法来创建正方形实例。 总结结构体可以让我们基于特定领域的规则创建有意义的自定义类型。通过使用结构体，可以将相互关联的数据组合起来，并为每条数据赋予有含义的名称，从而使代码更加清晰。方法可以让我们为结构体实例指定特殊的行为，而关联函数则可以将那些不需要实例的特定功能放置到结构体的命名空间中。 参考资料The Rust Programming Language]]></content>
      <categories>
        <category>Rust</category>
      </categories>
      <tags>
        <tag>Program</tag>
        <tag>OOP</tag>
        <tag>Class</tag>
        <tag>Function</tag>
        <tag>Type</tag>
        <tag>Development</tag>
        <tag>Rust</tag>
        <tag>Struct</tag>
        <tag>Method</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Vim 配置光标形状和颜色（Windows Terminal、xterm）]]></title>
    <url>%2F2021%2F06%2F08%2Fchange-vim-cursor-shape-and-color-in-xterm-or-windows-terminal%2F</url>
    <content type="text"><![CDATA[Windows Terminal 里的 Vim 设置了浅的配色，刚好光标默认是白色的竖线，不容易看出来。很有点费眼睛。 想把光标改成其他颜色的方块样式。因为不是 gvim，guicursor 之类的配置不起作用。上网查了些资料，测试如下配置可以正常生效（貌似这个 Terminal 是属于 xterm 那一类）：123456789101112131415" Set cursor shape and colorif &amp;term =~ "xterm" " INSERT mode let &amp;t_SI = "\&lt;Esc&gt;[6 q" . "\&lt;Esc&gt;]12;blue\x7" " REPLACE mode let &amp;t_SR = "\&lt;Esc&gt;[3 q" . "\&lt;Esc&gt;]12;black\x7" " NORMAL mode let &amp;t_EI = "\&lt;Esc&gt;[2 q" . "\&lt;Esc&gt;]12;green\x7"endif" 1 -&gt; blinking block 闪烁的方块" 2 -&gt; solid block 不闪烁的方块" 3 -&gt; blinking underscore 闪烁的下划线" 4 -&gt; solid underscore 不闪烁的下划线" 5 -&gt; blinking vertical bar 闪烁的竖线" 6 -&gt; solid vertical bar 不闪烁的竖线 其中各配置项的含义如下： &amp;t_SI 表示插入模式 &amp;t_SR 表示替换模式 &amp;t_EI 表示 Normal 模式 . 号左边的 &quot;\&lt;Esc&gt;[6 q&quot; 用来配置光标的形状。其中 6 的取值可以是 1 - 6，分别指代不同的光标样式（参考前面的注释） . 号右边的 &quot;\&lt;Esc&gt;]12;blue\x7&quot; 用来配置光标颜色，其中的 blue 可以替换为其他颜色名词 设置光标颜色时也可以使用 RGB 颜色，格式为 rgb:RR/GG/BB。比如纯白色的光标即为 &quot;\&lt;Esc&gt;]12;rgb:FF/FF/FF\x7&quot;。 若只想设置光标形状，直接去掉 . 号以及右边的颜色配置部分即可。如 let &amp;t_SR = &quot;\&lt;Esc&gt;[3 q&quot;。同理，只想修改颜色时也可以将 . 号左边的形状配置部分删掉。. 号在这里的作用其实是字符串拼接，方便区分形状配置部分和颜色配置部分而已。去掉 . 号直接将两部分配置写在一个字符串里也是可以的。即 let &amp;t_SR = &quot;\&lt;Esc&gt;[3 q&quot; . &quot;\&lt;Esc&gt;]12;black\x7&quot; 等同于 let &amp;t_SR = &quot;\&lt;Esc&gt;[3 q\&lt;Esc&gt;]12;black\x7&quot; Normal 模式（绿色方块）： 插入模式（蓝色竖线）： 替换模式（黑色下划线）： 参考资料Cursor color in xterm; change accordingly to the syntax in vim]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Tools</tag>
        <tag>Configuration</tag>
        <tag>Tricks</tag>
        <tag>Vim</tag>
        <tag>Xterm</tag>
        <tag>Terminal</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[The Rust programming language 读书笔记——所有权与 Move 机制]]></title>
    <url>%2F2021%2F06%2F07%2Fthe-rust-programming-language-reading-notes-ownership-and-move%2F</url>
    <content type="text"><![CDATA[所有权概念是 Rust 语言的核心功能 Rust 没有垃圾回收（GC）机制 Rust 通过所有权和相关工具保障内存安全 所有语言都需要管理自己在运行时使用的计算机内存空间。使用垃圾回收机制的语言（Java、Python）会在运行时定期检查并回收没有被继续使用的内存；另外一些语言（C、C++）则需要程序员手动地分配和释放内存。 Rust 采用第三种方式：它使用包含特定规则的所有权系统来管理内存。这套规则允许编译器在编译过程中执行检查工作，不会产生任何的运行时开销。 栈与堆栈和堆都是代码在运行时可以使用的内存空间。所有存储在栈中的数据必须拥有一个已知且固定的大小。在编译期无法确定大小的数据只能存放在堆中。 堆空间的管理较为松散。当希望将数据放入堆中时，可以请求特定大小的空间，操作系统会根据请求在堆中找到一块足够大的可用空间，并把指向这块空间地址的指针返回给我们。这个过程称为分配。 由于指针（内存地址）的大小是固定的且可以在编译期确定，因此可以将指针存放在栈中。通过指针指向的地址访问指针所指向的具体数据。 由于多了指针跳转的环节，访问堆上的数据要慢于访问栈上的数据。许多系统编程语言都需要程序员去记录代码中分配的堆空间，最小化堆上的冗余，并及时清理无用数据以避免耗尽内存空间。所有权的概念就是为了将上述问题交给 Rust 处理，减轻程序员的这部分心智负担。 所有权规则 Rust 中的每一个值都有一个对应的变量作为它的拥有者 在同一时间内，值有且只有一个拥有者 当所有者离开自己的作用域时，它拥有的值就会被释放掉 变量作用域作用域是一个对象在程序中有效的范围。 如：1234&#123; // 变量 s 还未声明，因此在这里不可用 let s = "hello"; // 从这里开始变量 s 变得可用 // 执行与 s 相关的操作&#125; // 作用域到这里结束，变量 s 不再可用 变量在进入作用域后变得有效 变量会保持自己的有效性直到离开自己的作用域 字符串字面量（如 let s = &quot;hello&quot;）属于被硬编码进程序的字符串值。很方便，但并不适用于所有场景。一是因为字符串字面量是不可变的，二是因为并不是所有字符串的值都能在编写代码时确定。比如需要获取用户的输入并保存。 Rust 提供了第二种字符串类型 String。String 会在堆上分配存储空间，因此能够处理未知大小的文本。12345let mut s = String::from("hello");s.push_str(", world!"); // push_str() 函数向 String 空间的尾部添加了一段字符串字面量println!("&#123;&#125;", s); // 这里会输出完整的 hello, world! 对于字符串字面量而言，由于在编译时就知道其内容，这部分硬编码的文本被直接嵌入到了可执行文件中。这也是访问字符串字面量异常高效的原因。对于 String 类型而言，为了支持一种可变的、可增长的类型，需要在堆上分配一块在编译时未知大小的内存来存放数据。当使用完 String 时，则需要通过某种方式来将这些内存归还给操作系统。 对于拥有 GC 机制的语言，GC 会替代程序员记录并清理那些不再使用的内存。而对于没有 GC 的语言，识别不再使用的内存并调用代码显式释放的工作就需要程序员来完成。假如忘记释放内存，就会造成内存泄漏；假如过早地释放内存，就会产生一个非法变量；假如重复释放同一块内存，就会产生无法预知的后果。 Rust 提供了另外一套解决方案：内存会在拥有它的变量离开作用域后自动地进行释放。1234&#123; // 变量 s 还未声明，因此在这里不可用 let s = String::from("hello"); // 从这里开始变量 s 变得可用 // 执行与 s 相关的操作&#125; // 作用域到这里结束，变量 s 失效 Rust 会在作用域结束的地方（即 } 处）自动回收分配给变量 s 的内存。 内存与分配对于整数类型的数据：12let x = 5;let y = x; 上述代码将整数值 5 绑定给变量 x，再创建一个 x 值的拷贝，绑定给变量 y。由于整数是已知固定大小的简单值，两个值 5 会同时被推入栈中。 对于 String 类型的数据：12let s1 = String::from("hello");let s2 = s1; 类似的代码，运行方式却并不一致。 String 的内存布局如下图： 对于绑定给变量 s1 的 String 来说，该字符串的文本内容（hello）保存在了堆上，同时在栈中保存着一个指向字符串内容的指针、一个长度和一个容量信息。 当将 s1 赋值给 s2 时，便复制了一次 String 的数据。这意味着我们复制了它存储在栈上的指针、长度和容量字段，而指针指向的堆上的数据并没有被复制。 变量 s1 和 s2 的内存布局如下图： 前面提到过，当一个变量离开当前的作用域时，Rust 会自动将变量使用的堆内存释放和回收。但若是有两个指针指向了同一个地址，就会导致如 s2 和 s1 离开自己的作用域时，Rust 会尝试重复释放相同的内存，进而有可能导致正在使用的数据发生损坏。为了确保内存安全，同时也避免复制分配的内存，Rust 在上述场景下会简单的将 s1 废弃。因此也就不需要在 s1 离开作用域后清理任何东西。这一行为即为 Move。 试图在 s2 创建完毕后访问 s1（如下所示）会导致编译错误。12345fn main() &#123; let s1 = String::from("hello"); let s2 = s1; // 变量 s1 在这里被废弃 print!("&#123;&#125;, world", s1); // 错误&#125; Rust 会报出 borrow of moved value: s1 错误。 Rust 永远不会自动创建数据的深度拷贝。 对于栈上数据的复制，比如：1234let x = 5;let y = x;println!("x = &#123;&#125;, y = &#123;&#125;", x, y); 上面的代码是完全合法的。因为整型的数据可以在编译时确定自己的大小，能够将数据完整地存储在栈中。对于这些类型而言，深度拷贝与浅度拷贝没有任何区别。 所有权与函数将值传递给函数在语义上类似于对变量进行赋值。将变量传递给函数将会触发移动或复制。 123456789101112131415161718fn main() &#123; let s = String::from("hello"); //变量 s 进入作用域 takes_ownership(s); // s 的值被移动进了函数 // 变量 s 从这里开始不再有效 let x = 5; // 变量 x 进入作用域 makes_copy(x); // 变量 x 被传递进了函数 // 但 i32 类型不受 Move 机制影响，因此这里 x 依旧可用&#125;fn takes_ownership(some_string: String) &#123; // some_string 进入作用域 print!("&#123;&#125;", some_string);&#125; // some_string 离开作用域，占用的内存被释放fn makes_copy(some_integer: i32) &#123; print!("&#123;&#125;", some_integer);&#125; // some_integer 离开作用域，没有特别的事情发生 在上述代码中，尝试在调用 takes_ownership 后使用变量 s 会导致编译错误。 函数在返回值的过程中也会发生所有权的转移。123456789101112131415fn main() &#123; let s1 = gives_ownership(); // gives_ownership 将它的返回值移动至变量 s1 中 let s2 = String::from("hello"); // 变量 s2 进入作用域 let s3 = takes_and_gives_back(s2); // s2 被移动进函数 takes_and_gives_back，而这个函数的返回值又被移动到了变量 s3 上&#125; // s3 和 s1 在这里离开作用域并被销毁，而 s2 已经移动了，因此不会发生任何事情fn gives_ownership() -&gt; String &#123; let some_string = String::from("hello"); // some_string 进入作用域 some_string // some_string 作为返回值移动至调用方&#125;// takes_and_gives_back 将取得一个 String 的所有权并将它作为结果返回fn takes_and_gives_back(a_string: String) -&gt; String &#123; a_string // a_string 作为返回值移动至调用方&#125; 变量的所有权转移总是遵循相同的模式：将一个值赋值给另一个变量时就会转移所有权。当一个持有堆数据的变量离开作用域时，它的数据就会被清理回收，除非这些数据的所有权被移动到了另一个变量上。 引用与借用参考如下示例代码：12345678910fn main() &#123; let s1 = String::from("hello"); let (s2, len) = calculate_length(s1); print!("The length of '&#123;&#125;' is &#123;&#125;", s2, len);&#125;fn calculate_length(s: String) -&gt; (String, usize) &#123; let length = s.len(); (s, length)&#125; 由于调用 caculate_length 会导致 String 移动到函数体内部，我们又需要在调用后继续使用该 String，因此不得不通过元组将 String 作为元素继续返回。 这种写法未免过于笨拙。在下面的代码中，新的 calculate_length 函数使用了 String 的引用作为参数而不会直接转移值的所有权。 123456789fn main() &#123; let s1 = String::from("hello"); let len = calculate_length(&amp;s1); print!("The length of '&#123;&#125;' is &#123;&#125;", s1, len);&#125;fn calculate_length(s: &amp;String) -&gt; usize &#123; s.len()&#125; 在新的代码中，调用 calculate_length 函数时使用了 &amp;s1 作为参数，且在该函数的定义中使用 &amp;String 替代了 String。&amp; 代表引用，允许在不获取所有权的情况下使用值。 &amp;s1 语法允许在不转移所有权的前提下创建一个指向 s1 值的引用。由于引用不持有值的所有权，当引用离开当前作用域时，它指向的值也不会被丢弃。当一个函数使用引用而不是值本身作为参数时，我们就不需要为了归还所有权而特意去返回值。毕竟引用根本没有取得所有权。 这种通过引用传递参数给函数的方法也称作借用。 可变引用与变量类似，引用默认是不可变的。Rust 不允许修改引用指向的值（除非声明为 mut）。 123456789fn main() &#123; let mut s = String::from("hello"); change(&amp;mut s); print!("&#123;&#125;", s)&#125;fn change(some_string: &amp;mut String) &#123; some_string.push_str(", world");&#125; 对于特定作用域中的特定数据，一次只能声明一个可变引用。比如：123456fn main() &#123; let mut s = String::from("hello"); let r1 = &amp;mut s; let r2 = &amp;mut s; println!("&#123;&#125;", r1)&#125; 就会出现 cannot borrow s as mutable more than once at a time 编译错误。这个规则使得引用的可变性只能以一种受到严格限制的方式使用。但另一方面，遵循这条限制性规则可以在编译时避免数据竞争。即不允许两个或两个以上的指针同时访问（且至少有一个指针会写入数据）同一空间。数据竞争会导致未定义的行为，往往难以在运行时进行跟踪，也就使得出现的 bug 更加难以被诊断和修复。 不能在拥有不可变引用的同时创建可变引用。编译时会报出 cannot borrow s as immutable because it is also borrowed as mutable 错误。1234567fn main() &#123; let mut s = String::from("hello"); let r1 = &amp;s; // 没问题 let r2 = &amp;s; // 没问题 let r3 = &amp;mut s; // 错误 println!("&#123;&#125;", r2)&#125; 不能在拥有不可变引用的同时创建可变引用，但可以同时存在多个不可变引用。因为对数据的只读操作不会影响到其他读取数据的用户。 Rust 编译器可以为用户提早（编译时而不是运行时）暴露那些潜在的 bug，并且明确指出出现问题的地方。用户就不再需要去追踪调试为何数据会在运行时发生了非预期的变化。 参考资料The Rust Programming Language]]></content>
      <categories>
        <category>Rust</category>
      </categories>
      <tags>
        <tag>Program</tag>
        <tag>Reference</tag>
        <tag>Rust</tag>
        <tag>Ownership</tag>
        <tag>Move</tag>
        <tag>Pointer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[The Rust programming language 读书笔记——通用编程概念]]></title>
    <url>%2F2021%2F06%2F04%2Fthe-rust-programming-language-reading-notes-common-concepts%2F</url>
    <content type="text"><![CDATA[一、变量Rust 中的变量默认是不可变的。 可以通过如下代码测试变量的不可变性： 使用 cargo new variables 命令创建新的 Rust 项目 进入新创建的 variables 目录，编辑 src/main.rs 源代码文件 123456fn main() &#123; let x = 10; println!("The value is &#123;&#125;", x); x = 20; println!("The value is &#123;&#125;", x);&#125; 运行 cargo run 命令编译并执行 Rust 程序，出现如下报错：1error[E0384]: cannot assign twice to immutable variable `x` 即 Rust 编译器不允许程序代码对不可变变量进行二次赋值。 可以通过 let mut 关键字声明可变变量。123456fn main() &#123; let mut x = 10; println!("The value is &#123;&#125;", x); x = 20; println!("The value is &#123;&#125;", x);&#125; 12345$ cargo run Finished dev [unoptimized + debuginfo] target(s) in 0.29s Running `target/debug/variables`The value is 10The value is 20 PS：在使用重型数据结构时，适当地使用可变性去修改一个实例，可能比重新返回一个新分配的实例更有效率；而数据结构更为轻量时，采用偏函数式的风格创建新变量来进行赋值，可能会使代码更易于理解。 常量 使用 const 而不是 let 关键字声明常量 声明常量时必须显式地标注值的类型 常量可以被声明在任何作用域中。在一个值需要被不同部分的代码共同引用时很有用处 无法将一个函数的返回值或其他需要在运行时计算的值绑定到常量上 const PI: f32 = 3.1415; ShadowShadow 的意思是，一个新声明的变量可以覆盖掉旧的同名变量。 123456fn main() &#123; let x = 5; let x = x + 1; let x = x * 2; println!("The value is &#123;&#125;", x);&#125; 123 Finished dev [unoptimized + debuginfo] target(s) in 0.00s Running `target/debug/variables`The value is 12 Shadow 机制允许在复用变量名称的同时改变变量的类型。比如下面的代码就是合法的：12let spaces = " ";let sapces = spaces.len(); 通过复用 spaces 这个名字，就不需要再声明诸如 spaces_str 和 spaces_num 之类的变量。 但如果使用 mut 关键字来模拟上述效果就会报错。12let mut spaces = " ";spaces = spaces.len(); 因为编译器拒绝修改变量的类型，即便该变量是可变的。Shadow 的机制在于使用 let 关键字重新声明了变量。 二、数据类型Rust 是一门静态类型语言，在编译过程中需要知道所有变量的具体类型。 大部分情况下，编译器可以自动推导出变量类型。但比如需要将 String 类型转换为数值类型时，就必须显式地添加类型标注：let guess: u32 = &quot;42&quot;.parse().expect(&quot;Not a number&quot;); 标量（Scalar）类型标量类型是单个值类型的统称。Rust 内置 4 种基础的标量类型：整数、浮点数、布尔值和字符。 整数类型 长度 有符号 无符号 8 bit i8 u8 16 bit i16 u16 32 bit i32 u32 64 bit i64 u64 有无符号代表了一个整数类型是否包含负数。即有符号的整数总是需要一个 bit 表示当前数值是否为正。对于一个 n bit 的有符号整数，其取值范围是 - 2 ^ (n - 1) 到 2 ^ (n - 1) - 1；长度为 n bit 的无符号整数，其取值范围则是 0 到 2 ^ n - 1。 浮点数类型包含 f32 和 f64 两种类型。Rust 默认会将未标注类型的浮点数推导为 f64。整数会推导为 i32。 字符类型Rust 中，char 类型使用单引号指定，字符串类型使用双引号指定。 char 类型占用 4 个字节，是一个 Unicode 标量值。let smile = &#39;😀&#39;; 复合类型复合类型可以将多个不同类型的值组合成一个类型。Rust 内置两种复合类型，元组和数组。 元组 元组每个位置上的值都有一个类型 元组拥有固定的长度。无法在声明结束后增加或减少其中的元素。 123fn main() &#123; let tup: (i32, f64, u8) = (500, 6.4, 1);&#125; 为了从元组中获取单个值，可以使用模式匹配来解构（拆包）元组。12345fn main() &#123; let tup = (500, 6.4, 1); let (x, y, z) = tup; print!("The value of y is: &#123;&#125;", y);&#125; 还可以通过索引使用点号访问元组中的元素。123456fn main() &#123; let x: (i32, f64, u8) = (500, 6.4, 1); let five_hundred = x.0; let six_point_four = x.1; let one = x.2;&#125; 数组 数组中的所有元素都必须是相同的类型 数组拥有固定的长度，一旦声明就无法更改大小 12345678910111213141516fn main() &#123; let months = [ "January", "February", "March", "April", "May", "June", "July", "August", "September", "October", "November", "December", ];&#125; 数组的类型标注：let a: [i32; 5] = [1, 2, 3, 4, 5]; 初始化含有相同元素的数组：let a = [3; 5];等价于 let a = [3, 3, 3, 3, 3]; 数组由一整块分配在栈上的内存组成，可以通过索引访问数组中的元素。123456fn main() &#123; let a = [1, 2, 3, 4, 5]; let first = a[0]; let second = a[1];&#125; 三、函数必须显式地声明每个参数（如果有）的数据类型。12345678fn main() &#123; another_function(5, 6);&#125;fn another_function(x: i32, y: i32) &#123; print!("The value of x is: &#123;&#125;", x); print!("The value of y is: &#123;&#125;", y);&#125; Rust 是一门基于表达式的语言。它将语句（statement）与表达式（expression）区分为两个不同的概念。 如使用 let 关键字创建变量并绑定值时的指令就是一条语句：let y = 6;语句不会返回值。 在 C 或 Ruby 语言里，赋值语句会返回所赋的值。因此可以使用 x = y =6 这样的语句，但 Rust 不支持这样的语法。 表达式会计算出某个值作为结果返回。如 5 + 6 就是表达式（返回 11），let y = 6 中的数字 6 也是表达式（返回 6 本身）。表达式本身可以作为语句的一部分。用来创建新作用域的大括号也是表达式。 123456789fn main() &#123; let x = 5; let y = &#123; let x = 3; x + 1 &#125;; println!("The value of y is: &#123;&#125;", y);&#125; 在上述代码中，let y = 后面大括号部分的内容就是一个表达式，它会将计算出的结果 4 作为返回值。该返回值接着通过赋值语句绑定给变量 y。结尾处的 x + 1 并没有添加分号。若加上分号，则这段代码就变成了语句而不会返回任何值。 函数可以向调用它的代码返回值。但需要在箭头后面声明值的类型。可以使用 return 关键字指定一个值提前从函数中返回，但大多数函数都隐式地返回了最后的表达式。12345678fn main() &#123; let x = plus_one(5); print!("The value of x is: &#123;&#125;", x);&#125;fn plus_one(x: i32) -&gt; i32 &#123; x + 1&#125; 上述代码会输出 6。但如果在 plus_one 函数末尾的 x + 1 处加上分号，该表达式就会变成语句（不返回任何值），最终在编译时报出 mismatched types 错误。原因是 plus_one 的声明中指定返回值类型为 i32，但由于语句不返回任何值，Rust 默认返回了一个空元组（()），导致实际的返回值类型与函数定义产生了冲突。 四、控制流if 表达式if 表达式必须产生一个 bool 类型的值，否则会触发编译错误。123456fn main() &#123; let number = 3; if number &#123; print!("number was three"); &#125;&#125; 在上面的代码中，if 表达式的计算结果为 3，而 Rust 期望获得一个 bool 值，因此编译时会爆出 mismatched types 错误。Rust 不会自动尝试将非布尔值转换为布尔类型。 if 是一个表达式。可以在 let 语句右侧使用 if 表达式来完成赋值。123456789fn main() &#123; let condition = true; let number = if condition &#123; 5 &#125; else &#123; 6 &#125;; print!("The value of number is &#123;&#125;", number);&#125; 整个 if 表达式的值取决于具体哪一个代码块得到了执行。因此，所有 if 分支可能返回的值都必须是同一种类型的。否则会触发编译错误。123456789fn main() &#123; let condition = true; let number = if condition &#123; 5 &#125; else &#123; "six" // 错误，类型不匹配 &#125;; print!("The value of number is &#123;&#125;", number);&#125; 循环Rust 提供了 3 种循环：loop、while、for。 从 loop 循环中返回值123456789101112fn main() &#123; let mut counter = 0; let result = loop &#123; counter += 1; if counter == 10 &#123; break counter * 2; &#125; &#125;; print!("The result is &#123;&#125;", result);&#125; break 关键字中断循环并返回 counter * 2。 while 循环1234567891011fn main() &#123; let mut number = 3; while number != 0 &#123; println!("&#123;&#125;!", number); number = number - 1; &#125; println!("LIFTOFF!!!");&#125; for 循环遍历集合中的元素1234567fn main() &#123; let a = [10, 20, 30, 40, 50]; for element in a.iter() &#123; println!("the value is: &#123;&#125;", element); &#125;&#125; for 循环重构前面使用 while 循环的代码123456fn main() &#123; for number in (1..4).rev() &#123; println!("&#123;&#125;!", number); &#125; println!("LIFTOFF!!!");&#125; 参考资料The Rust Programming Language]]></content>
      <categories>
        <category>Rust</category>
      </categories>
      <tags>
        <tag>Function</tag>
        <tag>Development</tag>
        <tag>Programming</tag>
        <tag>Basic</tag>
        <tag>Rust</tag>
        <tag>Variable</tag>
        <tag>Loop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Vim 8.1 懒人配置手册（包含 C/C++、Python、Rust 自动补全，基于 coc.nvim）]]></title>
    <url>%2F2021%2F05%2F30%2Fvim-configuration-with-coc-support-rust-c-python-complete%2F</url>
    <content type="text"><![CDATA[先说点废话。 最近装了 WSL2，想着作为 Win10 内置的 Linux 系统，应该比一般的虚拟机要来得平滑一些。毕竟虚拟机还要装 Virtualbox，每次开机还要多点几下，还要等它启动。怕麻烦。。。 可能习惯问题，喜欢在 Linux 下配置开发环境（学习写代码）。总感觉 Windows 上略显磨叽了一些。也许我道行不够。碰到需要编译的东西，一套工具装起来过于繁琐，直接 sudo apt install gcc 不香吗。 可惜 WSL 没有桌面支持，装不上 Vscode。对于工具的选择，我的原则是简单够用就好，Vscode 即是目前我的最爱（非重度编码）。开箱即用，功能满足基本需求。无非代码高亮、自动补全、查看定义、语法检查、代码格式化，再加个跳转。 于是决定在 Windows 系统上安装 Vscode，借助 Remote WSL 插件“远程”编码。可惜有 BUG，一直不能正常工作。Github 上提了 issue，没人理。无奈转向 Vim。 一开始用的补全插件是 YouCompleteMe，还要编译。虽然敲个命令就能自动执行，但光 Rust 支持就在本地给我搞了将近 1G 的文件，不太能忍。网上查了下，貌似已经很老的机制了。于是转向较新一点的 coc.nvim。据说支持各种 LSP（这个缩写。。。），也不懂，能用就行。 废话结束。 一、效果截图 二、NERDTree 文件浏览器vim 用的自带的 8.1 版本，有内置的插件管理。一般的插件安装流程是直接将插件源代码 clone 到 ~/.vim/pack/vendor/start 路径下（没有就创建），再在 vim 中运行 :helptags ~/.vim/pack/vendor/start/someplugin/doc 命令生成帮助文档（这一步可选）。 vim 会自动检测已经下载的插件。vendor 路径下可以有 start 和 opt 两个目录，start 路径下的插件会在 vim 启动时自动加载，opt 路径下的插件则需要通过 :packadd 命令手动加载。 vendor 也可以是其他名称，同位置下也可以有多个相同结构的目录，方便对不同类型的插件分别进行管理。省事起见，这里所有的插件都放在 ~/.vim/pack/vendor/start 下面。 NERDTree 文件浏览器插件安装：12cd ~/.vim/pack/vendor/startgit clone https://github.com/scrooloose/nerdtree 在 ~/.vimrc 文件中添加配置：12345678" 进入 vim 时自动开启 NERDTreeautocmd VimEnter * NERDTree | wincmd p" 若关闭某个 buff 后 NERDTree 是仅剩的最后一个 buff，则自动关闭 NERDTreeautocmd BufEnter * if tabpagenr('$') == 1 &amp;&amp; winnr('$') == 1 &amp;&amp; exists('b:NERDTree') &amp;&amp; b:NERDTree.isTabTree() | quit | endif" 使用 Ctrl+n 快捷键打开或关闭 NERDTreennoremap &lt;C-n&gt; :NERDTreeToggle&lt;CR&gt; 更多配置选项参考 VimAwesome 三、vim-airline 与配色安装 vim-airline 状态栏美化插件：git clone https://github.com/vim-airline/vim-airline 关于配色，萝卜青菜各有所爱。懒得去一个一个试。准备了两套，vim-one 深色和 gruvbox 浅色。 vim-one 插件安装：git clone https://github.com/rakr/vim-one 添加配置：12345" 深色背景set bg=dark" 启用 one 配色colorscheme one 效果截图： 安装 gruvbox 插件：git clone https://github.com/morhetz/gruvbox 修改配置文件（注释掉 colorscheme one）：123set bg=light" colorscheme oneautocmd vimenter * ++nested colorscheme gruvbox 效果截图： 更复杂的配置可自行在 VimAwesome 搜索对应的插件，或者进入相应的 Github 主页查看。 四、coco.nvim 代码补全与语法检查安装 nodejs &gt;= 10.12，官网上写的是通过 curl -sL install-node.now.sh/lts | bash 命令安装。我个人建议使用 nvm 安装最新的 lts 版本。此处不赘述。 安装 coc.nvim 插件：git clone https://github.com/neoclide/coc.nvim.git 为了得到某种编程语言的补全功能，还需要安装对应语言的 coc 扩展以及代码补全后端（LSP）。比如 C/C++ 对应的 coc 扩展为 coc-clangd，LSP 为 clangd。两个都需要。 安装 coc 扩展的方法非常简单，进入 vim 后运行 :CocInstall extension_name 命令即可。比如使用 :CocInstall coc-clangd 命令安装 coc-clangd 扩展。coc 扩展我遇到的都比较小，安装非常迅速，也会单独开一个窗口显示进度信息。 coc 扩展安装完成后，打开对应的源代码文件，比如 vim test.c，vim 就会自动在本地环境中寻找对应的 LSP（C/C++ 语言是 clangd）。 若 clangd 此时并未安装，vim 就会提示你运行某个命令（在 vim 内部）自动安装该依赖。这里有个坑。不知道是不是网络的问题，我复制运行了 vim 提供的命令，一直显示下载中，几个小时不见下载完成。。。 好在可以手动安装 clangd，退出 vim 直接运行 sudo apt install clangd 即可。此时 coc.nvim 对于 C/C++ 的补全支持即安装配置完成。 Rust 和 Python 语言支持对于 Rust 语言，需要先安装 coc-rust-analyzer 扩展：:CocInstall coc-rust-analyzer这一步简单迅速。 安装 rust-src：rustup component add rust-src 接着还必须安装针对 Rust 的 LSP（rust-analyzer）。鉴于安装 clangd 时出现的曲折，我决定手动安装 rust-analyzer。诡异的事情发生了。手动安装的可执行程序不被 coc 识别。无奈下尝试 vim 中的自动安装居然成功了。。。 方法是用 vim 新建任意一个 rust 源文件（vim test.rs），vim 会自动弹出提示，找不到 rust-analyzer，是否自动安装，选择 Yes 即可。这里的安装过程居然异乎寻常的快。安装完成后可能不会立即生效，会尝试创建索引。多打开几个文件试试。 至于 Python，安装 coc-pyright：:CocInstall coc-pyright 印象中并没有做其他操作，对于 Python 的支持就自动生效了，也许是安装扩展的时候自动安装了对应的 LSP。 对于其他语言的支持，可参考 Using coc extensions。 coc.nvim 示例配置（从官方 Github 上 copy 的，主要是一些快捷键的映射，可根据需求删减。没细看）：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165" Set internal encoding of vim, not needed on neovim, since coc.nvim using some" unicode characters in the file autoload/float.vimset encoding=utf-8" TextEdit might fail if hidden is not set.set hidden" Some servers have issues with backup files, see #649.set nobackupset nowritebackup" Give more space for displaying messages.set cmdheight=2" Having longer updatetime (default is 4000 ms = 4 s) leads to noticeable" delays and poor user experience.set updatetime=300" Don't pass messages to |ins-completion-menu|.set shortmess+=c" Always show the signcolumn, otherwise it would shift the text each time" diagnostics appear/become resolved.if has("nvim-0.5.0") || has("patch-8.1.1564") " Recently vim can merge signcolumn and number column into one set signcolumn=numberelse set signcolumn=yesendif" Use tab for trigger completion with characters ahead and navigate." NOTE: Use command ':verbose imap &lt;tab&gt;' to make sure tab is not mapped by" other plugin before putting this into your config.inoremap &lt;silent&gt;&lt;expr&gt; &lt;TAB&gt; \ pumvisible() ? "\&lt;C-n&gt;" : \ &lt;SID&gt;check_back_space() ? "\&lt;TAB&gt;" : \ coc#refresh()inoremap &lt;expr&gt;&lt;S-TAB&gt; pumvisible() ? "\&lt;C-p&gt;" : "\&lt;C-h&gt;"function! s:check_back_space() abort let col = col('.') - 1 return !col || getline('.')[col - 1] =~# '\s'endfunction" Use &lt;c-space&gt; to trigger completion.if has('nvim') inoremap &lt;silent&gt;&lt;expr&gt; &lt;c-space&gt; coc#refresh()else inoremap &lt;silent&gt;&lt;expr&gt; &lt;c-@&gt; coc#refresh()endif" Make &lt;CR&gt; auto-select the first completion item and notify coc.nvim to" format on enter, &lt;cr&gt; could be remapped by other vim plugininoremap &lt;silent&gt;&lt;expr&gt; &lt;cr&gt; pumvisible() ? coc#_select_confirm() \: "\&lt;C-g&gt;u\&lt;CR&gt;\&lt;c-r&gt;=coc#on_enter()\&lt;CR&gt;"" Use `[g` and `]g` to navigate diagnostics" Use `:CocDiagnostics` to get all diagnostics of current buffer in location list.nmap &lt;silent&gt; [g &lt;Plug&gt;(coc-diagnostic-prev)nmap &lt;silent&gt; ]g &lt;Plug&gt;(coc-diagnostic-next)" GoTo code navigation.nmap &lt;silent&gt; gd &lt;Plug&gt;(coc-definition)nmap &lt;silent&gt; gy &lt;Plug&gt;(coc-type-definition)nmap &lt;silent&gt; gi &lt;Plug&gt;(coc-implementation)nmap &lt;silent&gt; gr &lt;Plug&gt;(coc-references)" Use K to show documentation in preview window.nnoremap &lt;silent&gt; K :call &lt;SID&gt;show_documentation()&lt;CR&gt;function! s:show_documentation() if (index(['vim','help'], &amp;filetype) &gt;= 0) execute 'h '.expand('&lt;cword&gt;') elseif (coc#rpc#ready()) call CocActionAsync('doHover') else execute '!' . &amp;keywordprg . " " . expand('&lt;cword&gt;') endifendfunction" Highlight the symbol and its references when holding the cursor.autocmd CursorHold * silent call CocActionAsync('highlight')" Symbol renaming.nmap &lt;leader&gt;rn &lt;Plug&gt;(coc-rename)" Formatting selected code.xmap &lt;leader&gt;f &lt;Plug&gt;(coc-format-selected)nmap &lt;leader&gt;f &lt;Plug&gt;(coc-format-selected)augroup mygroup autocmd! " Setup formatexpr specified filetype(s). autocmd FileType typescript,json setl formatexpr=CocAction('formatSelected') " Update signature help on jump placeholder. autocmd User CocJumpPlaceholder call CocActionAsync('showSignatureHelp')augroup end" Applying codeAction to the selected region." Example: `&lt;leader&gt;aap` for current paragraphxmap &lt;leader&gt;a &lt;Plug&gt;(coc-codeaction-selected)nmap &lt;leader&gt;a &lt;Plug&gt;(coc-codeaction-selected)" Remap keys for applying codeAction to the current buffer.nmap &lt;leader&gt;ac &lt;Plug&gt;(coc-codeaction)" Apply AutoFix to problem on the current line.nmap &lt;leader&gt;qf &lt;Plug&gt;(coc-fix-current)" Map function and class text objects" NOTE: Requires 'textDocument.documentSymbol' support from the language server.xmap if &lt;Plug&gt;(coc-funcobj-i)omap if &lt;Plug&gt;(coc-funcobj-i)xmap af &lt;Plug&gt;(coc-funcobj-a)omap af &lt;Plug&gt;(coc-funcobj-a)xmap ic &lt;Plug&gt;(coc-classobj-i)omap ic &lt;Plug&gt;(coc-classobj-i)xmap ac &lt;Plug&gt;(coc-classobj-a)omap ac &lt;Plug&gt;(coc-classobj-a)" Remap &lt;C-f&gt; and &lt;C-b&gt; for scroll float windows/popups.if has('nvim-0.4.0') || has('patch-8.2.0750') nnoremap &lt;silent&gt;&lt;nowait&gt;&lt;expr&gt; &lt;C-f&gt; coc#float#has_scroll() ? coc#float#scroll(1) : "\&lt;C-f&gt;" nnoremap &lt;silent&gt;&lt;nowait&gt;&lt;expr&gt; &lt;C-b&gt; coc#float#has_scroll() ? coc#float#scroll(0) : "\&lt;C-b&gt;" inoremap &lt;silent&gt;&lt;nowait&gt;&lt;expr&gt; &lt;C-f&gt; coc#float#has_scroll() ? "\&lt;c-r&gt;=coc#float#scroll(1)\&lt;cr&gt;" : "\&lt;Right&gt;" inoremap &lt;silent&gt;&lt;nowait&gt;&lt;expr&gt; &lt;C-b&gt; coc#float#has_scroll() ? "\&lt;c-r&gt;=coc#float#scroll(0)\&lt;cr&gt;" : "\&lt;Left&gt;" vnoremap &lt;silent&gt;&lt;nowait&gt;&lt;expr&gt; &lt;C-f&gt; coc#float#has_scroll() ? coc#float#scroll(1) : "\&lt;C-f&gt;" vnoremap &lt;silent&gt;&lt;nowait&gt;&lt;expr&gt; &lt;C-b&gt; coc#float#has_scroll() ? coc#float#scroll(0) : "\&lt;C-b&gt;"endif" Use CTRL-S for selections ranges." Requires 'textDocument/selectionRange' support of language server.nmap &lt;silent&gt; &lt;C-s&gt; &lt;Plug&gt;(coc-range-select)xmap &lt;silent&gt; &lt;C-s&gt; &lt;Plug&gt;(coc-range-select)" Add `:Format` command to format current buffer.command! -nargs=0 Format :call CocAction('format')" Add `:Fold` command to fold current buffer.command! -nargs=? Fold :call CocAction('fold', &lt;f-args&gt;)" Add `:OR` command for organize imports of the current buffer.command! -nargs=0 OR :call CocAction('runCommand', 'editor.action.organizeImport')" Add (Neo)Vim's native statusline support." NOTE: Please see `:h coc-status` for integrations with external plugins that" provide custom statusline: lightline.vim, vim-airline.set statusline^=%&#123;coc#status()&#125;%&#123;get(b:,'coc_current_function','')&#125;" Mappings for CoCList" Show all diagnostics.nnoremap &lt;silent&gt;&lt;nowait&gt; &lt;space&gt;a :&lt;C-u&gt;CocList diagnostics&lt;cr&gt;" Manage extensions.nnoremap &lt;silent&gt;&lt;nowait&gt; &lt;space&gt;e :&lt;C-u&gt;CocList extensions&lt;cr&gt;" Show commands.nnoremap &lt;silent&gt;&lt;nowait&gt; &lt;space&gt;c :&lt;C-u&gt;CocList commands&lt;cr&gt;" Find symbol of current document.nnoremap &lt;silent&gt;&lt;nowait&gt; &lt;space&gt;o :&lt;C-u&gt;CocList outline&lt;cr&gt;" Search workspace symbols.nnoremap &lt;silent&gt;&lt;nowait&gt; &lt;space&gt;s :&lt;C-u&gt;CocList -I symbols&lt;cr&gt;" Do default action for next item.nnoremap &lt;silent&gt;&lt;nowait&gt; &lt;space&gt;j :&lt;C-u&gt;CocNext&lt;CR&gt;" Do default action for previous item.nnoremap &lt;silent&gt;&lt;nowait&gt; &lt;space&gt;k :&lt;C-u&gt;CocPrev&lt;CR&gt;" Resume latest coc list.nnoremap &lt;silent&gt;&lt;nowait&gt; &lt;space&gt;p :&lt;C-u&gt;CocListResume&lt;CR&gt; 五、彩蛋smile在 vim 中运行 :smile 效果： 123456789101112131415161718192021222324252627:smile oooo$$$$$$$$$$$$oooo oo$$$$$$$$$$$$$$$$$$$$$$$$o oo$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$o o$ $$ o$ o $ oo o$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$o $$ $$ $$o$ oo $ $ &quot;$ o$$$$$$$$$ $$$$$$$$$$$$$ $$$$$$$$$o $$$o$$o$ &quot;$$$$$$o$ o$$$$$$$$$ $$$$$$$$$$$ $$$$$$$$$$o $$$$$$$$ $$$$$$$ $$$$$$$$$$$ $$$$$$$$$$$ $$$$$$$$$$$$$$$$$$$$$$$ $$$$$$$$$$$$$$$$$$$$$$$ $$$$$$$$$$$$$ $$$$$$$$$$$$$$ &quot;&quot;&quot;$$$ &quot;$$$&quot;&quot;&quot;&quot;$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$ &quot;$$$ $$$ o$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$ &quot;$$$o o$$&quot; $$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$ $$$o $$$ $$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$&quot; &quot;$$$$$$ooooo$$$$o o$$$oooo$$$$$ $$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$ o$$$$$$$$$$$$$$$$$ $$$$$$$$&quot;$$$$ $$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$ $$$$&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot; &quot;&quot;&quot;&quot; $$$$ &quot;$$$$$$$$$$$$$$$$$$$$$$$$$$$$&quot; o$$$ &quot;$$$o &quot;&quot;&quot;$$$$$$$$$$$$$$$$$$&quot;$$&quot; $$$ $$$o &quot;$$&quot;&quot;$$$$$$&quot;&quot;&quot;&quot; o$$$ $$$$o o$$$&quot; &quot;$$$$o o$$$$$$o&quot;$$$$o o$$$$ &quot;$$$$$oo &quot;&quot;$$$$o$$$$$o o$$$$&quot;&quot; &quot;&quot;$$$$$oooo &quot;$$$o$$$$$$$$$&quot;&quot;&quot; &quot;&quot;$$$$$$$oo $$$$$$$$$$ &quot;&quot;&quot;&quot;$$$$$$$$$$$ $$$$$$$$$$$$ $$$$$$$$$$&quot; &quot;$$$&quot;&quot;&quot;&quot; Kill Sheep 小游戏针对 vim 8.2 版本，可在 Windows 系统中安装 gvim 8.2。 进入 C:\Users\xxx\vimfiles\pack\vendor\start 路径下（没有就创建），clone 源代码：git clone https://github.com/vim/killersheep.git 打开 gvim，最大化，运行 :KillKillKill 命令即可进入游戏。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Tools</tag>
        <tag>Configuration</tag>
        <tag>Vim</tag>
        <tag>Editor</tag>
        <tag>IDE</tag>
        <tag>Rust</tag>
        <tag>Completer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Windows Terminal 美化（wsl2、zsh、天气、数字时钟、ASCII Logo、彩色动画）]]></title>
    <url>%2F2021%2F05%2F26%2Fwindows-terminal-beautify-wsl2-weather-digital-clock-splash-animation%2F</url>
    <content type="text"><![CDATA[上次用 WSL（Windows Subsystem for Linux）要追溯到好几年前了。当时 WSL 刚出来不久，抱着尝鲜的态度试着装了，想着万一能用，就不用装虚拟机了。 结果遇到了 BUG，就再也没用过。最近才听说 WSL2 已经有了，类似虚拟机的机制，好用很多。再次装来试试，目前还没遇到啥问题。记录下初始的美化步骤。 一、效果截图天气、日历、数字时钟、Linux 发行版的 logo、系统信息 123456789101112131415161718192021222324252627282930Weather report: Hangzhou May 2021 Su Mo Tu We Th Fr Sa ┌────────────────────────────┐ _`/&quot;&quot;.-. Rain shower, mist 1 │ ┏━┓┏━┓ ┏━┓┏━┓ ┏━┓┏┳┓ │ ,\_( ). 21 °C 2 3 4 5 6 7 8 │ ┃┃┃┣━┓╹┃┃┃┣━┓ ┣━┛┃┃┃ │ /(___(__) ↓ 15 km/h 9 10 11 12 13 14 15 │ ┗━┛┗━┛╹┗━┛┗━┛ ╹ ╹ ╹ │ ‘ ‘ ‘ ‘ 3 km 16 17 18 19 20 21 22 └────────────────────────────┘ ‘ ‘ ‘ ‘ 0.8 mm 23 24 25 26 27 28 29 30 31 _-`````-, ,- &apos;- . starky@xxxxxx .&apos; .- - | | - -. `. ---------------- /.&apos; / `. \ OS: Ubuntu 20.04.2 LTS on Windows 10 x86_64:/ : _... ..._ `` : Kernel: 5.4.72-microsoft-standard-WSL2:: : /._ .`:&apos;_.._\. || : Uptime: 38 mins:: `._ ./ ,` : \ . _.&apos;&apos; . Packages: 736 (dpkg)`:. / | -. \-. \_ / Shell: zsh 5.8 \:._ _/ .&apos; .@) \@) ` `\ ,.&apos; Terminal: /dev/pts/0 _/,--&apos; .- .\,-.`--`. CPU: Intel i7-10850H (12) @ 2.712GHz ,&apos;/&apos;&apos; (( \ ` ) Memory: 118MiB / 12466MiB (0%) /&apos;/&apos; \ `-&apos; ( CPU Usage: 1% &apos;/&apos;&apos; `._,-----&apos; Disk (/): 3.3G / 251G (2%) &apos;&apos;/&apos; .,---&apos; Battery1: 100% [Full] &apos;&apos;/&apos; ;: Local IP: xx.xx.xx.xx &apos;&apos;/&apos;&apos; &apos;&apos;/ Public IP: xx.xx.xx.xx &apos;&apos;/&apos;&apos;/&apos;&apos; &apos;/&apos;/&apos; `;GNU 启动动画 动图（加载慢） 二、安装 WSL2 和 Windows Terminal参考微软官方文档 Windows Subsystem for Linux Installation Guide for Windows 10 很详细，不用再看其他文章了。 三、oh-my-zsh进入 wsl，安装 zsh：sudo apt updatesudo apt install zsh -y 安装 oh-my-zsh：sh -c &quot;$(curl -fsSL https://raw.githubusercontent.com/robbyrussell/oh-my-zsh/master/tools/install.sh)&quot; 安装 Powerline9k 主题：git clone https://github.com/bhilburn/powerlevel9k.git ~/.oh-my-zsh/custom/themes/powerlevel9k 启用 Powerline9k 主题：编辑 ~/.zshrc 配置文件，修改 ZSH_THEME 项的内容为 ZSH_THEME=&quot;powerlevel9k/powerlevel9k&quot; 安装字体：访问 nerd-fonts 的 Github release 页，下载某种字体的压缩包（如 JetBrainsMono.zip），解压后在 Windows 系统上安装字体文件（有些时候可能需要使用管理员权限安装） 修改 Windows Terminal 的默认字体： 四、oh-my-zh 插件安装 zsh-autosuggestions：git clone https://github.com/zsh-users/zsh-autosuggestions ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-autosuggestions 安装 zsh-syntax-highlighting：git clone https://github.com/zsh-users/zsh-syntax-highlighting.git ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-syntax-highlighting 启用插件（包含默认插件）：修改 ~/.zshrc 配置文件的 plugins 项，内容如下（可按需添加或删减）123456789101112131415plugins=( cargo command-not-found docker git golang npm nvm rust sudo systemd web-search zsh-autosuggestions zsh-syntax-highlighting) 插件安装完成后，如上图中的情况，直接按键盘上的右方向键即可自动补全命令。 五、neofetch 获取 Linux 发行版 ASCII logo 和系统信息安装 neofetch：sudo apt install neofetch 效果如下：123456789101112131415161718192021222324$ neofetch --ascii_distro raspbian `.::///+:/-. --///+//-:`` starky@xxxxxx `+oooooooooooo: `+oooooooooooo: ---------------- /oooo++//ooooo: ooooo+//+ooooo. OS: Ubuntu 20.04.2 LTS on Windows 10 x86_64 `+ooooooo:-:oo- +o+::/ooooooo: Kernel: 5.4.72-microsoft-standard-WSL2 `:oooooooo+`` `.oooooooo+- Uptime: 11 mins `:++ooo/. :+ooo+/.` Packages: 736 (dpkg) ...` `.----.` ``.. Shell: zsh 5.8 .::::-``:::::::::.`-:::-` Terminal: /dev/pts/0 -:::-` .:::::::-` `-:::- CPU: Intel i7-10850H (12) @ 2.712GHz `::. `.--.` `` `.---.``.::` Memory: 107MiB / 12466MiB (0%) .::::::::` -::::::::` ` CPU Usage: 0% .::` .:::::::::- `::::::::::``::. Disk (/): 3.3G / 251G (2%)-:::` ::::::::::. ::::::::::.`:::- Battery1: 100% [Full]:::: -::::::::. `-:::::::: :::: Local IP: xx.xx.xx.xx-::- .-:::-.``....``.-::-. -::- Public IP: xx.xx.xx.xx .. `` .::::::::. `..`.. -:::-` -::::::::::` .:::::` :::::::` -::::::::::` :::::::. .::::::: -::::::::. :::::::: `-:::::` ..--.` ::::::. `...` `...--..` `...` .:::::::::: `.-::::-` neofetch 默认会输出当前系统的 logo，这里写个脚本（random_distro.sh）随机获取某个 Linux 发行版的 logo。12345678# random_distro.shdistro_list=('Alpine' 'Anarchy' 'Android' 'Antergos' 'antiX' 'AOSC' 'ArcoLinux' 'ArchBox' 'ARCHlabs' 'ArchStrike' 'XFerience' 'ArchMerge' 'Arch' 'Artix' 'Arya' 'Bedrock' 'BlackArch' 'BLAG' 'BlankOn' 'BlueLight' 'bonsai' 'BSD' 'BunsenLabs' 'Calculate' 'Carbs' 'CentOS' 'Chakra' 'Chapeau' 'Chrom' 'Cleanjaro' 'ClearOS' 'Clear_Linux' 'Clover' 'Condres' 'Container_Linux' 'CRUX' 'Debian' 'Deepin' 'DesaOS' 'Devuan' 'DracOS' 'DragonFly' 'Drauger' 'Elementary' 'EndeavourOS' 'Endless' 'Exherbo' 'Fedora' 'Feren' 'FreeBSD' 'FreeMiNT' 'Frugalware' 'Funtoo' 'GalliumOS' 'Gentoo' 'Pentoo' 'GNU' 'GoboLinux' 'Grombyang' 'Guix' 'Haiku' 'Huayra' 'Hyperbola' 'janus' 'Kali' 'KaOS' 'KDE_neon' 'Kogaion' 'Korora' 'KSLinux' 'Kubuntu' 'LEDE' 'LFS' 'Linux_Lite' 'LMDE' 'Lubuntu' 'Lunar' 'macos' 'Mageia' 'Mandriva' 'Manjaro' 'Maui' 'Mer' 'Minix' 'LinuxMint' 'MX_Linux' 'Namib' 'Neptune' 'NetBSD' 'Netrunner' 'NixOS' 'Nurunner' 'NuTyX' 'OBRevenge' 'OpenBSD' 'OpenIndiana' 'OpenMandriva' 'OpenWrt' 'osmc' 'Oracle' 'Parabola' 'Pardus' 'Parrot' 'Parsix' 'TrueOS' 'PCLinuxOS' 'Peppermint' 'popos' 'Porteus' 'PostMarketOS' 'Puppy' 'PureOS' 'Qubes' 'Radix' 'Raspbian' 'Reborn_OS' 'Redstar' 'Redcore' 'Redhat' 'Refracted_Devuan' 'Rosa' 'sabotage' 'Sabayon' 'Sailfish' 'SalentOS' 'Scientific' 'Septor' 'SharkLinux' 'Siduction' 'SliTaz' 'SmartOS' 'Solus' 'Source_Mage' 'Sparky' 'Star' 'SteamOS' 'SunOS' 'openSUSE_Leap' 'openSUSE' 'SwagArch' 'Tails' 'Trisquel' 'Ubuntu-Budgie' 'Ubuntu-GNOME' 'Ubuntu-MATE' 'Ubuntu-Studio' 'Void' 'Obarun' 'windows10' 'Windows7' 'Xubuntu')length=$&#123;#distro_list[@]&#125;distro=$&#123;distro_list[$RANDOM % $length]&#125;neofetch --ascii_distro $distro 运行 bash random_distro.sh 即可获取随机的 Linux 发行版 logo。 为了使该脚本可以在 Terminal 启动时自动运行，可以添加如下一条命令到 ~/.zshrc 配置文件末尾：bash /path/to/random_distro.sh 六、Shell 脚本显示天气和数字时钟weather-clock.sh 脚本从网上找的，没做改动（参考文章 Terminal splash screen with Weather, Calendar, Time &amp; Sysinfo?）。代码如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136#!/bin/bash# NAME: now# PATH: $HOME/bin# DESC: Display current weather, calendar and time# CALL: Called from terminal or ~/.bashrc# DATE: Apr 6, 2017. Modified: May 24, 2019.# UPDT: 2019-05-24 If Weather unavailable nicely formatted error message.# NOTE: To display all available toilet fonts use this one-liner:# for i in $&#123;TOILET_FONT_PATH:=/usr/share/figlet&#125;/*.&#123;t,f&#125;lf; do j=$&#123;i##*/&#125;; toilet -d "$&#123;i%/*&#125;" -f "$j" "$&#123;j%.*&#125;"; done# Setup for 92 character wide terminalDateColumn=34 # Default is 27 for 80 character line, 34 for 92 character lineTimeColumn=61 # Default is 49 for " " " " 61 " " " "# Replace Edmonton with your city name, GPS, etc. See: curl wttr.in/:helpcurl wttr.in/Hangzhou?0 --silent --max-time 3 &gt; /tmp/now-weather# Timeout #. Increase for slow connection---^readarray aWeather &lt; /tmp/now-weatherrm -f /tmp/now-weather# Was valid weather report found or an error message?if [[ "$&#123;aWeather[0]&#125;" == "Weather report:"* ]] ; then WeatherSuccess=true echo "$&#123;aWeather[@]&#125;"else WeatherSuccess=false echo "+============================+" echo "| Weather unavailable now!!! |" echo "| Check reason with command: |" echo "| |" echo "| curl wttr.in/Edmonton?0 |" # Replace Edmonton with your city echo "| --silent --max-time 3 |" echo "+============================+" echo " "fiecho " " # Pad blank lines for calendar &amp; time to fit#--------- DATE -------------------------------------------------------------# calendar current month with today highlighted.# colors 00=bright white, 31=red, 32=green, 33=yellow, 34=blue, 35=purple,# 36=cyan, 37=whitetput sc # Save cursor position.# Move up 9 linesi=0while [ $((++i)) -lt 10 ]; do tput cuu1; doneif [[ "$WeatherSuccess" == true ]] ; then # Depending on length of your city name and country name you will: # 1. Comment out next three lines of code. Uncomment fourth code line. # 2. Change subtraction value and set number of print spaces to match # subtraction value. Then place comment on fourth code line. Column=$((DateColumn - 10)) tput cuf $Column # Move x column number # Blank out ", country" with x spaces printf " "else tput cuf $DateColumn # Position to column 27 for date displayfi# -h needed to turn off formating: https://askubuntu.com/questions/1013954/bash-substring-stringoffsetlength-error/1013960#1013960cal &gt; /tmp/terminal1# -h not supported in Ubuntu 18.04. Use second answer: https://askubuntu.com/a/1028566/307523tr -cd '\11\12\15\40\60-\136\140-\176' &lt; /tmp/terminal1 &gt; /tmp/terminalCalLineCnt=1Today=$(date +"%e")printf "\033[32m" # color green -- see list above.while IFS= read -r Cal; do printf "%s" "$Cal" if [[ $CalLineCnt -gt 2 ]] ; then # See if today is on current line &amp; invert background tput cub 22 for (( j=0 ; j &lt;= 18 ; j += 3 )) ; do Test=$&#123;Cal:$j:2&#125; # Current day on calendar line if [[ "$Test" == "$Today" ]] ; then printf "\033[7m" # Reverse: [ 7 m printf "%s" "$Today" printf "\033[0m" # Normal: [ 0 m printf "\033[32m" # color green -- see list above. tput cuf 1 else tput cuf 3 fi done fi tput cud1 # Down one line tput cuf $DateColumn # Move 27 columns right CalLineCnt=$((++CalLineCnt))done &lt; /tmp/terminalprintf "\033[00m" # color -- bright white (default)echo ""tput rc # Restore saved cursor position.#-------- TIME --------------------------------------------------------------tput sc # Save cursor position.# Move up 8 linesi=0while [ $((++i)) -lt 9 ]; do tput cuu1; donetput cuf $TimeColumn # Move 49 columns right# Do we have the toilet package?if hash toilet 2&gt;/dev/null; then echo " $(date +"%I:%M %P") " | \ toilet -f future --filter border &gt; /tmp/terminal# Do we have the figlet package?elif hash figlet 2&gt;/dev/null; then# echo $(date +"%I:%M %P") | figlet &gt; /tmp/terminal date +"%I:%M %P" | figlet &gt; /tmp/terminal# else use standard fontelse# echo $(date +"%I:%M %P") &gt; /tmp/terminal date +"%I:%M %P" &gt; /tmp/terminalfiwhile IFS= read -r Time; do printf "\033[01;36m" # color cyan printf "%s" "$Time" tput cud1 # Up one line tput cuf $TimeColumn # Move 49 columns rightdone &lt; /tmp/terminaltput rc # Restore saved cursor position.exit 0 运行效果：123456789$ bash weather-clock.shWeather report: Hangzhou May 2021 Su Mo Tu We Th Fr Sa ┌────────────────────────────┐ _`/&quot;&quot;.-. Rain shower, mist 1 │ ┏━┓┏━┓ ┏━┓┏━┓ ┏━┓┏┳┓ │ ,\_( ). 21 °C 2 3 4 5 6 7 8 │ ┃┃┃┣━┓╹┃┃┃┗━┫ ┣━┛┃┃┃ │ /(___(__) ↓ 15 km/h 9 10 11 12 13 14 15 │ ┗━┛┗━┛╹┗━┛┗━┛ ╹ ╹ ╹ │ ‘ ‘ ‘ ‘ 3 km 16 17 18 19 20 21 22 └────────────────────────────┘ ‘ ‘ ‘ ‘ 0.8 mm 23 24 25 26 27 28 29 30 31 其中获取天气的关键代码为 curl wttr.in/Hangzhou?0 --silent --max-time 3，可自行改为自己所在的城市。 同样，为了使 Terminal 在启动时能自动运行该脚本，在 ~/.zshrc 配置文件的末尾（random-distro.sh 上面一行）添加如下内容：bash /path/to/weather-clock.sh PS：数字时钟的正常显示需要依赖 toilet 软件。安装 toilet：sudo apt install toilet 七、pipe.sh 生成 Terminal 启动动画获取 pipe.sh 程序：git clone https://github.com/pipeseroni/pipes.sh.git 提取项目中的 pipes.sh/pipes.sh 源文件到任意路径下。为了使 Terminal 在启动时能自动运行该脚本，在 ~/.zshrc 配置文件的末尾（weather-clock.sh 上面一行）添加如下内容：bash /path/to/pipes.sh -p 5 运行效果： 按下空格键可终止动画。 GIF 版本：]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Shell</tag>
        <tag>Terminal</tag>
        <tag>Zsh</tag>
        <tag>Beautify</tag>
        <tag>Neofetch</tag>
        <tag>Ascii Art</tag>
        <tag>Weather</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Miniconda 和 poetry 搭建 Python 开发环境（支持多版本、依赖管理）]]></title>
    <url>%2F2021%2F05%2F19%2Fbuild-python-development-environment-with-miniconda-and-poetry%2F</url>
    <content type="text"><![CDATA[基于自己的日常习惯测试整理，通过 Windows 系统演示（Linux 系统操作步骤大同小异）。Miniconda 用来提供 conda 命令管理多个 Python 版本（如 Python 3.8、Python 3.9）；poetry 则用来创建基于项目的虚拟环境，维护对应的包依赖关系。 一、效果演示conda 命令查看安装的 Python 版本：123456C:\Users\Administrator&gt;conda env list# conda environments:#base * C:\Users\xniu\Miniconda3python2.7.18 C:\Users\xniu\Miniconda3\envs\python2.7.18python3.9.4 C:\Users\xniu\Miniconda3\envs\python3.9.4 poetry 查看某个项目的包依赖关系：12345678910111213141516(python3.9.4) C:\Users\Administrator\projects\auto-test&gt;poetry showcertifi 2020.12.5 Python package for providing Mozilla&apos;s CA Bundle.chardet 4.0.0 Universal encoding detector for Python 2 and 3idna 2.10 Internationalized Domain Names in Applications (IDNA)requests 2.25.1 Python HTTP for Humans.selenium 3.141.0 Python bindings for Seleniumurllib3 1.26.4 HTTP library with thread-safe connection pooling, file post, and more.(python3.9.4) C:\Users\Administrator\projects\auto-test&gt;poetry show -trequests 2.25.1 Python HTTP for Humans.|-- certifi &gt;=2017.4.17|-- chardet &gt;=3.0.2,&lt;5|-- idna &gt;=2.5,&lt;3`-- urllib3 &gt;=1.21.1,&lt;1.27selenium 3.141.0 Python bindings for Selenium`-- urllib3 * 二、安装 MinicondaMiniconda 软件提供了 conda 命令，可以用来创建基于不同 Python 版本的虚拟环境。 访问 Miniconda 官网，下载对应系统版本的安装包并安装。 安装完成后，添加 conda 命令的路径（安装目录下的 Scripts 目录）到 PATH 环境变量。其路径一般为 C:\Users\xxx\Miniconda3\Scripts\。 添加完成后，打开一个新的命令提示符，运行 conda 命令看是否有反应。 创建基于不同 Python 版本的虚拟环境conda create -n python3.9.4 python=3.9.4上述命令会创建一个新的 Python 虚拟环境，并安装 Python 3.9.4。 其中 -n 选项用于指定该虚拟环境的名称，方便后续通过 conda activate xxx 启用该虚拟环境；python=3.9.4 命令用于安装指定版本的 Python 程序。 虚拟环境创建成功后，即可使用 conda activate python3.9.4 命令启用该虚拟环境。此后在该命令提示符环境下任何 python 命令都会自动使用 Python3.9.4 执行。 conda env list 命令可以查看现有的 Python 虚拟环境。 PS：更多 conda 命令可参考 Command reference。 三、安装 poetrypoetry 的安装可参考 poetry 官方文档。对于 Windows 系统可直接打开一个 PowerShell 窗口，运行以下命令：(Invoke-WebRequest -Uri https://raw.githubusercontent.com/python-poetry/poetry/master/get-poetry.py -UseBasicParsing).Content | python - 安装成功后，poetry 命令会自动添加到 PATH 环境变量中。 创建基于项目的虚拟环境打开一个新的命令提示符，使用 conda activate python3.9.4 命令激活某个 Python 版本。 进入到项目路径下，运行 poetry init 命令初始化配置。1234567891011121314151617181920212223242526272829303132(python3.9.4) C:\Users\Administrator\projects\python\test-poetry&gt;poetry initThis command will guide you through creating your pyproject.toml config.Package name [test-poetry]:Version [0.1.0]:Description []:Author [None, n to skip]: nLicense []:Compatible Python versions [^3.9]:Would you like to define your main dependencies interactively? (yes/no) [yes] noWould you like to define your development dependencies interactively? (yes/no) [yes] noGenerated file[tool.poetry]name = &quot;test-poetry&quot;version = &quot;0.1.0&quot;description = &quot;&quot;authors = [&quot;Your Name &lt;you@example.com&gt;&quot;][tool.poetry.dependencies]python = &quot;^3.9&quot;[tool.poetry.dev-dependencies][build-system]requires = [&quot;poetry-core&gt;=1.0.0&quot;]build-backend = &quot;poetry.core.masonry.api&quot;Do you confirm generation? (yes/no) [yes] yes 上述操作会在项目目录下自动创建 pyproject.toml 配置文件，内容如下：1234567891011121314[tool.poetry]name = "test-poetry"version = "0.1.0"description = ""authors = ["Your Name &lt;you@example.com&gt;"][tool.poetry.dependencies]python = "^3.9"[tool.poetry.dev-dependencies][build-system]requires = ["poetry-core&gt;=1.0.0"]build-backend = "poetry.core.masonry.api" 使用 poetry shell 命令自动创建基于当前项目的虚拟环境并激活该环境：12345(python3.9.4) C:\Users\Administrator\projects\python\test-poetry&gt;poetry shellCreating virtualenv test-poetry-thSlgjIV-py3.9 in C:\Users\Administrator\AppData\Local\pypoetry\Cache\virtualenvsSpawning shell within C:\Users\Administrator\AppData\Local\pypoetry\Cache\virtualenvs\test-poetry-thSlgjIV-py3.9Microsoft Windows [Version 10.0.18363.1316](c) 2019 Microsoft Corporation. All rights reserved. 运行 code . 命令使用 VSCode 软件打开本项目，此时即可在 IDE 中切换到新创建的基于本项目的虚拟环境（VSCode 已经安装了 Python 插件）。 安装依赖包poetry add xxx 命令可以用来在当前环境中安装某个依赖包：123456789101112131415(python3.9.4) C:\Users\Administrator\projects\python\test-poetry&gt;poetry add requestsUsing version ^2.25.1 for requestsUpdating dependenciesResolving dependencies...Writing lock filePackage operations: 5 installs, 0 updates, 0 removals • Installing certifi (2020.12.5) • Installing chardet (4.0.0) • Installing idna (2.10) • Installing urllib3 (1.26.4) • Installing requests (2.25.1) poetry add xxx -D 命令可以用来安装针对开发环境的某个依赖包（用 poetry remove 命令卸载此类包时也需要指定 -D 选项）：12345678910111213(python3.9.4) C:\Users\Administrator\projects\python\test-poetry&gt;poetry add autopep8 -DUsing version ^1.5.7 for autopep8Updating dependenciesResolving dependencies...Writing lock filePackage operations: 3 installs, 0 updates, 0 removals • Installing pycodestyle (2.7.0) • Installing toml (0.10.2) • Installing autopep8 (1.5.7) 同时，安装的依赖包信息也会自动添加到 pyproject.toml 配置文件中：12345678910111213141516[tool.poetry]name = "test-poetry"version = "0.1.0"description = ""authors = ["Your Name &lt;you@example.com&gt;"][tool.poetry.dependencies]python = "^3.9"requests = "^2.25.1"[tool.poetry.dev-dependencies]autopep8 = "^1.5.7"[build-system]requires = ["poetry-core&gt;=1.0.0"]build-backend = "poetry.core.masonry.api" 后续配置新的环境时，pyproject.toml 可以发挥类似 requirements.txt 文件的作用。即借助此文件中的配置，可以直接使用 peotry install 命令自动安装文件中包含的依赖项。 维护环境依赖使用 poetry show 命令查看当前安装的依赖包列表：123456789(python3.9.4) C:\Users\Administrator\projects\python\test-poetry&gt;poetry showautopep8 1.5.7 A tool that automatically formats Python code to conform to the PEP 8 style guidecertifi 2020.12.5 Python package for providing Mozilla&apos;s CA Bundle.chardet 4.0.0 Universal encoding detector for Python 2 and 3idna 2.10 Internationalized Domain Names in Applications (IDNA)pycodestyle 2.7.0 Python style guide checkerrequests 2.25.1 Python HTTP for Humans.toml 0.10.2 Python Library for Tom&apos;s Obvious, Minimal Languageurllib3 1.26.4 HTTP library with thread-safe connection pooling, file post, and more. 使用 poetry show -t 命令查看当前环境中各包之间的依赖关系：123456789(python3.9.4) C:\Users\Administrator\projects\python\test-poetry&gt;poetry show -tautopep8 1.5.7 A tool that automatically formats Python code to conform to the PEP 8 style guide|-- pycodestyle &gt;=2.7.0`-- toml *requests 2.25.1 Python HTTP for Humans.|-- certifi &gt;=2017.4.17|-- chardet &gt;=3.0.2,&lt;5|-- idna &gt;=2.5,&lt;3`-- urllib3 &gt;=1.21.1,&lt;1.27 若此时使用 poetry remove autopep8 -D 命令移除 autopep8，则之前自动安装的 pycodestyle、toml 依赖项也会被移除。1234567891011(python3.9.4) C:\Users\Administrator\projects\python\test-poetry&gt;poetry remove autopep8 -DUpdating dependenciesResolving dependencies...Writing lock filePackage operations: 0 installs, 0 updates, 3 removals • Removing autopep8 (1.5.7) • Removing pycodestyle (2.7.0) • Removing toml (0.10.2) PS：更多 poetry 命令和用法可参考官方文档：Poetry Commands]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Program</tag>
        <tag>Development</tag>
        <tag>Poetry</tag>
        <tag>Miniconda</tag>
        <tag>Conda</tag>
        <tag>Package</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django（drf）配合 Vue Element 实现文件上传下载功能]]></title>
    <url>%2F2021%2F03%2F13%2Fdjango-drf-and-vue-element-file-upload-and-download%2F</url>
    <content type="text"><![CDATA[后台代码Models编辑 models.py 代码，通过 FileField 字段记录文件信息：123456789from django.db import modelsclass FilesModel(models.Model): file = models.FileField(upload_to='uploads/') class Meta: db_table = 'files_storage' ordering = ['-id'] Serializer这里使用 Django REST framework 实现后端 REST API，需要创建序列化器 serializers.py，内容如下：123456789from rest_framework import serializers# files 是 app 的名字from files import modelsclass FilesSerializer(serializers.ModelSerializer): class Meta: model = models.FilesModel fields = '__all__' Views编辑 views.py 代码，内容如下：1234567from rest_framework.viewsets import ModelViewSetfrom files import models, serializersclass FileViewSet(ModelViewSet): queryset = models.FilesModel.objects.all() serializer_class = serializers.FilesSerializer Urls在 files 路径下新建 urls.py 文件，填写路由配置：12345678910from django.urls import include, pathfrom rest_framework import routersfrom files import viewsrouter = routers.DefaultRouter()router.register(r'files', views.FileViewSet)urlpatterns = [ path('', include(router.urls))] 在项目总配置路径下（settings.py 所在的路径）编辑根路由配置文件 urls.py：1234567from django.contrib import adminfrom django.urls import path, includeurlpatterns = [ path('admin/', admin.site.urls), path('storage/', include('files.urls'))] 测试后端 API运行后台服务 python manage.py runserver 0.0.0.0:8000，访问 http://xx.xx.xx.xx:8000/storage/files/，界面如下： 测试上传文件，效果如下： 前端代码（手动上传）借助 Element UI 的 upload 组件，Vue 代码（index.vue）如下：123456789101112131415161718192021222324252627282930&lt;template&gt; &lt;div&gt; &lt;el-upload ref=&quot;upload&quot; drag action=&quot;http://xx.xx.xx.xx:8000/storage/files/&quot; :auto-upload=&quot;false&quot; :on-success=&quot;onSuccess&quot; &gt; &lt;i class=&quot;el-icon-upload&quot; /&gt; &lt;div class=&quot;el-upload__text&quot;&gt;将文件拖到此处，或&lt;em&gt;点击上传&lt;/em&gt;&lt;/div&gt; &lt;/el-upload&gt; &lt;el-button style=&quot;margin-left: 10px;&quot; size=&quot;small&quot; type=&quot;success&quot; @click=&quot;submitUpload&quot;&gt;上传到服务器&lt;/el-button&gt; &lt;/div&gt;&lt;/template&gt;&lt;script&gt;export default &#123; name: &apos;UploadDemo&apos;, methods: &#123; submitUpload() &#123; this.$refs.upload.submit() &#125;, onSuccess() &#123; this.$message.success(&apos;上传成功&apos;) &#125; &#125;&#125;&lt;/script&gt; 其中 el-upload 组件的 action 属性用于指定后台 API 的 URI；:auto-upload 属性用于设置是否自动上传（这里设置为 false，手动触发上传动作）；:on-success 属性用于指定上传成功后触发的方法。 submitUpload() 中的 this.$refs.upload.submit() 方法触发文件上传动作。 界面如下： 测试文件上传： 后台数据如下：12345678910[ &#123; "file": "http://172.20.23.34:8000/storage/files/uploads/template.html", "id": 18 &#125;, &#123; "file": "http://172.20.23.34:8000/storage/files/uploads/20171215091830_55126_hSnPtZR.png", "id": 17 &#125;] 文件上传的同时添加其他数据修改数据库模型编辑后端 models.py 文件，添加其他字段：12345678910from django.db import modelsclass FilesModel(models.Model): name = models.CharField(max_length=20, default='') file = models.FileField(upload_to='uploads/') class Meta: db_table = 'files_storage' ordering = ['-id'] 数据库迁移后，重启后台 Web 服务。 后台数据如下：123456789101112[ &#123; "file": "http://172.20.23.34:8000/storage/files/uploads/template.html", "id": 18, "name": "" &#125;, &#123; "file": "http://172.20.23.34:8000/storage/files/uploads/20171215091830_55126_hSnPtZR.png", "id": 17, "name": "" &#125;] 修改前端代码添加其他数据的输入界面，同时将附加数据绑定到 el-upload 组件中：123456789101112131415161718192021222324252627282930313233343536373839404142&lt;template&gt; &lt;div&gt; &lt;el-label&gt;名称&lt;/el-label&gt; &lt;el-input v-model=&quot;fileData.name&quot; style=&quot;width: 20%&quot; /&gt; &lt;el-upload ref=&quot;upload&quot; drag class=&quot;upload-demo&quot; action=&quot;http://xx.xx.xx.xx:8000/storage/files/&quot; :data=&quot;fileData&quot; :auto-upload=&quot;false&quot; :on-success=&quot;onSuccess&quot; style=&quot;padding: 30px&quot; &gt; &lt;i class=&quot;el-icon-upload&quot; /&gt; &lt;div class=&quot;el-upload__text&quot;&gt;将文件拖到此处，或&lt;em&gt;点击上传&lt;/em&gt;&lt;/div&gt; &lt;/el-upload&gt; &lt;el-button style=&quot;margin-left: 10px;&quot; size=&quot;small&quot; type=&quot;success&quot; @click=&quot;submitUpload&quot;&gt;上传到服务器&lt;/el-button&gt; &lt;/div&gt;&lt;/template&gt;&lt;script&gt;export default &#123; name: &apos;UploadDemo&apos;, data() &#123; return &#123; fileData: &#123; name: &apos;&apos; &#125; &#125; &#125;, methods: &#123; submitUpload() &#123; this.$refs.upload.submit() &#125;, onSuccess() &#123; this.$message.success(&apos;上传成功&apos;) &#125; &#125;&#125;&lt;/script&gt; 其中 el-upload 组件的 :data 属性用于指定文件上传时附加的数据（类型为 JavaScript 对象）。 文件上传测试： 上传完成，后台数据如下：1234567891011121314151617[ &#123; "file": "http://172.20.23.34:8000/storage/files/uploads/AnyDesk.exe", "id": 19, "name": "测试文件" &#125;, &#123; "file": "http://172.20.23.34:8000/storage/files/uploads/template.html", "id": 18, "name": "" &#125;, &#123; "file": "http://172.20.23.34:8000/storage/files/uploads/20171215091830_55126_hSnPtZR.png", "id": 17, "name": "" &#125;] 文件下载修改后台视图代码（views.py），添加文件下载的 API 响应逻辑： 123456789101112131415from rest_framework.viewsets import ModelViewSetfrom files import models, serializersfrom rest_framework.decorators import actionfrom django.http import FileResponseclass FileViewSet(ModelViewSet): queryset = models.FilesModel.objects.all() serializer_class = serializers.FilesSerializer @action(methods=['get', 'post'], detail=True) def download(self, request, pk=None, *args, **kwargs): file_obj = self.get_object() response = FileResponse(open(file_obj.file.path, 'rb')) return response 此时访问 http://xx.xx.xx.xx:8000/storage/files/[id]/download/ 链接，即可直接下载上传到服务器上的文件。 1234$ curl -o anydesk.exe 172.20.23.34:8000/storage/files/19/download/ % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed100 3584k 100 3584k 0 0 102M 0 --:--:-- --:--:-- --:--:-- 102M 参考资料Element UI 官方文档Django 官方文档]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Web</tag>
        <tag>Django</tag>
        <tag>Vue</tag>
        <tag>drf</tag>
        <tag>Element</tag>
        <tag>File</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基本算法问题的 Python 解法——图（Graph）问题]]></title>
    <url>%2F2021%2F02%2F20%2Fclassic-compute-problems-with-python-graph-problems%2F</url>
    <content type="text"><![CDATA[图（Graph）是一种用来对某些现实问题进行建模的抽象的数学结构，这些问题从逻辑上可以被划分成一系列相互连接的节点。其中的节点称为顶点（vertex），顶点之间的连接称为边（edge）。比如地铁线路就可以看作由图表示成的运输网络。每一个顶点都代表一个地铁站，而顶点之间的边则表示两个地铁站之间的路径。如果想知道某个站点到另一个站点的最短路径，图算法就能发挥作用。实际上，图算法可以被应用到任何类型的网络问题中。 map as graph 123456789101112131415# edge.pyfrom __future__ import annotationsfrom dataclasses import dataclass@dataclassclass Edge: u: int # the "from" vertex v: int # the "to" vertex def reversed(self) -&gt; Edge: return Edge(self.v, self.u) def __str__(self) -&gt; str: return f"&#123;self.u&#125; -&gt; &#123;self.v&#125;" 上面代码中的 Edge 类表示两个顶点之间的连接（即“边”），每个顶点都由整数索引表示。其中 u 用来表示第一个顶点，v 表示第二个顶点。这里只关注非方向性的 graph，edge 是双向的。而在有向图（digraph）中，edge 可以是单向的。reversed() 方法用来返回当前 edge 的逆向形式。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172# graph.pyfrom typing import TypeVar, Generic, List, Optionalfrom edge import EdgeV = TypeVar('V') # type of the vertices in the graphclass Graph(Generic[V]): def __init__(self, vertices: List[V] = []) -&gt; None: self._vertices: List[V] = vertices self._edges: List[List[Edge]] = [[] for _ in vertices] @property def vertex_count(self) -&gt; int: return len(self._vertices) # Number of vertices @property def edge_count(self) -&gt; int: return sum(map(len, self._edges)) # Number of edges # Add a vertex to the graph and return its index def add_vertex(self, vertex: V) -&gt; int: self._vertices.append(vertex) self._edges.append([]) # Add empty list for containing edges return self.vertex_count - 1 # Return index of added vertex # This is an undirected graph, # so we always add edges in both directions def add_edge(self, edge: Edge) -&gt; None: self._edges[edge.u].append(edge) self._edges[edge.v].append(edge.reversed()) # Add an edge using vertex indices (convenience method) def add_edge_by_indices(self, u: int, v: int) -&gt; None: edge: Edge = Edge(u, v) self.add_edge(edge) # Add an edge by looking up vertex indices (convenience method) def add_edge_by_vertices(self, first: V, second: V) -&gt; None: u: int = self._vertices.index(first) v: int = self._vertices.index(second) self.add_edge_by_indices(u, v) # Find the vertex at a specific index def vertex_at(self, index: int) -&gt; V: return self._vertices[index] # Find the index of a vertex in the graph def index_of(self, vertex: V) -&gt; int: return self._vertices.index(vertex) # Find the vertices that a vertex at some index is connected to def neighbors_for_index(self, index: int) -&gt; List[V]: return list(map(self.vertex_at, [e.v for e in self._edges[index]])) # Look up a vertice's index and find its neighbors (convenience method) def neighbors_for_vertex(self, vertex: V) -&gt; List[V]: return self.neighbors_for_index(self.index_of(vertex)) # Return all of the edges associated with a vertex at some index def edges_for_index(self, index: int) -&gt; List[Edge]: return self._edges[index] # Look up the index of a vertex and return its edges (convenience method) def edges_for_vertex(self, vertex: V) -&gt; List[Edge]: return self.edges_for_index(self.index_of(vertex)) # Make it easy to pretty-print a Graph def __str__(self) -&gt; str: desc: str = "" for i in range(self.vertex_count): desc += f"&#123;self.vertex_at(i)&#125; -&gt; &#123;self.neighbors_for_index(i)&#125;\n" return desc Graph 类聚焦于 graph 的核心角色，即将顶点用边连接起来。_vertices 列表是 Graph 类的核心，每个顶点都会被存储在该列表中。但是之后在实际引用时会使用顶点在列表中的索引。顶点本身有可能会是非常复杂的数据类型，但其索引一定会是 int 类型，相对而言更加方便使用。graph 数据类型可以使用 adjacency lists 方式实现，每个顶点都拥有一个列表，里面包含了这个顶点连接的其他顶点。这里使用了由 edge 组成的列表再组成的列表（_edges），每个顶点都拥有一个由 edge 组成的列表，这些 edge 表示该顶点与其他顶点的连接关系。 Graph 类中实现的方法的简单介绍： vertex_count 属性：获取 graph 中顶点的数量 edge_count 属性：获取 graph 中边的数量 add_vertex 方法：添加一个新的孤立的顶点并返回其索引 add_edge 方法：添加一条边（双向，参数是 Edge 对象） add_edge_by_indices 方法：通过顶点索引添加新的边（参数是边的两个顶点的索引 u、v） add_edge_by_vertices 方法：通过顶点添加新的边（参数是边的两个顶点（Vertex）对象） vertex_at 方法：通过特定的索引查询顶点 index_of 方法：根据顶点返回其索引 neighbors_for_index 方法：根据某个顶点的索引获取其临近的顶点（参数为顶点索引） neighbors_for_vertex 方法：根据某个顶点获取其临近的顶点（参数为顶点对象） edges_for_index 方法：根据某个顶点的索引获取与其连接的边（参数为顶点索引） edges_for_vertex 方法：根据某个顶点获取与其连接的边（参数为顶点对象） __str__ 方法：友好的方式输出整个 graph 补充测试代码：1234567891011121314151617181920212223242526272829303132# graph.py continuedif __name__ == "__main__": # test basic Graph construction city_graph: Graph[str] = Graph(["Seattle", "San Francisco", "Los Angeles", "Riverside", "Phoenix", "Chicago", "Boston", "New York", "Atlanta", "Miami", "Dallas", "Houston", "Detroit", "Philadelphia", "Washington"]) city_graph.add_edge_by_vertices("Seattle", "Chicago") city_graph.add_edge_by_vertices("Seattle", "San Francisco") city_graph.add_edge_by_vertices("San Francisco", "Riverside") city_graph.add_edge_by_vertices("San Francisco", "Los Angeles") city_graph.add_edge_by_vertices("Los Angeles", "Riverside") city_graph.add_edge_by_vertices("Los Angeles", "Phoenix") city_graph.add_edge_by_vertices("Riverside", "Phoenix") city_graph.add_edge_by_vertices("Riverside", "Chicago") city_graph.add_edge_by_vertices("Phoenix", "Dallas") city_graph.add_edge_by_vertices("Phoenix", "Houston") city_graph.add_edge_by_vertices("Dallas", "Chicago") city_graph.add_edge_by_vertices("Dallas", "Atlanta") city_graph.add_edge_by_vertices("Dallas", "Houston") city_graph.add_edge_by_vertices("Houston", "Atlanta") city_graph.add_edge_by_vertices("Houston", "Miami") city_graph.add_edge_by_vertices("Atlanta", "Chicago") city_graph.add_edge_by_vertices("Atlanta", "Washington") city_graph.add_edge_by_vertices("Atlanta", "Miami") city_graph.add_edge_by_vertices("Miami", "Washington") city_graph.add_edge_by_vertices("Chicago", "Detroit") city_graph.add_edge_by_vertices("Detroit", "Boston") city_graph.add_edge_by_vertices("Detroit", "Washington") city_graph.add_edge_by_vertices("Detroit", "New York") city_graph.add_edge_by_vertices("Boston", "New York") city_graph.add_edge_by_vertices("New York", "Philadelphia") city_graph.add_edge_by_vertices("Philadelphia", "Washington") print(city_graph) 运行结果：123456789101112131415Seattle -&gt; ['Chicago', 'San Francisco']San Francisco -&gt; ['Seattle', 'Riverside', 'Los Angeles']Los Angeles -&gt; ['San Francisco', 'Riverside', 'Phoenix']Riverside -&gt; ['San Francisco', 'Los Angeles', 'Phoenix', 'Chicago']Phoenix -&gt; ['Los Angeles', 'Riverside', 'Dallas', 'Houston']Chicago -&gt; ['Seattle', 'Riverside', 'Dallas', 'Atlanta', 'Detroit']Boston -&gt; ['Detroit', 'New York']New York -&gt; ['Detroit', 'Boston', 'Philadelphia']Atlanta -&gt; ['Dallas', 'Houston', 'Chicago', 'Washington', 'Miami']Miami -&gt; ['Houston', 'Atlanta', 'Washington']Dallas -&gt; ['Phoenix', 'Chicago', 'Atlanta', 'Houston']Houston -&gt; ['Phoenix', 'Dallas', 'Atlanta', 'Miami']Detroit -&gt; ['Chicago', 'Boston', 'Washington', 'New York']Philadelphia -&gt; ['New York', 'Washington']Washington -&gt; ['Atlanta', 'Miami', 'Detroit', 'Philadelphia'] 寻找最短路径在 graph 理论中，任意两个顶点之间的所有连线（边）称为路径。即从一个顶点到达另一个顶点需要走过的所有路径。在一个未加权的 graph 中（即不考虑边的长度），寻找最短的路径意味着从起始顶点到目标顶点之间经过的边最少。可以使用宽度优先搜索（breadth-first search, BFS）算法查找两个顶点之间的最短路径。（BFS 算法的具体实现可参考 基本算法问题的 Python 解法（递归与搜索）中的迷宫问题）。 BFS 部分代码如下：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162# generic_search.pyfrom __future__ import annotationsfrom typing import TypeVar, Generic, List, Callable, Deque, Set, OptionalT = TypeVar('T')class Node(Generic[T]): def __init__(self, state: T, parent: Optional[Node]) -&gt; None: self.state: T = state self.parent: Optional[Node] = parentclass Queue(Generic[T]): def __init__(self) -&gt; None: self._container: Deque[T] = Deque() @property def empty(self) -&gt; bool: return not self._container # not is true for empty container def push(self, item: T) -&gt; None: self._container.append(item) def pop(self) -&gt; T: return self._container.popleft() # FIFO def __repr__(self) -&gt; str: return repr(self._container)def bfs(initial: T, goal_test: Callable[[T], bool], successors: Callable[[T], List[T]]) -&gt; Optional[Node[T]]: # frontier is where we've yet to go frontier: Queue[Node[T]] = Queue() frontier.push(Node(initial, None)) # explored is where we've been explored: Set[T] = &#123;initial&#125; # keep going while there is more to explore while not frontier.empty: current_node: Node[T] = frontier.pop() current_state: T = current_node.state # if we found the goal, we're done if goal_test(current_state): return current_node # check where we can go next and haven't explored for child in successors(current_state): if child in explored: # skip children we already explored continue explored.add(child) frontier.push(Node(child, current_node)) return None # went through everything and never found goaldef node_to_path(node: Node[T]) -&gt; List[T]: path: List[T] = [node.state] # work backwards from end to front while node.parent is not None: node = node.parent path.append(node.state) path.reverse() return path 继续补充 graph.py 代码如下：123456789101112# graph.py continuedif __name__ == "__main__":# ... from generic_search import bfs, Node, node_to_path bfs_result: Optional[Node[V]] = bfs("Boston", lambda x: x == "Miami", city_graph.neighbors_for_vertex) if bfs_result is None: print("No solution found using breadth-first search!") else: path: List[V] = node_to_path(bfs_result) print("Path from Boston to Miami:") print(path) bfs() 函数接受三个参数：初始状态、用于检测当前状态是否符合目标状态的 Callable（可调用对象）、用于寻找达成目标状态的路径的 Callable。若需要寻找 Boston 到 Miami 的最短路径（不考虑加权的情况），则初始状态为顶点 “Boston”，用于状态检测的 Callable 则判断当前顶点是否为 “Miami”。 运行效果：12Path from Boston to Miami:['Boston', 'Detroit', 'Washington', 'Miami'] 加权图之前的计算中，最短路径只考虑经过的站点最少，而未将站点之间的路程计算在内。若需要将路程包含进去，则可以为 edge 加上权重来表示该 edge 对应的距离。 为了实现加权的 graph，需要实现 Edge 的子类 WeightedEdge 以及 Graph 的子类 WeightedGraph。每一个 WeightedEdge 对象都有一个关联的 float 类型的属性用来表示权重。123456789101112131415161718# weighted_edge.pyfrom __future__ import annotationsfrom dataclasses import dataclassfrom edge import Edge@dataclassclass WeightedEdge(Edge): weight: float def reversed(self) -&gt; WeightedEdge: return WeightedEdge(self.v, self.u, self.weight) # so that we can order edges by weight to find the minimum weight edge def __lt__(self, other: WeightedEdge) -&gt; bool: return self.weight &lt; other.weight def __str__(self) -&gt; str: return f"&#123;self.u&#125; &#123;self.weight&#125;&gt; &#123;self.v&#125;" WeightedEdge 子类添加了一个 weight 属性，通过 __lt__() 方法实现了 &lt; 操作符，令 WeightedEdge 对象成为可比较的，使得返回 weight 最小的 edge 成为可能。 1234567891011121314151617181920212223242526272829303132# weighted_graph.pyfrom typing import TypeVar, Generic, List, Tuplefrom graph import Graphfrom weighted_edge import WeightedEdgeV = TypeVar('V') # type of the vertices in the graphclass WeightedGraph(Generic[V], Graph[V]): def __init__(self, vertices: List[V] = []) -&gt; None: self._vertices: List[V] = vertices self._edges: List[List[WeightedEdge]] = [[] for _ in vertices] def add_edge_by_indices(self, u: int, v: int, weight: float) -&gt; None: edge: WeightedEdge = WeightedEdge(u, v, weight) self.add_edge(edge) # call superclass version def add_edge_by_vertices(self, first: V, second: V, weight: float) -&gt; None: u: int = self._vertices.index(first) v: int = self._vertices.index(second) self.add_edge_by_indices(u, v, weight) def neighbors_for_index_with_weights(self, index: int) -&gt; List[Tuple[V, float]]: distance_tuples: List[Tuple[V, float]] = [] for edge in self.edges_for_index(index): distance_tuples.append((self.vertex_at(edge.v), edge.weight)) return distance_tuples def __str__(self) -&gt; str: desc: str = "" for i in range(self.vertex_count): desc += f"&#123;self.vertex_at(i)&#125; -&gt; &#123;self.neighbors_for_index_with_weights(i)&#125;\n" return desc WeightedGraph 类继承自 Graph，在原来的基础上对某些需要适应 weight 属性的方法做了对应的修改。 补充 weighted_graph.py 代码，测试运行效果：123456789101112131415161718192021222324252627282930313233343536# weighted_graph.py continuedif __name__ == "__main__": city_graph2: WeightedGraph[str] = WeightedGraph(["Seattle", "San Francisco", "Los Angeles", "Riverside", "Phoenix", "Chicago", "Boston", "New York", "Atlanta", "Miami", "Dallas", "Houston", "Detroit", "Philadelphia", "Washington"]) city_graph2.add_edge_by_vertices("Seattle", "Chicago", 1737) city_graph2.add_edge_by_vertices("Seattle", "San Francisco", 678) city_graph2.add_edge_by_vertices("San Francisco", "Riverside", 386) city_graph2.add_edge_by_vertices("San Francisco", "Los Angeles", 348) city_graph2.add_edge_by_vertices("Los Angeles", "Riverside", 50) city_graph2.add_edge_by_vertices("Los Angeles", "Phoenix", 357) city_graph2.add_edge_by_vertices("Riverside", "Phoenix", 307) city_graph2.add_edge_by_vertices("Riverside", "Chicago", 1704) city_graph2.add_edge_by_vertices("Phoenix", "Dallas", 887) city_graph2.add_edge_by_vertices("Phoenix", "Houston", 1015) city_graph2.add_edge_by_vertices("Dallas", "Chicago", 805) city_graph2.add_edge_by_vertices("Dallas", "Atlanta", 721) city_graph2.add_edge_by_vertices("Dallas", "Houston", 225) city_graph2.add_edge_by_vertices("Houston", "Atlanta", 702) city_graph2.add_edge_by_vertices("Houston", "Miami", 968) city_graph2.add_edge_by_vertices("Atlanta", "Chicago", 588) city_graph2.add_edge_by_vertices("Atlanta", "Washington", 543) city_graph2.add_edge_by_vertices("Atlanta", "Miami", 604) city_graph2.add_edge_by_vertices("Miami", "Washington", 923) city_graph2.add_edge_by_vertices("Chicago", "Detroit", 238) city_graph2.add_edge_by_vertices("Detroit", "Boston", 613) city_graph2.add_edge_by_vertices("Detroit", "Washington", 396) city_graph2.add_edge_by_vertices("Detroit", "New York", 482) city_graph2.add_edge_by_vertices("Boston", "New York", 190) city_graph2.add_edge_by_vertices("New York", "Philadelphia", 81) city_graph2.add_edge_by_vertices("Philadelphia", "Washington", 123) print(city_graph2) 运行效果：123456789101112131415Seattle -&gt; [('Chicago', 1737), ('San Francisco', 678)]San Francisco -&gt; [('Seattle', 678), ('Riverside', 386), ('Los Angeles', 348)]Los Angeles -&gt; [('San Francisco', 348), ('Riverside', 50), ('Phoenix', 357)]Riverside -&gt; [('San Francisco', 386), ('Los Angeles', 50), ('Phoenix', 307), ('Chicago', 1704)]Phoenix -&gt; [('Los Angeles', 357), ('Riverside', 307), ('Dallas', 887), ('Houston', 1015)]Chicago -&gt; [('Seattle', 1737), ('Riverside', 1704), ('Dallas', 805), ('Atlanta', 588), ('Detroit', 238)]Boston -&gt; [('Detroit', 613), ('New York', 190)]New York -&gt; [('Detroit', 482), ('Boston', 190), ('Philadelphia', 81)]Atlanta -&gt; [('Dallas', 721), ('Houston', 702), ('Chicago', 588), ('Washington', 543), ('Miami', 604)]Miami -&gt; [('Houston', 968), ('Atlanta', 604), ('Washington', 923)]Dallas -&gt; [('Phoenix', 887), ('Chicago', 805), ('Atlanta', 721), ('Houston', 225)]Houston -&gt; [('Phoenix', 1015), ('Dallas', 225), ('Atlanta', 702), ('Miami', 968)]Detroit -&gt; [('Chicago', 238), ('Boston', 613), ('Washington', 396), ('New York', 482)]Philadelphia -&gt; [('New York', 81), ('Washington', 123)]Washington -&gt; [('Atlanta', 543), ('Miami', 923), ('Detroit', 396), ('Philadelphia', 123)] 在加权图中搜索最短路径寻找某个起点城市到另一个城市的所有路线中花费最小的一条，属于单源头最短路径（single-source shortest path）问题，即从加权图中的某个顶点到任意的另外一个顶点的最短路径。 Dijkstra 算法 可以用来解决单源头最短路径问题。该算法从某个起始顶点开始，可以找出加权图中所有其他顶点到起始顶点的最短路径。从某个顶点开始按照远近关系依次遍历完所有顶点并记录其总的花费（从起始顶点到当前顶点），若重复出现的顶点花费更小，则令其替换已有的记录。 具体步骤如下： 将起始顶点加入到优先级队列中 从优先级队列中弹出一个顶点（一开始就是起始顶点）作为当前顶点 查看与当前顶点临近的所有顶点，若某一个之前没有被记录到，或某个顶点按照当前路径的花费低于已有的最小记录，则记录其到起始顶点的距离（作为新的最小记录）及生成该距离的最后一条边（记录路径），并将该顶点 push 到优先级队列中（令其作为之后的“当前”顶点） 重复前面两步直到优先级队列为空 返回所有顶点到起始顶点的最小距离及路径 12345678910111213141516171819202122# priority_queue.pyfrom typing import TypeVar, Generic, Listfrom heapq import heappush, heappopT = TypeVar('T')class PriorityQueue(Generic[T]): def __init__(self) -&gt; None: self._container: List[T] = [] @property def empty(self) -&gt; bool: return not self._container def push(self, item: T) -&gt; None: heappush(self._container, item) def pop(self) -&gt; T: return heappop(self._container) def __repr__(self) -&gt; str: return repr(self._container) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118# dijkstra.pyfrom __future__ import annotationsfrom typing import TypeVar, List, Optional, Tuple, Dictfrom dataclasses import dataclassfrom mst import WeightedPath, print_weighted_pathfrom weighted_graph import WeightedGraphfrom weighted_edge import WeightedEdgefrom priority_queue import PriorityQueueV = TypeVar('V') # type of the vertices in the graph@dataclassclass DijkstraNode: vertex: int distance: float def __lt__(self, other: DijkstraNode) -&gt; bool: return self.distance &lt; other.distance def __eq__(self, other: DijkstraNode) -&gt; bool: return self.distance == other.distancedef dijkstra(wg: WeightedGraph[V], root: V) -&gt; Tuple[List[Optional[float]], Dict[int, WeightedEdge]]: first: int = wg.index_of(root) # distances are unknown at first distances: List[Optional[float]] = [None] * wg.vertex_count distances[first] = 0 # the root is 0 away from the root path_dict: Dict[int, WeightedEdge] = &#123;&#125; # how we got to each vertex pq: PriorityQueue[DijkstraNode] = PriorityQueue() pq.push(DijkstraNode(first, 0)) while not pq.empty: u: int = pq.pop().vertex # explore the next closest vertex dist_u: float = distances[u] # should already have seen it # look at every edge/vertex from current vertex for we in wg.edges_for_index(u): # the old distance from starting vertex to this vertex dist_v: float = distances[we.v] # no old distance or found shorter path if dist_v is None or dist_v &gt; we.weight + dist_u: # update distance to this vertex distances[we.v] = we.weight + dist_u # update the edge on the shortest path to this vertex path_dict[we.v] = we # explore this vertex soon pq.push(DijkstraNode(we.v, we.weight + dist_u)) return distances, path_dict# Helper function to get easier access to dijkstra resultsdef distance_array_to_vertex_dict(wg: WeightedGraph[V], distances: List[Optional[float]]) -&gt; Dict[V, Optional[float]]: distance_dict: Dict[V, Optional[float]] = &#123;&#125; for i in range(len(distances)): distance_dict[wg.vertex_at(i)] = distances[i] return distance_dict# Takes a dictionary of edges to reach each node and returns a list of# edges that goes from `start` ot `end`def path_dict_to_path(start: int, end: int, path_dict: Dict[int, WeightedEdge]) -&gt; WeightedPath: if len(path_dict) == 0: return [] edge_path: WeightedPath = [] e: WeightedEdge = path_dict[end] edge_path.append(e) while e.u != start: e = path_dict[e.u] edge_path.append(e) return list(reversed(edge_path))if __name__ == "__main__": city_graph2: WeightedGraph[str] = WeightedGraph(["Seattle", "San Francisco", "Los Angeles", "Riverside", "Phoenix", "Chicago", "Boston", "New York", "Atlanta", "Miami", "Dallas", "Houston", "Detroit", "Philadelphia", "Washington"]) city_graph2.add_edge_by_vertices("Seattle", "Chicago", 1737) city_graph2.add_edge_by_vertices("Seattle", "San Francisco", 678) city_graph2.add_edge_by_vertices("San Francisco", "Riverside", 386) city_graph2.add_edge_by_vertices("San Francisco", "Los Angeles", 348) city_graph2.add_edge_by_vertices("Los Angeles", "Riverside", 50) city_graph2.add_edge_by_vertices("Los Angeles", "Phoenix", 357) city_graph2.add_edge_by_vertices("Riverside", "Phoenix", 307) city_graph2.add_edge_by_vertices("Riverside", "Chicago", 1704) city_graph2.add_edge_by_vertices("Phoenix", "Dallas", 887) city_graph2.add_edge_by_vertices("Phoenix", "Houston", 1015) city_graph2.add_edge_by_vertices("Dallas", "Chicago", 805) city_graph2.add_edge_by_vertices("Dallas", "Atlanta", 721) city_graph2.add_edge_by_vertices("Dallas", "Houston", 225) city_graph2.add_edge_by_vertices("Houston", "Atlanta", 702) city_graph2.add_edge_by_vertices("Houston", "Miami", 968) city_graph2.add_edge_by_vertices("Atlanta", "Chicago", 588) city_graph2.add_edge_by_vertices("Atlanta", "Washington", 543) city_graph2.add_edge_by_vertices("Atlanta", "Miami", 604) city_graph2.add_edge_by_vertices("Miami", "Washington", 923) city_graph2.add_edge_by_vertices("Chicago", "Detroit", 238) city_graph2.add_edge_by_vertices("Detroit", "Boston", 613) city_graph2.add_edge_by_vertices("Detroit", "Washington", 396) city_graph2.add_edge_by_vertices("Detroit", "New York", 482) city_graph2.add_edge_by_vertices("Boston", "New York", 190) city_graph2.add_edge_by_vertices("New York", "Philadelphia", 81) city_graph2.add_edge_by_vertices("Philadelphia", "Washington", 123) distances, path_dict = dijkstra(city_graph2, "Los Angeles") name_distance: Dict[str, Optional[int]] = distance_array_to_vertex_dict(city_graph2, distances) print("Distances from Los Angeles:") for key, value in name_distance.items(): print(f"&#123;key&#125; : &#123;value&#125;") print("") print("Shortest path from Los Angelges to Boston:") path: WeightedPath = path_dict_to_path(city_graph2.index_of("Los Angeles"), city_graph2.index_of("Boston"), path_dict) print_weighted_path(city_graph2, path) 运行结果：1234567891011121314151617181920212223Distances from Los Angeles:Seattle : 1026San Francisco : 348Los Angeles : 0Riverside : 50Phoenix : 357Chicago : 1754Boston : 2605New York : 2474Atlanta : 1965Miami : 2340Dallas : 1244Houston : 1372Detroit : 1992Philadelphia : 2511Washington : 2388Shortest path from Los Angelges to Boston:Los Angeles 50&gt; RiversideRiverside 1704&gt; ChicagoChicago 238&gt; DetroitDetroit 613&gt; BostonTotal Weight: 2605 参考资料Classic Computer Science Problems in Pythondavecom/ClassicComputerScienceProblemsInPython]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Program</tag>
        <tag>Advanced</tag>
        <tag>DataStructure</tag>
        <tag>Algorithm</tag>
        <tag>Graph</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基本算法问题的 Python 解法——约束满足问题（CSP）]]></title>
    <url>%2F2021%2F02%2F03%2Fclassic-compute-problems-with-python-constraint-satisfaction-problems%2F</url>
    <content type="text"><![CDATA[由计算工具解决的很大一部分问题都可以归类为约束满足问题（CSPs, constraint-satisfaction problems）。CSP 一般包含三个基本概念：变量（variables）、域（domains）和约束条件（constraints）。 比如需要在星期五为 Joe、Mary、Sue 三个人安排一场会议，要求 Sue 必须和另外的至少一个人同时在场。针对此问题： Joe、Mary、Sue 三个人即为变量（variables） 每个人（变量）各自空闲的时间点即为对应的域（domains）。比如变量 Mary 在下午 2 点和 3 点的时候有空，这两个时间点即为变量 Mary 对应的域 约束条件（constraints）有两点：Sue 必须在场；除 Sue 以外至少还需要另一人到场 构建 CSP 框架约束条件通过 Constraint 类实现。该类中包含被约束的变量以及测试其是否满足约束的 satisfied() 方法。确定是否满足约束条件是针对某个特定的 CSP 的核心逻辑，该 satisfied() 方法必须为抽象方法，由子类覆盖后发挥实际作用，以满足不同问题的不同约束条件。123456789101112131415# csp.pyfrom typing import Generic, TypeVar, Dict, List, Optionalfrom abc import ABC, abstractmethodV = TypeVar('V') # variable typeD = TypeVar('D') # domain typeclass Constraint(Generic[V, D], ABC): def __init__(self, variables: List[V]) -&gt; None: self.variables = variables @abstractmethod def satisfied(self, assignment: Dict[V, D]) -&gt; bool: pass 约束满足框架的核心部分代码是 CSP 类，该类集中处理变量、域和约束条件。CSP 的类型使用 Generic，目的是使其足够灵活，能够处理各种类型的 variables 和 domains。其中 variables 是 list 类型，domains 是由 variable 和对应的 list （所有可能的值）关联成的 dict 类型，constraints 则是由 variable 和对应的 list（约束条件列表）关联成的 dict 类型。 __init__() 初始化方法会创建 constraints 字典，将 variables 中的值作为键，每个键关联一个空列表。add_constraint() 方法遍历 variables 中的值（同时也是 constraints 中的键），将对应的 constraint 添加到 constraints 字典的该 variable 键关联的列表中。从而完成对 variables、domains、constraints 三类数据的初始化。1234567891011121314151617# csp.pyclass CSP(Generic[V, D]): def __init__(self, variables: List[V], domains: Dict[V, List[D]]) -&gt; None: self.variables = variables self.domains = domains self.constraints: Dict[V, List[Constraint[V, D]]] = &#123;&#125; for variable in self.variables: self.constraints[variable] = [] if variable not in self.domains: raise LookupError("Every variable should have a domain assigned to it") def add_constraint(self, constraint: Constraint[V, D]) -&gt; None: for variable in constraint.variables: if variable not in self.variables: raise LookupError("Variable in constraint not in CSP") else: self.constraints[variable].append(constraint) consistent() 方法用于检查给定的 variable 对应的每一个约束条件是否一一符合当前预设的方案。这个临时的方案用 assignment 表示。即先有某个 variable，然后为其选择对应的 domain 中的任意一个值作为临时的 assignment，再检查该 assignment 是否符合对应的 variable 关联的所有约束条件。123456# csp.pydef consistent(self, variable: V, assignment: Dict[V, D]) -&gt; bool: for constraint in self.constraints[variable]: if not constraint.satisfied(assignment): return False return True 约束满足框架还需要一个简单的 backtracking 搜索用于查找问题的解决方案。Backtracking 意味着一旦在搜索路径的某个节点上终止，则返回到上一个已知的搜索节点，选择另一条搜索路径。有点类似于深度优先搜索（DFS, depth-first search）。12345678910111213141516def backtracking_search(self, assignment: Dict[V, D] = &#123;&#125;) -&gt; Optional[Dict[V, D]]: # assignment is complete if every variable is assigned if len(assignment) == len(self.variables): return assignment # get all variables in the CSP but not in the assignment unassigned: List[V] = [v for v in self.variables if v not in assignment] first: V = unassigned[0] for value in self.domains[first]: local_assignment = assignment.copy() local_assignment[first] = value if self.consistent(first, local_assignment): result: Optional[Dict[V, D]] = self.backtracking_search(local_assignment) if result is not None: return result return None 逐条分析以上代码：12if len(assignment) == len(self.variables): return assignment 上面的 backtracking 搜索采用了递归的形式，此 if 语句则提供了一种递归的终止条件。即当所有 variable 都被赋予了合法的值时，意味着其中一种搭配方案已被找到，则停止进一步的搜索，返回该搭配方案。 12unassigned: List[V] = [v for v in self.variables if v not in assignment]first: V = unassigned[0] 取出 variables 中第一个还未被赋值（未做选择）的 variable，作为下一步中进行赋值（做决定）和约束条件测试的对象。 1234if self.consistent(first, local_assignment): result: Optional[Dict[V, D]] = self.backtracking_search(local_assignment) if result is not None: return result 为前面未赋值的某个 variable “做决定”。将对应的 domain 中所有存在的值依次赋值给该 variable，形成一个新的方案（local_assignment）。若该方案符合所有的约束条件（通过 consistent() 方法检测），则借助递归进行下一轮对另一个 variable 的赋值，直到触发终止条件（所有 variable 都被赋值）。 return None 若针对某个特定的 variable，已经检查完 domain 中包含的所有可能的值，仍没有找到符合要求的方案，则返回 None 表示没有解决。这会导致 backtracking 搜索结束本轮 for 循环，返回到递归的上一层中的 for 循环，尝试为上一步中已赋值的 variable 做出不同的决定，并继续递归（或回溯）下去。 地图上色问题假如有一张澳大利亚地图，需要按州进行上色。要求所有相邻的州不能有相同的颜色。 借助前面构建的约束符合框架，实现代码如下：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647# map_coloring.pyfrom csp import Constraint, CSPfrom typing import Dict, List, Optionalclass MapColoringConstraint(Constraint[str, str]): def __init__(self, place1: str, place2: str) -&gt; None: super().__init__([place1, place2]) self.place1 = place1 self.place2 = place2 def satisfied(self, assignment: Dict[str, str]) -&gt; bool: # if either place is not in the assignment, then it is not # yet possible for their colors to be conflicting if self.place1 not in assignment or self.place2 not in assignment: return True # check the color assigned to place1 is not the same as the # color assigned to place2 return assignment[self.place1] != assignment[self.place2]if __name__ == "__main__": variables: List[str] = ["Western Australia", "Northern Territory", "South Australia", "Queensland", "New South Wales", "Victoria", "Tasmania"] domains: Dict[str, List[str]] = &#123;&#125; for variable in variables: domains[variable] = ["red", "green", "blue"] csp: CSP[str, str] = CSP(variables, domains) csp.add_constraint(MapColoringConstraint("Western Australia", "Northern Territory")) csp.add_constraint(MapColoringConstraint("Western Australia", "South Australia")) csp.add_constraint(MapColoringConstraint("South Australia", "Northern Territory")) csp.add_constraint(MapColoringConstraint("Queensland", "Northern Territory")) csp.add_constraint(MapColoringConstraint("Queensland", "South Australia")) csp.add_constraint(MapColoringConstraint("Queensland", "New South Wales")) csp.add_constraint(MapColoringConstraint("New South Wales", "South Australia")) csp.add_constraint(MapColoringConstraint("Victoria", "South Australia")) csp.add_constraint(MapColoringConstraint("Victoria", "New South Wales")) csp.add_constraint(MapColoringConstraint("Victoria", "Tasmania")) solution: Optional[Dict[str, str]] = csp.backtracking_search() if solution is None: print("No solution found") else: print(solution)# =&gt; &#123;'Western Australia': 'red', 'Northern Territory': 'green', 'South# Australia': 'blue', 'Queensland': 'red', 'New South Wales': 'green',# 'Victoria': 'red', 'Tasmania': 'green'&#125; 简单梳理一下程序逻辑： 在上述 CSP 中，地图中的 7 个州即为 variables（为方便，以 a、b、c、d、e、f、g 代替）variables = [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;, &#39;e&#39;, &#39;f&#39;, &#39;g&#39;] 每个州都可以涂成红绿蓝三种颜色（假设用 1、2、3 指代）中的任何一种，各 variable 对应的所有颜色即组成对应 variable 的 domain：domains = {&#39;a&#39;: [1, 2, 3], &#39;b&#39;: [1, 2, 3], &#39;c&#39;: [1, 2, 3], &#39;d&#39;: [1, 2, 3], &#39;e&#39;: [1, 2, 3], &#39;f&#39;: [1, 2, 3], &#39;g&#39;: [1, 2, 3]} constraints 的逻辑在 MapColoringConstraints 类中实现，即已经涂色的相邻的两个州色彩须不一致。比如 a 与 b 相邻，则该 constraint 的表示如下：MapColoringConstraint(&#39;a&#39;, &#39;b&#39;)而所有的 constraints 都会关联到对应的以 variable 为键的字典中。即若 a 同时与 b 和 c 相邻，则变量 a 的 constraints 表示为：{&#39;a&#39;: [MapColoringConstraint(&#39;a&#39;, &#39;b&#39;), MapColoringConstraint(&#39;a&#39;, &#39;c&#39;)]} backtrack_search() 方法的执行流程为： 在 variables 中取第一个未被赋值（涂色）的 variable，为其赋予对应 domain 中的某个数值作为临时方案 用该 variable 对应的所有 constraints 测试上述临时方案的可行性。若符合要求，则借助递归开启下一轮循环，继续为另一个未被赋值（涂色）的 variable 赋值 若不符合要求，则继续本轮循环，为本 variable 赋予 domain 中的另一个值 若对应 domain 中的所有值赋予 variable 后都不能符合约束要求，则返回 None。本轮循环结束，回到递归的上一轮继续循环，为上一轮中已赋值的 variable 赋予不同的值，延续递归操作 若所有 variable 都已被赋值，则返回 variable 及其对应的值作为最终的解决方案；若所有循环（递归/回溯）结束，返回结果最终为 None，则表示无法找到合理的解决方案 国际象棋的八王后问题国际象棋的棋盘由 8x8 的方格组成，棋子落于方格上。而棋子王后能够吃掉处于同一行、同一列、同一斜线上的任何一个敌方棋子。八王后问题是指需要将 8 个王后放置到国际象棋棋盘上且彼此之间不会产生冲突（即不会有任意两枚棋子位于同一行、同一列或者同一斜线上）。 其中一种可能的解决方案如下图： 实现代码如下：12345678910111213141516171819202122232425262728293031323334353637from csp import Constraint, CSPfrom typing import Dict, List, Optionalclass QueensConstraint(Constraint[int, int]): def __init__(self, columns: List[int]) -&gt; None: super().__init__(columns) self.columns = columns def satisfied(self, assignment: Dict[int, int]) -&gt; bool: # q1c = queen 1 column, q1r = queen 1 row for q1c, q1r in assignment.items(): # q2c = queen 2 column for q2c in range(q1c + 1, len(self.columns) + 1): if q2c in assignment: q2r = assignment[q2c] if q1r == q2r: # same row? return False if abs(q1r - q2r) == abs(q1c - q2c): # same diagonal? return False return Trueif __name__ == "__main__": columns: List[int] = [1, 2, 3, 4, 5, 6, 7, 8] rows: Dict[int, List[int]] = &#123;&#125; for column in columns: rows[column] = [1, 2, 3, 4, 5, 6, 7, 8] csp: CSP[int, int] = CSP(columns, rows) csp.add_constraint(QueensConstraint(columns)) solution: Optional[Dict[int, int]] = csp.backtracking_search() if solution is None: print("No solution found!") else: print(solution)# =&gt; &#123;1: 1, 2: 5, 3: 8, 4: 6, 5: 3, 6: 7, 7: 2, 8: 4&#125; 简单解释下 satisfied() 方法中的两个 for 循环。assignment 采用类似 {1: 1, 2: 5, 3: 8, 4: 6, 5: 3, 6: 7, 7: 2, 8: 4} 的字典类型（一开始会短一些），上述两个 for 循环的作用在于，先以棋盘上的第一列为标准，若第一列与剩余的几列不存在冲突，则去掉第一列，再比较第二列与剩余的几列是否存在冲突，以此类推。一旦出现任何冲突，则返回 False。 参考资料Classic Computer Science Problems in Pythondavecom/ClassicComputerScienceProblemsInPython]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Program</tag>
        <tag>Advanced</tag>
        <tag>DataStructure</tag>
        <tag>Algorithm</tag>
        <tag>CSP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 设计模式——反模式]]></title>
    <url>%2F2021%2F02%2F03%2Fpython-design-patterns-anti-pattern%2F</url>
    <content type="text"><![CDATA[软件设计模式提供了一套规则或标准，能够帮助开发人员在设计层面进行决策。不良的设计主要表现在四个方面： 不动性：开发的应用程序非常难以重用 刚性：任何小的更改需求都会导致软件的太多部分必须进行相应的改动，牵一发而动全身 脆弱性：应用程序的任何更改都会导致现有系统变得非常容易崩溃 粘滞性：由于架构层面的修改非常困难，修改必须由开发人员在代码层面或环境本身中进行 软件开发反模式在软件开发过程中，往往会偏离最初的代码结构，原因一般有： 开发人员的想法会随着开发过程的推进而发生变化 用例通常会随着客户的反馈而进行更改 最初设计的数据结构可能会随功能或可伸缩性等方面的考虑而发生变化 基于上述原因，软件通常需要进行重构。 意大利面条式代码 典型成因包括： 对面向对象编程和分析的无知 没有考虑产品的架构或设计 快餐式思维 问题： 结构的重用性会降到最低 维护工作量过高 进行修改时，扩展性和灵活性会降低 金锤 金锤的意思是一把锤子搞定所有的钉子（解决所有问题）。软件开发人员或团队通常会有一种倾向，一头扎进一个成熟的解决方案，而不管其是否满足适用性。 典型成因： 来自不了解具体问题的高层的建议 某解决方案在过去多次验证有效，但当前项目有不同的背景和要求 公司已被这种技术“绑架”，或员工们因为顺手对这种技术情有独钟 金锤的影响： 痴迷于一个解决方案，并把它应用于所有软件项目 不是通过功能，而是通过开发中使用的技术来描述产品 没有满足需求，造成与用户的预期不符 熔岩流 熔岩流与软件应用中的死代码或一段用不到的代码有关，人们害怕一旦对其进行修改就会破坏其他东西。随着时间的流逝，这段代码会一直留在软件中并固化其位置，就像熔岩变成硬岩一样。 熔岩流的成因： 在产品中有大量的试错代码 由一个人单独编写的代码，未经审查的情况下移交给了其他开发团队 软件架构或设计的初始思想是通过代码库实现的，但没有人能理解 熔岩流的症状： 开发的测试工作具有很低的代码覆盖率 代码中含有莫名其妙的注释 过时的接口，或开发人员需要围绕既有代码展开工作 复制粘贴式编程 原因： 新手开发者不习惯编写代码或不知道如何开发 快速修复 bug 或急就章式的开发 代码重复，无法满足跨模块标准化以及代码结构化的要求 缺乏长远打算或深谋远虑 后果： 多个软件应用存在同种类型的问题 维护成本更高，bug 的生命周期也会变得更长 较少的模块化代码库，相同的代码会散落于多处 继承问题 软件架构反模式重新发明轮子 原因： 缺乏中央文档或存储库来讲解架构级问题和存放已实现的解决方案 社区或公司内的技术领袖之间缺乏沟通 组织中遵循的流程是从头开始构建的 后果： 解决一个标准问题的方案太多，其中有许多考虑得并不周全 会耗费工程团队更多的时间和资源，导致预算超标，完成时间延后 封闭的系统架构、重复劳动和糟糕的风险管理 供应商套牢 原因： 熟悉供应商公司的权威人士以及技术采购的可能折扣 基于营销和销售业务人员而不是技术评估选择的技术 在当前项目中使用经过验证的技术，即使它不适合当前项目的需要 技术人员已经接受过相关技术的培训 后果： 公司产品的发布周期和维护周期直接取决于供应商的发布时间 该产品是围绕该技术而不是根据客户的要求开放的 产品上市时间不可靠，不能满足客户的期望 委员会设计 原因： 根据组织的流程，产品的架构或设计是由众多的利益相关者批准的 没有指定单独的联系人或负责设计的架构师 由营销或技术专家确定设计优先级，而不是客户反馈 症状： 开发人员和架构师之间的观点冲突，即使在设计完成后依旧如此 过于复杂的设计，很难记录 规格或设计的任何改动都需要经过多次审查，导致实现延迟]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Program</tag>
        <tag>OOP</tag>
        <tag>Design</tag>
        <tag>Advanced</tag>
        <tag>Pattern</tag>
        <tag>Development</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 设计模式——模板方法模式]]></title>
    <url>%2F2020%2F12%2F29%2Fpython-design-patterns-template-pattern%2F</url>
    <content type="text"><![CDATA[行为模式主要关注对象的响应性，处理对象之间的交互以实现更强大的功能。模板方法模式即为一种行为设计模式。比如可以将制作饮料的步骤定义为模板方法中的算法，子类就能使用模板方法来实现沏茶的步骤。且步骤的改变（即子类的具体实现）并不会影响原始算法的结构。这样模板方法模式中的子类就可以通过覆盖来创建不同的行为。 模板方法模式适用于以下场景： 当多个算法或类实现类似或相同逻辑的时候 在子类中实现算法有助于减少重复代码的时候 子类可以通过覆盖实现多种不同行为的时候 模板方法模式的主要意图： 使用基本操作定义算法的框架 重新定义子类的某些操作，无需修改算法的结构 实现代码重用并避免重复工作 利用通用接口或功能实现 AbstractClass：在抽象方法的帮助下定义算法的操作或步骤。这些步骤将被具体的子类覆盖 template_method()：定义算法的框架。在模板方法中调用抽象方法定义的步骤以形成序列或算法 ConcreteClass：实现需要算法子类关注的特定步骤 12345678910111213141516171819202122232425262728293031323334353637from abc import ABCMeta, abstractmethodclass Compiler(metaclass=ABCMeta): @abstractmethod def collectSource(self): pass @abstractmethod def compileToObject(self): pass @abstractmethod def run(self): pass def compileAndRun(self): self.collectSource() self.compileToObject() self.run()class iOSCompiler(Compiler): def collectSource(self): print("Collecting Swift Source Code") def compileToObject(self): print("Compiling Swift code to LLVM bitcode") def run(self): print("Program runing on runtime environment")iOS = iOSCompiler()iOS.compileAndRun()# =&gt; Collecting Swift Source Code# =&gt; Compiling Swift code to LLVM bitcode# =&gt; Program runing on runtime environment 现实中的模板方法模式123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384from abc import abstractmethod, ABCMetaclass Trip(metaclass=ABCMeta): @abstractmethod def setTransport(self): pass @abstractmethod def day1(self): pass @abstractmethod def day2(self): pass @abstractmethod def day3(self): pass @abstractmethod def returnHome(self): pass def itinerary(self): self.setTransport() self.day1() self.day2() self.day3() self.returnHome()class VeniceTrip(Trip): def setTransport(self): print("Take a boat and find your way in the Grand Canal") def day1(self): print("Visit St Mark's Basilica in St Mark's Square") def day2(self): print("Appreciate Doge's Palace") def day3(self): print("Enjoy the food near the Rialto Bridge") def returnHome(self): print("Get souovenirs for friends and get back")class MaldivesTrip(Trip): def setTransport(self): print("On foot, on any island, Wow!") def day1(self): print("Enjoy the marine life of Banana Reef") def day2(self): print("Go for the water sports and snorkelling") def day3(self): print("Relax on the beach and enjoy the sun") def returnHome(self): print("Don't feel like leaving the beach..")class TravelAgency: def arrange_trip(self): choice = input("What kind of place you'd like to go historical or to a beach? ") if choice == 'historical': self.trip = VeniceTrip() self.trip.itinerary() if choice == 'beach': self.trip = MaldivesTrip() self.trip.itinerary()TravelAgency().arrange_trip()# =&gt; What kind of place you'd like to go historical or to a beach? beach# =&gt; On foot, on any island, Wow!# =&gt; Enjoy the marine life of Banana Reef# =&gt; Go for the water sports and snorkelling# =&gt; Relax on the beach and enjoy the sun# =&gt; Don't feel like leaving the beach.. 抽象类 Trip 是一个接口，定义了不同日子使用的交通方式和参观地点等细节 setTransport 是一个抽象方法，由 ConcreteClass 实现，作用是设置交通方式 day1()、day2()、day3() 抽象方法定义了特定日期所参观的地点 itinerary() 模板方法则用于创建完整的行程 VeniceTrip 类和 MaldivesTrip 类是 Trip 接口的具体实现 模板方法的优点和缺点优点： 没有代码重复 使用继承而不是合成，只有为数不多的几个方法需要重写 灵活性，允许子类决定如何实现算法中的步骤 缺点： 调试和理解模板方法模式中的流程序列时可能会令人困惑 模板框架的维护可能是一个问题，任何层次（底层或高层）的变更都可能对实现造成干扰]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Program</tag>
        <tag>OOP</tag>
        <tag>Design</tag>
        <tag>Advanced</tag>
        <tag>Pattern</tag>
        <tag>Development</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 设计模式——观察者模式]]></title>
    <url>%2F2020%2F12%2F29%2Fpython-design-patterns-observer-pattern%2F</url>
    <content type="text"><![CDATA[行为型模式中的观察者模式创建型模式（比如单例模式）是基于对象的创建机制的，这些模式隔离了对象的创建细节，使得实现这些细节的代码能够与要创建的对象类型相互独立。结构型模式（比如门面模式）用于设计对象和类的结构，使得它们能够相互协作以形成更大的结构。重点是结构的简化以及识别类和对象之间的关系。行为型模式（比如观察者模式）主要关注的是对象的责任，处理对象之间的交互，以实现更复杂的功能。对象之间应该可以彼此交互，且应该是松耦合的。 在观察者设计模式中，对象（主题）维护了一个依赖（观察者）列表，以便主题可以使用观察者定义的任何方法通知所有观察者它所发生的变化。 在分布式应用中，多个服务通常是通过彼此交互来实现更大型的操作的。服务可以执行多种操作，但它们执行的操作会直接或很大程度上取决于其交互的服务对象的状态。比如用户注册的示例，其中用户服务负责响应用户在网站上的各种操作。假设有另一个电子邮件的服务，其作用是监视用户的状态并向用户发送电子邮件。若用户刚刚注册，则用户服务将调用电子邮件服务的方法，向用户发送邮件进行账户验证。若账户经过了验证，但信用度较低，则电子邮件服务将监视用户服务并向用户发送信用度过低的电子邮件警报。 因此，若应用中存在一个许多其他服务所依赖的核心服务，该核心服务就会成为观察者必须观察/监视变化的主题。当主题发生变化时，观察者应该改变自身对象状态，或者采取某些动作。从属服务监视核心服务的状态变化是观察者设计模式的经典情境。 观察者模式的主要目标 定义了对象之间的一对多依赖关系，使得对象中的任何更改都将自动通知给其他依赖对象 封装了主题的核心组件 观察者模式的基本实现 12345678910111213141516171819202122232425262728293031323334class Subject: def __init__(self): self.__observers = [] def register(self, observer): self.__observers.append(observer) def notifyAll(self, *args, **kwargs): for observer in self.__observers: observer.notify(subject, *args, **kwargs)class Observer1: def __init__(self, subject): subject.register(self) def notify(self, subject, *args): print(type(self).__name__, ':: Got', args, 'From', subject)class Observer2: def __init__(self, subject): subject.register(self) def notify(self, subject, *args): print(type(self).__name__, ':: Got', args, 'From', subject)subject = Subject()observer1 = Observer1(subject)observer2 = Observer2(subject)subject.notifyAll('notification')# =&gt; Observer1 :: Got ('notification',) From &lt;__main__.Subject object at 0x7f9a1276fa60&gt;# =&gt; Observer2 :: Got ('notification',) From &lt;__main__.Subject object at 0x7f9a1276fa60&gt; 现实中的观察者模式12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485class NewsPublisher: def __init__(self): self.__subscribers = [] self.__latestNews = None def attach(self, subscriber): self.__subscribers.append(subscriber) def detach(self): return self.__subscribers.pop() def subscribers(self): return [type(x).__name__ for x in self.__subscribers] def notifySubscribers(self): for sub in self.__subscribers: sub.update() def addNews(self, news): self.__latestNews = news def getNews(self): return "Got News: " + self.__latestNewsfrom abc import ABCMeta, abstractmethodclass Subscriber(metaclass=ABCMeta): @abstractmethod def update(self): passclass SMSSubscriber(Subscriber): def __init__(self, publisher): self.publisher = publisher self.publisher.attach(self) def update(self): print(type(self).__name__, self.publisher.getNews())class EmailSubscriber(Subscriber): def __init__(self, publisher): self.publisher = publisher self.publisher.attach(self) def update(self): print(type(self).__name__, self.publisher.getNews())class AnyOtherSubscriber: def __init__(self, publisher): self.publisher = publisher self.publisher.attach(self) def update(self): print(type(self).__name__, self.publisher.getNews())if __name__ == '__main__': news_publisher = NewsPublisher() for Subscriber in [SMSSubscriber, EmailSubscriber, AnyOtherSubscriber]: Subscriber(news_publisher) print("\nSubscribers: ", news_publisher.subscribers()) news_publisher.addNews('Hello World') news_publisher.notifySubscribers() print("\nDetached: ", type(news_publisher.detach()).__name__) print("\nSubscribers: ", news_publisher.subscribers()) news_publisher.addNews('My second news') news_publisher.notifySubscribers()# =&gt; Subscribers: ['SMSSubscriber', 'EmailSubscriber', 'AnyOtherSubscriber']# =&gt; SMSSubscriber Got News: Hello World# =&gt; EmailSubscriber Got News: Hello World# =&gt; AnyOtherSubscriber Got News: Hello World# =&gt; Detached: AnyOtherSubscriber# =&gt; Subscribers: ['SMSSubscriber', 'EmailSubscriber']# =&gt; SMSSubscriber Got News: My second news# =&gt; EmailSubscriber Got News: My second news 松耦合与观察者模式松耦合架构的特性： 降低了一个元素内发生的更改可能对其他元素产生意外影响的风险 使得测试、维护和故障排除工作更加简单 系统可以轻松地分解为可定义的元素 观察者模式提供了一种实现主体和观察者松耦合的对象设计模式： 主题对观察者唯一的了解就是它实现的一个特定的接口 可以随时添加任意的新观察者 添加新观察者时，完全无需修改主题 观察者或主题没有绑定在一起，可以彼此独立使用。观察者可以在任何地方重复使用 主题或观察者中的变化不会相互影响 观察者模式的优缺点优点： 使得彼此交互的对象之间保持松耦合 使得可以在无需对主题或观察者进行任何修改的情况下高效地发送数据到其他对象 可以随时添加/删除观察者 缺点： 观察者接口必须由具体观察者实现，涉及继承，且无法进行组合 若实现不当的话，观察者可能会增加复杂性，导致性能降低 在软件应用中，通知有时是不可靠的，并导致竞争条件不一致]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Program</tag>
        <tag>OOP</tag>
        <tag>Design</tag>
        <tag>Advanced</tag>
        <tag>Pattern</tag>
        <tag>Development</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 设计模式——状态模式]]></title>
    <url>%2F2020%2F12%2F26%2Fpython-design-patterns-state-pattern%2F</url>
    <content type="text"><![CDATA[行为模式关注的是对象的响应性，它们通过对象之间的交互以实现更复杂的功能。状态模式是一种行为设计模式，在该模式中，一个对象可以基于其内部状态封装多个行为。比如根据收音机的基本状态（AM/FM），当调谐到 AM 或 FM 频道时，扫描频道的行为就会相应地发生动态的改变。 123456789101112131415161718192021222324252627282930313233343536373839from abc import abstractmethod, ABCMetaclass State(metaclass=ABCMeta): @abstractmethod def handle(self): passclass ConcreteStateB(State): def handle(self): print("ConcreteStateB")class ConcreteStateA(State): def handle(self): print("ConcreteStateA")class Context(State): def __init__(self): self.state = None def getState(self): return self.state def setState(self, state): self.state = state def handle(self): self.state.handle()context = Context()stateA = ConcreteStateA()stateB = ConcreteStateB()context.setState(stateA)context.handle()# =&gt; ConcreteStateA State：定义 Handle() 抽象方法的接口。需要通过 ConcreteState 类实现 ConcreteState：实现 Handle() 方法，可以根据状态变化定义执行的实际操作 Context：接收客户端请求，维护着对象当前状态的引用，以根据请求调用具体的行为 简单示例1234567891011121314151617181920212223242526272829303132333435363738394041from abc import abstractmethod, ABCMetaclass State(metaclass=ABCMeta): @abstractmethod def doThis(self): passclass StartState(State): def doThis(self): print("TV Switching ON...")class StopState(State): def doThis(self): print("TV Switching OFF...")class TVContext(State): def __init__(self): self.state = None def getState(self): return self.state def setState(self, state): self.state = state def doThis(self): self.state.doThis()context = TVContext()context.getState()start = StartState()stop = StopState()context.setState(stop)context.doThis()# =&gt; TV Switching OFF... 真实用例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960class ComputerState: name = "state" allowed = [] def switch(self, state): if state.name in self.allowed: print("current:", self, " =&gt; switching to", state.name) self.__class__ = state else: print("Current:", self, " =&gt; switching to", state.name, "not possible.") def __str__(self): return self.nameclass Off(ComputerState): name = "off" allowed = ['on']class On(ComputerState): name = "on" allowed = ['off', 'suspend', 'hibernate']class Suspend(ComputerState): name = "suspend" allowed = ['on']class Hibernate(ComputerState): name = "hibernate" allowed = ['on']class Computer: def __init__(self): self.state = Off() def change(self, state): self.state.switch(state)if __name__ == '__main__': comp = Computer() comp.change(On) comp.change(Off) comp.change(On) comp.change(Suspend) comp.change(Hibernate) comp.change(On) comp.change(Hibernate)# =&gt; current: off =&gt; switching to on# =&gt; current: on =&gt; switching to off# =&gt; current: off =&gt; switching to on# =&gt; current: on =&gt; switching to suspend# =&gt; Current: suspend =&gt; switching to hibernate not possible.# =&gt; current: suspend =&gt; switching to on# =&gt; current: on =&gt; switching to Hibernate 状态模式的优点 在状态设计模式中，对象的行为是其状态的函数结果，且行为在运行时依旧状态而改变。这消除了对 if/else 或 switch/case 条件逻辑的依赖 使用状态模式，实现多态行为是很方便的，并且易于添加状态来支持额外的行为 状态模式提高了聚合性，针对状态的行为被聚合到 ConcreteState 类中，放置在代码的同一个地方 状态模式不仅改善了扩展应用程序行为时的灵活性，且提高了代码的可维护性。一个 ConcreteState 类即对应一种行为 状态模式的缺点 类爆炸：由于每个状态都需要在 ConcreteState 中定义，可能导致创建太多功能较为单一的类。既增加了代码量，又使得状态机的结构更加难以审查 随着新行为的引入，Context 类需要进行相应的更新以处理每个行为，使得上下文行为更容易受到每个新行为的影响]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Program</tag>
        <tag>OOP</tag>
        <tag>Design</tag>
        <tag>Advanced</tag>
        <tag>Development</tag>
        <tag>Patterns</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 设计模式——命令模式]]></title>
    <url>%2F2020%2F12%2F26%2Fpython-design-patterns-command-pattern%2F</url>
    <content type="text"><![CDATA[命令模式是一种行为设计模式。可以用来实现比如安装软件时的安装向导功能，通常安装向导会通过多个步骤根据用户的选择了解用户的偏好。安装向导首先启动一个名为 Command 的对象，用于存储在向导的多个步骤中用户指定的选项。当用户在最后一个步骤中点击完成按钮时，Command 对象就会运行 execute() 方法，该方法会考察所有存储的选项并完成相应的安装过程。 命令模式通常包含以下术语： Command 对象了解 Receiver 对象的情况，并能调用其方法 调用者（Invoker）方法的参数值存储在 Command 对象中 调用者知道如何执行命令 客户端（Client）用来创建 Command 对象并设置其接收者 命令模式的主要意图： 将请求封装为对象 可用不同的请求对客户端进行参数化 允许将请求保存在队列中 提供面向对象的回调 命令模式的适用场景： 根据需要执行的操作对对象进行参数化 将操作添加到队列并在不同地点执行请求 创建一个结构根据较小的操作来完成高级操作 代码示例12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152from abc import ABCMeta, abstractmethodclass Order(metaclass=ABCMeta): @abstractmethod def execute(self): passclass BuyStockOrder(Order): def __init__(self, stock): self.stock = stock def execute(self): self.stock.buy()class SellStockOrder(Order): def __init__(self, stock): self.stock = stock def execute(self): self.stock.sell()class Agent: def __init__(self): self.__orderQueue = [] def placeOrder(self, order): self.__orderQueue.append(order) order.execute()class StockTrade: def buy(self): print("You will buy stocks") def sell(self): print("You will sell stocks")if __name__ == '__main__': stock = StockTrade() buyStock = BuyStockOrder(stock) sellStock = SellStockOrder(stock) agent = Agent() agent.placeOrder(buyStock) agent.placeOrder(sellStock)# =&gt; You will buy stocks# =&gt; You will sell stocks Order 类 -&gt; Command 对象 BuyStockOrder 和 SellStockOrder 类 -&gt; ConcreteCommand 对象，为交易系统定义适当的操作 StockTrade 类 -&gt; Receiver 对象，定义了多个方法（动作）可以被 ConcreteCommand 调用以买入或卖出股票 Agent 类 -&gt; Invoker 对象，作为客户端和 StockTrade 的中介，执行客户下达的订单 命令模式的优点： 将调用操作的类与知道如何执行该操作的类解耦 借助队列系统，可以创建一系列命令 添加新命令更加容易，无需更改现有代码 可以使用命令模式定义回滚系统 命令模式的缺点： 为了实现目标，需要大量的类和对象进行协作 每个单独的命令都是一个 ConcreteCommand 类，增加了实现和维护的成本]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Program</tag>
        <tag>OOP</tag>
        <tag>Design</tag>
        <tag>Advanced</tag>
        <tag>Development</tag>
        <tag>Patterns</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 设计模式——门面模式]]></title>
    <url>%2F2020%2F12%2F25%2Fpython-design-patterns-facade-pattern%2F</url>
    <content type="text"><![CDATA[门面（facade）指建筑物的表面，尤其是最有吸引力的那一面。当人们从建筑物旁边经过时，可以欣赏其外部面貌，而不必了解其本身结构的复杂性。门面在隐藏内部复杂性的同时，也为客户端提供了一个可以轻松访问的接口。 比如需要到某个商店买东西，但对于该商店的布局并不清楚。可以直接找店主说明需要哪些东西，由店主将这些商品找到并提供给顾客。即店主作为购物的接口，顾客无需了解具体商品的位置。 门面设计模式的特点： 为子系统的一组接口提供一个统一的高级接口，帮助客户端以更简单的方式使用这些子系统 门面并不是封装子系统，而是对底层子系统进行组合。即用单个接口对象表示复杂的子系统 门面 一个接口，知道某个请求应该交由那个子系统处理 通过组合的方式将客户端的请求委派给相应的子系统对象 系统 实现子系统的功能，由一组负责不同任务的类来表示 处理门面对象分配的工作，但并不知道门面也不引用它 客户端 会实例化门面 会向门面提出请求 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990class EventManager: def __init__(self): print("Event Manager:: Let me talk to the folks\n") def arrange(self): self.hotelier = Hotelier() self.hotelier.bookHotel() self.florist = Florist() self.florist.setFlowerRequirements() self.caterer = Caterer() self.caterer.setCuisine() self.musician = Musician() self.musician.setMusicType()class Hotelier: def __init__(self): print("Arranging the Hotel for Marriage? --") def __isAvailable(self): print("Is the Hotel free for the event on given day?") return True def bookHotel(self): if self.__isAvailable(): print("Registered the Booking\n\n")class Florist: def __init__(self): print("Flower Decorations for the Event? --") def setFlowerRequirements(self): print("Carnations, Roses and Lilies would be used for Decorations\n\n")class Caterer: def __init__(self): print("Food Arrangements for the Event --") def setCuisine(self): print("Chinese &amp; Continental Cuisine to be served\n\n")class Musician: def __init__(self): print("Musical Arrangements for the Marriage --") def setMusicType(self): print("Jazz and Classical will be played\n\n")class You: def __init__(self): print("You:: Whoa! Marriage Arrangements??!!!") def asskEventManager(self): print("You:: Let's Contact the Event Manager\n\n") em = EventManager() em.arrange() def __del__(self): print("You:: Thanks to Event Manager, all preparations done!")you = You()you.asskEventManager()# =&gt; You:: Whoa! Marriage Arrangements??!!!# =&gt; You:: Let's Contact the Event Manager# =&gt; Event Manager:: Let me talk to the folks# =&gt; Arranging the Hotel for Marriage? --# =&gt; Is the Hotel free for the event on given day?# =&gt; Registered the Booking# =&gt; Flower Decorations for the Event? --# =&gt; Carnations, Roses and Lilies would be used for Decorations# =&gt; Food Arrangements for the Event --# =&gt; Chinese &amp; Continental Cuisine to be served# =&gt; Musical Arrangements for the Marriage --# =&gt; Jazz and Classical will be played# =&gt; You:: Thanks to Event Manager, all preparations done! 最少知识原则门面能够将客户端与实现具体功能的子系统解耦，其背后的设计原理即最少知识原则。 在设计系统时，对于创建的每个对象，都应该考察与之交互的类的数量，以及交互的方式 避免创建许多彼此紧密耦合的类。若类之间存在大量的依赖关系，系统就会变得难以维护，应坚决避免]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Program</tag>
        <tag>OOP</tag>
        <tag>Design</tag>
        <tag>Advanced</tag>
        <tag>Development</tag>
        <tag>Patterns</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 设计模式——代理模式]]></title>
    <url>%2F2020%2F12%2F25%2Fpython-design-patterns-proxy-pattern%2F</url>
    <content type="text"><![CDATA[代理通常是指一个介于寻求方和提供方之间的中介系统。寻求发是发出请求的一方，而提供方则是根据请求提供资源的一方。在设计模式中，代理通常是封装实际服务对象的装饰器或代理人，可以为其包装的对象提供附加功能同时无需改变对象本身的代码。其主要目的是为其他对象提供一个代理者或占位符，从而控制对实际对象的访问。 代理设计模式的主要工作： 为其他对象提供代理，实现对原始对象的访问控制 可以用作一个中间层或接口，以支持分布式访问 通过增加代理，保护真正的组件不受意外影响 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162# agent.pyfrom abc import ABCMeta, abstractmethodclass Payment(metaclass=ABCMeta): @abstractmethod def do_pay(self): passclass Bank(Payment): def __init__(self): self.card = None self.account = None def __getAccount(self): self.account = self.card return self.account def __hasFunds(self): print("Bank:: Checking if Account ", self.__getAccount(), " has enough funds") return True def setCard(self, card): self.card = card def do_pay(self): if self.__hasFunds(): print("Bank:: Paying the merhant") return True else: print("Bank:: Sorry, not enough funds") return Falseclass DebitCard(Payment): def __init__(self): self.bank = Bank() def do_pay(self): card = input("Proxy:: Punch in Card Number: ") self.bank.setCard(card) return self.bank.do_pay()class You: def __init__(self): print("You:: Let's buy the Denim shirt") self.debitCard = DebitCard() self.isPurchased = None def make_payment(self): self.isPurchased = self.debitCard.do_pay() def __del__(self): if self.isPurchased: print("You:: Denim shirt is Mine :-)") else: print("You:: I should earn more :(")you = You()you.make_payment() 123456$ python agent.pyYou:: Let's buy the Denim shirtProxy:: Punch in Card Number: 12345Bank:: Checking if Account 12345 has enough fundsBank:: Paying the merhantYou:: Denim shirt is Mine :-) 关于类 You（对应 UML 图中的 client）的解释： 该类用于实现客户端的行为 __init__() 会调用代理并将其实例化 make_payment() 方法表示购买动作，会在内部调用代理的付款方法 关于类 Bank（对应 UML 图中的 RealSubject）的解释： 该类实际完成从顾客账户向商家划账的动作（do_pay()） 该类提供了多个方法来处理有关付款的一系列逻辑（__getAccount()、__hasFunds()、do_pay() 等） 通过 setCard() 方法从代理处获取借记卡信息 关于 DebitCard 类（对应 UML 图中的 Proxy）的解释： 该类用于实现代理的行为，充当真实主题（银行）的代理 顾客需要付款时，无需跑去银行提款再回到商家支付，而是调用 DebitCard 的 do_pay() 方法 DebitCard 类在内部控制真实主题（Bank）的创建，并向银行提供借记卡的详细信息 Bank 对象在内部对账户进行检查并完成支付动作 代理模式的优点 可以通过缓存笨重的对象或频繁访问的对象来提高应用程序的性能 可以提供对于真实对象的访问授权 远程代理还便于与远程服务器进行交互，并监视系统 门面模式与代理模式的比较 代理模式 门面模式 为其他对象提供代理或占位符，以控制对原始对象的访问 为类的大型子系统提供了一个简单的接口 代理对象具有与目标对象相同的接口，并保存目标对象的引用 实现了子系统之间通信和依赖性的最小化 充当客户端和被封装的对象之间的中介 提供了单一的简单接口]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Program</tag>
        <tag>OOP</tag>
        <tag>Design</tag>
        <tag>Advanced</tag>
        <tag>Development</tag>
        <tag>Patterns</tag>
        <tag>Proxy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 设计模式——工厂模式]]></title>
    <url>%2F2020%2F12%2F22%2Fpython-design-patterns-factory-pattern%2F</url>
    <content type="text"><![CDATA[在面向对象编程中，工厂表示一个负责创建其他类型的对象的类。通常作为工厂的类会实现多个关联的方法，客户端通过某些参数调用这些方法，工厂则负责创建对应类型的对象并返回给客户端。 工厂模式的优点： 松耦合。对象的创建独立于类的实现 客户端无需了解创建对象的类，只需知道需要传递的接口、方法和参数即可。简化了客户端的实现 可以轻松地在工厂中添加其他类来创建其他类型的对象，无需更改客户端代码 工厂可以重用现有的对象 简单工厂模式123456789101112131415161718192021222324252627from abc import ABCMeta, abstractmethodclass Animal(metaclass=ABCMeta): @abstractmethod def do_say(self): passclass Dog(Animal): def do_say(self): print("Bhow Bhow")class Cat(Animal): def do_say(self): print("Meow Meow")class ForestFactory: def make_sound(self, object_type): return eval(object_type)().do_say()if __name__ == '__main__': ff = ForestFactory() animal = input("Which animal should make sound, Dog or Cat\n") ff.make_sound(animal) 12345678$ python make_sound.pyWhich animal should make sound, Dog or CatDogBhow Bhow$ python make_sound.pyWhich animal should make sound, Dog or CatCatMeow Meow 工厂方法模式工厂方法模式的特点： 定义一个接口来创建对象，但工厂本身并不负责创建动作，而是由其子类决定实例化哪些类 工厂方法的创建是通过继承而不是通过实例化来完成的 工厂方法使设计更具有定制性。可以返回相同的实例或子类 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162from abc import ABCMeta, abstractmethodclass Section(metaclass=ABCMeta): @abstractmethod def describe(self): passclass PersonalSection(Section): def describe(self): print("Personal Section")class AlbumSection(Section): def describe(self): print("Album Section")class PatentSection(Section): def describe(self): print("Patent Section")class PublicationSection(Section): def describe(self): print("Publication Section")class Profile(metaclass=ABCMeta): def __init__(self): self.sections = [] self.createProfile() @abstractmethod def createProfile(self): pass def getSections(self): return self.sections def addSections(self, section): self.sections.append(section)class linkedin(Profile): def createProfile(self): self.addSections(PersonalSection()) self.addSections(PatentSection()) self.addSections(PublicationSection())class facebook(Profile): def createProfile(self): self.addSections(PersonalSection()) self.addSections(AlbumSection())if __name__ == '__main__': profile_type = input("Which Profile you'd like to create?\n[LinkedIn or FaceBook] ") profile = eval(profile_type.lower())() print("Creating Profile...", type(profile).__name__) print("Profile has sections --", profile.getSections()) 12345$ python profile.pyWhich Profile you'd like to create?[LinkedIn or FaceBook] LinkedInCreating Profile... linkedinProfile has sections -- [&lt;__main__.PersonalSection object at 0x7f3d25e53c70&gt;, &lt;__main__.PatentSection object at 0x7f3d25e53ca0&gt;, &lt;__main__.PublicationSection object at 0x7f3d25e53df0&gt;] Profile 抽象类代表 Creator，提供了 createProfile() 工厂方法，用于创建带有适当板块的个人信息界面。但 Profile 并不清楚某个特定界面应该具有哪些板块，如 Facebook 需要提供个人信息板块和相册区。createProfile() 工厂方法实际是由 Profile 的子类去实现的。 两个 Profile 的子类 linkedin 和 facebook 代表 ConcreteCreator，每个类都实现了 createProfile 方法，该方法在运行时创建多个板块（ConcreteProducts）。 工厂方法模式的优点 强大的灵活性，代码更加通用。实现哪些类取决于接口（Product），而不是 ConcreteProduct 类 松耦合。创建对象的代码与使用对象的代码是分离的。客户端不需要关心传递哪些参数以及需要实例化哪些类 抽象工厂模式抽象工厂模式的主要目的是提供一个接口来创建一系列相关的对象，而无需指定具体的类。因此可以帮助客户端一次使用来自一个产品/系列的多个对象。比如正在开发的应用是平台无关的，则需要对不同平台下的各种依赖项（包括操作系统、文件系统调用等）进行抽象处理，由抽象工厂为各个平台创建所需的服务，客户端就不必直接创建平台对象了。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576from abc import ABCMeta, abstractmethodclass PizzaFactory(metaclass=ABCMeta): @abstractmethod def createVegPizza(self): pass @abstractmethod def createNonVegPizza(self): passclass IndianPizzaFactory(PizzaFactory): def createVegPizza(self): return DeluxVeggiePizza() def createNonVegPizza(self): return ChickenPizza()class USPizzaFactory(PizzaFactory): def createVegPizza(self): return MexicanVegPizza() def createNonVegPizza(self): return HamPizza()class VegPizza(metaclass=ABCMeta): @abstractmethod def prepare(self, VegPizza): passclass NonVegPizza(metaclass=ABCMeta): @abstractmethod def serve(self, VegPizza): passclass DeluxVeggiePizza(VegPizza): def prepare(self): print("Prepare ", type(self).__name__)class ChickenPizza(NonVegPizza): def serve(self, VegPizza): print(type(self).__name__, " is served with Chicken on ", type(VegPizza).__name__)class MexicanVegPizza(VegPizza): def prepare(self): print("Prepare ", type(self).__name__)class HamPizza(NonVegPizza): def serve(self, VegPizza): print(type(self).__name__, " is served with Ham on ", type(VegPizza).__name__)class PizzaStore: def makePizzas(self): for factory in [IndianPizzaFactory(), USPizzaFactory()]: self.factory = factory self.NonVegPizza = self.factory.createNonVegPizza() self.VegPizza = self.factory.createVegPizza() self.VegPizza.prepare() self.NonVegPizza.serve(self.VegPizza)pizza = PizzaStore()pizza.makePizzas()# =&gt; Prepare DeluxVeggiePizza# =&gt; ChickenPizza is served with Chicken on DeluxVeggiePizza# =&gt; Prepare MexicanVegPizza# =&gt; HamPizza is served with Ham on MexicanVegPizza 工厂方法与抽象工厂方法的比较 工厂方法 抽象工厂方法 向客户端开放了一个创建对象的方法 包含一个或多个工厂方法来创建一个系列的相关对象 使用继承和子类决定要创建哪个对象 使用组合将创建对象的任务委托给其他类 工厂方法用于创建一个产品 抽象工厂方法用于创建相关产品的系列]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Design</tag>
        <tag>Advanced</tag>
        <tag>Pattern</tag>
        <tag>Factory</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django models 详解之聚合查询（aggregate）与分组查询（annotate）]]></title>
    <url>%2F2020%2F12%2F22%2Fdjango-models-aggregate-and-annotate%2F</url>
    <content type="text"><![CDATA[一、测试代码及数据models.py 代码 12345678910111213from django.db import modelsclass Author(models.Model): name = models.CharField(max_length=100) age = models.IntegerField()class Book(models.Model): name = models.CharField(max_length=300) price = models.DecimalField(max_digits=10, decimal_places=2) authors = models.ManyToManyField(Author) pubdate = models.DateField() 测试数据 authors：12345678910111213141516171819202122[ &#123; "id": 1, "name": "路人甲", "age": 10 &#125;, &#123; "id": 2, "name": "路人乙", "age": 18 &#125;, &#123; "id": 3, "name": "路人丙", "age": 28 &#125;, &#123; "id": 4, "name": "路人丁", "age": 50 &#125;] books：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980[ &#123; "id": 1, "name": "人之初", "price": "38.80", "pubdate": "2020-12-01", "authors": [ 1 ] &#125;, &#123; "id": 2, "name": "性本善", "price": "28.40", "pubdate": "2020-06-01", "authors": [ 2 ] &#125;, &#123; "id": 3, "name": "性相近", "price": "15.20", "pubdate": "2019-10-01", "authors": [ 3 ] &#125;, &#123; "id": 4, "name": "习相远", "price": "35.20", "pubdate": "2019-07-01", "authors": [ 4 ] &#125;, &#123; "id": 5, "name": "苟不教", "price": "5.20", "pubdate": "2018-07-01", "authors": [ 1, 3, 4 ] &#125;, &#123; "id": 6, "name": "性乃迁", "price": "55.20", "pubdate": "2018-12-01", "authors": [ 2, 3, 4 ] &#125;, &#123; "id": 7, "name": "教之道", "price": "33.20", "pubdate": "2018-12-23", "authors": [ 2, 3 ] &#125;, &#123; "id": 8, "name": "贵以专", "price": "27.20", "pubdate": "2017-12-23", "authors": [ 1, 4 ] &#125;] 二、常用聚合操作获取所有书籍的数量：12&gt;&gt;&gt; Book.objects.count()8 获取由路人甲参与著作的所有书籍的数量：12&gt;&gt;&gt; Book.objects.filter(authors__name__contains='路人甲').count()3 获取所有书籍的平均价格：12&gt;&gt;&gt; Book.objects.all().aggregate(Avg('price'))&#123;'price__avg': Decimal('29.800000')&#125; 获取所有书籍中的最高价格：12&gt;&gt;&gt; Book.objects.all().aggregate(Max('price'))&#123;'price__max': Decimal('55.20')&#125; 涉及到一对多或多对多关系的聚合查询计算每一位作者各自参与写作了多少本书：12345678&gt;&gt;&gt; from django.db.models import Count&gt;&gt;&gt; authors=Author.objects.annotate(num_books=Count('book'))&gt;&gt;&gt; authors&lt;QuerySet [&lt;Author: Author object (1)&gt;, &lt;Author: Author object (2)&gt;, &lt;Author: Author object (3)&gt;, &lt;Author: Author object (4)&gt;]&gt;&gt;&gt;&gt; authors[0].num_books3&gt;&gt;&gt; authors.values_list('name', 'num_books')&lt;QuerySet [('路人甲', 3), ('路人乙', 3), ('路人丙', 4), ('路人丁', 4)]&gt; 即作者包含路人甲的书籍有3本，以此类推。 计算每一位作者各自参与写作的书籍数量，根据书籍出版年份是否在2020年以前分界：12345678910&gt;&gt;&gt; from django.db.models import Q&gt;&gt;&gt; before_2020 = Count('book', filter=Q(book__pubdate__lt='2020-01-01'))&gt;&gt;&gt; after_2020 = Count('book', filter=Q(book__pubdate__gt='2020-01-01'))&gt;&gt;&gt; authors = Author.objects.annotate(before_2020=before_2020).annotate(after_2020=after_2020)&gt;&gt;&gt; authors&lt;QuerySet [&lt;Author: Author object (1)&gt;, &lt;Author: Author object (2)&gt;, &lt;Author: Author object (3)&gt;, &lt;Author: Author object (4)&gt;]&gt;&gt;&gt;&gt; authors[0].before_20202&gt;&gt;&gt; authors.values_list('name', 'before_2020', 'after_2020')&lt;QuerySet [('路人甲', 2, 1), ('路人乙', 2, 1), ('路人丙', 4, 0), ('路人丁', 4, 0)]&gt; 即作者包含路人甲的书籍，2020年以前出版的有2本，2020年以后出版的有1本。以此类推。 获取每一位作者各自参与著作的书籍数量，将输出结果按书籍数量由大到小的顺序排序：12345&gt;&gt;&gt; authors = Author.objects.annotate(num_books=Count('book')).order_by('-num_books')&gt;&gt;&gt; authors&lt;QuerySet [&lt;Author: Author object (3)&gt;, &lt;Author: Author object (4)&gt;, &lt;Author: Author object (1)&gt;, &lt;Author: Author object (2)&gt;]&gt;&gt;&gt;&gt; authors.values_list('name', 'num_books')&lt;QuerySet [('路人丙', 4), ('路人丁', 4), ('路人甲', 3), ('路人乙', 3)]&gt; 三、aggregate在聚合查询中，Django 支持通过 aggregate() 方法从整个 QuerySet 中计算出一个汇总数据。如获取所有书籍的平均价格：123&gt;&gt;&gt; from django.db.models import Avg&gt;&gt;&gt; Book.objects.all().aggregate(Avg('price'))&#123;'price__avg': Decimal('29.800000')&#125; 上述语句中的 all() 可以省略。aggregate() 的参数表示我们想要做聚合计算的那一列数据，其中的 &#39;price&#39; 即表示 Book 模型的 price 字段。 aggregate() 对于 QuerySet 来说是一种终止语句，会返回字典形式的键值对作为计算结果。其中的键会根据聚合的字段自动生成，也可以手动指定：12&gt;&gt;&gt; Book.objects.all().aggregate(average_price=Avg('price'))&#123;'average_price': Decimal('29.800000')&#125; 如果想要同时完成多个聚合查询操作，可以为 aggregate() 添加多个参数：123&gt;&gt; from django.db.models import Avg, Max, Min&gt;&gt;&gt; Book.objects.aggregate(Avg('price'), Max('price'), Min('price'))&#123;'price__avg': Decimal('29.800000'), 'price__max': Decimal('55.20'), 'price__min': Decimal('5.20')&#125; 四、annotate借助 annotate() 方法，Django 可以从 QuerySet 的每一个对象中计算出对应的独立的汇总数据。比如想获得 Book 模型中每一本书的作者的数量：12345678&gt;&gt;&gt; from django.db.models import Count&gt;&gt;&gt; q = Book.objects.annotate(num_authors=Count('authors'))&gt;&gt;&gt; q&lt;QuerySet [&lt;Book: Book object (1)&gt;, &lt;Book: Book object (2)&gt;, &lt;Book: Book object (3)&gt;, &lt;Book: Book object (4)&gt;, &lt;Book: Book object (5)&gt;, &lt;Book: Book object (6)&gt;, &lt;Book: Book object (7)&gt;, &lt;Book: Book object (8)&gt;]&gt;&gt;&gt;&gt; q[0].num_authors1&gt;&gt;&gt; q.values_list('name', 'num_authors')&lt;QuerySet [('人之初', 1), ('性本善', 1), ('性相近', 1), ('习相远', 1), ('苟不教', 3), ('性乃迁', 3), ('教之道', 2), ('贵以专', 2)]&gt; 不同于 aggregate()，annotate() 对于 QuerySet 来说并不是终止语句，annotate() 方法的输出结果仍是 QuerySet 对象。该对象可以继续执行被 QuerySet 支持的任意操作，如 filter()、order_by() 等，甚至另一个 annotate()。 五、join &amp; aggregate某些情况下，你想要聚合的字段并不属于当前正在查询的模型，而是属于关联于当前模型的另一个模型。在对这些字段进行聚合查询时，Django 允许使用与 filter() 中相同的用于指定关联字段的双下划线语法。 比如想要获取每一位作者所著书籍的价格区间：1234&gt;&gt;&gt; from django.db.models import Max, Min&gt;&gt;&gt; authors = Author.objects.annotate(min_price=Min('book__price'), max_price=Max('book__price'))&gt;&gt;&gt; authors.values_list('name', 'min_price', 'max_price')&lt;QuerySet [('路人甲', Decimal('5.20'), Decimal('38.80')), ('路人乙', Decimal('28.40'), Decimal('55.20')), ('路人丙', Decimal('5.20'), Decimal('55.20')), ('路人丁', Decimal('5.20'), Decimal('55.20'))]&gt; 即作者为路人甲的书籍中，最低的价格为 5.20，最高的价格为 38.80。 六、filter() 或 order_by() 应用到 annotate()如查找所有多人合著（作者数量大于 1）的书籍列表：12345&gt;&gt;&gt; books = Book.objects.annotate(num_authors=Count('authors')).filter(num_authors__gt=1)&gt;&gt;&gt; books&lt;QuerySet [&lt;Book: Book object (5)&gt;, &lt;Book: Book object (6)&gt;, &lt;Book: Book object (7)&gt;, &lt;Book: Book object (8)&gt;]&gt;&gt;&gt;&gt; books.values_list('name', 'num_authors')&lt;QuerySet [('苟不教', 3), ('性乃迁', 3), ('教之道', 2), ('贵以专', 2)]&gt; 根据作者数量对全部书籍进行排序：12345&gt;&gt;&gt; books = Book.objects.annotate(num_authors=Count('authors')).order_by('num_authors')&gt;&gt;&gt; books&lt;QuerySet [&lt;Book: Book object (2)&gt;, &lt;Book: Book object (4)&gt;, &lt;Book: Book object (1)&gt;, &lt;Book: Book object (3)&gt;, &lt;Book: Book object (8)&gt;, &lt;Book: Book object (7)&gt;, &lt;Book: Book object (5)&gt;, &lt;Book: Book object (6)&gt;]&gt;&gt;&gt;&gt; books.values_list('name', 'num_authors')&lt;QuerySet [('性本善', 1), ('习相远', 1), ('人之初', 1), ('性相近', 1), ('教之道', 2), ('贵以专', 2), ('苟不教', 3), ('性乃迁', 3)]&gt; 参考资料Django 官方文档 —— Aggregation]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Web</tag>
        <tag>Django</tag>
        <tag>Models</tag>
        <tag>Aggregate</tag>
        <tag>Annotate</tag>
        <tag>Backend</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Vue.js 截获 Ctrl+S 组合键以及自动保存（提交）功能的实现]]></title>
    <url>%2F2020%2F12%2F14%2Fvue-js-add-auto-save-and-ctrl-s%2F</url>
    <content type="text"><![CDATA[默认情况下，Chrome 中按下 Ctrl+S 组合键会进入“保存网页”界面，并不会与网页中的具体内容做交互。 最近在做一个前端基于 Vue 的在线文档，希望网页中按下 Ctrl+S 组合件就能触发提交动作，将前端数据的改动存储到后端数据库中。并且不管用户是否操作，每隔特定时间也会自动提交文档的当前内容到后端，实现自动保存的功能。 示例代码如下：12345678910111213141516171819202122232425262728293031323334&lt;template&gt; &lt;button @click=&quot;save(&apos;button&apos;)&quot;&gt;保存&lt;/button&gt;&lt;/template&gt;&lt;script&gt;export default &#123; mounted() &#123; document.addEventListener(&apos;keydown&apos;, this.saveContent) this.timer = setInterval(() =&gt; &#123; this.save(&apos;timer&apos;) &#125;, 10 * 1000) &#125;, beforeDestroy() &#123; document.removeEventListener(&apos;keydown&apos;, this.saveContent) clearInterval(this.timer) &#125;, methods: &#123; save(type) &#123; console.log(`content saved by $&#123;type&#125;`) &#125;, saveContent(e) &#123; var key = window.event.keyCode ? window.event.keyCode : window.event.which if (key === 83 &amp;&amp; e.ctrlKey) &#123; this.save(&apos;hot key&apos;) e.preventDefault() &#125; &#125; &#125;&#125;&lt;/script&gt;]]></content>
      <categories>
        <category>JavaScript</category>
      </categories>
      <tags>
        <tag>Program</tag>
        <tag>Development</tag>
        <tag>Web</tag>
        <tag>Frontend</tag>
        <tag>JavaScript</tag>
        <tag>Vue.js</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django REST framework 使用 MongoDB 作为数据库后端]]></title>
    <url>%2F2020%2F11%2F27%2Fpython-django-rest-framework-and-mongodb%2F</url>
    <content type="text"><![CDATA[想写个前后端分离的项目，需要在数据库中存储非常复杂的 JSON 格式（包含多层嵌套）的数据，又不想将 JSON 数据转为文本后以 Text 的格式存到 Mysql 数据库中。 因此想尝试下文档型数据库 MongoDB，其用来存放数据的文档结构，本身就是非常类似 JSON 对象的 BSON（Binary JSON）。 但 Django 的官方版本目前还未支持 NoSQL 数据库（参考 FAQ），MongoDB 官方文档建议借助 Djongo 组件完成到原生 Django ORM 的对接。Djongo 实际上是一个 SQL 到 MongoDB 的翻译器。通过 Django 的 admin 应用可以向 MongoDB 中添加或修改文档，其他 Django 模块如 contrib、auth、session 等也可以在不做任何改动的情况下正常使用。 项目初始化安装需要用到的 Python 模块，初始化项目：1234$ pip install djongo djangorestframework$ django-admin startproject mongo_test$ cd mongo_test$ django-admin startapp blogs 修改项目配置文件（mongo_test/settings.py），添加数据库配置：12345678...DATABASES = &#123; 'default': &#123; 'ENGINE': 'djongo', 'NAME': 'mongo_test', &#125;&#125;... 数据库迁移，创建管理员账户，运行 WEB 服务：123$ python manage.py migrate$ python manage.py createsuperuser$ python manage.py runserver 0.0.0.0:8000 访问 http://127.0.0.1:8000/admin ，进入 Django 管理员后台，各部分功能使用正常： 此时访问 MongoDB 数据库，可以查询到存入的数据：123456789101112131415161718192021222324252627282930313233343536// mongo shell&gt; show dbsadmin 0.000GBapscheduler 0.000GBconfig 0.000GBlocal 0.000GBmongo_test 0.000GB&gt; use mongo_testswitched to db mongo_test&gt; show collections;__schema__auth_groupauth_group_permissionsauth_permissionauth_userauth_user_groupsauth_user_user_permissionsdjango_admin_logdjango_content_typedjango_migrationsdjango_session&gt; db.auth_user.find().pretty()&#123; &quot;_id&quot; : ObjectId(&quot;5fc0a6a4e7b96c382fa9ccd8&quot;), &quot;id&quot; : 1, &quot;password&quot; : &quot;pbkdf2_sha256$180000$XL0v3lLCM1RW$rnw4qzoTUtwgc5EoKfB4yaaVEu1jTid8yuBVl0Y6P5Q=&quot;, &quot;last_login&quot; : ISODate(&quot;2020-11-27T07:11:55.492Z&quot;), &quot;is_superuser&quot; : true, &quot;username&quot; : &quot;admin&quot;, &quot;first_name&quot; : &quot;&quot;, &quot;last_name&quot; : &quot;&quot;, &quot;email&quot; : &quot;&quot;, &quot;is_staff&quot; : true, &quot;is_active&quot; : true, &quot;date_joined&quot; : ISODate(&quot;2020-11-27T07:11:31.955Z&quot;)&#125; Django REST framework在配置文件 mongo_test/settings.py 中的 INSTALLED_APPS 配置项下添加 rest_framework 和 blogs 两个应用：123456789101112...INSTALLED_APPS = [ 'django.contrib.admin', 'django.contrib.auth', 'django.contrib.contenttypes', 'django.contrib.sessions', 'django.contrib.messages', 'django.contrib.staticfiles', 'rest_framework', 'blogs']... 数据库模型（Models）编辑 blogs/models.py 文件，创建数据库模型，内容如下：123456789from djongo import modelsclass Blog(models.Model): title = models.CharField(max_length=50) content = models.TextField() class Meta: db_table = 'mongo_blog' 序列化器（Serializers）创建 blogs/serializers.py 文件，内容如下：12345678from blogs.models import Blogfrom rest_framework.serializers import ModelSerializerclass BlogSerializer(ModelSerializer): class Meta: model = Blog fields = '__all__' 视图（Views）编辑 blogs/views.py 文件，内容如下：12345678from blogs.models import Blogfrom blogs.serializers import BlogSerializerfrom rest_framework.viewsets import ModelViewSetclass BlogViewSet(ModelViewSet): queryset = Blog.objects.all() serializer_class = BlogSerializer 路由（URLs）创建 blogs/urls.py 文件，内容如下：12345678910from django.urls import include, pathfrom rest_framework import routersfrom blogs import viewsrouter = routers.DefaultRouter()router.register(r'blog', views.BlogViewSet)urlpatterns = [ path('', include(router.urls))] 根路由编辑项目路由配置文件 mongo_test/urls.py，内容如下：1234567from django.contrib import adminfrom django.urls import path, includeurlpatterns = [ path('admin/', admin.site.urls), path('', include('blogs.urls')),] 访问 http://127.0.0.1/blog ，利用 POST 方法新增数据以测试 REST API 运行效果： 结果爆出 TypeError 错误（int() argument must be a string, a bytes-like object or a number, not &#39;ObjectId&#39;）： 重新访问 http://127.0.0.1:8000/blog ，发现新增的数据已添加到数据库中，只是 id 项为 null：1234567[ &#123; "id": null, "title": "Blog", "content": "This is a TEST Blog" &#125;] 导致基于 REST API 的 CRUD 操作都是不能正常执行的。 ObjectId实际上按照上述方式存入数据库的数据是以下格式：1234567// mongo shell&gt; db.mongo_blog.findOne()&#123; &quot;_id&quot; : ObjectId(&quot;5fc0ae2ea7795c8c4ddae815&quot;), &quot;title&quot; : &quot;Blog&quot;, &quot;content&quot; : &quot;This is a TEST Blog&quot;&#125; 修改数据库模型（blogs/models.py），令其包含 _id 字段：12345678910from djongo import modelsclass Blog(models.Model): _id = models.ObjectIdField() title = models.CharField(max_length=50) content = models.TextField() class Meta: db_table = 'mongo_blog' 刷新 http://127.0.0.1:8000/blog 页面，此时数据显示正常，也可以通过 POST 方法正常添加数据（_id 项留空，会自动生成）： Retrieve上述实现仍有部分问题，实际上只有新值数据（Create）和获取数据列表（List）能够正常运行。而 CRUD 中的 Retrieve、Update、Delete 都会报出 404 错误。即无法通过 _id 获取对应的数据对象。 比如访问 http://127.0.0.1:8000/blog/5fc0b18e60870125f0ed846d/ ： 原因是 MongoDB 中的 _id 是 OjbectId 类型，与 Django REST framework 用于检索的 _id 类型不一致，导致无法通过 _id 找到对应的对象。需要在中间做一步转换工作（将字符串形式的 _id 转换为 ObjectId 形式）。12345// mongo shell&gt; db.mongo_blog.find(&#123;&quot;_id&quot;: &quot;5fc0b18e60870125f0ed846d&quot;&#125;)&gt;&gt; db.mongo_blog.find(&#123;&quot;_id&quot;: ObjectId(&quot;5fc0b18e60870125f0ed846d&quot;)&#125;)&#123; &quot;_id&quot; : ObjectId(&quot;5fc0b18e60870125f0ed846d&quot;), &quot;title&quot; : &quot;Blog2&quot;, &quot;content&quot; : &quot;This is another Blog&quot; &#125; 查看 ModelViewSet 源代码通过查看 ModelViewSet 的源代码，发现后台对 Retrieve 操作的响应逻辑是由mixinx.RetrieveModelMixin 类实现的，其中获取某个特定对象的函数是 self.get_object()：12345678class RetrieveModelMixin: """ Retrieve a model instance. """ def retrieve(self, request, *args, **kwargs): instance = self.get_object() serializer = self.get_serializer(instance) return Response(serializer.data) 进一步查找，发现 get_object() 函数是在 generics.GenericAPIVie 类中实现的，其代码为：12345678910111213141516171819202122232425262728class GenericAPIView(views.APIView): def get_object(self): &quot;&quot;&quot; Returns the object the view is displaying. You may want to override this if you need to provide non-standard queryset lookups. Eg if objects are referenced using multiple keyword arguments in the url conf. &quot;&quot;&quot; queryset = self.filter_queryset(self.get_queryset()) # Perform the lookup filtering. lookup_url_kwarg = self.lookup_url_kwarg or self.lookup_field assert lookup_url_kwarg in self.kwargs, ( &apos;Expected view %s to be called with a URL keyword argument &apos; &apos;named &quot;%s&quot;. Fix your URL conf, or set the `.lookup_field` &apos; &apos;attribute on the view correctly.&apos; % (self.__class__.__name__, lookup_url_kwarg) ) filter_kwargs = &#123;self.lookup_field: self.kwargs[lookup_url_kwarg]&#125; obj = get_object_or_404(queryset, **filter_kwargs) # May raise a permission denied self.check_object_permissions(self.request, obj) return obj 其中最关键的两句为：12filter_kwargs = &#123;self.lookup_field: self.kwargs[lookup_url_kwarg]&#125;obj = get_object_or_404(queryset, **filter_kwargs) {self.lookup_field: self.kwargs[lookup_url_kwarg]} 决定了最终 MongoDB 会以怎样的方式和条件检索某个对象。 实现自己的 ModelViewSet综上，为了让 CURD 操作中的 URD 能够通过 _id（ObjectId）检索获取特定对象，可以实现自己的 ModelViewSet 类，重写 get_object() 方法。 新建 blogs/mongo_viewset.py 文件，内容如下：1234567891011121314151617181920212223242526272829from bson import ObjectIdfrom django.shortcuts import get_object_or_404from rest_framework.viewsets import ModelViewSetclass MongoModelViewSet(ModelViewSet): def get_object(self): queryset = self.filter_queryset(self.get_queryset()) # Perform the lookup filtering. lookup_url_kwarg = self.lookup_url_kwarg or self.lookup_field assert lookup_url_kwarg in self.kwargs, ( 'Expected view %s to be called with a URL keyword argument ' 'named "%s". Fix your URL conf, or set the `.lookup_field` ' 'attribute on the view correctly.' % (self.__class__.__name__, lookup_url_kwarg) ) if self.lookup_field == '_id': filter_kwargs = &#123;self.lookup_field: ObjectId(self.kwargs[self.lookup_field])&#125; else: filter_kwargs = &#123;self.lookup_field: self.kwargs[self.lookup_url_kwarg]&#125; obj = get_object_or_404(queryset, **filter_kwargs) # May raise a permission denied self.check_object_permissions(self.request, obj) return obj 最主要的改动即：1234if self.lookup_field == &apos;_id&apos;: filter_kwargs = &#123;self.lookup_field: ObjectId(self.kwargs[self.lookup_field])&#125;else: filter_kwargs = &#123;self.lookup_field: self.kwargs[self.lookup_url_kwarg]&#125; 视图代码 blogs/views.py 改为如下版本：123456789from blogs.models import Blogfrom blogs.serializers import BlogSerializerfrom blogs.mongo_viewset import MongoModelViewSetclass BlogViewSet(MongoModelViewSet): queryset = Blog.objects.all() serializer_class = BlogSerializer lookup_field = '_id' 此时访问 http://172.20.23.34:8000/blog/5fc0b18e60870125f0ed846d/ 即可正常显示，即能够通过 _id（ObjectId）获取对应的数据对象。 由此 CRUD 操作全部可以正常支持。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Development</tag>
        <tag>Web</tag>
        <tag>Django</tag>
        <tag>Backend</tag>
        <tag>REST</tag>
        <tag>MongoDB</tag>
        <tag>NoSQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 设计模式——单例模式]]></title>
    <url>%2F2020%2F11%2F27%2Fpython-design-patterns-singleton%2F</url>
    <content type="text"><![CDATA[单例模式即确保类有且只有一个特定类型的对象，并提供全局访问点。因此通常用于日志记录、数据库操作、打印机后台处理程序等。这些程序在运行过程中只生成一个实例，避免对同一资源产生相互冲突的请求。 特点： 确保类有且只有一个对象被创建 为唯一对象提供访问点，令其可被全局访问 控制共享资源的并行访问 经典单例模式1234567891011121314151617181920212223class Singleton(object): def __new__(cls, name): if not hasattr(cls, 'instance'): cls.instance = super().__new__(cls) return cls.instance def __init__(self, name): self.name = names1 = Singleton('Singleton1')print(s1)# =&gt; &lt;__main__.Singleton object at 0x7efc1b006220&gt;print(s1.name)# =&gt; Singleton1s2 = Singleton('Singleton2')print(s2)# =&gt; &lt;__main__.Singleton object at 0x7efc1b006220&gt;print(s2.name)# =&gt; Singleton2print(s1.name)# =&gt; Singleton2 在上面的代码中，通过定义 __new__ 方法控制对象的创建。方法 hasattr 则用于检查对象 cls 是否具有 instance 属性（即确认该类是否已经生成了一个对象）。若 instance 属性不存在，则使用 super().__new__() 方法创建新的实例；若 instance 属性存在，则分配已有的实例给变量。因此当 s2 = Singleton(&#39;Singleton2&#39;) 执行时，hasattr 发现对象实例已存在（s1），因此直接将已有的对象分配给 s2。s1 和 s2 实际是同一个对象实例。 Monostate（单态）模式Monostate 模式即类的所有实例对象共享相同的状态。123456789101112131415class Borg: __shared_state = &#123;&#125; def __init__(self): self.__dict__ = self.__shared_stateb = Borg()b1 = Borg()print(b is b1) # =&gt; Falseb.x = 4print(b.x) # =&gt; 4print(b1.x) # =&gt; 4b1.x = 6print(b1.x) # =&gt; 6print(b.x) # =&gt; 6 在上述代码中，通过将类变量 __shared_state 赋值给实例变量 __dict__（__dict__ 变量用于存储实例对象的属性等状态），使得类生成的所有对象实例都共享同一状态。即 b 和 b1 是 Borg 类创建的不同的实例对象，但用于保存实例状态的 b.__dict__ 和 b1.__dict__ 却是相同的（即都是 Borg.__shared_state）。因此 b 的属性 x 若发生改变，同样的变化也会体现到 b1 中。 也可以通过修改 __new__ 方法来实现 Borg 模式：1234567891011121314151617181920class Borg: __shared_state = &#123;&#125; def __new__(cls, name): obj = super().__new__(cls) obj.__dict__ = cls.__shared_state return obj def __init__(self, name): self.name = nameb1 = Borg('Borg1')print(b1.name) # =&gt; Borg1b2 = Borg('Borg2')print(b2.name) # =&gt; Borg2print(b1.name) # =&gt; Borg2b1.name = 'Borg'print(b1.name) # =&gt; Borgprint(b2.name) # =&gt; Borgprint(b1 is b2) # =&gt; False 通过元类实现单例模式1234567891011121314151617181920class MetaSingleton(type): def __init__(self, *args, **kwargs): self.__instance = None def __call__(self, *args, **kwargs): if not self.__instance: self.__instance = super().__call__(*args, **kwargs) return self.__instanceclass Logger(metaclass=MetaSingleton): passlogger1 = Logger()logger2 = Logger()print(logger1, logger2)# =&gt; &lt;__main__.Logger object at 0x7fac8af577c0&gt; &lt;__main__.Logger object at# 0x7fac8af577c0&gt;print(logger1 is logger2) # =&gt; True 单例模式的实际应用DB 操作1234567891011121314151617181920212223242526272829import sqlite3class MetaSingleton(type): def __init__(self, *args, **kwargs): self.__instance = None def __call__(self, *args, **kwargs): if not self.__instance: self.__instance = super().__call__(*args, **kwargs) return self.__instanceclass Database(metaclass=MetaSingleton): connection = None def connect(self): if self.connection is None: self.connection = sqlite3.connect("db.sqlite3") self.cursorobj = self.connection.cursor() return self.cursorobjdb1 = Database().connect()db2 = Database().connect()print(db1, db2)# =&gt; &lt;sqlite3.Cursor object at 0x7f810d6f8260&gt; &lt;sqlite3.Cursor object at# 0x7f810d6f8260&gt;print(db1 is db2)# =&gt; True 监控服务12345678910111213141516171819202122232425262728293031323334353637383940414243class HealthCheck: _instance = None def __new__(cls, *args, **kwargs): if not HealthCheck._instance: HealthCheck._instance = super().__new__(cls, *args, **kwargs) return HealthCheck._instance def __init__(self): self._servers = [] def addServer(self): self._servers.append("Server 1") self._servers.append("Server 2") self._servers.append("Server 3") self._servers.append("Server 4") def changeServer(self): self._servers.pop() self._servers.append("Server 5")hc1 = HealthCheck()hc2 = HealthCheck()hc1.addServer()print("Schedule health check for servers (1) ...")for i in range(4): print("Checking ", hc1._servers[i])hc2.changeServer()print("Schedule health check for servers (2) ...")for i in range(4): print("Checking ", hc2._servers[i])# =&gt; Schedule health check for servers (1) ...# =&gt; Checking Server 1# =&gt; Checking Server 2# =&gt; Checking Server 3# =&gt; Checking Server 4# =&gt; Schedule health check for servers (2) ...# =&gt; Checking Server 1# =&gt; Checking Server 2# =&gt; Checking Server 3# =&gt; Checking Server 5]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>OOP</tag>
        <tag>Advanced</tag>
        <tag>Development</tag>
        <tag>DesignPattern</tag>
        <tag>Project</tag>
        <tag>MetaClass</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Fluent Python 笔记 —— 装饰器和闭包]]></title>
    <url>%2F2020%2F11%2F19%2Ffluent-python-decorator-and-closure%2F</url>
    <content type="text"><![CDATA[装饰器函数装饰器用于在源码中“标记”函数，以某种方式增强函数的行为。它是一种以另一个函数（被装饰的函数）为参数的可调用对象，可能会处理被装饰的函数并将其返回，或者将其替换为另一个函数。 装饰器严格来说只是语法糖。假如有个名为 decorate 的装饰器：123@decoratedef target(): print('running target()') 上述代码效果等同于如下写法：1234def target(): print('running target()')target = decorate(target) 即原来的 target 函数会被替换为 decorate(target) 返回的函数。12345678910111213&gt;&gt;&gt; def deco(func):... def inner():... print('running inner()')... return inner...&gt;&gt;&gt; @deco... def target():... print('running target()')...&gt;&gt;&gt; target()running inner()&gt;&gt;&gt; target&lt;function deco.&lt;locals&gt;.inner at 0x7ff01bad9a60&gt; 如上述代码，deco 返回 inner 函数对象，使用 deco 装饰 target，调用被装饰的 target 实际会运行 inner。target 对象变为 inner 的引用。 装饰器有如下两大特性： 能把被装饰的函数替换成其他函数 装饰器在加载模块时立即执行 装饰器何时执行装饰器会在被装饰的函数定义之后立即运行，这通常是在 Python 加载模块时。 参考如下 registration.py 模块：123456789101112131415161718192021222324252627registry = []def register(func): print(f'running register(&#123;func&#125;)') registry.append(func) return func@registerdef f1(): print('running f1()')@registerdef f2(): print('running f2()')def f3(): print('running f3()')def main(): print('running main()') print('registry -&gt;', registry) f1() f2() f3()if __name__ == '__main__': main() 运行后输出如下：1234567running register(&lt;function f1 at 0x7fbc852d43a0&gt;)running register(&lt;function f2 at 0x7fbc852d4430&gt;)running main()registry -&gt; [&lt;function f1 at 0x7fbc852d43a0&gt;, &lt;function f2 at 0x7fbc852d4430&gt;]running f1()running f2()running f3() Python 加载模块后，装饰器 register 会在其他函数之前运行，将被装饰的函数（f1 和 f2）的引用添加到 registry 列表中。原本的函数 f1 和 f2，以及未被装饰的 f3，则只在 main 明确调用它们时才执行。 如果导入 registration.py 模块（不作为脚本运行），输出如下：123&gt;&gt;&gt; import registrationrunning register(&lt;function f1 at 0x7f8fbd8c3b80&gt;)running register(&lt;function f2 at 0x7f8fbd8c3c10&gt;) 函数装饰器在导入模块时立即执行，而被装饰的函数只在明确调用时执行。突出了导入时和运行时之间的区别。 变量作用域规则测试如下函数，它读取两个变量的值，一个是局部变量 a，是函数的参数；另一个是未被定义的变量 b：12345678910&gt;&gt;&gt; def f1(a):... print(a)... print(b)...&gt;&gt;&gt; f1(3)3Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt; File "&lt;stdin&gt;", line 3, in f1NameError: name 'b' is not defined 运行时变量 a 的值正常输出，接着报出 name ‘b’ is not defined。 若先给全局变量 b 赋值，再调用 f1 函数，就不会报错：1234&gt;&gt;&gt; b = 6&gt;&gt;&gt; f1(3)36 但如下代码的结果可能会让人意想不到：123456789101112&gt;&gt;&gt; b=6&gt;&gt;&gt; def f2(a):... print(a)... print(b)... b = 9...&gt;&gt;&gt; f2(3)3Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt; File "&lt;stdin&gt;", line 3, in f2UnboundLocalError: local variable 'b' referenced before assignment 代码运行后首先输出了 3（print(a)），但是第二个语句 print(b) 执行报错。按照直觉第二个 print 语句应该输出 6，因为全局变量 b 已经在函数执行之前赋值，局部变量 b 的赋值动作也是在 print 语句后面。 事实上是，Python 在编译函数定义体时，会判断 b 是局部变量，Python 会尝试从本地环境获取 b。调用 f2(3) 时，f2 的定义体尝试获取局部变量 b 的值，发现 b 没有绑定后报错。 如果在函数内部赋值时想让解释器把 b 当成全局变量，需要使用 global 声明：123456789101112131415161718&gt;&gt;&gt; b = 6&gt;&gt;&gt; def f3(a):... global b... print(a)... print(b)... b = 9...&gt;&gt;&gt; f3(3)36&gt;&gt;&gt; b9&gt;&gt;&gt; f3(3)39&gt;&gt;&gt; b = 30&gt;&gt;&gt; b30 闭包闭包指延伸了作用域的函数，其中包含函数定义体中引用、不在定义体中定义的非全局变量。 计算移动平均值（不断增加的系列值的均值）的类：123456789# average_oo.pyclass Averager: def __init__(self): self.series = [] def __call__(self, new_value): self.series.append(new_value) total = sum(self.series) return total/len(self.series) 效果如下：12345678&gt;&gt;&gt; from average_oo import Averager&gt;&gt;&gt; avg = Averager()&gt;&gt;&gt; avg(10)10.0&gt;&gt;&gt; avg(11)10.5&gt;&gt;&gt; avg(12)11.0 以下代码是同样功能的函数式实现：123456789def make_averager(): series = [] def averager(new_value): series.append(new_value) total = sum(series) return total / len(series) return averager 12345678&gt;&gt;&gt; from average import make_averager&gt;&gt;&gt; avg = make_averager()&gt;&gt;&gt; avg(10)10.0&gt;&gt;&gt; avg(11)10.5&gt;&gt;&gt; avg(12)11.0 第一个例子中，Averager 类的实例 avg 存储历史值的位置很明显：通过 self.series 实例属性。第二个例子中，series 是 make_averager 函数的局部变量，但调用 avg(10) 时，make_averager 函数已经返回，它的本地作用域也就不存在了。 在 averager 函数中，series 是自由变量（free variable），指未在本地作用域中绑定的变量。 闭包是一种函数，它会保留定义函数时存在的自由变量的绑定。这样在调用函数时，即便定义作用域不可用了，通过闭包仍能使用那些绑定。 nolocal前面实现 make_averager 函数的方式效率并不高，把所有值存储在历史列表中，在每次调用 averager 时使用 sum 求和。更好的实现方式是，只存储目前的总和以及元素个数，只使用这两个值计算均值。1234567891011# average2.pydef make_averager(): count = 0 total = 0 def averager(new_value): count += 1 total += new_value return total / count return averager 上述代码运行后会报出如下错误：12345678&gt;&gt;&gt; from average2 import make_averager&gt;&gt;&gt; avg = make_averager()&gt;&gt;&gt; avg(10)Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt; File "/home/starky/program/python/algorithm/average2.py", line 6, in averager count += 1UnboundLocalError: local variable 'count' referenced before assignment 原因在于，当 count 是数字或其他任何不可变类型时，count += 1 的作用等同于 count = count + 1。导致在 averager 的定义体中为 count 赋值了，将 count 变成了局部变量。total 变量也是如此。之前的 series 变量没有出现此问题，原因是只调用了 series.append，列表作为可变对象，并不存在重新赋值的情况。 Python 3 中引入了 nolocal 声明，其作用是把变量标记为自由变量。为 nolocal 声明的变量赋予新值后，闭包中保存的绑定也会更新。1234567891011def make_averager(): count = 0 total = 0 def averager(new_value): nonlocal count, total count += 1 total += new_value return total / count return averager 实现一个简单的装饰器1234567891011121314151617181920212223242526import timedef clock(func): def clocked(*args): t0 = time.perf_counter() result = func(*args) elapsed = time.perf_counter() - t0 name = func.__name__ arg_str = ', '.join(repr(arg) for arg in args) print('[%0.8fs] %s(%s) -&gt; %r' % (elapsed, name, arg_str, result)) return result return clocked@clockdef snooze(seconds): time.sleep(seconds)@clockdef factorial(n): return 1 if n &lt; 2 else n * factorial(n - 1)if __name__ == '__main__': print('*' * 40, 'Calling snooze(.123)') snooze(.123) print('*' * 40, 'Calling factorial(6)') print('6! =', factorial(6)) 执行结果如下：12345678910**************************************** Calling snooze(.123)[0.12318703s] snooze(0.123) -&gt; None**************************************** Calling factorial(6)[0.00000168s] factorial(1) -&gt; 1[0.00003647s] factorial(2) -&gt; 2[0.00006038s] factorial(3) -&gt; 6[0.00008216s] factorial(4) -&gt; 24[0.00010411s] factorial(5) -&gt; 120[0.00012920s] factorial(6) -&gt; 7206! = 720 在上述代码中：123@clockdef factorial(n): return 1 if n &lt; 2 else n * factorial(n - 1) 等同于：123def factorial(n): return 1 if n &lt; 2 else n * factorial(n - 1)factorial = clock(factorial) factorial 会作为 func 参数传递给 clock，返回 clocked 函数。Python 解释器在背后会把 clocked 赋值给 factorial。此后，每次调用 factorial(n)，实际执行的都是 clocked(n)。总体步骤如下： 记录初始时间 t0 调用原来的 factorial 函数，保存结果 计算执行的时间 格式化收集到的数据 返回第二步保存的结果 以上即装饰器的典型行为：将被装饰的函数替换为新函数，二者接收同样的参数，（通常）返回被装饰函数本该返回的值，并做些额外的操作。 参数化装饰器参数化的注册装饰器 为了便于启用或禁用 register 的函数注册功能，可以为其提供一个可选的 active 参数，设为 False 时，不注册被装饰的函数。从概念上讲，这个新的 register 函数不是装饰器，而是装饰器工厂函数，用来返回真正的装饰器。123456789101112131415161718192021222324# registration_param.pyregistry = set()def register(active=True): def decorate(func): print('running register(active=%s)-&gt;decorate(%s)' % (active, func)) if active: registry.add(func) else: registry.discard(func) return func return decorate@register(active=False)def f1(): print('running f1()')@register()def f2(): print('running f2()')def f3(): print('running f3()') 运行效果：12345&gt;&gt;&gt; import registration_paramrunning register(active=False)-&gt;decorate(&lt;function f1 at 0x7fa801b1bc10&gt;)running register(active=True)-&gt;decorate(&lt;function f2 at 0x7fa801b1bca0&gt;)&gt;&gt;&gt; registration_param.registry&#123;&lt;function f2 at 0x7fa801b1bca0&gt;&#125; decorate 是装饰器，必须返回一个函数。register 是装饰器工厂函数，返回 decorate。只有 active 参数的值为 True 时才注册 func；若 active 不为真，且 func 在 registry 中，则将 func 移除。@register 工厂函数必须作为函数调用，传入所需参数（或 @register()）。 若不使用 @ 句法，也可以像常规函数那样使用 register：123456789101112131415&gt;&gt;&gt; from registration_param import *running register(active=False)-&gt;decorate(&lt;function f1 at 0x7fc32e6d0b80&gt;)running register(active=True)-&gt;decorate(&lt;function f2 at 0x7fc32e6d0c10&gt;)&gt;&gt;&gt; registry&#123;&lt;function f2 at 0x7fc32e6d0c10&gt;&#125;&gt;&gt;&gt; register()(f3)running register(active=True)-&gt;decorate(&lt;function f3 at 0x7fc32e6d0af0&gt;)&lt;function f3 at 0x7fc32e6d0af0&gt;&gt;&gt;&gt; registry&#123;&lt;function f2 at 0x7fc32e6d0c10&gt;, &lt;function f3 at 0x7fc32e6d0af0&gt;&#125;&gt;&gt;&gt; register(active=False)(f2)running register(active=False)-&gt;decorate(&lt;function f2 at 0x7fc32e6d0c10&gt;)&lt;function f2 at 0x7fc32e6d0c10&gt;&gt;&gt;&gt; registry&#123;&lt;function f3 at 0x7fc32e6d0af0&gt;&#125; 参数化的 clock 装饰器12345678910111213141516171819202122232425262728293031# clock_param.pyimport timeDEFAULT_FMT = '[&#123;elapsed:0.8f&#125;s] &#123;name&#125;(&#123;args&#125;) -&gt; &#123;result&#125;'def clock(fmt=DEFAULT_FMT): def decorate(func): def clocked(*_args): t0 = time.time() _result = func(*_args) elapsed = time.time() - t0 name = func.__name__ args = ', '.join(repr(arg) for arg in _args) result = repr(_result) print(fmt.format(**locals())) return _result return clocked return decorateif __name__ == '__main__': @clock() def snooze(seconds): time.sleep(seconds) for i in range(3): snooze(.123)# =&gt; [0.12320948s] snooze(0.123) -&gt; None# =&gt; [0.12319684s] snooze(0.123) -&gt; None# =&gt; [0.12318802s] snooze(0.123) -&gt; None 1234567891011121314# clock_param2.pyimport timefrom clock_param import clock@clock('&#123;name&#125;(&#123;args&#125;) dt=&#123;elapsed:0.3f&#125;s')def snooze(seconds): time.sleep(seconds)for i in range(3): snooze(.123)# =&gt; snooze(0.123) dt=0.123s# =&gt; snooze(0.123) dt=0.123s# =&gt; snooze(0.123) dt=0.123s 标准库中的装饰器——单分派泛函数假设需要开发一个调试 Web 应用的工具，能够生成 HTML 来显示不同类型的 Python 对象。12345import htmldef htmlize(obj): content = html.escape(repr(obj)) return '&lt;pre&gt;&#123;&#125;&lt;/pre&gt;'.format(content) 上述函数适用于任何 Python 类型。但如果想做进一步扩展，使其能够用不同的方式显示不同的类型： str：把内部换行符替换为 ‘\\n’，使用 进行格式化 int：以十进制和十六进制显示数字 list：输出 HTML 列表，根据各个元素的类型进行格式化 Python 不支持重载方法或函数，因此不能使用不同的签名定义 htmlize 的变体，也无法使用不同的方式处理不同的数据类型。一种常见的做法是将 htmlize 变成一个分派函数，使用一系列 if/elif/elif 调用专门的函数，如 htmlize_str、htmlize_int 等。但这样不便于模块的扩展，且显得笨拙。分派函数 htmlize 会随着时间推移变得很大，与各个专门函数之间的耦合也很紧密。 Python 中的 functools.singledispatch 装饰器可以把整体方案拆分为等多个模块，甚至可以为无法修改的类提供专门函数。使用 @singledispatch 装饰的普通函数会变成泛函数（generic function），根据第一个参数的类型以不同方式执行相同操作的一组函数。 12345678910111213141516171819202122232425# htmlize.pyfrom functools import singledispatchfrom collections import abcimport numbersimport html@singledispatchdef htmlize(obj): content = html.escape(repr(obj)) return '&lt;pre&gt;&#123;&#125;&lt;/pre&gt;'.format(content)@htmlize.register(str)def _(text): content = html.escape(text).replace('\n', '&lt;br&gt;\n') return '&lt;p&gt;&#123;0&#125;&lt;/p&gt;'.format(content)@htmlize.register(numbers.Integral)def _(n): return '&lt;pre&gt;&#123;0&#125; (0x&#123;0:x&#125;)&lt;/pre&gt;'.format(n)@htmlize.register(tuple)@htmlize.register(abc.MutableSequence)def _(seq): inner = '&lt;/li&gt;\n&lt;li&gt;'.join(htmlize(item) for item in seq) return '&lt;ul&gt;\n&lt;li&gt;' + inner + '&lt;/li&gt;\n&lt;/ul&gt;' 123456789101112131415&gt;&gt;&gt; from htmlize import htmlize&gt;&gt;&gt; htmlize(&#123;1, 2, 3&#125;)'&lt;pre&gt;&#123;1, 2, 3&#125;&lt;/pre&gt;'&gt;&gt;&gt; htmlize(abs)'&lt;pre&gt;&amp;lt;built-in function abs&amp;gt;&lt;/pre&gt;'&gt;&gt;&gt; htmlize('Heimlich &amp; Co.\n- a game')'&lt;p&gt;Heimlich &amp;amp; Co.&lt;br&gt;\n- a game&lt;/p&gt;'&gt;&gt;&gt; htmlize(42)'&lt;pre&gt;42 (0x2a)&lt;/pre&gt;'&gt;&gt;&gt; print(htmlize(['alpha', 66, &#123;3, 2, 1&#125;]))&lt;ul&gt;&lt;li&gt;&lt;p&gt;alpha&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;pre&gt;66 (0x42)&lt;/pre&gt;&lt;/li&gt;&lt;li&gt;&lt;pre&gt;&#123;1, 2, 3&#125;&lt;/pre&gt;&lt;/li&gt;&lt;/ul&gt; @singledispatch 标记处理 object 类型的基函数。各个专门函数使用 @&lt;base_function&gt;.register(&lt;type&gt;) 装饰。为每个需要特殊处理的类型注册一个函数，numbers.Integral 是 int 的抽象基类。只要可能，注册的专门函数应该尽量处理抽象基类（如 numbers.Integral 和 abc.MutableSequence），不要处理具体实现（如 int 和 list）。这样代码支持的兼容类型会更广泛（支持抽象基类现有的和未来的具体子类），比如用户可能通过子类化 numbers.Integral 实现固定位数的 int 类型。可以叠放多个 register 装饰器，让同一个函数支持不同类型。 @singledispatch 可以在系统的任何地方和任何模块中注册专门函数，还可以为不是自己编写的或者不能修改的类添加自定义函数。 参考资料Fluent Python]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Program</tag>
        <tag>Advanced</tag>
        <tag>Function</tag>
        <tag>Closure</tag>
        <tag>Decorator</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python Cookbook —— 元编程]]></title>
    <url>%2F2020%2F11%2F19%2Fpython-cookbook-metaprogramming%2F</url>
    <content type="text"><![CDATA[一、函数装饰器1234567891011121314151617181920212223242526import timefrom functools import wrapsdef timethis(func): ''' Decorator that reports the execution time. ''' @wraps(func) def wrapper(*args, **kwargs): start = time.time() result = func(*args, **kwargs) elapsed = time.time() - start print(func.__name__, elapsed) return result return wrapper@timethisdef countdown(n): while n &gt; 0: n -= 1countdown(1000000)# =&gt; countdown 0.29901695251464844 装饰器负责接收某个函数作为参数，然后返回一个新的函数作为输出。下面的代码：123@timethisdef countdown(n): ... 实际上等同于123def countdown(n): ...countdown = timethis(countdown) 装饰器内部通常要定义一个接收任意参数（*args, **kwargs）的函数，即 wrapper()。在 wrapper 函数里，调用原始的作为参数传入的函数（func）并获取其结果，再根据需求添加上执行其他操作的代码（比如计时、日志等）。最后新创建的 wrapper 函数被返回并替换掉被装饰的函数（countdown），从而在不改变被装饰函数自身代码的情况下，为其添加额外的行为。 二、带参数的装饰器12345678910111213141516171819202122232425262728293031323334353637383940from functools import wrapsimport loggingdef logged(level, name=None, message=None): &apos;&apos;&apos; Add logging to a function. level is the logging level, name is the logger name, and message is the log message. &apos;&apos;&apos; logging.basicConfig( level=logging.DEBUG, format=&apos;%(asctime)s - %(name)s - %(levelname)s - %(message)s&apos;) def decorate(func): logname = name if name else func.__module__ log = logging.getLogger(logname) logmsg = message if message else func.__name__ @wraps(func) def wrapper(*args, **kwargs): log.log(level, logmsg) return func(*args, **kwargs) return wrapper return decorate# Example use@logged(logging.WARNING)def spam(): pass@logged(logging.INFO, name=&apos;Example&apos;, message=&apos;This is log message&apos;)def foo(): passspam()foo()# =&gt; 2019-10-24 09:22:25,780 - __main__ - WARNING - spam# =&gt; 2019-10-24 09:22:25,783 - Example - INFO - This is log message 最外层的函数 logged() 用于接收传入装饰器的参数，并使这些参数能够被装饰器中的内部函数（decorate()）访问。内部函数 decorate 则用于实现装饰器的“核心逻辑”，即接收某个函数作为参数，通过定义一个新的内部函数（wrapper）添加某些行为，再将这个新的函数返回作为被装饰函数的替代品。 在类中定义的装饰器1234567891011121314151617181920212223242526272829303132333435363738from functools import wrapsclass A: # Decorator as an instance method def decorator1(self, func): @wraps(func) def wrapper(*args, **kwargs): print('Decorator 1') return func(*args, **kwargs) return wrapper #Decorator as a class method @classmethod def decorator2(cls, func): @wraps(func) def wrapper(*args, **kwargs): print('Decorator 2') return func(*args, **kwargs) return wrapper# As an instance methoda = A()@a.decorator1def spam(): passspam()# =&gt; Decorator 1# As a class method@A.decorator2def grok(): passgrok()# =&gt; Decorator 2 利用装饰器向原函数中添加参数123456789101112131415161718192021222324from functools import wrapsimport inspectdef optional_debug(func): if 'debug' in inspect.getfullargspec(func).args: raise TypeError('debug argument already defined') @wraps(func) def wrapper(*args, debug=False, **kwargs): if debug: print('Calling', func.__name__) return func(*args, **kwargs) return wrapper@optional_debugdef add(x, y): print(x + y)add(2, 3)# =&gt; 5add(2, 3, debug=True)# =&gt; Calling add# =&gt; 5 装饰器修改类的定义1234567891011121314151617181920212223242526def log_getattribute(cls): orig_getattribute = cls.__getattribute__ def new_getattribute(self, name): print('getting: ', name) return orig_getattribute(self, name) cls.__getattribute__ = new_getattribute return cls@log_getattributeclass A: def __init__(self, x): self.x = x def spam(self): passa = A(42)print(a.x)a.spam()# =&gt; getting: x# =&gt; 42# =&gt; getting: spam 类装饰器可以用来重写类的部分定义以修改其行为，作为一种直观的类继承或元类的替代方式。比如上述功能也可以通过类继承来实现：1234567891011121314151617class LoggedGetattribute: def __getattribute__(self, name): print('getting: ', name) return super().__getattribute__(name)class A(LoggedGetattribute): def __init__(self, x): self.x = x def spam(self): passa = A(42)print(a.x)a.spam() 在某些情况下，类装饰器的方案要更为直观一些，并不会向继承层级中引入新的依赖。同时由于不使用 super() 函数，速度也稍快一点。 使用元类控制实例的创建Python 中的类可以像函数那样调用，同时创建实例对象：1234567class Spam: def __init__(self, name): self.name = namea = Spam('Guido')b = Spam('Diana') 如果开发人员想要自定义创建实例的行为，可以通过元类重新实现一遍 __call__() 方法。假设在调用类时不创建任何实例：1234567891011121314 class NoInstance(type): def __call__(self, *args, **kwargs): raise TypeError("Can't instantiate directly")class Spam(metaclass=NoInstance): @staticmethod def grok(x): print('Spam.grok')Spam.grok(42) # Spam.groks = Spam()# TypeError: Can't instantiate directly 元类实现单例模式单例模式即类在创建对象时，单一的类确保只生成唯一的实例对象。1234567891011121314151617# singleton.pyclass Singleton(type): def __init__(self, *args, **kwargs): self.__instance = None super().__init__(*args, **kwargs) def __call__(self, *args, **kwargs): if self.__instance is None: self.__instance = super().__call__(*args, **kwargs) return self.__instance else: return self.__instanceclass Spam(metaclass=Singleton): def __init__(self): print('Creating Spam') 123456789&gt;&gt;&gt; from singleton import *&gt;&gt;&gt; a = Spam()Creating Spam&gt;&gt;&gt; b = Spam()&gt;&gt;&gt; a is bTrue&gt;&gt;&gt; c = Spam()&gt;&gt;&gt; a is cTrue 强制检查类定义中的代码规范可以借助元类监控普通类的定义代码。通常的方式是定义一个继承自 type 的元类并重写其 __new__() 或 __init__() 方法。123456class MyMeta(type): def __new__(cls, clsname, bases, clsdict): # clsname is name of class being defined # bases is tuple of base classes # clsdict is class dictionary return super().__new__(cls, clsname, bases, clsdict) 123456class MyMeta(type): def __init__(self, clsname, bases, clsdict): # clsname is name of class being defined # bases is tuple of base classes # clsdict is class dictionary return super().__init__(clsname, bases, clsdict) 为了使用元类，通常会先定义一个供其他对象继承的基类：12345678class Root(metaclass=MyMeta): passclass A(Root): passclass B(Root): pass 元类的重要特性在于，它允许用户在类定义时检查类的内容。在重写的 __init__() 方法内部，可以方便地检查 class dictionary、base class 或者其他与类定义相关的内容。此外，当元类指定给某个普通类以后，该普通类的所有子类也都会继承元类的定义。 下面是一个用于检查代码规范的元类，确保方法的命名里只包含小写字母：123456789101112131415161718192021class NoMixedCaseMeta(type): def __new__(cls, clsname, bases, clsdict): for name in clsdict: if name.lower() != name: raise TypeError('Bad attribute name: ' + name) return super().__new__(cls, clsname, bases, clsdict)class Root(metaclass=NoMixedCaseMeta): passclass A(Root): def foo_bar(self): passclass B(Root): def fooBar(self): pass# TypeError: Bad attribute name: fooBar 元类的定义中重写 __new__() 还是 __init__() 方法取决于你想以何种方式产出类。__new__() 方法生效于类创建之前，通常用于对类的定义进行改动（通过修改 class dictionary 的内容）；__init__() 方法生效于类创建之后，通常是与已经生成的类对象进行交互。比如 super() 函数只在类实例被创建后才能起作用。 以编程的方式定义类可以通过编程的方式创建类，比如从字符串中产出类的源代码。types.new_class() 函数可以用来初始化新的类对象，只需要向其提供类名、父类（以元组的形式）、关键字参数和一个用来更新 class dictionary 的回调函数。1234567891011121314151617181920212223242526# Methodsdef __init__(self, name, shares, price): self.name = name self.shares = shares self.price = pricedef cost(self): return self.shares * self.pricecls_dict = &#123; '__init__': __init__, 'cost': cost,&#125;# Make a classimport typesStock = types.new_class('Stock', (), &#123;&#125;, lambda ns: ns.update(cls_dict))Stock.__module__ = __name__s = Stock('ACME', 50, 91.1)print(s)# =&gt; &lt;__main__.Stock object at 0x7f0e3b62edc0&gt;print(s.cost())# =&gt; 4555.0 通常形式的类定义代码：12class Spam(Base, debug=True, typecheck=False): ... 转换成对应的 type.new_class() 形式的代码：123Spam = types.new_class('Spam', (Base,), &#123;'debug': True, 'typecheck': False&#125;, lambda ns: ns.update(cls_dict)) 从代码中产出类对象在某些场景下是很有用的，比如 collections.nametupe() 函数：1234&gt;&gt;&gt; import collections&gt;&gt;&gt; Stock = collections.namedtuple('Stock', ['name', 'shares', 'price'])&gt;&gt;&gt; Stock&lt;class '__main__.Stock'&gt; 下面是一个类似 namedtuple 功能的实现代码：1234567891011121314151617181920212223242526272829303132333435import operatorimport typesimport sysdef named_tuple(classname, fieldnames): # Populate a dictionary of field property accessors cls_dict = &#123; name: property(operator.itemgetter(n)) for n, name in enumerate(fieldnames) &#125; # Make a __new__ function and add to the class dict def __new__(cls, *args): if len(args) != len(fieldnames): raise TypeError('Expected &#123;&#125; arguments'.format(len(fieldnames))) return tuple.__new__(cls, args) cls_dict['__new__'] = __new__ # Make the class cls = types.new_class(classname, (tuple,), &#123;&#125;, lambda ns: ns.update(cls_dict)) cls.__module__ = sys._getframe(1).f_globals['__name__'] return clsPoint = named_tuple('Point', ['x', 'y'])print(Point)# =&gt; &lt;class '__main__.Point'&gt;p = Point(4, 5)print(p.x)# =&gt; 4print(p.y)# =&gt; 5p.x = 2# =&gt; AttributeError: can't set attribute 在定义时初始化类成员在类定义时完成初始化或其他设置动作，是元类的经典用法（元类在类定义时触发）。12345678910111213141516171819202122232425262728293031323334import operatorclass StructTupleMeta(type): def __init__(cls, *args, **kwargs): super().__init__(*args, **kwargs) for n, name in enumerate(cls._fields): setattr(cls, name, property(operator.itemgetter(n)))class StructTuple(tuple, metaclass=StructTupleMeta): _fields = [] def __new__(cls, *args): if len(args) != len(cls._fields): raise ValueError('&#123;&#125; arguments required'.format(len(cls._fields))) return super().__new__(cls, args)class Stock(StructTuple): _fields = ['name', 'shares', 'price']class Point(StructTuple): _fields = ['x', 'y']s = Stock('ACME', 50, 91.1)print(s)# =&gt; ('ACME', 50, 91.1)print(s[0])# =&gt; ACMEprint(s.name)# =&gt; ACMEs.shares = 23# =&gt; AttributeError: can't set attribute 在上面的代码中，StructTupleMeta 元类从 _fields 类属性中读取属性名列表并将其转换成属性方法。operator.itemgetter() 函数负责创建访问方法（accessor function），property() 函数负责将它们转换成属性（property）。 StructTuple 类用作供其他类继承的基类。其中的 __new__() 方法负责创建新的实例对象。不同于 __init__()，__new__() 方法会在实例创建之前触发，由于 tuple 是不可变对象，创建之后即无法被修改，因此这里使用 __new__()。 参考资料Python Cookbook, 3rd Edition]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Program</tag>
        <tag>OOP</tag>
        <tag>Class</tag>
        <tag>Advanced</tag>
        <tag>MetaProgramming</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Fluent Python 笔记 —— 字典与集合]]></title>
    <url>%2F2020%2F11%2F08%2Ffluent-python-dicts-and-sets%2F</url>
    <content type="text"><![CDATA[一、映射类型标准库里的所有映射类型都是利用 dict 实现的，它们有个共同的限制：其中的键必须是可散列的数据类型。关于可散列的数据类型的定义：若某对象是可散列的，则它的散列值在其整个生命周期中是保持不变的。该对象需要实现 __hash__ 方法和 __qe__ 方法（跟其他键做比较）。如果两个可散列对象是相等的，那么他们的散列值一定相等。 不可变数据类型中的 str、bytes 和数字都是可散列类型。虽然元组本身是不可变序列，但元组中的元素有可能是其他可变类型的引用。只有当一个元组中包含的所有元素都是可散列类型的情况下，该元组才是可散列的。12345678&gt;&gt;&gt; tt = (1, 2, (30, 40))&gt;&gt;&gt; hash(tt)-3907003130834322577&gt;&gt;&gt; tl = (1, 2, [30, 40])&gt;&gt;&gt; hash(tl)Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;TypeError: unhashable type: 'list' 一般用户自定义类型的对象都是可散列的，因此这些对象在比较时都是不相等的。若某个对象实现了 __eq__ 方法，并且在方法中用到了该对象的内部状态，则只有当这些内部状态都是不可变类型的情况下，该对象才是可散列的。 二、字典推导123456789101112&gt;&gt;&gt; DIAL_CODES = [... (86, 'China'),... (91, 'India'),... (1, 'America'),... (55, 'Brazil'),... (7, 'Russia')... ]&gt;&gt;&gt; country_code = &#123;country: code for code, country in DIAL_CODES&#125;&gt;&gt;&gt; country_code&#123;'China': 86, 'India': 91, 'America': 1, 'Brazil': 55, 'Russia': 7&#125;&gt;&gt;&gt; &#123;code: country.upper() for country, code in country_code.items() if code &lt; 66&#125;&#123;1: 'AMERICA', 55: 'BRAZIL', 7: 'RUSSIA'&#125; 三、setdefault 处理找不到的键以下代码的写法：1my_dict.setdefault(key, []).append(new_value) 其效果等同于如下代码：123if key not in my_dict: my_dict[key] = []my_dict[key].append(new_value) 都是获取 my_dict 中 key 键对应的值（如果该 key 不存在，则新增 key 并令其值为 []），然后向 key 对应的值（列表）中添加新元素。只不过后者至少要进行两次键查询（如果键不存在，则执行三次键查询），而使用 setdefault 只需要一次键查询就可以完成整个操作。setdefault 在获取 key 键对应的值时，如果该 key 不存在，则把 key 和空列表直接放进映射并返回空列表，因而不需要执行第二次键值查找。 四、弹性键查询defaultdictcollections.defaultdict 可以做到即便某个键在映射里不存在，也能够在读取这个键的时候得到一个默认值。只需要用户在初始化 defaultdict 对象时，为其指定一个创建默认值的方法。即在实例化 defaultdict 的时候，向构造方法提供一个可调用对象，该对象会在 __getitem__ 碰到找不到的键的时候被调用，让 __getitem__ 返回某种默认值。 如 dd = defaultdict(list)。若键 new-key 在 dd 中不存在，表达式 dd[&#39;new-key&#39;] 会执行以下操作： 调用 list() 创建一个新列表 把新列表作为值，new-key 作为键存放到 dd 中 返回新列表的引用 这个用来生成默认值得可调用对象（list()）存放在名为 default_factory 的实例属性里。若创建 defaultdict 的时候没有指定 default_factory，则查询不存在的键会触发 KeyError。这些特性背后依赖的是 __missing__ 特殊方法，这个特殊方法是所有映射类型都可以选择性地去支持的。 missing所有的映射类型处理找不到的键的时候，都会涉及到 __missing__ 方法。虽然基类 dict 并没有定义这个方法，但如果有一个类继承了 dict，该类提供 __missing__ 方法，则 __getitem__ 找不到键的时候，会自动调用 __missing__ 而不会抛出 KeyError 异常。 __missing__ 方法只会被 __getitem__ 调用（比如在表达式 d[k] 中）。提供 __missing__ 方法对 get 或 __contains__ 方法的使用没有影响。 如需要实现一种自定义的映射类型，在查询的时候将映射里的键都转换成 str。示例代码如下：1234567891011121314class StrKeyDict(dict): def __missing__(self, key): if isinstance(key, str): raise KeyError(key) return self[str(key)] def get(self, key, default=None): try: return self[key] except KeyError: return default def __contains__(self, key): return key in self.keys() or str(key) in self.keys() 运行结果如下：123456789101112131415161718192021222324&gt;&gt;&gt; from strkeydict import StrKeyDict&gt;&gt;&gt; d = StrKeyDict([('2', 'two'), ('4', 'four')])&gt;&gt;&gt; d['2']'two'&gt;&gt;&gt; d[4]'four'&gt;&gt;&gt; d[1]Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt; File "/home/starky/program/python/algorithm/strkeydict.py", line 5, in __missing__ return self[str(key)] File "/home/starky/program/python/algorithm/strkeydict.py", line 4, in __missing__ raise KeyError(key)KeyError: '1'&gt;&gt;&gt; d.get('2')'two'&gt;&gt;&gt; d.get(4)'four'&gt;&gt;&gt; d.get(1, 'N/A')'N/A'&gt;&gt;&gt; 2 in dTrue&gt;&gt;&gt; 1 in dFalse StrKeyDict 继承自 dict，如果找不到的键本身是字符串，抛出 KeyError 异常；如果找不到的键不是字符串，则将其转换成字符串后再进行查找。get 方法把查找工作用 self[key] 的形式委托给 __getitem__，这样在确定查找失败之前，还能通过 __missing__ 在给某个键一个机会。 isinstance(key, str) 在 __missing__ 中是必须的，若 str(k) 不是一个存在的键，代码就会陷入无限递归。因为 __missing__ 最后一行中的 self[str(key)] 会调用 __getitem__，而 str(key) 又不存在，则 __missing__ 又会被调用。 __contains__ 方法也是必须的，因为从 dict 继承到的 __contians__ 方法不会在找不到键时调用 __missing__ 方法。 不可变 Map标准库里所有的映射类型都是可变的。从 Python 3.3 开始，types 模块引入了一个名为 MappingProxyType 的封装类，可以从普通映射创建一个只读的映射视图。12345678910111213141516&gt;&gt;&gt; from types import MappingProxyType&gt;&gt;&gt; d = &#123;1: 'A'&#125;&gt;&gt;&gt; d_proxy = MappingProxyType(d)&gt;&gt;&gt; d_proxymappingproxy(&#123;1: 'A'&#125;)&gt;&gt;&gt; d_proxy[1]'A'&gt;&gt;&gt; d_proxy[2] = 'x'Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;TypeError: 'mappingproxy' object does not support item assignment&gt;&gt;&gt; d[2] = 'B'&gt;&gt;&gt; d_proxymappingproxy(&#123;1: 'A', 2: 'B'&#125;)&gt;&gt;&gt; d_proxy[2]'B' 集合集合的本质是许多唯一对象的聚集，可用于去重：123&gt;&gt;&gt; l = ['spam', 'spam', 'eggs', 'spam']&gt;&gt;&gt; set(l)&#123;'spam', 'eggs'&#125; 除了保证唯一性，集合还实现了很多基础的中缀运算符。比如 a | b 求并集，a &amp; b 求交集，a - b 求差集等。合理使用这些运算符可以省去不必要的循环和逻辑操作，使代码行数更少且更易读。 比如有一个电子邮件地址的集合 haystack，还要维护一个较小的电子邮件集合 needles，然后求出 needles 中有多少地址同时也出现在了 heystack 里。求 needles 的元素在 heystack 中出现的次数，两个变量都是集合类型，则只用一行代码即可实现：1found = len(needles &amp; haystack) 若不使用交集操作的话，则需要通过以下代码实现：123for n in needles: if n in haystack: found += 1 散列表散列表（Hash Map）是一种稀疏数组（即总是有空白元素的数组），散列表中的单元通常叫做表元（bucket）。在 dict 背后的散列表中，每个键值对都占用一个表元，每个表元都包含两个部分，对键的引用和对值的引用。因为所有表元的大小一致，可以通过偏移量来读取某个表元。 Python 会保证大概三分之一的表元是空的，当快要达到这个阈值时，原有的散列表会被复制到一个更大的空间里。把对象存入散列表中之前，需要先计算该元素键的散列值。 内置的 hash() 函数可用于计算所有内置类型对象的散列值。若自定义对象调用 hash()，实际上运行的是自定义的 __hash__ 方法。若两个对象比较时大小相等，则它们的散列值也必须相等。即 1 == 1.0 为真，则 hash(1) == hash(1.0) 也必须为真。 为了让散列值能够作为散列表索引使用，这些散列值必须在索引空间内尽量分散开。意味着在理想状态下，越是相似但不相等的对象，其散列值差别也越大。123456&gt;&gt;&gt; hash(1.0001)230584300921345&gt;&gt;&gt; hash(1.0002)461168601842689&gt;&gt;&gt; hash(1.0003)691752902764033 散列表算法为了获取 my_dict[search_key] 背后的值，Python 首先会调用 hash(search_key) 来计算 search_key 的散列值，将该值最低的几位数字作为偏移量（具体几位作为偏移量，需依据当前散列表的大小），在散列表里查找表元。若查找出的表元为空，则抛出 KeyError 异常。若表元非空，该表元中会有一对 found_key: found_value。Python 会检查 search_key == found_key 是否为真，若为真则返回 found_value。若 search_key 与 found_key 不匹配，这种情况称为散列冲突。算法会在散列值中另外再取几位数字，用特殊方法处理一下，把新得到的数字作为索引继续寻找表元并重复以上步骤。 字典的特性键必须是可散列的可散列对象满足以下三个要求： 支持 hash() 函数，且通过 __hash__() 得到的散列值是不变的 支持通过 __eq__() 方法检测相等性 若 a == b 为真，则 hash(a) == hash(b) 也为真 所有用户自定义对象默认都是可散列的，其散列值有 id() 获取，且都不相等。 字典在内存上开销巨大字典使用了散列表，散列表又必须是稀疏的，从而导致字典在空间的使用上效率低下。若需要存放数量巨大的记录，放在由元组或具名元组构成的列表中会是比较好的选择。用元组取代字典存储记录，一是避免了散列表所耗费的空间，二是无需把记录中字段的名字在每个元素里都存一遍，从而节省空间。在用户自定义类型中，__slots__ 属性可以改变实例属性的存储方式，由 dict 变为 tuple。 字典的键查询很快dict 的实现是典型的空间换时间。字典类型有着巨大的内存开销，但是它提供了无视数据量大小的快速访问。 键的次序取决于添加顺序 集合的特性集合里的元素必须是可散列的 集合很消耗内存 可以很高效地判断元素是否存在于某个集合中 元素的次序取决于被添加到集合里的次序 参考资料Fluent Python]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Program</tag>
        <tag>DataStructure</tag>
        <tag>Map</tag>
        <tag>Dict</tag>
        <tag>Hash</tag>
        <tag>Set</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Fluent Python 笔记 —— 使用一等函数实现设计模式]]></title>
    <url>%2F2020%2F11%2F08%2Ffluent-python-first-class-function-and-design-patterns%2F</url>
    <content type="text"><![CDATA[经典的策略模式 “策略模式”：定义一系列算法，把它们一一封装，并且使它们之间可以相互替换。本模式使得算法可以独立于使用它的对象而变化 电商领域有个明显的功能可以使用“策略”模式，即根据客户的属性或订单中的商品计算折扣。假如有如下折扣规则： 有 1000 或以上积分的顾客，每个订单享 5% 折扣 同一订单中，单个商品数量达到 20 个或以上，享 10% 折扣 订单中的不同商品达到 10 个或以上，享 7% 折扣 如上述 UML 图，上下文提供服务，会把一些计算委托给实现不同算法的可互换组件。本例中是订单 Order，会根据不同算法计算促销折扣；策略指实现不同算法的组件共用的接口。本例中是名为 Promotion 的抽象类；具体策略即“策略”的具体子类，FidelityPromo、BulkPromo 和 LargeOrderPromo 是这里实现的三个具体策略。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768from abc import ABC, abstractmethodfrom collections import namedtupleCustomer = namedtuple('Customer', 'name fidelity')class LineItem: def __init__(self, product, quantity, price): self.product = product self.quantity = quantity self.price = price def total(self): return self.price * self.quantityclass Order: # 上下文 def __init__(self, customer, cart, promotion=None): self.customer = customer self.cart = list(cart) self.promotion = promotion def total(self): if not hasattr(self, '__total'): self.__total = sum(item.total() for item in self.cart) return self.__total def due(self): if self.promotion is None: discount = 0 else: discount = self.promotion.discount(self) return self.total() - discount def __repr__(self): fmt = '&lt;Order total: &#123;:.2f&#125; due: &#123;:.2f&#125;&gt;' return fmt.format(self.total(), self.due())class Promotion(ABC): # 策略：抽象基类 @abstractmethod def discount(self, order): """返回折扣金额"""class FidelityPromo(Promotion): """为积分为1000或以上的顾客提供5%折扣""" def discount(self, order): return order.total() * .05 if order.customer.fidelity &gt;= 1000 else 0class BulkItemPromo(Promotion): """单个商品为20个或以上时提供10%折扣""" def discount(self, order): discount = 0 for item in order.cart: if item.quantity &gt;= 20: discount += item.total() * .1 return discountclass LargeOrderPromo(Promotion): """订单中的不同商品达到10个或以上时提供7%折扣""" def discount(self, order): distinct_items = &#123;item.product for item in order.cart&#125; if len(distinct_items) &gt;= 10: return order.total() * .07 return 0 执行效果：1234567891011121314&gt;&gt;&gt; from order import *&gt;&gt;&gt; joe = Customer('John Doe', 0)&gt;&gt;&gt; ann = Customer('Ann Smith', 1100)&gt;&gt;&gt; cart = [LineItem('banana', 4, .5), LineItem('apple', 10, 1.5), LineItem('watermellon', 5, 5.0)]&gt;&gt;&gt; Order(joe, cart, FidelityPromo())&lt;Order total: 42.00 due: 42.00&gt;&gt;&gt;&gt; Order(ann, cart, FidelityPromo())&lt;Order total: 42.00 due: 39.90&gt;&gt;&gt;&gt; banana_cart = [LineItem('banana', 30, .5), LineItem('apple', 10, 1.5)]&gt;&gt;&gt; Order(joe, banana_cart, BulkItemPromo())&lt;Order total: 30.00 due: 28.50&gt;&gt;&gt;&gt; long_order = [LineItem(str(item_code), 1, 1.0) for item_code in range(10)]&gt;&gt;&gt; Order(joe, long_order, LargeOrderPromo())&lt;Order total: 10.00 due: 9.30&gt; 使用函数实现策略模式1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253from collections import namedtupleCustomer = namedtuple('Customer', 'name fidelity')class LineItem: def __init__(self, product, quantity, price): self.product = product self.quantity = quantity self.price = price def total(self): return self.price * self.quantityclass Order: def __init__(self, customer, cart, promotion=None): self.customer = customer self.cart = list(cart) self.promotion = promotion def total(self): if not hasattr(self, '__total'): self.__total = sum(item.total() for item in self.cart) return self.__total def due(self): if self.promotion is None: discount = 0 else: discount = self.promotion(self) return self.total() - discount def __repr__(self): fmt = '&lt;Order total: &#123;:.2f&#125; due: &#123;:.2f&#125;&gt;' return fmt.format(self.total(), self.due())def fidelity_promo(order): return order.total() * .05 if order.customer.fidelity &gt;= 1000 else 0def bulk_item_promo(order): discount = 0 for item in order.cart: if item.quantity &gt;= 20: discount += item.total() * .1 return discountdef large_order_promo(order): distince_items = &#123;item.product for item in order.cart&#125; if len(distince_items) &gt;= 10: return order.total() * .07 return 0 计算折扣只需调用 self.promotion() 函数，无需涉及到抽象类。各个策略都是由函数实现的。为了把折扣策略应用到 Order 实例上，只需把促销函数作为参数传入。 执行效果：1234567891011121314&gt;&gt;&gt; from order_fun import *&gt;&gt;&gt; joe = Customer('John Doe', 0)&gt;&gt;&gt; ann = Customer('Ann Smith', 1100)&gt;&gt;&gt; cart = [LineItem('banana', 4, .5), LineItem('apple', 10, 1.5), LineItem('watermellon', 5, 5.0)]&gt;&gt;&gt; Order(joe, cart, fidelity_promo)&lt;Order total: 42.00 due: 42.00&gt;&gt;&gt;&gt; Order(ann, cart, fidelity_promo)&lt;Order total: 42.00 due: 39.90&gt;&gt;&gt;&gt; banana_cart = [LineItem('banana', 30, .5), LineItem('apple', 10, 1.5)]&gt;&gt;&gt; Order(joe, banana_cart, bulk_item_promo)&lt;Order total: 30.00 due: 28.50&gt;&gt;&gt;&gt; long_order = [LineItem(str(item_code), 1, 1.0) for item_code in range(10)]&gt;&gt;&gt; Order(joe, long_order, large_order_promo)&lt;Order total: 10.00 due: 9.30&gt; 具体策略一般没有内部状态，只是处理上下文中的数据。因此一定要使用普通的函数，而无需编写只有一个方法的类，再去实现另一个类声明的单函数接口。函数比用户自定义类的实例更为轻量，各个策略函数在 Python 编译模块时只会创建一次。即普通的函数是可共享的对象，可以同时在多个上下文中使用。 选择最佳策略在上述代码的基础上，添加 best_promo 函数计算所有折扣，并返回额度最大的。1234promos = [fidelity_promo, bulk_item_promo, large_order_promo]def best_promo(order): return max(promo(order) for promo in promos) 与其他几个 *_promo 函数一样，best_promo 函数的参数是一个 Order 实例。使用生成器表达式把 order 传给 promos 列表中的各个函数，返回折扣额度最大的促销策略。虽然上述代码可用且易于阅读，但若想添加新的促销策略，除了定义新的折扣函数以外，还应注意记得把新定义的函数添加到 promos 列表中。 可以使用内置的 globals() 函数找出模块中的全部策略。globals() 函数会返回一个字典，包含针对当前模块的全局符号表（对函数或方法来说，“当前模块”指的是定义它们的模块，而不是调用它们的模块）。123456promos = [globals()[name] for name in globals() if name.endswith('_promo') and name != 'best_promo']def best_promo(order): return max(promo(order) for promo in promos) 动态收集促销折扣函数更为显式的一种方案是使用简单的装饰器。参考下一章节。 使用装饰器改进策略模式使用注册装饰器可以改进前面的电商促销折扣示例。之前的主要问题是，定义体中有函数名称，但 best_promo 用来判断哪个折扣幅度最大的 promos 列表中也有函数名称。这种重复导致新增折扣函数后可能会忘记把它添加到 promos 列表中，导致 best_promos 忽略新策略且不报错，为系统引入了不易察觉的缺陷。 123456789101112131415161718192021222324252627promos = []def promotion(promo_func): promos.append(promo_func) return promo_func@promotiondef fidelity(order): return order.total() * .05 if order.customer.fidelity &gt;= 1000 else 0@promotiondef bulk_item(order): discount = 0 for item in order.cart: if item.quantity &gt;= 20: discount += item.total() * .1 return discount@promotiondef large_order(order): discount_items = &#123;item.product for item in order.cart&#125; if len(discount_items) &gt;= 10: return order.total() * .07 return 0def best_promo(order): return max(promo(order) for promo in promos) promos 列表起初是空的，promotion 把 promo_func 添加到 promos 列表中，然后原封不动地将其返回。即被 @promotion 装饰的函数都会提前添加到 promos 列表中（装饰器在被装饰的函数定义之后（通常是在导入时）立即运行）。 相比于之前的方案，此方案有以下几个优点： 促销函数无需使用特殊的名称（即不用以 _promo 结尾） @promotion 装饰器突出了被装饰的函数的作用，还便于临时禁用某个促销策略：只需把装饰器注释掉 促销折扣策略可以在其他模块中定义，只要使用 @promotion 装饰即可 参考资料Fluent Python]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Program</tag>
        <tag>Advanced</tag>
        <tag>Functional</tag>
        <tag>DesignPattern</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Fluent Python 笔记 —— 可迭代对象、迭代器和生成器]]></title>
    <url>%2F2020%2F10%2F18%2Ffluent-python-iterator-and-generator%2F</url>
    <content type="text"><![CDATA[迭代是数据处理的基石。扫描内存中放不下的数据集时，通常需要一种惰性获取数据项的方式，即按需一次获取一个数据项。这就是迭代器模式。 在 Python 中，所有序列类型都支持迭代。在语言内部，迭代器用于支持以下操作： for 循环 构建和扩展序列类型 逐行遍历文本文件 列表推导、字典推导和集合推导 元组拆包 调用函数时，使用 * 拆包实参 可迭代对象以下代码实现了一个 Sentence 类，通过索引从文本中提取单词：123456789101112131415161718import reimport reprlibRE_WORD = re.compile('\w+')class Sentence: def __init__(self, text): self.text = text self.words = RE_WORD.findall(text) def __getitem__(self, index): return self.words[index] def __len__(self): return len(self.words) def __repr__(self): return f'Sentence(&#123;reprlib.repr(self.text)&#125;)' 效果如下：12345678910111213141516&gt;&gt;&gt; from sentence import Sentence&gt;&gt;&gt; s = Sentence('"The time has come," the Walrus said,')&gt;&gt;&gt; sSentence('"The time ha... Walrus said,')&gt;&gt;&gt; for word in s:... print(word)...ThetimehascometheWalrussaid&gt;&gt;&gt; list(s)['The', 'time', 'has', 'come', 'the', 'Walrus', 'said'] 上面创建的 Sentence 实例是可迭代的。因此该实例对象可被 for 循环调用、可以用于构建列表等。 迭代的机制Python 解释器需要迭代对象 x 时，会自动执行 iter(x)。其作用如下： 检查对象是否实现了 __iter__ 方法，如已实现则调用 __iter__，返回一个迭代器对象 若对象没有实现 __iter__ 方法，但实现了 __getitem__ 方法，Python 会创建一个迭代器，尝试按顺序（从索引 0 开始）获取元素 若上述尝试失败，抛出 TypeError 异常（X object is not iterable） 所有 Python 序列都实现了 __iter__ 方法，因此都支持迭代操作。 可迭代对象与迭代器的对比可迭代对象指通过 iter 函数调用可以获取迭代器的对象。即对象实现了能够返回迭代器的 __iter__ 方法，该对象就是可迭代的；或者实现了 __getitem__ 方法，且其参数是从 0 开始的索引，则对象也可以迭代。 一个简单的 for 循环背后也是有迭代器的作用的：1234567&gt;&gt;&gt; s = 'ABC'&gt;&gt;&gt; for char in s:... print(char)...ABC 使用 while 循环模拟效果如下：123456789101112&gt;&gt;&gt; s = 'ABC'&gt;&gt;&gt; it = iter(s)&gt;&gt;&gt; while True:... try:... print(next(it))... except StopIteration:... del it... break...ABC 使用可迭代的对象（字符串 s）创建迭代器 it 不断在迭代器 it 上调用 next 函数，获取下一个字符 若已获取到最后一个字符，迭代器抛出 StopIteration 异常 捕获 StopIteration 异常，释放 it 对象，退出循环 Python 语言内部会自动处理 for 循环和其他迭代上下文（如列表推导等）中的 StopIteration 异常。 迭代器（如前面的 it）实现了无参数的 __next__ 方法，返回序列中的下一个元素；若没有元素了，则抛出 StopIteration 异常。Python 中的迭代器还实现了 __iter__ 方法，返回该迭代器本身（即确保迭代器本身也是可迭代对象） 典型的迭代器关于可迭代对象与迭代器之间的区别，可以参考如下代码：1234567891011121314151617181920212223242526272829303132import reimport reprlibRE_WORD = re.compile('\w+')class Sentence: def __init__(self, text): self.text = text self.words = RE_WORD.findall(text) def __repr__(self): return f'Sentence(&#123;reprlib.repr(self.text)&#125;)' def __iter__(self): return SentenceIterator(self.words)class SentenceIterator: def __init__(self, words): self.words = words self.index = 0 def __next__(self): try: word = self.words[self.index] except IndexError: raise StopIteration() self.index += 1 return word def __iter__(self): return self 根据迭代器协议，可迭代对象 Sentence 中的 __iter__ 方法会实例化并返回一个迭代器（SentenceIterator），而 SentenceIterator 作为迭代器实现了 __next__ 和 __iter__ 方法。 构建可迭代对象时出现错误的原因经常是混淆了可迭代对象与迭代器。可迭代对象通过内部的 __iter__ 方法返回一个实例化的迭代器对象；而迭代器要实现 __next__ 方法返回单个元素，此外还需要实现 __iter__ 方法返回迭代器本身。 生成器函数1234567891011121314151617181920212223242526272829303132import reimport reprlibRE_WORD = re.compile('\w+')class Sentence: def __init__(self, text): self.text = text self.words = RE_WORD.findall(text) def __repr__(self): return f'Sentence(&#123;reprlib.repr(self.text)&#125;)' def __iter__(self): return SentenceIterator(self.words)class SentenceIterator: def __init__(self, words): self.words = words self.index = 0 def __next__(self): try: word = self.words[self.index] except IndexError: raise StopIteration() self.index += 1 return word def __iter__(self): return self 实现可迭代对象，相较于之前的代码，符合 Python 习惯的方式是用生成器函数替换手动实现的迭代器 SentenceIterator 类。 只要 Python 函数的定义体中有 yield 关键字，则该函数就是生成器函数。调用生成器函数会返回一个生成器对象。1234567891011121314151617181920212223242526&gt;&gt;&gt; def gen_123():... yield 1... yield 2... yield 3...&gt;&gt;&gt; gen_123&lt;function gen_123 at 0x7f63e57f0f80&gt;&gt;&gt;&gt; gen_123()&lt;generator object gen_123 at 0x7f63e57dc950&gt;&gt;&gt;&gt; for i in gen_123():... print(i)...123&gt;&gt;&gt; g = gen_123()&gt;&gt;&gt; next(g)1&gt;&gt;&gt; next(g)2&gt;&gt;&gt; next(g)3&gt;&gt;&gt; next(g)Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;StopIteration 把生成器对象传递给 next() 函数时，其行为与迭代器一致。 惰性求值re.finditer 是 re.findall 函数的惰性版本，返回的不是结果列表而是一个生成器，按需生成 re.MatchObject 实例。即只在需要时才生成下一个单词。 123456789101112131415import reimport reprlibRE_WORD = re.compile('\w+')class Sentence: def __init__(self, text): self.text = text def __repr__(self): return f'Sentence(&#123;reprlib.repr(self.text)&#125;)' def __iter__(self): for match in RE_WORD.finditer(self.text): yield match.group() finditer 函数返回一个迭代器，包含 self.text 中匹配 RE_WORD 的单词，产出 MatchObject 实例。match.group() 方法从 MatchObject 实例中提取匹配正则表达式的具体文本。 生成器函数已极大地简化了代码，但使用生成器表达式能够把代码变得更为简短。123456789101112131415161718&gt;&gt;&gt; def gen_AB():... print('start')... yield 'A'... print('continue')... yield 'B'... print('end.')...&gt;&gt;&gt; res = (x * 3 for x in gen_AB())&gt;&gt;&gt; res&lt;generator object &lt;genexpr&gt; at 0x7f4619324ad0&gt;&gt;&gt;&gt; for i in res:... print('--&gt;', i)...start--&gt; AAAcontinue--&gt; BBBend. 可以看出，生成器表达式会产出生成器。1234567891011121314import reimport reprlibRE_WORD = re.compile('\w+')class Sentence: def __init__(self, text): self.text = text def __repr__(self): return f'Sentence(&#123;reprlib.repr(self.text)&#125;)' def __iter__(self): return (match.group() for match in RE_WORD.finditer(self.text)) 标准库中的生成器函数用于过滤的生成器函数 模块 函数 说明 itertools compress(it, selector_it) 并行处理两个可迭代对象。若 selector_it 中的元素是真值，产出 it 中对应的元素 itertools dropwhile(predicate, it) 把可迭代对象 it 中的元素传给 predicate，跳过 predicate(item) 为真值的元素，在 predicate(item) 为假时停止，产出剩余（未跳过）的所有元素（不再继续检查） 内置 filter(predicate, it) 把 it 中的各个元素传给 predicate，若 predicate(item) 返回真值，产出对应元素 itertools filterfalse(predicate, it) 与 filter 函数类似，不过 predicate(item) 返回假值时产出对应元素 itertools takewhile(predicate, it) predicate(item) 返回真值时产出对应元素，然后立即停止不再继续检查 itertools islice(it, stop) 或 islice(it, start, stop, step=1) 产出 it 的切片，作用类似于 s[:stop] 或 s[start:stop:step，不过 it 可以是任何可迭代对象，且实现的是惰性操作 12345678910111213141516&gt;&gt;&gt; def vowel(c):... return c.lower() in 'aeiou'...&gt;&gt;&gt; list(filter(vowel, 'Aardvark'))['A', 'a', 'a']&gt;&gt;&gt; import itertools&gt;&gt;&gt; list(itertools.filterfalse(vowel, 'Aardvark'))['r', 'd', 'v', 'r', 'k']&gt;&gt;&gt; list(itertools.dropwhile(vowel, 'Aardvark'))['r', 'd', 'v', 'a', 'r', 'k']&gt;&gt;&gt; list(itertools.compress('Aardvark', (1,0,1,1,0,1)))['A', 'r', 'd', 'a']&gt;&gt;&gt; list(itertools.islice('Aardvark', 4))['A', 'a', 'r', 'd']&gt;&gt;&gt; list(itertools.islice('Aardvark', 1, 7, 2))['a', 'd', 'a'] 用于映射的生成器函数 模块 函数 说明 itertools accumulate(it, [func]) 产出累积的总和。若提供了 func，则把 it 中的前两个元素传给 func，再把计算结果连同下一个元素传给 func，以此类推，产出结果 内置 enumerate(it, start=0) 产出由两个元素构成的元组，结构是 (index, item)。其中 index 从 start 开始计数，item 则从 it 中获取 内置 map(func, it1, [it2, ..., itN]) 把 it 中的各个元素传给 func，产出结果；若传入 N 个可迭代对象，则 func 必须能接受 N 个参数，且并行处理各个可迭代对象 123456789101112131415&gt;&gt;&gt; list(enumerate('albatroz', 1))[(1, 'a'), (2, 'l'), (3, 'b'), (4, 'a'), (5, 't'), (6, 'r'), (7, 'o'), (8, 'z')]&gt;&gt;&gt; import operator&gt;&gt;&gt; list(map(operator.mul, range(11), range(11)))[0, 1, 4, 9, 16, 25, 36, 49, 64, 81, 100]&gt;&gt;&gt; list(map(operator.mul, range(11), [2, 4, 8]))[0, 4, 16]&gt;&gt;&gt; list(map(lambda a, b: (a, b), range(11), [2, 4, 8]))[(0, 2), (1, 4), (2, 8)]&gt;&gt;&gt; import itertools&gt;&gt;&gt; sample = [5, 4, 2, 8, 7, 6, 3, 0, 9, 1]&gt;&gt;&gt; list(itertools.accumulate(sample))[5, 9, 11, 19, 26, 32, 35, 35, 44, 45]&gt;&gt;&gt; list(itertools.accumulate(sample, max))[5, 5, 5, 8, 8, 8, 8, 8, 9, 9] 合并多个可迭代对象的生成器函数 模块 函数 说明 itertools chain(it1, ..., itN) 先产出 it1 中的所有元素，然后产出 it2 中的所有元素，以此类推，无缝连接 itertools chain.from_iterable(it) 产出 it 生成的各个可迭代对象中的元素，一个接一个无缝连接；it 中的元素应该为可迭代对象（即 it 是嵌套了可迭代对象的可迭代对象） itertools product(it1, ..., itN, repeat=1) 计算笛卡尔积。从输入的各个可迭代对象中获取元素，合并成 N 个元素组成的元组，与嵌套的 for 循环效果一样。repeat 指明重复处理多少次输入的可迭代对象 内置 zip(it1, ..., itN) 并行从输入的各个可迭代对象中获取元素，产出由 N 个元素组成的元组。只要其中任何一个可迭代对象到头了，就直接停止 itertools zip_longest(it1, ..., itN, fillvalue=None) 并行从输入的各个可迭代对象中获取元素，产出由 N 个元素组成的元组，等到最长的可迭代对象到头后才停止。空缺的值用 fillvalue 填充 123456789101112131415&gt;&gt;&gt; import itertools&gt;&gt;&gt; list(itertools.chain('ABC', range(2)))['A', 'B', 'C', 0, 1]&gt;&gt;&gt; list(itertools.chain(enumerate('ABC')))[(0, 'A'), (1, 'B'), (2, 'C')]&gt;&gt;&gt; list(itertools.chain.from_iterable(enumerate('ABC')))[0, 'A', 1, 'B', 2, 'C']&gt;&gt;&gt; list(zip('ABC', range(5)))[('A', 0), ('B', 1), ('C', 2)]&gt;&gt;&gt; list(zip('ABC', range(5), [10, 20, 30, 40]))[('A', 0, 10), ('B', 1, 20), ('C', 2, 30)]&gt;&gt;&gt; list(itertools.zip_longest('ABC', range(5)))[('A', 0), ('B', 1), ('C', 2), (None, 3), (None, 4)]&gt;&gt;&gt; list(itertools.zip_longest('ABC', range(5), fillvalue='?'))[('A', 0), ('B', 1), ('C', 2), ('?', 3), ('?', 4)] 123456789&gt;&gt;&gt; list(itertools.product('ABC', range(2)))[('A', 0), ('A', 1), ('B', 0), ('B', 1), ('C', 0), ('C', 1)]&gt;&gt;&gt; suits = 'spades hearts diamonds clubs'.split()&gt;&gt;&gt; list(itertools.product('AK', suits))[('A', 'spades'), ('A', 'hearts'), ('A', 'diamonds'), ('A', 'clubs'), ('K', 'spades'), ('K', 'hearts'), ('K', 'diamonds'), ('K', 'clubs')]&gt;&gt;&gt; list(itertools.product('ABC'))[('A',), ('B',), ('C',)]&gt;&gt;&gt; list(itertools.product('ABC', repeat=2))[('A', 'A'), ('A', 'B'), ('A', 'C'), ('B', 'A'), ('B', 'B'), ('B', 'C'), ('C', 'A'), ('C', 'B'), ('C', 'C')] 把输入的各个元素扩展成多个输出元素的生成器函数|模块|函数|说明||-|-|-||itertools|combinations(it, out_len)|把可迭代对象 it 产出的 out_len 个元素组合在一起产出||itertools|combinations_with_replacement(it, out_len)|把 it 产出的 out_len 个元素组合在一起产出，包含相同元素的组合||itertools|count(start=0, step=1)|从 start 开始不断产出数字，按 step 指定的步幅增加||itertools|cycle(it)|从 it 中产出各个元素，存储各个元素的副本，然后按顺序重复不断地产出各个元素||itertools|permutations(it, out_len=None)|把 out_len 个 it 产出的元素排列在一起，然后产出这些排列；out_len 的默认值等于 len(list(it))||itertools|repeat(item, [times])|重复不断地产出指定的元素，除非提供 times 指定次数| 123456789101112131415161718&gt;&gt;&gt; import itertools&gt;&gt;&gt; ct = itertools.count()&gt;&gt;&gt; next(ct)0&gt;&gt;&gt; next(ct), next(ct), next(ct)(1, 2, 3)&gt;&gt;&gt; list(itertools.islice(itertools.count(1, .3), 3))[1, 1.3, 1.6]&gt;&gt;&gt; cy = itertools.cycle('ABC')&gt;&gt;&gt; next(cy)'A'&gt;&gt;&gt; list(itertools.islice(cy, 7))['B', 'C', 'A', 'B', 'C', 'A', 'B']&gt;&gt;&gt; rp = itertools.repeat(7)&gt;&gt;&gt; next(rp), next(rp)(7, 7)&gt;&gt;&gt; list(itertools.repeat(8, 4))[8, 8, 8, 8] 123456789&gt;&gt;&gt; import itertools&gt;&gt;&gt; list(itertools.combinations('ABC', 2))[('A', 'B'), ('A', 'C'), ('B', 'C')]&gt;&gt;&gt; list(itertools.combinations_with_replacement('ABC', 2))[('A', 'A'), ('A', 'B'), ('A', 'C'), ('B', 'B'), ('B', 'C'), ('C', 'C')]&gt;&gt;&gt; list(itertools.permutations('ABC', 2))[('A', 'B'), ('A', 'C'), ('B', 'A'), ('B', 'C'), ('C', 'A'), ('C', 'B')]&gt;&gt;&gt; list(itertools.product('ABC', repeat=2))[('A', 'A'), ('A', 'B'), ('A', 'C'), ('B', 'A'), ('B', 'B'), ('B', 'C'), ('C', 'A'), ('C', 'B'), ('C', 'C')] 用于重新排列元素的生成器函数|模块|函数|说明||-|-|-||itertools|groupby(it, key=None)|产出由两个元素组成的元素，形式为 (key, group)，其中 key 是分组标准，group 是生成器，用于产出分组里的元素||内置|reversed(seq)|从后向前，倒序产出 seq 中的元素；seq 必须是序列，或者实现了 __reversed__ 特殊方法的对象||itertools|tee(it, n=2)|产出一个有 n 个生成器组成的元组，每个生成器都可以独立地产出输入的可迭代对象中的元素| 1234567891011121314151617181920212223242526&gt;&gt;&gt; import itertools&gt;&gt;&gt; animals = ['duck', 'eagle', 'rat', 'giraffe', 'bear', 'bat', 'dolphin', 'shark', 'lion']&gt;&gt;&gt; animals.sort(key=len)&gt;&gt;&gt; animals['rat', 'bat', 'duck', 'bear', 'lion', 'eagle', 'shark', 'giraffe', 'dolphin']&gt;&gt;&gt; for length, group in itertools.groupby(animals, len):... print(length, '-&gt;', list(group))...3 -&gt; ['rat', 'bat']4 -&gt; ['duck', 'bear', 'lion']5 -&gt; ['eagle', 'shark']7 -&gt; ['giraffe', 'dolphin']&gt;&gt;&gt;&gt;&gt;&gt; g1, g2 = itertools.tee('ABC')&gt;&gt;&gt; next(g1)'A'&gt;&gt;&gt; next(g2)'A'&gt;&gt;&gt; next(g2)'B'&gt;&gt;&gt; list(g1)['B', 'C']&gt;&gt;&gt; list(g2)['C']&gt;&gt;&gt; list(zip(*itertools.tee('ABC')))[('A', 'A'), ('B', 'B'), ('C', 'C')] PS：itertools.groupby 假定输入的可迭代对象已按照分组标准完成排序 读取迭代器，返回单个值的函数 模块 函数 说明 内置 all(it) it 中的所有元素都为真值时返回 True，否则返回 False；all([]) 返回 True 内置 any(it) 只要 it 中有元素为真值就返回 True，否则返回 False；any([]) 返回 False 内置 max(it, [key=], [default=]) 返回 it 中值最大的元素；key 是排序函数，与 sorted 中的一样；若可迭代对象为空，返回 default 内置 min(it, [key=], [default=]) 返回 it 中值最小的元素；key 是排序函数；若可迭代对象为空，返回 default functools reduce(func, it, [initial]) 把前两个元素传给 func，然后把计算结果和第三个元素传给 func，以此类推，返回最后的结果。若提供了 initial，则将其作为第一个元素传入 内置 sum(it, start=0) it 中所有元素的总和，若提供可选的 start，会把它也加上 1234567891011121314&gt;&gt;&gt; all([1, 2, 3])True&gt;&gt;&gt; all([1, 0, 3])False&gt;&gt;&gt; all([])True&gt;&gt;&gt; any([1, 2, 3])True&gt;&gt;&gt; any([1, 0, 3])True&gt;&gt;&gt; any([0, 0.0])False&gt;&gt;&gt; any([])False 123456&gt;&gt;&gt; import functools&gt;&gt;&gt; functools.reduce(lambda a, b: a * b, range(1, 6))120&gt;&gt;&gt; import operator&gt;&gt;&gt; functools.reduce(operator.mul, range(1, 6))120 参考资料Fluent Python]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Program</tag>
        <tag>Advanced</tag>
        <tag>Functional</tag>
        <tag>Generator</tag>
        <tag>Iterator</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 与数据结构 —— 基于数组的序列类型]]></title>
    <url>%2F2020%2F10%2F13%2Fpython-and-data-structure-array-based-list%2F</url>
    <content type="text"><![CDATA[Python 中的序列类型包含内置的 list、tuple、str 等，它们有很多明显的共同点。比如都支持通过索引语法（seq[k]）获取序列中的某个特定元素；底层的结构都是用数组来实现的。 Low-Level Array计算机系统一般都包含有数量庞大的内存空间，为了跟踪具体某段数据实际的存储位置，计算机加入了称为内存地址（memory address）的抽象形式。每个字节的存储空间都会关联一个独特的数字作为其地址。 计算机的内存为 random access memory (RAM)，即任意一个 byte 内存的读取与写入耗费的时间都是 O(1)。 通常来说，编程语言会跟踪每一个标识符及其对应的值的位置（内存地址）。为了方便起见，一组相关联的变量则可以保存在一段连续的内存中，即数组（array）。比如字符串实际上是由独立的字符组成的有序的序列： 数组中的每一个元素都占据同样大小的存储空间，使得任意一个元素的访问和更新都可以通过索引以常量的时间完成。 存储引用的数组假如需要用数组保存如下的一个姓名列表：[Rene, Joseph, Janet, Jonas, Helen, Virginia, ...] 使用数组的话，则需要确保数组中的每一个元素都占据同样的内存大小。但字符串本身具有差异很大的长度。当然可以尝试为数组中的每一个元素都分配足够大的内存空间，保证即使最长的字符串也不会超出，但这样必然会导致内存的浪费。 Python 的方案是在 list 或 tuple 中不保存字符串对象本身而是保存其引用。即列表中只是包含一系列内存地址，每一个地址都指向对应元素实际的存储位置。 即便每一个字符串的大小并不相同，列表中保存的字符串的内存地址却都是固定的（64bit）。 由于 list 和 tuple 中保存的是引用，导致同一个对象有可能成为多个 list 中某个元素的实体。比如对某个 list 执行分片操作后，分片中的元素实际上和母列表中对应的元素指向同样的对象。如 temp = primes[3:6]： 若对分片中的某个元素重新赋值，则直接将该元素替换为新的内存地址（指向新对象）即可，如执行 temp[2] = 15： 一个更显著的例子如 counters = [0] * 8，生成的 counters 列表中的 8 个数字 0 实际上都指向了同一个数字对象。这种方式初看上去很容易出现混乱，但是得益于数字本身是不可变对象，即使为数组中的某个元素重新赋值（比如 counters[2] = 1），也只是将当前位置保存的对数字 0 的引用替换为指向新的数字 1 的引用。而原数字 0 本身不发生任何变化，即数组中指向数字 0 的其他元素不受任何影响。 存储数值的数组前面提到过字符串是一种存储一系列字符（而不是这些字符的地址）的序列，这种更直接的形式称为 compact array。它相对于前面的存储引用的数组有着性能上的优势，且需要更少的内存。 比如存储一个包含一百万个 64-bit 整数的序列，理论上只需要 64 MB 的内存。实际上 Python 里的 list 需要 4 到 5 倍的容量去储存这些值。此外对于计算而言，compact array 将有关联的数据直接保存在一段连续的内存中，容易获得更高的性能。 动态数组在创建 low-level 数组时，其大小必须精确地、显式地声明，从而使系统可以为其分配适当的内存空间。但是这些内存附近的空间有可能提供给其他数据用于储存，因此数组在初始化后其容量一般是固定的，其中保存的元素的数量不能超越这个限制。 Python 的 list 类采用了一种称为 dynamic array 的机制，使其看上去没有长度的限制，其容量可以随着元素的添加不断地增长。list 实例会在底层维护着一个容量比当前 list 更大的数组，这个容量更大的数组使得为 list 添加元素变得方便很多。当元素数量持续增长直到数组中预留的空间也要被用尽时，list 会申请一个容量更大的数组替代之前容量较小的数组，旧的数组则被系统回收。 测试代码：123456789import sysdata = []for count in range(20): data.append(None) length = len(data) size = sys.getsizeof(data) print('Length: &#123;0:3d&#125;; Size in bytes: &#123;1:4d&#125;'.format(length, size)) 输出结果：1234567891011121314151617181920Length: 1; Size in bytes: 88Length: 2; Size in bytes: 88Length: 3; Size in bytes: 88Length: 4; Size in bytes: 88Length: 5; Size in bytes: 120Length: 6; Size in bytes: 120Length: 7; Size in bytes: 120Length: 8; Size in bytes: 120Length: 9; Size in bytes: 184Length: 10; Size in bytes: 184Length: 11; Size in bytes: 184Length: 12; Size in bytes: 184Length: 13; Size in bytes: 184Length: 14; Size in bytes: 184Length: 15; Size in bytes: 184Length: 16; Size in bytes: 184Length: 17; Size in bytes: 256Length: 18; Size in bytes: 256Length: 19; Size in bytes: 256Length: 20; Size in bytes: 256 可以看出随着数组长度的增加，其占据的内存空间不是成比例而是分段地进行扩展的。 动态数组的 Python 实现1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950import ctypesclass DynamicArray: def __init__(self): self._n = 0 self._capacity = 1 self._A = self._make_array(self._capacity) def __len__(self): return self._n def __getitem__(self, k): if not 0 &lt;= k &lt; self._n: raise IndexError('invalid index') return self._A[k] def append(self, obj): if self._n == self._capacity: self._resize(2 * self._capacity) self._A[self._n] = obj self._n += 1 def _resize(self, c): B = self._make_array(c) for k in range(self._n): B[k] = self._A[k] self._A = B self._capacity = c def _make_array(self, c): return (c * ctypes.py_object)() def insert(self, k, value): if self._n == self._capacity: self._resize(2 * self._capacity) for j in range(self._n, k, -1): self._A[j] = self._A[j - 1] self._A[k] = value self._n += 1 def remove(self, value): for k in range(self._n): if self._A[k] == value: for j in range(k, self._n - 1): self._A[j] = self._A[j + 1] self._A[self._n - 1] = None self._n -= 1 return raise ValueError('value not found') Python 中 List 的性能 操作 时间 len(data) O(1) data[j] O(1) data.count(value) O(n) data.index(value) O(k + 1)，k 指从左起 value 第一次出现的位置 value in data O(k + 1)，k 指从左起 value 第一次出现的位置 data1 == data2 O(k + 1)，此处 k 指从左起第一次出现不同元素的位置 data[j:k] O(k - j + 1)，j 和 k 指分片的起止位置 data1 + data2 O(n1 + n2) c * data O(cn) data[j] = val O(1) data.append(value) O(1) data.insert(k, value) O(n - k + 1) data.pop() O(1) data.pop(k) O(n - k) data1 += data2 O(n2) data.reverse() O(n) data.sort() O(nlogn) 参考资料Data Structures and Algorithms in Python]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Program</tag>
        <tag>Advanced</tag>
        <tag>DataStructure</tag>
        <tag>List</tag>
        <tag>Array</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 与数据结构 —— 链表及其应用]]></title>
    <url>%2F2020%2F10%2F13%2Fpython-and-data-structure-linked-list%2F</url>
    <content type="text"><![CDATA[list 的局限Python 的 list 类是经过高度优化的，在需要存储数据时是一个很优秀的选择。但仍有以下几点需要注意的劣势： 动态数组的长度通常会大于实际存储的元素的数量 当存储的元素数量不断增长时，动态数组扩展边界的性能较低 靠近数组中间位置的插入和删除操作性能相对较低 基于数组的序列和链表都能够以特定顺序存储元素，但是各自实现的方式差异很大。数组提供了一种更为“中心化”的表示方式，用一大段连续的内存存储多个元素的引用；而链表则更为“分散化”，将每个元素表示为轻量的节点（Node），每个节点都维护着包含元素本身及一个或多个相邻节点的引用，通过引用将多个节点最终连接成线性顺序的序列。 链表无法高效地通过数字索引读取其中元素的值，但是可以避免前面提到过的基于数组的序列的三点劣势。 单链表单链表是最简单的一种形式，组成线性序列的每个节点都包含元素本身及下一个节点的引用。 第一个和最后一个节点分别称为 head 和 tail。从 head 节点开始，跟随每个节点的 next 引用从首节点一直移动到尾部节点的过程，即为链表遍历。 向链表头部插入新元素的步骤： 创建包含新元素的新的节点对象 将新节点的 next 引用指向当前的 head 节点 将新节点设置为链表的新 head 节点 向链表尾部插入新元素的步骤： 创建一个包含新元素的新的节点 将新节点的 next 引用指向 None 作为新的 tail 节点 将当前 tail 节点的 next 引用改为指向上面的新节点 通过单链表实现 Stack 数据结构12345678910111213141516171819202122232425262728293031323334353637383940# linkstack.pyclass EmptyError(Exception): passclass Node: __slots__ = '_element', '_next' def __init__(self, element, next): self._element = element self._next = nextclass LinkedStack: def __init__(self): self._head = None self._size = 0 def __len__(self): return self._size def is_empty(self): return self._size == 0 def push(self, e): self._head = Node(e, self._head) self._size += 1 def top(self): if self.is_empty(): raise EmptyError('Stack is empty') return self._head._element def pop(self): if self.is_empty(): raise EmptyError('Stack is empty') answer = self._head._element self._head = self._head._next self._size -= 1 return answer 运行效果如下：12345678910111213141516171819&gt;&gt;&gt; from linkstack import LinkedStack&gt;&gt;&gt; s = LinkedStack()&gt;&gt;&gt; s.push(1)&gt;&gt;&gt; s.push(2)&gt;&gt;&gt; s.push(3)&gt;&gt;&gt; len(s)3&gt;&gt;&gt; s.pop()3&gt;&gt;&gt; s.pop()2&gt;&gt;&gt; s.pop()1&gt;&gt;&gt; s.pop()Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt; File "/home/starky/program/python/algorithm/linkstack.py", line 35, in pop raise EmptyError('Stack is empty')linkstack.EmptyError: Stack is empty 性能 操作 时间 S.push(e) O(1) S.pop() O(1) S.top() O(1) len(S) O(1) S.is_empty() O(1) 单链表实现 Queue 数据结构1234567891011121314151617181920212223242526272829303132333435363738394041424344454647class Node: __slots__ = '_element', '_next' def __init__(self, element, next): self._element = element self._next = nextclass EmptyError(Exception): passclass LinkedQueue: def __init__(self): self._head = None self._tail = None self._size = 0 def __len__(self): return self._size def is_empty(self): return self._size == 0 def first(self): if self.is_empty(): raise EmptyError('Queue is empty') return self._head._element def dequeue(self): if self.is_empty(): raise EmptyError('Queue is empty') answer = self._head._element self._head = self._head._next self._size -= 1 if self.is_empty(): self._tail = None return answer def enqueue(self, e): newest = Node(e, None) if self.is_empty(): self._head = newest else: self._tail._next = newest self._tail = newest self._size += 1 运行效果如下：12345678910111213141516171819&gt;&gt;&gt; from linkqueue import LinkedQueue&gt;&gt;&gt; q = LinkedQueue()&gt;&gt;&gt; q.enqueue(1)&gt;&gt;&gt; q.enqueue(2)&gt;&gt;&gt; q.enqueue(3)&gt;&gt;&gt; len(q)3&gt;&gt;&gt; q.dequeue()1&gt;&gt;&gt; q.dequeue()2&gt;&gt;&gt; q.dequeue()3&gt;&gt;&gt; q.dequeue()Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt; File "/home/starky/program/python/algorithm/linkqueue.py", line 32, in dequeue raise EmptyError('Queue is empty')linkqueue.EmptyError: Queue is empty 双链表 图中的 header 和 trailer 节点实际上不保存任何元素，这些“dummy”节点称为 sentinels。目的是保证逻辑的一致性，即任何情况下插入新节点，都能确保其左右两边至少各有一个旧节点。 插入新节点示意图： 代码实现：12345678910111213141516171819202122232425262728293031323334353637383940# doublylinked.pyclass Node: __slots__ = '_element', '_prev', '_next' def __init__(self, element, prev, next): self._element = element self._prev = prev self._next = nextclass DoublyLinkedBase: def __init__(self): self._header = Node(None, None, None) self._trailer = Node(None, None, None) self._header._next = self._trailer self._trailer._prev = self._header self._size = 0 def __len__(self): return self._size def is_empty(self): return self._size == 0 def _insert_between(self, e, predecessor, successor): newest = Node(e, predecessor, successor) predecessor._next = newest successor._prev = newest self._size += 1 return newest def _delete_node(self, node): predecessor = node._prev successor = node._next predecessor._next = successor successor._prev = predecessor self._size -= 1 element = node._element node._prev = node._next = node._element = None return element 双链表实现 Deque 数据结构12345678910111213141516171819202122232425262728293031323334# linkdeque.pyfrom doublylinked import DoublyLinkedBaseclass EmptyError(Exception): passclass LinkedDeque(DoublyLinkedBase): def first(self): if self.is_empty(): raise EmptyError("Deque is empty") return self._header._next._element def last(self): if self.is_empty(): raise Empty("Deque is empty") return self._trailer._prev._element def insert_first(self, e): self._insert_between(e, self._header, self._header._next) def insert_last(self, e): self._insert_between(e, self._trailer._prev, self._trailer) def delete_first(self): if self.is_empty(): raise EmptyError("Deque is empty") return self._delete_node(self._header._next) def delete_last(self): if self.is_empty(): raise EmptyError("Deque is empty") return self._delete_node(self._trailer._prev) 运行效果如下：1234567891011121314&gt;&gt;&gt; from linkdeque import LinkedDeque&gt;&gt;&gt; dq = LinkedDeque()&gt;&gt;&gt; dq.insert_first(2)&gt;&gt;&gt; dq.insert_first(1)&gt;&gt;&gt; dq.insert_last(3)&gt;&gt;&gt; dq.insert_last(4)&gt;&gt;&gt; len(dq)4&gt;&gt;&gt; dq.delete_first()1&gt;&gt;&gt; dq.delete_last()4&gt;&gt;&gt; len(dq)2 Link-Based vs. Array-BasedArray-Based 序列的优势： 提供 O(1) 时间下基于整数索引（index）对元素的访问。而链表访问第 k 个元素则需要 O(k) 时间（从链表头部开始遍历） 在不考虑动态数组边界扩展的情况下，各种操作在基于数组的序列中性能更高（步骤相对较少），虽然整体上和链表一样时间都是 O(1) 与链表结构相比，基于数组的序列需要更少的内存。基于数组或链表的序列实际上保存的都是对象的引用，因此这部分占据的内存空间是一致的。数组在最差的情况下（即刚刚扩展过边界的动态数组）需要为 2n 个对象引用收集内存；而单链表本身就需要为 2n 个对象引用提供空间（每个节点都包含元素本身和下一个节点的引用），双链表则为 3n Link-Based 序列的优势： 链表结构能够支持任意位置下插入和删除操作只耗费 O(1) 时间。而基于数组的序列在尾部插入和删除元素是常数时间，在任意索引为 k 的位置插入或移除元素则需要 O(n - k + 1) 时间。 参考资料Data Structures and Algorithms in Python]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Program</tag>
        <tag>Advanced</tag>
        <tag>DataStructure</tag>
        <tag>List</tag>
        <tag>Array</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Fluent Python 笔记——序列类型及其丰富的操作]]></title>
    <url>%2F2020%2F10%2F12%2Ffluent-python-list-and-its-operation%2F</url>
    <content type="text"><![CDATA[序列的分类Python 标准库用 C 语言实现了丰富的序列类型的数据结构，如： 容器序列（能存放不同类型的数据）：list、tuple、collections.deque 等 扁平序列（只容纳同一类型的数据）：str、bytes、bytearray、memoryview、array.array 12&gt;&gt;&gt; a_list = [1, '2', True, [1, 2, 3], 4.5]&gt;&gt;&gt; a_str = 'helloworld' 容器序列存放的是对象的引用，扁平序列存放的是值。即扁平序列是一段连续的内存空间。123456789&gt;&gt;&gt; a_list = [1, '2', True, [1, 2, 3], 4.5]&gt;&gt;&gt; embedded_list = a_list[3]&gt;&gt;&gt; embedded_list[1, 2, 3]&gt;&gt;&gt; embedded_list.append(4)&gt;&gt;&gt; embedded_list[1, 2, 3, 4]&gt;&gt;&gt; a_list[1, '2', True, [1, 2, 3, 4], 4.5] 序列还可以按照是否可变（能够被修改）进行分类： 可变序列：list、bytearray、array.array、collections.deque、memoryview 不可变序列：tuple、str、bytes 123456789&gt;&gt;&gt; a_list = [1, 2, 3]&gt;&gt;&gt; a_list[0] = 2&gt;&gt;&gt; a_list[2, 2, 3]&gt;&gt;&gt; a_tuple = (1, 2, 3)&gt;&gt;&gt; a_tuple[0] = 2Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;TypeError: 'tuple' object does not support item assignment 列表推导for 循环：1234567&gt;&gt;&gt; symbols = '!@#$%'&gt;&gt;&gt; codes = []&gt;&gt;&gt; for symbol in symbols:... codes.append(ord(symbol))...&gt;&gt;&gt; codes[33, 64, 35, 36, 37] 列表推导：1234&gt;&gt;&gt; symbols = '!@#$%'&gt;&gt;&gt; codes = [ord(symbol) for symbol in symbols]&gt;&gt;&gt; codes[33, 64, 35, 36, 37] 通常的原则是，只用列表推导创建新的列表，并尽量保持简短。 列表推导（包括集合推导、字典推导）、生成器表达式在 Python3 中有自己的局部作用域。123456&gt;&gt;&gt; x = 'ABC'&gt;&gt;&gt; dummy = [ord(x) for x in x]&gt;&gt;&gt; x'ABC'&gt;&gt;&gt; dummy[65, 66, 67] 列表推导与 filter/map 的比较：1234567&gt;&gt;&gt; symbols = '$¢£¥€¤'&gt;&gt;&gt; beyond_ascii = [ord(s) for s in symbols if ord(s) &gt; 127]&gt;&gt;&gt; beyond_ascii[162, 163, 165, 8364, 164]&gt;&gt;&gt; beyond_ascii = list(filter(lambda c: c &gt; 127, map(ord, symbols)))&gt;&gt;&gt; beyond_ascii[162, 163, 165, 8364, 164] 作为记录的元组元组其实是一种数据记录（Record），其中的每个元素都对应记录中一个字段的数据，字段在元组中的位置则可以用来区分其含义。123456789&gt;&gt;&gt; lax_coordinates = (33.9425, -118.408056)&gt;&gt;&gt; city, year, pop, area = ('Tokyo', 2003, 32450, 8014)&gt;&gt;&gt; traveler_ids = [('USA', '31195855'), ('BRA', 'CE342567'), ('ESP', 'XDA205856')]&gt;&gt;&gt; for country, _ in traveler_ids:... print(country)...USABRAESP 元组拆包元组拆包可以应用到任何可迭代对象上，唯一的要求即可迭代对象中的元素数量与接收这些元素的空档数一致（除非用 * 忽略多余的元素）。 元组拆包（平行赋值）：123456&gt;&gt;&gt; lax_coordinates = (33.9425, -118.408056)&gt;&gt;&gt; latitude, longitude = lax_coordinates&gt;&gt;&gt; latitude33.9425&gt;&gt;&gt; longitude-118.408056 不使用中间变量交换两个变量的值：1234567&gt;&gt;&gt; a = 1&gt;&gt;&gt; b = 2&gt;&gt;&gt; a, b = b, a&gt;&gt;&gt; a2&gt;&gt;&gt; b1 使用 * 运算符把一个可迭代对象拆开作为函数的参数：12345&gt;&gt;&gt; divmod(20, 8)(2, 4)&gt;&gt;&gt; t = (20, 8)&gt;&gt;&gt; divmod(*t)(2, 4) 元组拆包可以方便一个函数以元组的方式返回多个值，调用函数的代码就可以轻松地（有选择地）接受这些值。1234&gt;&gt;&gt; import os&gt;&gt;&gt; _, filename = os.path.split('/home/luciano/.ssh/idrsa.pub')&gt;&gt;&gt; filename'idrsa.pub' 用 * 处理多余的元素：123456789101112&gt;&gt;&gt; a, b, *rest = range(5)&gt;&gt;&gt; a, b, rest(0, 1, [2, 3, 4])&gt;&gt;&gt; a, b, *rest = range(3)&gt;&gt;&gt; a, b, rest(0, 1, [2])&gt;&gt;&gt; a, b, *rest = range(2)&gt;&gt;&gt; a, b, rest(0, 1, [])&gt;&gt;&gt; a, *body, c, d = range(5)&gt;&gt;&gt; a, body, c, d(0, [1, 2], 3, 4) 具名元组collections.namedtuple 可以用来创建一个带字段名的元组和一个有名字的类，便于对程序进行调试。其类实例消耗的内存与元组是一样的，跟普通的对象实例相比更小一些（不用 __dict__ 存放实例的属性）。1234567891011&gt;&gt;&gt; from collections import namedtuple&gt;&gt;&gt; City = namedtuple('City', 'name country population coordinates')&gt;&gt;&gt; tokyo = City('Tokyo', 'JP', 36.933, (35.689722, 139.691667))&gt;&gt;&gt; tokyoCity(name='Tokyo', country='JP', population=36.933, coordinates=(35.689722, 139.691667))&gt;&gt;&gt; tokyo.population36.933&gt;&gt;&gt; tokyo.coordinates(35.689722, 139.691667)&gt;&gt;&gt; tokyo[1]'JP' 创建具名元组需要传入两个参数，第一个是类名，第二个是类的各个字段的名称。后者可以是多个字符串组成的可迭代对象或由空格分隔开的字段名组成的字符串。可以通过字段名或位置获取某个字段的信息。 具名元组的 _fields 属性包含由这个类中所有字段名称组成的元组；_asdict() 方法可以把具名元组以 collections.OrderedDict 的形式返回。 切片关于切片和区间忽略最后一个元素在切片和区间操作里不包含最后一个元素是 Python 的风格，同时也符合 C 和其他以 0 为起始下标的语言的习惯。部分原因如下： 当只有最后一个位置信息时，可以快速看出区间里包含多少个元素：range(3) 和 my_list[:3] 都返回 3 个元素 起止位置都可见时，可以快速算出区间的长度（stop - start），如切片 my_list[3:6] 即包含 6 - 3 = 3 个元素 可以利用任意一个下标把序列分割成不重叠的两部分（my_list[:x] 和 my_list[x:]） step可以用 s[a:b:c] 的形式对 s 在 a 和 b 之间以 c 为间隔取值。c 值还可以为负，表示反向取值。1234567&gt;&gt;&gt; s = 'bicycle'&gt;&gt;&gt; s[::3]'bye'&gt;&gt;&gt; s[::-1]'elcycib'&gt;&gt;&gt; s[::-2]'eccb' 对 seq[start:stop:step] 求值时，Python 会调用 seq.__getitem__(slice(start, stop, step))。 对切片赋值如果把切片放在赋值语句左边，或把它作为 del 操作的对象，则可以对切片所属的序列进行拼接、切除或就地修改等操作。123456789101112131415&gt;&gt;&gt; l = list(range(10))&gt;&gt;&gt; l[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]&gt;&gt;&gt; l[2:5] = [20, 30]&gt;&gt;&gt; l[0, 1, 20, 30, 5, 6, 7, 8, 9]&gt;&gt;&gt; del l[5:7]&gt;&gt;&gt; l[0, 1, 20, 30, 5, 8, 9]&gt;&gt;&gt; l[3::2] = [11, 22]&gt;&gt;&gt; l[0, 1, 20, 11, 5, 22, 9]&gt;&gt;&gt; l[2:5] = [100]&gt;&gt;&gt; l[0, 1, 100, 22, 9] 需要注意的是，在对切片进行赋值操作时，赋值语句的右侧必须是个可迭代对象。 对序列使用 + 和 *Python 程序员一般默认序列都会支持 + 和 * 的拼接操作。在拼接过程中，两个被操作的序列不会发生任何改动，Python 会创建一个新的包含拼接结果的序列。12345&gt;&gt;&gt; l = [1, 2, 3]&gt;&gt;&gt; l * 5[1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3]&gt;&gt;&gt; 5 * 'abcd''abcdabcdabcdabcdabcd' 如果 a * n 语句中序列 a 里的元素是对其他可变对象的引用的话，这个式子的结果可能会出乎意料。比如用 my_list = [[]] * 3 来初始化一个有列表组成的列表，实际上得到的列表里包含的三个元素是三个引用，且这三个引用都指向同一列表。123456&gt;&gt;&gt; weird_board = [['-'] * 3] * 3&gt;&gt;&gt; weird_board[['-', '-', '-'], ['-', '-', '-'], ['-', '-', '-']]&gt;&gt;&gt; weird_board[1][2] = 'O'&gt;&gt;&gt; weird_board[['-', '-', 'O'], ['-', '-', 'O'], ['-', '-', 'O']] 其错误的本质等同于如下代码：12345678&gt;&gt;&gt; row = ['-'] * 3&gt;&gt;&gt; board = []&gt;&gt;&gt; for i in range(3):... board.append(row)...&gt;&gt;&gt; board[1][2] = 'O'&gt;&gt;&gt; board[['-', '-', 'O'], ['-', '-', 'O'], ['-', '-', 'O']] 即追加同一个行对象（row）到游戏币（board） 正确的做法代码如下：123456&gt;&gt;&gt; board = [['-'] * 3 for i in range(3)]&gt;&gt;&gt; board[['-', '-', '-'], ['-', '-', '-'], ['-', '-', '-']]&gt;&gt;&gt; board[1][2] = 'O'&gt;&gt;&gt; board[['-', '-', '-'], ['-', '-', 'O'], ['-', '-', '-']] 等同于如下代码：12345678910&gt;&gt;&gt; board = []&gt;&gt;&gt; for i in range(3):... row = ['-'] * 3... board.append(row)...&gt;&gt;&gt; board[['-', '-', '-'], ['-', '-', '-'], ['-', '-', '-']]&gt;&gt;&gt; board[1][2] = 'O'&gt;&gt;&gt; board[['-', '-', '-'], ['-', '-', 'O'], ['-', '-', '-']] 即每次迭代中都新建了一个列表，作为新的一行（row）追加到游戏板子（board） 序列的增量赋值增量赋值运算符 += 和 *= 的行为取决于第一个操作对象。+= 调用的特殊方法是 __iadd__（自增）。如果某个类没有实现该方法，Python 会退一步调用 __add__。 如 a += b 就会调用 a 中实现的 __iadd__ 方法，同时对于可变序列（如 list、bytearray、array.array），该方法的行为类似于 a.extend(b)，在 a 上就地改动。如 a 没有实现 __iadd__，a += b 的效果就类似于 a = a + b，计算 a + b 得到一个新的对象，再把这个对象赋值给 a。 *= 对应的是 __imul__。1234567891011121314&gt;&gt;&gt; l = [1, 2, 3]&gt;&gt;&gt; id(l)2888988078920&gt;&gt;&gt; l *= 2&gt;&gt;&gt; l[1, 2, 3, 1, 2, 3]&gt;&gt;&gt; id(l)2888988078920&gt;&gt;&gt; t = (1, 2, 3)&gt;&gt;&gt; id(t)2888988799688&gt;&gt;&gt; t *= 2&gt;&gt;&gt; id(t)2888988107592 作为可变对象的列表运用增量乘法后，ID 没变；而作为不可变对象的元组运用增量乘法后，新的元组被创建。 因此对于不可变序列做重复拼接操作效率会很低，每次都会有一个新对象。但字符串除外，由于对字符串做 += 等操作太普遍，CPython 专门做了优化。在为字符串初始化内存时，程序会预留额外的可扩展空间。 list.sort 与 sortedlist.sort 方法会就地排序列表，即在原列表的基础上完成排序，不会再另外复制一份。也因此其返回值为 None。内置的 sorted 函数则会新建一个列表作为返回值。它可以接收任何形式的可迭代对象（包含不可变序列和生成器），最后返回的始终是排序好的列表。123456789101112&gt;&gt;&gt; fruits = ['grape', 'raspberry', 'apple', 'banana']&gt;&gt;&gt; sorted(fruits)['apple', 'banana', 'grape', 'raspberry']&gt;&gt;&gt; fruits['grape', 'raspberry', 'apple', 'banana']&gt;&gt;&gt; sorted(fruits, key=len)['grape', 'apple', 'banana', 'raspberry']&gt;&gt;&gt; fruits['grape', 'raspberry', 'apple', 'banana']&gt;&gt;&gt; fruits.sort()&gt;&gt;&gt; fruits['apple', 'banana', 'grape', 'raspberry'] 参考资料Fluent Python]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Program</tag>
        <tag>Advanced</tag>
        <tag>DataStructure</tag>
        <tag>List</tag>
        <tag>Array</tag>
        <tag>Slice</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Fluent Python 笔记 —— 对象引用、可变性及其影响]]></title>
    <url>%2F2020%2F10%2F12%2Ffluent-python-object-reference-and-mutation%2F</url>
    <content type="text"><![CDATA[别名Python 中的变量类似于 Java 中的引用式变量，可以理解为附加在对象上的“标注”。 比如下面代码中的变量 a 和 b 实际上指向同一个列表：12345&gt;&gt;&gt; a = [1, 2, 3]&gt;&gt;&gt; b = a&gt;&gt;&gt; a.append(4)&gt;&gt;&gt; b[1, 2, 3, 4] 对于引用式变量的赋值，“把变量分配给对象”的说法更为合理。即赋值语句的右边先执行，右侧产生的对象在赋值前就已经创建了。 尝试运行以下代码：12345678910111213&gt;&gt;&gt; class Gizmo:... def __init__(self):... print(f'Gizmo id: &#123;id(self)&#125;')...&gt;&gt;&gt; x = Gizmo()Gizmo id: 140497327120880&gt;&gt;&gt; y = Gizmo() * 10Gizmo id: 140497327319840Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;TypeError: unsupported operand type(s) for *: 'Gizmo' and 'int'&gt;&gt;&gt; dir()['Gizmo', '__annotations__', '__builtins__', '__doc__', '__loader__', '__name__', '__package__', '__spec__', 'x'] 输出的 Gizmo id: ... 是创建 Gizmo 实例时的副作用。因此 y = Gizmo() * 10 实际上创建了一个新的 Gizmo 实例，但绝不会创建变量 y。因为赋值语句右侧在求值时抛出了异常。因此在赋值语句中，对象在右边创建或获取，之后左边的变量才会绑定给对象。类似于给对象贴标签，贴的多个标签就是别名。 相等性两个变量指向同一个对象：123456789&gt;&gt;&gt; charles = &#123;'name': 'Charles L. Dodgson', 'born': 1832&#125;&gt;&gt;&gt; lewis = charles&gt;&gt;&gt; lewis is charlesTrue&gt;&gt;&gt; id(charles), id(lewis)(140497326642048, 140497326642048)&gt;&gt;&gt; lewis['balance'] = 950&gt;&gt;&gt; charles&#123;'name': 'Charles L. Dodgson', 'born': 1832, 'balance': 950&#125; lewis 是 charles 的别名，两者指向同一个对象。 alex 绑定具有同样内容的另一个对象：12345&gt;&gt;&gt; alex = &#123;'name': 'Charles L. Dodgson', 'born': 1832, 'balance': 950&#125;&gt;&gt;&gt; alex == charlesTrue&gt;&gt;&gt; alex is charlesFalse 则 alex 指代的对象与赋值给 charles 的对象内容一样，但两者绑定的是不同的对象。 每个变量都有标识、类型和值。对象一旦创建，其标识绝不会变。可以将标识理解为对象在内存中的地址。is 运算符比较两个对象的标识，id() 函数返回对象标识的整数表示。 == 与 is 的选择== 用于比较两个对象的值（对象中保存的数据），而 is 比较对象的标识。通常在比较时关注的是值而不是标识，因此 == 出现的几率比 is 要高。 但在变量和单例值之间比较时，应使用 is。比如用 is 检查变量绑定的值是不是 None：x is None 或 x is not None。原因是 is 运算符比 == 速度更快。因为它不能重载，Python 不用寻找并调用特殊方法，而是直接比较两个整数 ID。而 a == b 是 a.__eq__(b) 的语法糖。多数内置类型使用更有意义的方式覆盖了 __eq__ 方法，因此相等性测试可能涉及大量处理工作。 元组的相对不可变性元组中保存的是对象的引用。即便元组本身不可变，若元素引用的对象是可变的，则该元素依然可变。即元组的不可变性指的是 tuple 数据结构的物理内容（即保存的引用）不可变，与引用指向的对象无关。 12345678910111213&gt;&gt;&gt; t1 = (1, 2, [30, 40])&gt;&gt;&gt; t2 = (1, 2, [30, 40])&gt;&gt;&gt; t1 == t2True&gt;&gt;&gt; id(t1[-1])140497326641856&gt;&gt;&gt; t1[-1].append(99)&gt;&gt;&gt; t1(1, 2, [30, 40, 99])&gt;&gt;&gt; id(t1[-1])140497326641856&gt;&gt;&gt; t1 == t2False 元组的值会随着其中元素引用的可变对象的变化而变化。元组中不可变的是元素的标识。 浅复制与深复制赋值列表（或多数内置的其他可变集合）最简单的方式是使用类型构造方法。12345678&gt;&gt;&gt; l1 = [3, [55, 44], (7, 8, 9)]&gt;&gt;&gt; l2 = list(l1)&gt;&gt;&gt; l2[3, [55, 44], (7, 8, 9)]&gt;&gt;&gt; l2 == l1True&gt;&gt;&gt; l2 is l1False list(l1) 会创建 l1 的副本。副本与源列表相等，但两者实际指向不同的对象。还可以使用同样效果的 l2 = l1[:] 语句。 但是，list 构造方法和 [:] 做的都是浅复制，即只复制最外层的容器，副本中的元素是源容器中元素的引用。若所有元素都是不可变的，则没有任何问题，还可以节省内存；若容器中存在可变的元素，则有可能导致意想不到的问题。12345678910111213141516171819202122&gt;&gt;&gt; l1 = [3, [66, 55, 44], (7, 8, 9)]&gt;&gt;&gt; l2 = list(l1)&gt;&gt;&gt; l1.append(100)&gt;&gt;&gt; l1[3, [66, 55, 44], (7, 8, 9), 100]&gt;&gt;&gt; l2[3, [66, 55, 44], (7, 8, 9)]&gt;&gt;&gt; l1[1].remove(55)&gt;&gt;&gt; l1[3, [66, 44], (7, 8, 9), 100]&gt;&gt;&gt; l2[3, [66, 44], (7, 8, 9)]&gt;&gt;&gt; l2[1] += [33, 22]&gt;&gt;&gt; l1[3, [66, 44, 33, 22], (7, 8, 9), 100]&gt;&gt;&gt; l2[3, [66, 44, 33, 22], (7, 8, 9)]&gt;&gt;&gt; l2[2] += (10, 11)&gt;&gt;&gt; l2[3, [66, 44, 33, 22], (7, 8, 9, 10, 11)]&gt;&gt;&gt; l1[3, [66, 44, 33, 22], (7, 8, 9), 100] l2 是 l1 的浅复制副本，但是二者引用同一个列表 [66, 55, 44] 和同一个元组 (7, 8, 9) 把 100 追加到 l1 中，对 l2 没有任何影响 把内部列表 l1[1] 中的 55 移除，l2 也会出现同样的改动。原因是 l2[1] 与 l1[1] 绑定的是同一个列表 同样的，l2[1] 引用的列表通过 += 运算符就地修改列表，这样的修改也会在 l1[1] 中体现 对元组来说，+= 运算符会创建一个新元组重新绑定给变量 l2[2]，等同于 l2[2] = l2[2] + (10, 11)，因此最终状态下 l1 和 l2 中的元组不再是同一个对象 对自定义对象做深复制和浅复制12345678910111213# bus.pyclass Bus: def __init__(self, passengers=None): if passengers is None: self.passengers = [] else: self.passengers = list(passengers) def pick(self, name): self.passengers.append(name) def drop(self, name): self.passengers.remove(name) 1234567891011121314&gt;&gt;&gt; from bus import Bus&gt;&gt;&gt; import copy&gt;&gt;&gt; bus1 = Bus(['Alice', 'Bill', 'Claire', 'David'])&gt;&gt;&gt; bus2 = copy.copy(bus1)&gt;&gt;&gt; bus3 = copy.deepcopy(bus1)&gt;&gt;&gt; id(bus1), id(bus2), id(bus3)(140247402065344, 140247402153680, 140247380561296)&gt;&gt;&gt; bus1.drop('Bill')&gt;&gt;&gt; bus2.passengers['Alice', 'Claire', 'David']&gt;&gt;&gt; id(bus1.passengers), id(bus2.passengers), id(bus3.passengers)(140247380615936, 140247380615936, 140247401717056)&gt;&gt;&gt; bus3.passengers['Alice', 'Bill', 'Claire', 'David'] 使用 copy 和 deepcopy 创建 3 个不同的 Bus 实例。bus1 中的 Bill 下车后，bus2 中也没有他了。原因是 bus2 是 bus1 的浅复制副本，他们共享同一个列表对象。bus3 是 bus1 的深复制副本，其 passengers 属性指向另一个不同的列表对象。 函数参数作为引用Python 唯一支持的参数传递模式是共享传参，多数面向对象语言都采用这一模式。在该模式中，函数内部的形参是实参的别名。上述方案导致函数可能会修改作为参数传入的可变对象，虽然无法修改那些对象的标识（即不能把一个对象替换为另一个对象）。 如下面这个简单的函数，在参数上调用 +=，分别将数字、列表、元组作为参数传入，效果如下：1234567891011121314151617181920&gt;&gt;&gt; def f(a, b):... a += b... return a...&gt;&gt;&gt; x = 1&gt;&gt;&gt; y = 2&gt;&gt;&gt; f(x, y)3&gt;&gt;&gt; a = [1, 2]&gt;&gt;&gt; b = [3, 4]&gt;&gt;&gt; f(a, b)[1, 2, 3, 4]&gt;&gt;&gt; a, b([1, 2, 3, 4], [3, 4])&gt;&gt;&gt; t = (10, 20)&gt;&gt;&gt; u = (30, 40)&gt;&gt;&gt; f(t, u)(10, 20, 30, 40)&gt;&gt;&gt; t, u((10, 20), (30, 40)) 不可变的数字 x 和元组 t 都没变，而可变对象列表 a 变了。 可变类型作为参数默认值函数的可选参数可以有默认值，但是应尽量避免使用可变对象作为参数的默认值。 123456789class HauntedBus: def __init__(self, passengers=[]): self.passengers = passengers def pick(self, name): self.passengers.append(name) def drop(self, name): self.passengers.remove(name) 123456789101112131415161718&gt;&gt;&gt; from hauntedbus import HauntedBus&gt;&gt;&gt; bus1 = HauntedBus(['Alice', 'Bill'])&gt;&gt;&gt; bus1.passengers['Alice', 'Bill']&gt;&gt;&gt; bus2 = HauntedBus()&gt;&gt;&gt; bus2.pick('Carrie')&gt;&gt;&gt; bus2.passengers['Carrie']&gt;&gt;&gt; bus3 = HauntedBus()&gt;&gt;&gt; bus3.passengers['Carrie']&gt;&gt;&gt; bus3.pick('Dave')&gt;&gt;&gt; bus2.passengers['Carrie', 'Dave']&gt;&gt;&gt; bus2.passengers is bus3.passengersTrue&gt;&gt;&gt; bus1.passengers['Alice', 'Bill'] 上述的代码表明，bus2.passengers 和 bus3.passengers 指向同一个列表，而 bus1.passengers 是不同的列表。即没有指定初始乘客的 HauntedBus 实例会共享同一个乘客列表。这个问题的根源在于，函数参数的默认值是在定义函数时计算的，因此该默认值成为了函数对象的属性。因此 self.passengers 变成了 passengers 参数默认值（可变的列表对象）的别名，导致后续的函数调用都会受到影响。1234&gt;&gt;&gt; HauntedBus.__init__.__defaults__(['Carrie', 'Dave'],)&gt;&gt;&gt; HauntedBus.__init__.__defaults__[0] is bus2.passengersTrue 函数对可变参数的修改如果定义的函数接收可变参数，应谨慎考虑调用方是否期望修改传入的参数。比如函数接收一个字典，且在处理过程中要修改它，则该副作用是否要体现到函数外部？ 如下面的 TwilightBus 实例：123456789101112class TwilightBus: def __init__(self, passengers=None): if passengers is None: self.passengers = [] else: self.passengers = passengers def pick(self, name): self.passengers.append(name) def drop(self, name): self.passengers.remove(name) 1234567&gt;&gt;&gt; from twilightbus import TwilightBus&gt;&gt;&gt; basketball_team = ['Sue', 'Tina', 'Maya', 'Diana', 'Pat']&gt;&gt;&gt; bus = TwilightBus(basketball_team)&gt;&gt;&gt; bus.drop('Tina')&gt;&gt;&gt; bus.drop('Pat')&gt;&gt;&gt; basketball_team['Sue', 'Maya', 'Diana'] basketball_team 中有 5 名学生，使用这队学生实例化 TwilightBus。两个学生从 bus 下车后，这两人就从篮球队（basketball_team）中消失了。 代码中 self.passengers = passengers 赋值语句把 self.passengers 变成 passengers 的别名，而后者是传给 __init__ 方法的实参（即 basketball_team）的别名。导致在 self.passengers 上调用 .remove() 和 .append() 方法实际上会修改传给构造方法的列表（basketball_team）。正确的做事是，校车维护自己的乘客列表。即在 __init__ 中，应该把 passengers 参数值的副本赋值给 self.passengers。12345def __init__(self, passengers=None): if passengers is None: self.passengers = [] else: self.passengers = list(passengers) 小结变量保存的是引用，此种行为对 Python 编程有很多实际影响： 简单的赋值操作不会创建副本 对于 += 或 *= 的增量赋值，若左边变量绑定的是不可变对象，则会创建新对象；若左边是可变对象，则就地修改该可变对象 为现有变量赋予新值，不会直接修改之前绑定的变量值。而是创建新的对象并绑定给变量（重新绑定） 函数参数以别名的形式传递，意味着函数可能会修改通过参数传入的可变对象。除非在本地创建可变对象的副本 使用可变类型作用函数参数的默认值有一定风险。若就地修改了参数，则默认值也会改变，进而影响以后使用默认值的调用 参考资料Fluent Python]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Program</tag>
        <tag>Advanced</tag>
        <tag>Mutation</tag>
        <tag>Reference</tag>
        <tag>Object</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[理解 JavaScript（ECMAScript 6）—— 异步编程]]></title>
    <url>%2F2020%2F09%2F19%2Funderstanding-javascript-ecmascript-6-async-programming%2F</url>
    <content type="text"><![CDATA[JavaScript 作为主要面向 Web 编程而创建的语言，其诞生初期即具有了应对异步的用户交互（如点击鼠标、按下键盘等）的能力。后续的 Node.js 引入了 callbacks 作为除事件模型以外的另一种实现异步编程的方式，而之后的 Promise 又使得 JavaScript 处理异步需求的能力更为强大。 一、异步编程基础Event Model当用户点击鼠标或按下键盘上的某个按键时，一个对应的特殊事件（比如 onclick）触发，该事件关联的一系列响应动作即被添加到工作队列中最终被执行。这是 JavaScript 中最基本的异步编程方式。1234567&lt;button id="my-btn"&gt;Click Me&lt;/button&gt; &lt;script&gt; let button = document.getElementById("my-btn") button.onclick = function(event) &#123; console.log("Clicked") &#125; &lt;/script&gt; callbackcallback 模式与基于事件模型的异步编程类似，异步代码都是在后续的某个特定时间点执行。不同的是，其异步执行的操作（函数）需要作为参数传递。12345678910111213let fs = require("fs")fs.readFile("example.txt", &#123; encoding: "utf8" &#125;, function(err, contents)&#123; if (err) &#123; throw err &#125; console.log(contents)&#125;)console.log("Hi!")// Hi!// This is an example text file 上面的例子使用了经典的 Node.js error-first 回调函数模式。readFile() 函数从硬盘读取某个文件，在文件读取完成之后执行 callback 函数。如果读取文件时发生错误，则传递给回调函数的 err 参数为错误对象；未发生错误则 content 参数中包含了读取的文件内容。 在 callback 模式中，readFile() 会立即开始执行，并且在文件读取的进度开启之后暂停。紧接着 readFile() 后面的 console.log(&quot;Hi!&quot;) 会立即执行并输出 Hi! 到屏幕上（此时 readFile() 处于暂停状态，回调函数中的 console.log(contents) 也并未执行）。文件读取结束后，一个新的任务（即 callback 函数以及传递给它的参数）被添加到任务队列中等待最终被执行。从输出中可以看到，Hi! 要先于 contents 被打印。 callback 模式的问题在于，当有过多的 callback 函数嵌套时，会出现称为 callback hell 的情况：123456789101112131415161718192021method1(function(err, result) &#123; if (err) &#123; throw err; &#125; method2(function(err, result) &#123; if (err) &#123; throw err; &#125; method3(function(err, result) &#123; if (err) &#123; throw err; &#125; method4(function(err, result) &#123; if (err) &#123; throw err; &#125; method5(result); &#125;); &#125;); &#125;);&#125;); 二、Promise除了前面提到的 callback hell 会使代码变得过于繁杂以至于难以理解和调试之外，callback 模式对于处理某些较复杂的逻辑也有一定的局限性。比如希望两个异步操作并行执行，在双方都完成之后提醒用户；或者两个异步任务同时开始但是只获取第一个任务执行完后的结果。在这些情景下，就需要同时追踪多个 callback 函数的状态。promise 则针对以上情况做了相应的提升。 promise 是一种对应异步操作执行结果的“占位符”。12// readFile “保证”会在未来的某个时间点完成let promise = readFile("example.txt") readFile() 并不会立即开始读取文件。相反，它会直接返回一个 promise 对象表示异步的读取操作，方便在后续的代码中通过这个 promise 对象访问读取任务的结果。该 promise 代表的结果是否可用取决于其生命周期所处的阶段。 Promise 生命周期promise 的生命周期起始于 pending (unsettled) 状态，表明对应的异步操作还未完成。比如前面的 let promise = readFile(&quot;example.txt&quot;)，在 readFile() 函数返回后 promise 就立即进入了 pending 状态。而异步操作最终完成时，promise 则进入 settled 状态，具体包含两种情况： Fulfilled：promise 对应的异步操作成功执行完毕 Rejected：promise 对应的异步操作未执行完毕（出现错误或其他情况） 内部属性 [[PromiseState]] 用来标记其生命周期状态（如 pending、fulfilled、rejected），该属性不对 promise 对象外部暴露，因此不可以人为修改 promise 对象的生命周期。但是可以在 promise 的状态改变时通过 then() 方法自动触发一系列动作。 所有的 promise 对象都具有 then() 方法，该方法可以接收两个函数作为参数。第一个参数为当 promise 状态为 fulfilled 时调用的函数，所有与异步操作相关的数据都会被传递给该函数；第二个参数为当 promise 状态为 rejected 时调用的函数。这两个参数都是可选的。123456789101112131415161718192021222324let promise = readFile("example.txt");promise.then(function(contents) &#123; // fulfillment console.log(contents)&#125;, function(err) &#123; // rejection console.error(err.message)&#125;);promise.then(function(contents) &#123; // fulfillment console.log(contents);&#125;);promise.then(null, function(err) &#123; // rejection console.error(err.message);&#125;);promise.catch(function(err) &#123; // rejectionconsole.error(err.message);&#125;); 创建 Promisepromise 可以使用 Promise 构造器创建，该构造器接收一个称为 executor 的函数作为参数，包含了初始化 promise 的代码。executor 接收 resolve()、reject() 两个函数作为参数，resolve() 将在 executor 执行成功后调用，传递 promise 已做好准备的信号；executor 执行失败了则调用 reject()。 一个 Promise 的完整示例：123456789101112131415161718192021222324252627282930let fs = require("fs")function readFile(filename) &#123; return new Promise(function(resolve, reject) &#123; fs.readFile(filename, &#123; encoding: "utf8" &#125;, function(err, contents) &#123; // check for errors if (err) &#123; reject(err) return &#125; // the read succeeded resolve(contents) &#125;) &#125;)&#125;let promise = readFile("example.txt")// listen for both fulfillment and rejectionpromise.then(function(contents) &#123; // fulfillment console.log(contents)&#125;, function(err) &#123; // rejection console.error(err.message)&#125;)console.log("Hi!")// Hi!// This is an example text file Promise 的执行流程参考如下代码：12345678910111213141516171819202122232425262728293031323334console.log("At code start")var delayedPromise = new Promise((resolve, reject) =&gt; &#123; console.log("delayedPromise executor") setTimeout(() =&gt; &#123; console.log("Resolving delayedPromise") resolve("Hello") &#125;, 1000)&#125;)console.log("After creating delayedPromise")delayedPromise.then(contents =&gt; &#123; console.log("delayedPromise resolve handled with", contents)&#125;)const immediatePromise = new Promise((resolve, reject) =&gt; &#123; console.log("immediatePromise executor") resolve("World")&#125;)immediatePromise.then(contents =&gt; &#123; console.log("immediatePromise resolve handled with", contents)&#125;)console.log("At code end")// At code start// delayedPromise executor// After creating delayedPromise// immediatePromise executor// At code end// immediatePromise resolve handled with World// Resolving delayedPromise// delayedPromise resolve handled with Hello 具体的执行逻辑为： 代码开始执行，通过 Promise 构造器创建一个 delayedPromise，其中的 console.log() 和 setTimeout()（也可以是其他异步操作）函数立即执行 delayedPromise 创建之后，其最终的结果和状态（是否成功执行）不能立即知晓，因此处于 pending 状态 调用 delayedPromise 的 then 方法，将一个当 promise 成功 resolve 后才执行的 callback 函数放到执行计划中 继续创建另一个 immediatePromise，该 promise 会在创建的过程中立即 resolve，因此其创建完成后即处于 resolved 状态 调用 immediatePromise 的 then 方法，注册一个当 promise 成功 resolve 后才执行的 callback 函数 从最终的结果中可以看出，即便 immediatePromise 在创建后即处于 resolved 状态，At code end 实际上是先于前面的 immediatePromise.then() 输出的。原因是 promise 被设计成专门针对异步操作，then() 方法中的 callback 会永远在当前事件循环中所有代码执行完后才开始触发。 因此实际的执行顺序为：At code start -&gt; 创建 delayedPromise -&gt; 通过 then() 注册 delayPromise 状态为 resolved 时触发的 callback -&gt; 创建 immediatePromise -&gt; 通过 then() 注册 immediatePromise 状态为 resolved 时触发的 callback -&gt; At code end -&gt; immediatePromise 先 resolved，其关联的 callback 执行 -&gt; delayedPromise resolved，其关联的 callback 执行 三、Chaining Promises截止到前面的介绍，promise 看起来只不过在 callback 的基础上做了一点点有限的提升。实际 promise 支持多种形式的连接，足以完成更加复杂的异步逻辑。 每次对 promise 的 then() 或 catch() 方法的调用，实际上都会创建和返回另一个 promise 对象。第二个 promise 对象只有在第一个 promise fulfilled 或 rejected 后才会被 resolve。123456789101112let p1 = new Promise(function(resolve, reject) &#123; resolve(42)&#125;)p1.then(function(value) &#123; console.log(value)&#125;).then(function() &#123; console.log("Finished")&#125;)// 42// Finished unchained 版本：1234567891011let p1 = new Promise(function(resolve, reject) &#123; resolve(42)&#125;)let p2 = p1.then(function(value) &#123; console.log(value)&#125;)p2.then(function() &#123; console.log("Finished")&#125;) p2.then() 也会返回一个 promise 对象，只不过它没有在代码中使用。 错误捕获Promise chaining 允许用户捕获之前的 promise 中出现的错误。12345678910let p1 = new Promise(function(resolve, reject) &#123; resolve(42)&#125;)p1.then(function(value) &#123; throw new Error("Boom!")&#125;).catch(function(error) &#123; console.log(error.message)&#125;)// Boom! p1 的 fulfillment handler 抛出异常，第二个 promise 的 catch() 方法通过它的 rejection handler 接收到该异常。同样的方式也适用于 rejection handler 抛出异常：123456789101112let p1 = new Promise(function(resolve, reject) &#123; throw new Error("Explosion!")&#125;)p1.catch(function(error) &#123; console.log(error.message) throw new Error("Boom!")&#125;).catch(function(error) &#123; console.log(error.message)&#125;)// Explosion!// Boom! executor 抛出异常触发 p1 的 rejection handler，该 handler 又抛出另一个异常触发第二个 promise 的 rejection handler。 Promise Chain 中的返回值Promise Chain 中另一个很重要的特性即在两个 promise 之间传递数据。之前的代码中，可以通过 executor 中的 resovle() 函数将值传递给该 promise 的 fulfillment handler。此外，还可以通过为 fulfillment handler 指定一个返回值，将该值沿着 promise chain 传递。12345678910111213let p1 = new Promise(function(resolve, reject) &#123; resolve(42)&#125;)p1.then(function(value) &#123; console.log(value) return value + 1&#125;).then(function(value) &#123; console.log(value)&#125;)// 42// 43 同样的操作也可以用在 rejection handler 上：12345678910111213let p1 = new Promise(function(resolve, reject) &#123; reject(42)&#125;)p1.catch(function(value) &#123; console.log(value) return value + 1&#125;).then(function(value) &#123; console.log(value)&#125;)// 42// 43 四、响应多个 Promise之前的代码中都是一次只响应一个 promise，但是有时候需要监控多个 promise 的状态并决定之后的动作。ECMAScript 6 提供了两种方法（Promise.all() 和 Promise.race）应对这些情况。 Promise.all()Promise.all() 方法只接收一个包含所有需要监控的 promise 的可迭代对象（如列表）作为参数，并且只有当这些需要监控的 promise 全部 resolved 时，Promise.all() 返回的 promise 才会 resolved。1234567891011121314151617181920let p1 = new Promise(function(resolve, reject) &#123; resolve(42)&#125;)let p2 = new Promise(function(resolve, reject) &#123; resolve(43)&#125;)let p3 = new Promise(function(resolve, reject) &#123; resolve(44)&#125;)let p4 = Promise.all([p1, p2, p3])p4.then(function(value) &#123; console.log(Array.isArray(value)) // true console.log(value[0]) // 42 console.log(value[1]) // 43 console.log(value[2]) // 44&#125;) Promise.all() 创建了 promise p4。只有当列表中的 promise p1，p2，p3 全部 fulfilled 之后，p4 最终才会 fulfilled。前面 3 个 promise resolve 的数字组成列表传递给 p4 的 fulfillment handler，这些数字与生产它们的 promise 的位置是一一对应的。 如果任意一个传入 Promise.all() 的 promise 状态是 rejected，则 Promise.all() 返回的 promise 也会立即 rejected，不会等待其他 promise 结束。1234567891011121314151617let p1 = new Promise(function(resolve, reject) &#123; resovle(42)&#125;)let p2 = new Promise(function(resolve, reject) &#123; reject(43)&#125;)let p3 = new Promise(function(resolve, reject) &#123; resolve(44)&#125;)let p4 = Promise.all([p1, p2, p3])p4.catch(function(value) &#123; console.log(Array.isArray(value)) // false console.log(value) // 43&#125;) 在上面的代码中，p2 的状态为 rejected，p4 的 rejection handler 会立即调用，不会等待 p1 和 p3 执行完毕（p1 和 p3 最终会执行完毕，只是 p4 不会等它们）。 Promise.race()Promise.race() 同样接收一个包含需要监控的多个 promise 的可迭代对象，返回一个新的 promise。但是不同于 Promise.all() 会等待所有监控中的 promise resolved，Promise.race() 会在列表中任意一个 promise resolve 后立即返回。123456789101112131415161718192021let p1 = new Promise(function(resolve, reject) &#123; setTimeout(resolve, 500, 42)&#125;)let p2 = new Promise(function(resolve, reject) &#123; setTimeout(resolve, 100, 43)&#125;)let p3 = new Promise(function(resolve, reject) &#123; setTimeout(resolve, 200, 44)&#125;)let p4 = Promise.race([p1, p2, p3])p4.then(function(value) &#123; console.log(value)&#125;)// 43 传递给 Promise.race() 的 promise 像是处在一个赛道中，看哪一个先执行完毕。如果第一个运行完的 promise 状态为 fulfilled，则最后返回的 promise 状态为 fulfilled；如果第一个运行完的 promise 状态为 rejected，则最后返回的 promise 状态为 rejected。 参考资料Understanding ECMAScript 6Secrets of the JavaScript Ninja, Second Edition]]></content>
      <categories>
        <category>Program</category>
      </categories>
      <tags>
        <tag>Program</tag>
        <tag>Advanced</tag>
        <tag>JavaScript</tag>
        <tag>Async</tag>
        <tag>ECMAScript6</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 中的特殊方法及其应用]]></title>
    <url>%2F2020%2F09%2F19%2Fmagic-methods-in-python%2F</url>
    <content type="text"><![CDATA[Python 中的特殊方法主要是为了被解释器调用的，因此应该尽量使用 len(my_object) 而不是 my_object.__len__() 这种写法。在执行 len(my_object) 时，Python 解释器会自行调用 my_object 中实现的 __len__ 方法。 除非有大量的元编程存在，直接调用特殊方法的频率应远小于实现它们的次数。 模拟数值类型可以通过在自定义对象中实现 __add__ 和 __mul__ 等特殊方法 ，令其支持 +、* 等运算符。如下面的模拟向量的 Vector 类：12345678910111213141516171819202122# vector.pyfrom math import hypotclass Vector: def __init__(self, x=0, y=0): self.x = x self.y = y def __repr__(self): return f'Vector(&#123;self.x&#125;, &#123;self.y&#125;)' def __abs__(self): return hypot(self.x, self.y) def __bool__(self): return bool(self.x or self.y) def __add__(self, other): return Vector(self.x + other.x, self.y + other.y) def __mul__(self, scalar): return Vector(self.x * scalar, self.y * scalar) 运行效果如下：12345678910&gt;&gt;&gt; from vector import Vector&gt;&gt;&gt; v1 = Vector(2, 4)&gt;&gt;&gt; v2 = Vector(2, 1)&gt;&gt;&gt; v1 + v2Vector(4, 5)&gt;&gt;&gt; v = Vector(3, 4)&gt;&gt;&gt; abs(v)5.0&gt;&gt;&gt; v * 3Vector(9, 12) 对象的字符串表示Python 有一个 repr 内置函数，能把一个对象用字符串的形式表示出来。实际上这种字符串表达是通过对象内部的 __repr__ 特殊方法定义的。默认情况下，在控制台里查看某个对象时，输出的字符串一般是 &lt;xxx object at 0x7fc99d6ab2e0&gt; 这种形式。 __repr__ 返回的字符串应该准确、无歧义，并尽可能表示出该对象是如何创建的。比如前面的 Vector 对象，其 __repr__ 中定义的字符串形式类似于 Vector(3, 4)，和对象初始化的语法非常近似。 __repr__ 和 __str__ 的区别在于，__str__ 是在向对象应用 str() 函数（或者用 print 函数打印某个对象）时被调用。其返回的字符串对终端用户更友好。如果只想实现其中一个特殊方法，__repr__ 应该是更优的选择。在对象没有实现 __str__ 方法的情况下，Python 解释器会用 __repr__ 代替。 1234567# myclass.pyclass MyClass: def __repr__(self): return 'MyClass' def __str__(self): return 'This is an instance of MyClass' 123456&gt;&gt;&gt; from myclass import MyClass&gt;&gt;&gt; my = MyClass()&gt;&gt;&gt; myMyClass&gt;&gt;&gt; print(my)This is an instance of MyClass 自定义布尔值Python 里有 bool 类型，但实际上任何对象都可以用在需要 bool 类型的上下文（比如 if 或 while 语句）中。为了判断某个值 x 的真假，Python 会调用 bool(x) 返回 True 或 False。 默认情况下，自定义类的实例总是为真。除非这个类对于 __bool__ 或 __len__ 方法有自己的实现。bool(x) 实际上调用了对象 x 中的 __bool__ 方法。如不存在 __bool__ 方法，则 bool(x) 会尝试调用 x.__len__()，返回 0 则为 False，否则为 True。 12345678910# boolclass.pyclass BoolClass: def __init__(self): self.list = [] def add(self, item): self.list.append(item) def __len__(self): return len(self.list) 1234567891011&gt;&gt;&gt; from boolclass import BoolClass&gt;&gt;&gt; b = BoolClass()&gt;&gt;&gt; len(b)0&gt;&gt;&gt; bool(b)False&gt;&gt;&gt; b.add(1)&gt;&gt;&gt; len(b)1&gt;&gt;&gt; bool(b)True 12345678910111213# boolclass.pyclass BoolClass: def __init__(self): self.list = [] def add(self, item): self.list.append(item) def __len__(self): return len(self.list) def __bool__(self): return bool(sum(self.list)) 123456789101112&gt;&gt;&gt; from boolclass import BoolClass&gt;&gt;&gt; b = BoolClass()&gt;&gt;&gt; b.add(1)&gt;&gt;&gt; len(b)1&gt;&gt;&gt; bool(b)True&gt;&gt;&gt; b.add(-1)&gt;&gt;&gt; len(b)2&gt;&gt;&gt; bool(b)False 参考资料Fluent Python]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Program</tag>
        <tag>OOP</tag>
        <tag>Object</tag>
        <tag>Methods</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JavaScript 解密 —— 数组（Array）及其函数式操作]]></title>
    <url>%2F2020%2F08%2F21%2Fsecrets-of-javascript-array%2F</url>
    <content type="text"><![CDATA[一、创建数组1234567891011const ninjas = ["Kuma", "Hattori", "Yagyu"]const samurai = new Array("Oda", "Tomoe")console.log(ninjas.length) // 3console.log(ninjas[ninjas.length-1]) // Yagyuconsole.log(ninjas[4]) // undefinedninjas[4] = "Ishi"console.log(ninjas.length) // 5console.log(ninjas) // [ 'Kuma', 'Hattori', 'Yagyu', &lt;1 empty item&gt;, 'Ishi' ]ninjas.length = 2console.log(ninjas) // [ 'Kuma', 'Hattori' ] 数组可以使用 [] 字面量或 Array() 构造器创建 可以使用索引访问数组中的元素。索引从 0 到 array.length - 1 数组的 length 属性表示数组的大小（即元素的个数）。读取超出索引范围的元素，会得到 undefined 给超出索引范围的元素赋值，会自动扩展数组 将数组的 length 属性改为一个较小的值，会自动删除溢出的元素 二、添加和移除元素 push：向数组尾部添加元素 unshift：向数组头部添加元素 pop：从数组尾部弹出元素 shift：从数组头部弹出元素 123456789101112131415const ninjas = []ninjas.push("Kuma")ninjas.push("Hattori")console.log(ninjas) // [ 'Kuma', 'Hattori' ]ninjas.unshift("Yagyu")console.log(ninjas) // [ 'Yagyu', 'Kuma', 'Hattori' ]const lastNinja = ninjas.pop()console.log(lastNinja) // Hattoriconsole.log(ninjas) // [ 'Yagyu', 'Kuma' ]const firstNinja = ninjas.shift()console.log(firstNinja) // Yagyuconsole.log(ninjas) // [ 'Kuma' ] 从任意位置添加或移除元素delete1234const ninjas = ["Yagyu", "Kuma", "Hattori", "Fuma"]delete ninjas[1]console.log(ninjas) // [ 'Yagyu', &lt;1 empty item&gt;, 'Hattori', 'Fuma' ]console.log(ninjas.length) // 4 用 delete 删除数组中的元素，实际上等于用 undefined 替换掉了删除的元素，导致数组中的对应位置出现一个“洞”（undefined），数组仍保持原来的长度。因此不符合最初的目的。 splice12345678910const ninjas = ["Yagyu", "Kuma", "Hattori", "Fuma"]var removedItems = ninjas.splice(1, 1)console.log(removedItems) // [ 'Kuma' ]console.log(ninjas.length) // 3console.log(ninjas) // [ 'Yagyu', 'Hattori', 'Fuma' ]removedItems = ninjas.splice(1, 2, "Mochizuki", "Yoshi", "Momochi")console.log(removedItems) // [ 'Hattori', 'Fuma' ]console.log(ninjas) // [ 'Yagyu', 'Mochizuki', 'Yoshi', 'Momochi' ]console.log(ninjas.length) // 4 移除元素时，splice 方法接收两个参数：被移除片段的起点的索引和片段的长度，返回移除的元素。 splice 方法还可用于在数组的指定位置插入元素。如上面代码中的 splice(1, 2, &quot;Mochizuki&quot;, &quot;Yoshi&quot;, &quot;Momochi&quot;) 即表示从索引 1 开始移除 2 个元素，并在该位置处添加 &quot;Mochizuki&quot;、&quot;Yoshi&quot;、&quot;Momochi&quot; 三个元素。 三、数组的一般操作遍历数组for1234567const ninjas = ["Yagyu", "Kuma", "Hattori"]for(let i = 0; i &lt; ninjas.length; i++)&#123; console.log(ninjas[i])&#125;// Yagyu// Kuma// Hattori forEach12345678const ninjas = ["Yagyu", "Kuma", "Hattori"]ninjas.forEach(ninja =&gt; &#123; console.log(ninja)&#125;)// Yagyu// Kuma// Hattori MappingforEach12345678910111213const ninjas = [ &#123;name: "Yagyu", weapon: "shuriken"&#125;, &#123;name: "Yoshi", weapon: "katana"&#125;, &#123;name: "Kuma", weapon: "wakizashi"&#125;]const weapons = []ninjas.forEach(ninja =&gt; &#123; weapons.push(ninja.weapon)&#125;)console.log(weapons)// [ 'shuriken', 'katana', 'wakizashi' ] map123456789const ninjas = [ &#123;name: "Yagyu", weapon: "shuriken"&#125;, &#123;name: "Yoshi", weapon: "katana"&#125;, &#123;name: "Kuma", weapon: "wakizashi"&#125;]const weapons = ninjas.map(ninja =&gt; ninja.weapon)console.log(weapons)// [ 'shuriken', 'katana', 'wakizashi' ] map 将其 callback 函数（即 ninja =&gt; ninja.weapon）应用到数组的每一个元素上，并以 callback 函数的返回值 ninja.weapon 为元素创建一个新的数组作为 map 的返回值。 Testingevery1234567891011const ninjas = [ &#123;name: "Yagyu", weapon: "shuriken"&#125;, &#123;name: "Yoshi" &#125;, &#123;name: "Kuma", weapon: "wakizashi"&#125;]const allNinjasAreNamed = ninjas.every(ninja =&gt; "name" in ninja)console.log(allNinjasAreNamed) // trueconst allNinjasAreArmed = ninjas.every(ninja =&gt; "weapon" in ninja)console.log(allNinjasAreArmed) // false every 方法接收一个 callback 函数，将其应用到数组的每一个元素。若所有的 callback 执行后的返回值都为 true，则 every 返回 true；否则 every 返回 false。 some1234567891011const ninjas = [ &#123;name: "Yagyu", weapon: "shuriken"&#125;, &#123;name: "Yoshi" &#125;, &#123;name: "Kuma", weapon: "wakizashi"&#125;]const allNinjasAreNamed = ninjas.every(ninja =&gt; "name" in ninja)console.log(allNinjasAreNamed) // trueconst allNinjasAreArmed = ninjas.some(ninja =&gt; "weapon" in ninja)console.log(allNinjasAreArmed) // true some 方法接收一个 callback 函数，将其应用到数组的每一个元素。若至少有一个数组元素应用 callback 函数后返回 true，则 some 返回 true；否则 some 返回 false。 即 every 可以检查数组的所有元素是否都满足一个特定的条件，该条件由 callback 函数指定。而 some 用于检查是否至少有一个数组元素满足某个特定条件。 Searchingfind1234567891011121314151617const ninjas = [ &#123;name: "Yagyu", weapon: "shuriken"&#125;, &#123;name: "Yoshi" &#125;, &#123;name: "Kuma", weapon: "wakizashi"&#125;]const ninjaWithWakizashi = ninjas.find(ninja =&gt; &#123; return ninja.weapon === "wakizashi"&#125;)console.log(ninjaWithWakizashi)// &#123; name: 'Kuma', weapon: 'wakizashi' &#125;const ninjaWithKatana = ninjas.find(ninja =&gt; &#123; return ninja.weapon === "katana"&#125;)console.log(ninjaWithKatana)// undefined find 方法会以 callback 函数为规则搜索数组中的元素，返回满足条件（callback 返回 true）的第一个元素。若所有元素都不满足，则返回 undefined。 filter123456789101112const ninjas = [ &#123;name: "Yagyu", weapon: "shuriken"&#125;, &#123;name: "Yoshi" &#125;, &#123;name: "Kuma", weapon: "wakizashi"&#125;]const armedNinjas = ninjas.filter(ninja =&gt; "weapon" in ninja)console.log(armedNinjas)// [// &#123; name: 'Yagyu', weapon: 'shuriken' &#125;,// &#123; name: 'Kuma', weapon: 'wakizashi' &#125;// ] filter 方法则用来以 callback 函数为规则搜索多个数组元素，符合条件（callback 返回 true）的多个元素以新数组的形式返回。 Finding index1234567const ninjas = ["Yagyu", "Yoshi", "Kuma", "Yoshi"]console.log(ninjas.indexOf("Yoshi")) // 1console.log(ninjas.lastIndexOf("Yoshi")) // 3const yoshiIndex = ninjas.findIndex(ninja =&gt; ninja === "Yoshi")console.log(yoshiIndex) // 1 Sortingsort 方法的常见形式：array.sort((a, b) =&gt; a - b) 传递给 sort 的 callback 函数（如上面的 (a, b) =&gt; a - b）即为具体的排序规则，可能达到的效果如下： callback 函数的返回值小于 0，则 a 排在 b 前面 callback 函数的返回值等于 0，则 a 与 b 在排序上权重相同 callback 函数的返回值大于 0，则 a 排在 b 后面 1234567891011121314const ninjas = [&#123;name: "Yoshi"&#125;, &#123;name: "Hattori"&#125;, &#123;name: "Kuma"&#125;]ninjas.sort(function(ninja1, ninja2)&#123; if(ninja1.name &lt; ninja2.name) &#123; return -1 &#125; if(ninja1.name &gt; ninja2.name) &#123; return 1 &#125; return 0&#125;)console.log(ninjas)// [ &#123; name: 'Hattori' &#125;, &#123; name: 'Kuma' &#125;, &#123; name: 'Yoshi' &#125; ]ninjas.sort((ninja1, ninja2) =&gt; ninja1.name.length - ninja2.name.length)console.log(ninjas)// [ &#123; name: 'Kuma' &#125;, &#123; name: 'Yoshi' &#125;, &#123; name: 'Hattori' &#125; ] AggregatingforEach12345678const numbers = [1, 2, 3, 4]let sum = 0numbers.forEach(number =&gt; &#123; sum += number&#125;)console.log(sum) // 10 reduce1234const numbers = [1, 2, 3, 4]const sum = numbers.reduce((aggregated, number) =&gt; aggregated + number, 0)console.log(sum) // 10 参考资料Secrets of the JavaScript Ninja, Second Edition]]></content>
      <categories>
        <category>JavaScript</category>
      </categories>
      <tags>
        <tag>Program</tag>
        <tag>DataStructure</tag>
        <tag>Development</tag>
        <tag>JavaScript</tag>
        <tag>Array</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Vue.js 通过 Event Bus 实现 Components 间的事件通信（实例）]]></title>
    <url>%2F2020%2F08%2F19%2Fvue-js-event-bus-for-communication-between-components%2F</url>
    <content type="text"><![CDATA[一、环境配置 vue create productapp --default npm install bootstrap@4.0.0 npm install --save core-js 二、源代码productapp/src/main.js：123456789101112131415import Vue from 'vue'import App from './App.vue'import "../node_modules/bootstrap/dist/css/bootstrap.min.css"Vue.config.productionTip = falsenew Vue(&#123; render: h =&gt; h(App), provide: function () &#123; return &#123; eventBus: new Vue() &#125; &#125;&#125;).$mount('#app') productapp/src/App.vue：123456789101112131415161718192021&lt;template&gt; &lt;div class=&quot;container-fluid&quot;&gt; &lt;div class=&quot;row&quot;&gt; &lt;div class=&quot;col-8 m-3&quot;&gt; &lt;product-display&gt;&lt;/product-display&gt; &lt;/div&gt; &lt;div class=&quot;col m-3&quot;&gt; &lt;product-editor&gt;&lt;/product-editor&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt;&lt;/template&gt;&lt;script&gt;import ProductDisplay from &quot;./components/ProductDisplay&quot;;import ProductEditor from &quot;./components/ProductEditor&quot;;export default &#123; name: &quot;App&quot;, components: &#123; ProductDisplay, ProductEditor &#125;&#125;;&lt;/script&gt; productapp/src/components/ProductDisplay.vue：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455&lt;template&gt; &lt;div&gt; &lt;table class=&quot;table table-sm table-striped table-bordered&quot;&gt; &lt;tr&gt; &lt;th&gt;ID&lt;/th&gt;&lt;th&gt;Name&lt;/th&gt;&lt;th&gt;Price&lt;/th&gt;&lt;th&gt;&lt;/th&gt; &lt;/tr&gt; &lt;tbody&gt; &lt;tr v-for=&quot;p in products&quot; v-bind:key=&quot;p.id&quot;&gt; &lt;td&gt;&#123;&#123; p.id &#125;&#125;&lt;/td&gt; &lt;td&gt;&#123;&#123; p.name &#125;&#125;&lt;/td&gt; &lt;td&gt;&#123;&#123; p.price &#125;&#125;&lt;/td&gt; &lt;td&gt; &lt;button class=&quot;btn btn-sm btn-primary&quot; @click=&quot;editProduct(p)&quot;&gt; Edit &lt;/button&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;/div&gt;&lt;/template&gt;&lt;script&gt; import Vue from &quot;vue&quot;; export default &#123; data: function () &#123; return &#123; products: [ &#123; id: 1, name: &quot;Kayak&quot;, price: 275 &#125;, &#123; id: 2, name: &quot;Lifejacket&quot;, price: 48.95 &#125;, &#123; id: 3, name: &quot;Soccer Ball&quot;, price: 19.50 &#125;, &#123; id: 4, name: &quot;Corner Flags&quot;, price: 39.95 &#125;, &#123; id: 5, name: &quot;Stadium&quot;, price: 79500 &#125;] &#125; &#125;, methods: &#123; editProduct(product) &#123; this.eventBus.$emit(&quot;edit&quot;, product); &#125;, processComplete(product) &#123; let index = this.products.findIndex(p =&gt; p.id == product.id); if (index == -1) &#123; return; &#125; else &#123; Vue.set(this.products, index, product); &#125; &#125; &#125;, inject: [&quot;eventBus&quot;], created() &#123; this.eventBus.$on(&quot;complete&quot;, this.processComplete); &#125; &#125;&lt;/script&gt; productapp/src/components/ProductEditor.vue：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647&lt;template&gt; &lt;div&gt; &lt;div class=&quot;form-group&quot;&gt; &lt;label&gt;ID&lt;/label&gt; &lt;input v-model.number=&quot;product.id&quot; class=&quot;form-control&quot; /&gt; &lt;label&gt;Name&lt;/label&gt; &lt;input v-model=&quot;product.name&quot; class=&quot;form-control&quot; /&gt; &lt;label&gt;Price&lt;/label&gt; &lt;input v-model.number=&quot;product.price&quot; class=&quot;form-control&quot; /&gt; &lt;/div&gt; &lt;div class=&quot;text-center&quot;&gt; &lt;button class=&quot;btn btn-primary&quot; @click=&quot;save&quot;&gt;Save&lt;/button&gt; &lt;button class=&quot;btn btn-secondary&quot; @click=&quot;cancel&quot;&gt;Cancel&lt;/button&gt; &lt;/div&gt; &lt;/div&gt;&lt;/template&gt;&lt;script&gt;export default &#123; data: function() &#123; return &#123; product: &#123;&#125; &#125;; &#125;, methods: &#123; startEdit(product) &#123; this.product = &#123; id: product.id, name: product.name, price: product.price &#125;; &#125;, save() &#123; this.eventBus.$emit(&quot;complete&quot;, this.product); console.log(`Edit Complete: $&#123;JSON.stringify(this.product)&#125;`); this.product = &#123;&#125; &#125;, cancel() &#123; this.product = &#123;&#125;; &#125; &#125;, inject: [&quot;eventBus&quot;], created() &#123; this.eventBus.$on(&quot;edit&quot;, this.startEdit); &#125;&#125;;&lt;/script&gt; 三、运行效果 参考资料Pro Vue.js 2]]></content>
      <categories>
        <category>Program</category>
      </categories>
      <tags>
        <tag>Program</tag>
        <tag>Development</tag>
        <tag>Web</tag>
        <tag>Vue</tag>
        <tag>Frontend</tag>
        <tag>JavaScript</tag>
        <tag>Components</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django REST framework 模型中 Many-to-many 关系的序列化]]></title>
    <url>%2F2020%2F08%2F19%2Fdjango-rest-framework-many-to-many-relations%2F</url>
    <content type="text"><![CDATA[一、模型12345678910111213141516171819# blogs/models.pyfrom django.db import modelsclass TagModel(models.Model): tag = models.CharField(max_length=20, unique=True) description = models.CharField(max_length=100) class Meta: db_table = 'blog_tags'class PostModel(models.Model): title = models.CharField(max_length=50) update_date = models.DateTimeField(auto_now=True) tags = models.ManyToManyField(TagModel) class Meta: db_table = 'blog_posts' 二、Serializers123456789101112131415# blogs/serializers.pyfrom rest_framework import serializersfrom blogs.models import TagModel, PostModelclass TagSerializer(serializers.ModelSerializer): class Meta: model = TagModel fields = '__all__'class PostSerializer(serializers.ModelSerializer): class Meta: model = PostModel fields = '__all__' 三、视图1234567891011121314# blogs/views.pyfrom rest_framework.viewsets import ModelViewSetfrom blogs.models import TagModel, PostModelfrom blogs.serializers import TagSerializer, PostSerializerclass TagViewSet(ModelViewSet): queryset = TagModel.objects.all() serializer_class = TagSerializerclass PostViewSet(ModelViewSet): queryset = PostModel.objects.all() serializer_class = PostSerializer 四、URLs123456789101112# blogs/urls.pyfrom django.urls import include, pathfrom rest_framework import routersfrom blogs.views import TagViewSet, PostViewSetrouter = routers.DefaultRouter()router.register(r'tags', TagViewSet)router.register(r'posts', PostViewSet)urlpatterns = [ path('', include(router.urls))] 修改项目总体的 URL 配置 urls.py：1234567from django.contrib import adminfrom django.urls import path, includeurlpatterns = [ path('admin/', admin.site.urls), path('blog/', include('blogs.urls'))] 五、效果123456789101112131415161718# GET http://127.0.0.1:8002/blog/tags/[ &#123; &quot;description&quot;: &quot;Technique for python programming&quot;, &quot;id&quot;: 1, &quot;tag&quot;: &quot;Python&quot; &#125;, &#123; &quot;description&quot;: &quot;Shell programming tricks&quot;, &quot;id&quot;: 2, &quot;tag&quot;: &quot;Shell&quot; &#125;, &#123; &quot;description&quot;: &quot;Linux system administration&quot;, &quot;id&quot;: 3, &quot;tag&quot;: &quot;Linux&quot; &#125;] 123456789101112# GET http://127.0.0.1:8002/blog/posts/[ &#123; &quot;id&quot;: 1, &quot;tags&quot;: [ 1, 3 ], &quot;title&quot;: &quot;Python for linux administration&quot;, &quot;update_date&quot;: &quot;2020-08-11T07:01:50.893976Z&quot; &#125;] 六、自定义序列化方式为了在获取 posts 列表时，每条 post 中的 tags 不仅仅显示 id，还显示对应的标签名称。修改代码如下：1234567891011121314151617181920212223# blogs/serializers.pyfrom rest_framework import serializersfrom blogs.models import TagModel, PostModelclass TagSerializer(serializers.ModelSerializer): class Meta: model = TagModel fields = '__all__'class TagsReadOnly(serializers.ModelSerializer): class Meta: model = TagModel fields = ['id', 'tag']class PostSerializer(serializers.ModelSerializer): tags = TagsReadOnly(many=True) class Meta: model = PostModel fields = '__all__' 效果：123456789101112131415161718# GET http://127.0.0.1:8002/blog/posts/[ &#123; &quot;id&quot;: 1, &quot;tags&quot;: [ &#123; &quot;id&quot;: 1, &quot;tag&quot;: &quot;Python&quot; &#125;, &#123; &quot;id&quot;: 3, &quot;tag&quot;: &quot;Linux&quot; &#125; ], &quot;title&quot;: &quot;Python for linux administration&quot;, &quot;update_date&quot;: &quot;2020-08-11T07:01:50.893976Z&quot; &#125;] 但此时若使用 POST 或 PUT 方法新增或更新数据，会报出如下错误（即在新增或修改 post 的同时，会尝试创建关联的 tags，但这些 tags 本就已经存在，从而导致冲突）：123456789&#123; "tags": [ &#123; "tag": [ "tag model with this tag already exists." ] &#125; ]&#125; 测试 POST 和 PUT 动作时使用的 JSON 数据格式如下：123456789&#123; "tags": [ &#123; "id": 1, "tag": "Python" &#125; ], "title": "An interesting post"&#125; 为了使 posts 接口在接收数据时支持列表类型的 tags（类似 &quot;tags&quot;: [1, 2, 3] 这种）且能够成功更新，可以选择覆盖 PostSerializer 的 to_internal_value 和 create 方法：1234567891011121314151617181920212223242526272829303132333435# blogs/serializers.pyfrom rest_framework import serializersfrom blogs.models import TagModel, PostModelclass TagSerializer(serializers.ModelSerializer): class Meta: model = TagModel fields = '__all__'class TagsReadOnly(serializers.ModelSerializer): class Meta: model = TagModel fields = ['id', 'tag']class PostSerializer(serializers.ModelSerializer): tags = TagsReadOnly(many=True) class Meta: model = PostModel fields = '__all__' def to_internal_value(self, data): return data def create(self, validated_data): tags_data = validated_data.pop('tags') post = PostModel.objects.create(**validated_data) tags = [TagModel.objects.get( pk=id) for id in tags_data] post.tags.set(tags) post.save() return post 其中 to_internal_value 方法用于验证前端传入的数据（上述代码中不做任何验证）；create 方法用于新增或更新后台数据。 此时再向 posts 接口 POST 或 PUT 数据时，就可以使用如下格式：1234&#123; "tags": [1, 2, 3], "title": "An interesting post"&#125;]]></content>
      <tags>
        <tag>Python</tag>
        <tag>Development</tag>
        <tag>Web</tag>
        <tag>Django</tag>
        <tag>REST</tag>
        <tag>API</tag>
        <tag>Relations</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django 借助 ldap3 自定义支持 LDAP 域账号的认证后端]]></title>
    <url>%2F2020%2F08%2F19%2Fdjango-add-ldap-authentication-backend-with-ldap3%2F</url>
    <content type="text"><![CDATA[一、项目初始化 pip install django ldap3 django-admin startproject auth_demo cd auth_demo django-admin startapp authldap python manage.py migrate 二、编写自定义认证后端Django 的认证系统支持的自定义插件，本质上是一个实现了 get_user(user_id) 和 authenticate(request, **credentials) 方法的类。其中 get_user 接收 user_id（可以是用户名、ID 等，但必须为 user 对象的主键）返回匹配的用户对象或 None；authenticate 接收 request 参数以及认证信息，根据最终的认证结果返回用户对象（认证通过）或 None（认证不通过）。 auth_demo/authldap/authbackends.py：12345678910111213141516171819202122232425262728293031323334353637# auth_demo/authldap/authbackends.pyfrom django.contrib.auth.backends import BaseBackendfrom django.contrib.auth.models import Userimport ldap3# 替换为实际的域控 IPLDAP_HOST = 'xx.xx.xx.xx'class LdapBackend(BaseBackend): def authenticate(self, request, username=None, password=None): if ldap_auth(username, password): try: user = User.objects.get(username=username) except User.DoesNotExist: user = User(username=username) if username.endswith('admin'): user.is_staff = True user.is_superuser = True user.save() return user return None def get_user(self, user_id): try: return User.objects.get(pk=user_id) except User.DoesNotExist: return None# @example.com 改为自己域环境的域名def ldap_auth(username, password): username = username + '@example.com'\ if '@' not in username else username server = ldap3.Server(LDAP_HOST, port=636, use_ssl=True) conn = ldap3.Connection(server, username, password) return conn.bind() 代码中的 ldap_auth 函数用于执行 LDAP 认证，将 Django 收到的认证信息传递给 LDAP 服务器，通过 Connection 对象的 bind() 方法确认用户名密码是否正确。正确返回 True，不正确则返回 False。 LDAP 认证通过后，再检查 User 数据库表中是否已包含该用户。若该用户存在，则直接返回对应的 User 对象；若该用户不存在（第一次登录），则先在 User 中创建同名的新用户并添加权限等，保存后返回刚创建的 User 对象。 假设所有需要添加 Django 管理员权限的账号名字都以 admin 结尾。。。 配置认证后端Django 认证时使用的插件列表由 settings.py 中的 AUTHENTICATION_BACKENDS 字段指定，默认值为 [&#39;django.contrib.auth.backends.ModelBackend&#39;]。因此为了用上前面创建的自定义认证后端，需在 auth_demo/auth_demo/settings.py 配置文件中添加以下内容：1234AUTHENTICATION_BACKENDS = [ 'auth_ldap.authbackends.LdapBackend', 'django.contrib.auth.backends.ModelBackend' ] 三、测试 运行 python manage.py runserver 0.0.0.0:8000 启动 Web 服务 在域中创建 testaccount 和 testaccount-admin 测试账号 访问 http://xx.xx.xx.xx:8000/admin 进入 Django 后台，用测试账号登录 效果如下： 四、初始化用户组首先梳理下之前代码的逻辑流程： Django 后台获取前端传入的认证信息，并转发给 LDAP 服务器进行认证 认证成功且该用户不在数据库中（首次登录），则创建对应的用户并将其返回；该用户已存在则直接返回 创建用户时，若用户名以 admin 结尾，则额外向其添加 staff 权限和管理员权限 认证失败返回 None 假设在数据库中创建新用户时，需要根据一定的规则对用户的属组进行初始化（比如名称以 admin 结尾的用户自动添加到 admin 组中）。最终的代码如下：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849# auth_demo/authldap/authbackends.pyfrom django.contrib.auth.backends import BaseBackendfrom django.contrib.auth.models import User, Groupimport ldap3# 替换为实际的域控 IPLDAP_HOST = &apos;xx.xx.xx.xx&apos;class LdapBackend(BaseBackend): def authenticate(self, request, username=None, password=None): if ldap_auth(username, password): try: user = User.objects.get(username=username) except User.DoesNotExist: user = User(username=username) user.save() if username.endswith(&apos;admin&apos;): user.is_staff = True user.is_superuser = True add_group(user) user.save() return user return None def get_user(self, user_id): try: return User.objects.get(pk=user_id) except User.DoesNotExist: return None# @example.com 改为自己域环境的域名def ldap_auth(username, password): username = username + &apos;@example.com&apos;\ if &apos;@&apos; not in username else username server = ldap3.Server(LDAP_HOST, port=636, use_ssl=True) conn = ldap3.Connection(server, username, password) return conn.bind()def add_group(user, groupname=&apos;admin&apos;): try: group = Group.objects.get(name=groupname) except Group.DoesNotExist: group = Group(name=groupname) group.save() group.user_set.add(user) group.save() 删除上一步中数据库里新建的 testaccount-admin 账号，重新登录测试。则 LDAP 认证成功后，Django 后台会在数据库中新建 testaccount-admin 账号并将其添加至 admin 用户组中（如 admin 组不存在则新建该用户组）。即在新建账号的同时初始化其属组。 参考资料Customizing authentication in Django]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Program</tag>
        <tag>Development</tag>
        <tag>Web</tag>
        <tag>Django</tag>
        <tag>Authentication</tag>
        <tag>LDAP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[理解 JavaScript（ECMAScript 6）—— Proxy 与 Reflection API]]></title>
    <url>%2F2020%2F08%2F19%2Funderstanding-javascript-ecmascript6-proxy-and-reflection-api%2F</url>
    <content type="text"><![CDATA[一、创建 Proxy12345678910let target = &#123;&#125;let proxy = new Proxy(target, &#123;&#125;)proxy.name = "proxy"console.log(proxy.name) // proxyconsole.log(target.name) // proxytarget.name = "target"console.log(proxy.name) // targetconsole.log(target.name) // target 在上面的例子中，由 Proxy 构造器创建的 proxy 对象会将自身的所有操作直接转发给 target。当 proxy.name 被赋值为 &quot;proxy&quot; 时，target 对象也会创建 name 属性并获得同样的值。实际上 proxy 对象本身并不创建和存储 name 属性，它只是转发对应的操作给 target。 类似的，proxy.name 与 target.name 的值始终保持一致，因为它们实际上都指向了 target.name。这也意味着给 target.name 赋予一个新的值时，该变化也会反映到 proxy.name 上。 使用 set Trap 验证属性Proxy 允许开发者主动拦截本该转发给 target 对象的底层操作，这些拦截行为通过 trap 实现。每个 trap 都可以覆盖 JavaScript 对象的某些内置行为，即 proxy 允许通过 trap 拦截并修改指向 target 对象的操作。 假设需要创建一个新添加的属性值只能是数字类型的对象，就可以借助 set trap 覆盖默认的赋值行为。代码如下：12345678910111213141516171819202122232425let target = &#123; name: "target"&#125;let proxy = new Proxy(target, &#123; set(trapTarget, key, value, receiver) &#123; if (!trapTarget.hasOwnProperty(key)) &#123; if (isNaN(value)) &#123; throw new TypeError("New property must be a number.") &#125; &#125; return Reflect.set(trapTarget, key, value, receiver) &#125;&#125;)proxy.count = 1console.log(proxy.count) // 1console.log(target.count) // 1proxy.name = "proxy"console.log(proxy.name) // proxyconsole.log(target.name) // proxyproxy.anotherName = "proxy"// TypeError: New property must be a number. set trap 中的四个参数含义如下： trapTarget：接收新属性的对象（即 proxy 指向的 target） key：新属性对应的 key value：新属性对应的 value receiver：通常为 proxy 自身 Reflect.set() 是与 set trap 相对应的原始方法，表示被覆盖前的默认的赋值行为。 使用 get Trap 令程序读取不存在属性时报错JavaScript 在读取不存在的属性时并不会报错，而是返回 undefined。12let target = &#123;&#125;console.log(target.name) // undefined 可以借助 get trap 修改读取对象属性时的默认行为：1234567891011121314let proxy = new Proxy(&#123;&#125;, &#123; get(trapTarget, key, receiver) &#123; if (!(key in receiver)) &#123; throw new TypeError("Property " + key + " doesn't exist.") &#125; return Reflect.get(trapTarget, key, receiver) &#125;&#125;)proxy.name = "proxy"console.log(proxy.name) // proxyconsole.log(proxy.nme)// TypeError: Property nme doesn't exist. 通过 deleteProperty Trap 防止删除属性JavaScript 中使用 delete 操作符删除对象的属性：123456789101112131415let target = &#123; name: "target", value: 42&#125;Object.defineProperty(target, "name", &#123; configurable: false &#125;)console.log("value" in target) // truelet result1 = delete target.valueconsole.log(result1) // trueconsole.log("value" in target) // falselet result2 = delete target.nameconsole.log(result2) // falseconsole.log("name" in target) // true 使用 deleteProxy Trap 防止属性被意外删除：123456789101112131415161718192021222324let target = &#123; name: "target", value: 42&#125;let proxy = new Proxy(target, &#123; deleteProperty(trapTarget, key) &#123; if (key === "value") &#123; return false &#125; else &#123; return Reflect.deleteProperty(trapTarget, key) &#125; &#125;&#125;)console.log("value" in proxy) // truelet result1 = delete proxy.valueconsole.log(result1) // falseconsole.log("value" in proxy) // truelet result2 = delete proxy.nameconsole.log(result2) // trueconsole.log("name" in proxy) // false 二、Proxy 的现实应用logging12345678910111213141516171819202122function makeLoggable(target) &#123; return new Proxy(target, &#123; get: (target, property) =&gt; &#123; console.log("Reading " + property) return target[property] &#125;, set: (target, property, value) =&gt; &#123; console.log("Writing value " + value + " to " + property) target[property] = value &#125; &#125;)&#125;let ninja = &#123; name: "Yoshi" &#125;ninja = makeLoggable(ninja)console.log(ninja.name)ninja.weapon = "sword"// Reading name// Yoshi// Writing value sword to weapon 性能测试123456789101112131415161718192021function isPrime(number) &#123; if (number &lt; 2) &#123; return false &#125; for (let i = 2; i &lt; number; i++) &#123; if (number % i === 0) &#123; return false &#125; &#125; return true&#125;isPrime = new Proxy(isPrime, &#123; apply: (target, thisArg, args) =&gt; &#123; console.time("isPrime") const result = target.apply(thisArg, args) console.timeEnd("isPrime") return result &#125;&#125;)console.log(isPrime(1358765377))// isPrime: 6815.107ms// true 自动添加属性12345678910111213141516171819202122function Folder() &#123; return new Proxy(&#123;&#125;, &#123; get: (target, property) =&gt; &#123; console.log("Reading " + property) if(!(property in target)) &#123; target[property] = new Folder() &#125; return target[property] &#125; &#125;)&#125;const rootFolder = new Folder()rootFolder.ninjasDir.firstNinjaDir.ninjaFile = "yoshi.txt"// Reading ninjasDir// Reading firstNinjaDirconsole.log(rootFolder.ninjasDir.firstNinjaDir.ninjaFile)// Reading ninjasDir// Reading firstNinjaDir// Reading ninjaFile// yoshi.txt 参考资料Understanding ECMAScript 6Secrets of the JavaScript Ninja, Second Edition]]></content>
      <categories>
        <category>Program</category>
      </categories>
      <tags>
        <tag>Program</tag>
        <tag>Advanced</tag>
        <tag>JavaScript</tag>
        <tag>Proxy</tag>
        <tag>ECMAScript6</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JavaScript 解密 —— 理解对象]]></title>
    <url>%2F2020%2F08%2F06%2Fsecrets-of-javascript-understanding-objects%2F</url>
    <content type="text"><![CDATA[一、prototypesPrototype 是一个对象，其中定义的属性和功能可以自动被其他对象访问。Prototype 可以发挥类似于传统的 OO 语言中类的作用，事实上 JavaScript 中的 prototype 主要用途就是编写 OO 形式的代码。 在 JavaScript 中，对象表示一系列已命名的属性及其属性值的合集。12345let obj = &#123; prop1: 1, // 属性值为基本类型 prop2: function () &#123;&#125;, // 属性值为函数 prop3: &#123;&#125; // 属性值为另一个对象&#125; 关联到某个对象上的属性可以很容易地被修改或删除。123456789101112let obj = &#123; prop1: 1, // 属性值为基本类型 prop2: function () &#123;&#125;, // 属性值为函数 prop3: &#123;&#125; // 属性值为另一个对象&#125;obj.prop1 = [] // 将 prop1 属性的值改为其他类型delete obj.prop2 // 删除某个属性obj.prop4 = "Hello" // 添加一个新的属性console.log(obj)// &#123; prop1: [], prop3: &#123;&#125;, prop4: 'Hello' &#125; 每个对象都可以拥有一个 prototype 的引用，在对象本身不包含某个属性时，可以由 prototype 对象去寻找该属性。12345678910111213141516171819const yoshi = &#123; skulk: true &#125;const hattori = &#123; sneak: true &#125;const kuma = &#123; creep: true &#125;console.log("skulk" in yoshi) // trueconsole.log("sneak" in yoshi) // falseconsole.log("creep" in yoshi) // false// 通过 setPrototypeOf 方法将 hattori 设置为// yoshi 对象的 prototype，yoshi 此时可以访问 sneak 属性Object.setPrototypeOf(yoshi, hattori)console.log("sneak" in yoshi) // trueconsole.log("creep" in yoshi) // false// 将 kuma 设置为 hattori 对象的 prototype，// yoshi 此时可以访问 creep 属性Object.setPrototypeOf(hattori, kuma)console.log("sneak" in yoshi) // trueconsole.log("creep" in yoshi) // true prototype 与对象构建JavaScript 中，最简单的创建新对象的语法如下：123456const warrior = &#123;&#125;warrior.name = 'Saito'warrior.occupation = 'marksman'console.log(warrior)// &#123; name: 'Saito', occupation: 'marksman' &#125; 对于有面向对象编程背景的人来说，上述方式看上去缺少了很多东西，比如没有一个用来初始化对象的类构造器。此外如果需要同时创建多个相同类型的对象，手动地一个一个为对象关联属性则工作量太大且容易出问题。 和其他常见的 OO 语言类似，JavaScript 也是使用 new 关键字通过构造器初始化对象。只不过该过程中被未出现类的定义，new 关键字实际上调用的是构造器函数。123456789101112131415function Ninja() &#123;&#125;// 每个函数都包含一个内置的 prototype 对象，可以被随意修改Ninja.prototype.swingSword = function () &#123; return true&#125;// 作为函数调用 Ninja()，不会创建任何对象const ninja1 = Ninja()console.log(ninja1) // undefined// 作为构造器调用，一个新的对象被创建且作为构造器函数的上下文（this），// 构造器函数的 prototype 成为新创建对象的 prototypeconst ninja2 = new Ninja()console.log(ninja2) // Ninja &#123;&#125;console.log(ninja2.swingSword()) // true 当某个函数作为构造器函数使用时，构造器函数的 prototype 将成为新创建对象的 prototype。在上面的例子中，用 swingSword 方法扩展了 Ninja.prototype，当 ninja2 对象被创建后，其 prototype 就被设置成 Ninja 的 prototype。因此，当后面我们访问 ninja2 对象的 swingSword 属性时，对于属性值的搜索最终传递给 Ninja 的 prototype 对象。此外，所有通过 Ninja 构造器创建的对象都可以访问 swingSword 方法。 实例属性当函数通过 new 关键字作为构造器调用时，其本身即成为新创建对象的上下文。除了通过 prototype 暴露属性外，还可以使用 this 关键字在构造器函数中初始化对象实例。12345678910111213141516function Ninja() &#123; this.swung = false // instance method this.swingSword = function () &#123; return !this.swung &#125;&#125;// prototype methodNinja.prototype.swingSword = function () &#123; return this.swung&#125;const ninja = new Ninja()console.log(ninja.swingSword()) // true 上面代码的执行结果表明，实例方法会覆盖同名的 prototype 方法。 在构造器函数中，this 关键字指向通过 new 创建的对象。因此构造器中添加的属性会直接在新的 ninja 对象中创建。当我们需要访问 ninja 对象的 swingSword 属性时，本就没有必要再从 prototype 中搜索。ninja 对象本身已经有了通过构造器创建的同名的实例属性。 简单来说，每个对象都包含自己版本的通过构造器创建的属性，同时也都能够访问 prototype 的属性（名字相同时实例属性优先）。 Side effects123456789101112131415161718192021222324252627282930// 定义一个构造器函数，具有 swung 属性function Ninja() &#123; this.swung = true&#125;// 通过 new 调用构造器函数，创建一个 Ninja 的实例const ninja1 = new Ninja()// 在 ninja1 实例创建之后，为 prototype 添加 swingSword 方法Ninja.prototype.swingSword = function() &#123; return this.swung&#125;// 即便 swingSword 方法是后面添加到 prototype 的，ninja1 实例仍能访问console.log(ninja1.swingSword()) // true// 完全覆盖 Ninja 的 prototypeNinja.prototype = &#123; pierce: function() &#123; return true &#125;&#125;// Ninja 的 prototype 被完全替换后，ninja1 实例仍能访问 swingSword 方法console.log(ninja1.swingSword()) // true// 再次新建 ninja2 实例，此时该实例不能访问 swingSword 但可以访问 pierce 方法const ninja2 = new Ninja()console.log(ninja2.pierce()) // trueconsole.log(ninja2.swingSword) // undefined 从上面代码中可以看出，构造器函数的 prototype 可以随意被替换，但之前已经生成的对象实例仍指向旧的 prototype。 Object typing123456789function Ninja() &#123;&#125;const ninja = new Ninja()console.log(typeof ninja) // objectconsole.log(ninja instanceof Ninja) // trueconsole.log(ninja.constructor === Ninja) // trueconst ninja2 = new ninja.constructor()console.log(ninja2 instanceof Ninja) // true instanceof 操作符可以帮助我们确定某个对象实例是否由某个特定的构造器创建。此外，还可以借助能够被所有实例访问的 constructor 属性，因为该属性一定指向原始的构造器函数。又因为 constructor 属性是对原始构造器的引用，它因此也可以用来实例化新的对象。 二、继承通过 prototype 实现继承12345678910function Person() &#123;&#125;Person.prototype.dance = function() &#123;&#125;function Ninja() &#123;&#125;Ninja.prototype = new Person()const ninja = new Ninja()console.log(ninja instanceof Ninja) // trueconsole.log(ninja instanceof Person) // trueconsole.log(typeof ninja.dance) // function 当我们通过 ninja 对象访问 dance 方法时，JavaScript 运行时首先试着检查 ninja 对象本身。ninja 本身并不包含 dance 属性，因此 ninja 对象的 prototype（即 person 对象）继续被搜索。person 对象也不包含 dance 属性，最终 person 对象的 prototype 被搜索检查，dance 方法被找到。这就是 JavaScript 中实现继承的方式。 覆盖 constructor 属性引发的问题在上面的代码中，通过创建一个新的 Person 对象作为 Ninja 构造器的 prototype，令 Ninja 成为 Person 的子类。这种方式会导致原始的 Ninja prototype 被替换，即丢失 constructor 属性。而 constructor 属性对于判断对象的起源非常重要。针对这个问题，可以使用 Object.defineProperty 方法为新的 Ninja.prototype 添加 constructor 属性，将其值设置为 Ninja：1234567891011121314function Person() &#123;&#125;Person.prototype.dance = function() &#123;&#125;function Ninja() &#123;&#125;Ninja.prototype = new Person()Object.defineProperty(Ninja.prototype, "constructor", &#123; enumerable: false, value: Ninja, writable: true&#125;)var ninja = new Ninja()console.log(ninja.constructor === Ninja) // true 三、ES6 中的 classES6 中引入了一个新的 class 关键字，为创建对象、实现继承提供了一种更为优雅的方式，不必再自己通过 prototype 手动实现。12345678910111213class Ninja&#123; constructor(name) &#123; this.name = name &#125; swingSword() &#123; return true &#125;&#125;var ninja = new Ninja("Yoshi")console.log(ninja instanceof Ninja) // trueconsole.log(ninja.name) // Yoshiconsole.log(ninja.swingSword()) // true class 关键字实际上是一种语法糖，上述代码等同于 ES5 中的如下代码：123456789101112function Ninja(name) &#123; this.name = name&#125;Ninja.prototype.swingSword = function() &#123; return true&#125;var ninja = new Ninja('Yoshi')console.log(ninja instanceof Ninja) // trueconsole.log(ninja.name) // Yoshiconsole.log(ninja.swingSword()) // true static methods12345678910111213141516171819class Ninja&#123; constructor(name, level) &#123; this.name = name this.level = level &#125; swingSword() &#123; return true &#125; static compare(ninja1, ninja2) &#123; return ninja1.level - ninja2.level &#125;&#125;var ninja1 = new Ninja("Yoshi", 4)var ninja2 = new Ninja("Hattori", 3)console.log("compare" in ninja1 || "compare" in ninja2) // falseconsole.log(Ninja.compare(ninja1, ninja2) &gt; 0) // trueconsole.log("swingSword" in Ninja) // false compare 这种 static methods 是类级别的代码，在 ES6 之前的代码中，可以这样实现：12function Ninja() &#123;&#125;Ninja.compare = function(ninja1, ninja2) &#123; ... &#125; 继承1234567891011121314151617181920212223242526272829303132class Person &#123; constructor(name) &#123; this.name = name &#125; dance() &#123; return true &#125;&#125;class Ninja extends Person &#123; constructor(name, weapon) &#123; super(name) this.weapon = weapon &#125; wieldWeapon() &#123; return true &#125;&#125;var person = new Person("Bob")console.log(person instanceof Person) // trueconsole.log(person.dance()) // trueconsole.log(person.name) // Bobconsole.log(person instanceof Ninja) // falseconsole.log(person.wieldWeapon) // undefinedvar ninja = new Ninja("Yoshi", "Wakizashi")console.log(ninja instanceof Ninja) // trueconsole.log(ninja.wieldWeapon()) // trueconsole.log(ninja instanceof Person) // trueconsole.log(ninja.name) // Yoshiconsole.log(ninja.dance()) // true 参考资料Secrets of the JavaScript Ninja, Second Edition]]></content>
      <categories>
        <category>Program</category>
      </categories>
      <tags>
        <tag>Program</tag>
        <tag>OOP</tag>
        <tag>Class</tag>
        <tag>JavaScript</tag>
        <tag>Inherit</tag>
        <tag>Object</tag>
        <tag>ECMAScript6</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 密码学实践 —— 凯撒密码]]></title>
    <url>%2F2020%2F08%2F05%2Fpython-cryptography-caesar-cipher%2F</url>
    <content type="text"><![CDATA[一、原理凯撒密码 是密码学中的一种简单的替换加密技术。明文中的所有字符都会替换为其按照字母表顺序向左（或向右）偏移一定量后得到的新字母，作为加密后密文。如当偏移量为 3 时，明文中所有的字母 A 将被替换成字母 D，B 替换成 E，以此类推。 若收到密文的同时已知加密时使用的偏移量，就可以快速地通过逆运算获取到最初的明文。 下面两张图展示了当偏移量为 8 时明文字母与密文字母的对应关系（图一即凯撒密码轮盘，外层为明文，内层为密文，可旋转以改变偏移量）以及实际的加密过程（图二）： PS：对一段明文消息连续应用多个不同的偏移量进行凯撒密码规则的加密，并不会增强安全等级。即轮盘的多次旋转，实际上等同于抵消后的一次旋转。多次应用的不同偏移量，最终等同于抵消后的一次偏移量，对于暴力破解来说并不会增加复杂度。如第一次对明文实施偏移 3 位的凯撒加密，再对生成的密文实施偏移 10 位的加密，实际上相当于对最初的明文实施了偏移 13 位的加密。 二、Python 实现凯撒密码源代码：1234567891011121314while True: key = input("Please input a key number (like 13):\n") or 13 mode = input("\nPlease input mode (encrypt or decrypt):\n") or "encrypt" symbols = 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz1234567890 !?.' # 根据数字 key 对字母表 symbols 进行偏移操作，形成密文字母表 ciphers ciphers = symbols[int(key):] + symbols[:int(key)] # 根据加密与解密动作，生成明文字母到密文字母（或密文到明文）的对应关系 transtab = str.maketrans(symbols, ciphers) if mode == 'encrypt' else str.maketrans(ciphers, symbols) message = input("\nPlease input plaintext or ciphertext:\n") # 完成明文到密文（或密文到明文）的转换 result = message.translate(transtab) print(f"\nThe result is: &#123;result&#125;\n\n") 运行效果如下：12345678910111213141516171819202122Please input a key number (like 13):13Please input mode (encrypt or decrypt):encryptPlease input plaintext or ciphertext:This is my secret message.The result is: guv6Jv6Jz!J6rp5r7Jzr66ntrMPlease input a key number (like 13):13Please input mode (encrypt or decrypt):decryptPlease input plaintext or ciphertext:guv6Jv6Jz!J6rp5r7Jzr66ntrMThe result is: This is my secret message. 三、Python 对凯撒密码的爆破即在加密用的 key 值未知的情况下，尝试所有可能的 key 值（0 到字母表长度减一）对密文进行解密，输出以查看解密出的明文是否有意义。 源代码：1234567891011symbols = 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz1234567890 !?.'ciphertext = input("Please input ciphertext:\n")for key in range(len(symbols)): ciphers = symbols[key:] + symbols[:key] transtab = str.maketrans(ciphers, symbols) plaintext = ciphertext.translate(transtab) print(f'Key #&#123;key&#125;: &#123;plaintext&#125;') 运行效果如下：12345678910111213141516171819Please input ciphertext:guv6Jv6Jz!J6rp5r7Jzr66ntrMKey #0: guv6Jv6Jz!J6rp5r7Jzr66ntrMKey #1: ftu5Iu5Iy I5qo4q6Iyq55msqLKey #2: est4Ht4Hx0H4pn3p5Hxp44lrpKKey #3: drs3Gs3Gw9G3om2o4Gwo33kqoJKey #4: cqr2Fr2Fv8F2nl1n3Fvn22jpnIKey #5: bpq1Eq1Eu7E1mkzm2Eum11iomHKey #6: aopzDpzDt6Dzljyl1DtlzzhnlGKey #7: ZnoyCoyCs5CykixkzCskyygmkFKey #8: YmnxBnxBr4BxjhwjyBrjxxfljEKey #9: XlmwAmwAq3AwigvixAqiwwekiDKey #10: Wklv.lv.p2.vhfuhw.phvvdjhCKey #11: Vjku?ku?o1?ugetgv?oguucigBKey #12: Uijt!jt!nz!tfdsfu!nfttbhfAKey #13: This is my secret message.Key #14: Sghr0hr0lx0rdbqds0ldrrZfd?Key #15: Rfgq9gq9kw9qcapcr9kcqqYec!... 参考资料Cracking Codes with Python]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Security</tag>
        <tag>Python</tag>
        <tag>Program</tag>
        <tag>Cryptography</tag>
        <tag>Cipher</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[理解 JavaScript（ECMAScript 6）—— Sets 与 Maps]]></title>
    <url>%2F2020%2F08%2F05%2Funderstanding-javascript-ecmascript-6-sets-and-maps%2F</url>
    <content type="text"><![CDATA[一、ECMAScript 5 中的 Sets 和 Maps在 ECMAScript 5 中，通常使用对象属性来模拟 set 和 map 数据类型：1234567var set = Object.create(null)set.foo = trueif (set.foo) &#123; // code to execute&#125; 用对象属性作为 set 中的非重复键值，将值设置为 true 表明其存在性。 12345var map = Object.create(null)map.foo = "bar"var value = map.fooconsole.log(value) // bar 不同于 set，map 常常需要获取其中保存的数据，而不仅仅用于检查 key 是否存在。 存在的问题用对象作为 set 和 map 适用于简单的场景，在某些情况下对象属性会显露出一定的局限性。比如所有的对象属性都必须是字符串类型，因此同一个对象中绝不能包含多个（转换成字符串后）一致的键。1234var map = Object.create(null)map[5] = "foo"console.log(map["5"]) // foo 数字类型的键 5 实际上在内部会被转换为字符串 &quot;5&quot;，因此 map[5] 和 map[&quot;5&quot;] 指向的是同一个值。 又比如：123456var map = Object.create(null)var key1 = &#123;&#125;var key2 = &#123;&#125;map[key1] = "foo"console.log(map[key2]) // foo 上面代码中的 map[key1] 和 map[key2] 也是指向同一值的。因为对象 key1 和 key2 转换为字符串后的值同为 &quot;[object Object]&quot;。 二、ECMAScript 6 中的 SetsECMAScript 6 中添加了 Set 类型，即一个包含不重复的有序值的列表。12345678910111213let set = new Set()set.add(5)set.add("5")console.log(set) // Set &#123; 5, '5' &#125;let set2 = new Set()let key1 = &#123;&#125;let key2 = &#123;&#125;set2.add(key1)set2.add(key2)console.log(set2.size) // 2 使用 set 去重：123456789101112let set = new Set()set.add(5)set.add("5")set.add(5)console.log(set) // Set &#123; 5, '5' &#125;let set2 = new Set([1, 2, 3, 4, 5, 5, 5, 5])console.log(set2) // Set &#123; 1, 2, 3, 4, 5 &#125;console.log(set2.has(2)) // trueconsole.log(set2.has(6)) // false forEach：1234567891011let set = new Set([1, 2])set.forEach(function(value, key, ownerSet) &#123; console.log(key + " " + value) console.log(ownerSet === set)&#125;)// 1 1// true// 2 2// true 如果需要在 forEach() 的回调函数中使用 this，可以将 this 作为 forEach() 的第二个参数传递：12345678910111213141516let set = new Set([1, 2])let processor = &#123; output(value) &#123; console.log(value) &#125;, process(dataSet) &#123; dataSet.forEach(function(value) &#123; this.output(value) &#125;, this) &#125;&#125;processor.process(set)// 1// 2 Set 与 Array 的互换：1234567let set = new Set([1, 2, 3, 3, 3, 4, 5])console.log(set)// Set &#123; 1, 2, 3, 4, 5 &#125;let array = [...set]console.log(array)// [ 1, 2, 3, 4, 5 ] 12345678function removeDuplicates(items) &#123; return [...new Set(items)]&#125;let numbers = [1, 2, 3, 3, 3, 4, 5]let noDuplicates = removeDuplicates(numbers)console.log(noDuplicates)// [ 1, 2, 3, 4, 5 ] 三、ECMAScript 6 中的 Maps123456let map = new Map()map.set("title", "Understanding ECMAScript 6")map.set("year", 2016)console.log(map.get("title")) // Understanding ECMAScript 6console.log(map.get("year")) // 2016 可以使用对象作为 Map 中的键，这些键不会转换为另一种形式，且都是唯一的：123456789let map = new Map()let key1 = &#123;&#125;let key2 = &#123;&#125;map.set(key1, 5)map.set(key2, 42)console.log(map.get(key1)) // 5console.log(map.get(key2)) // 42 Map 方法：1234567891011121314151617let map = new Map()map.set("name", "Nicholas")map.set("age", 25)console.log(map.size) // 2console.log(map.has("name")) // trueconsole.log(map.get("name")) // Nicholasmap.delete("name")console.log(map.has("name")) // falseconsole.log(map.get("name")) // undefinedconsole.log(map.size) // 1map.clear()console.log(map.has("age")) // falseconsole.log(map.get("age")) // undefinedconsole.log(map.size) // 0 forEach：12345678910let map = new Map([["name", "Nicholas"], ["age", 25]])map.forEach(function(value, key, ownerMap) &#123; console.log(key + " " + value) console.log(ownerMap === map)&#125;)// name Nicholas// true// age 25// true 参考资料Understanding ECMAScript 6]]></content>
      <categories>
        <category>Program</category>
      </categories>
      <tags>
        <tag>Program</tag>
        <tag>Development</tag>
        <tag>Web</tag>
        <tag>JavaScript</tag>
        <tag>EMCAScript6</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[理解 JavaScript（ECMAScript 6）—— 基于 destructuring 的数据访问]]></title>
    <url>%2F2020%2F08%2F05%2Funderstanding-javascript-ecmascript-6-destructuring%2F</url>
    <content type="text"><![CDATA[在 ECMAScript 5 及早期版本中，从对象和数组中获取数据会导致很多冗余代码：12345678910let options = &#123; repeat: true, save: false&#125;let repeat = options.repeat, save = options.saveconsole.log(repeat, save)// true false Object Destrcturing123456789let node = &#123; type: "Identifier", name: "foo"&#125;let &#123; type, name &#125; = nodeconsole.log(type, name)// Identifier foo destructuring 也可以为已经初始化过的变量赋值：1234567891011let node = &#123; type: "Identifier", name: "foo"&#125;let type = "Literal", name = 5;(&#123; type, name &#125; = node)console.log(type, name)// Identifier foo ({ type, name } = node) 外面的小括号是必须的。只有大括号的话，大括号会被看成语句块，而语句块不能位于等号左边。 destructuring 赋值语句可以出现在代码的任何位置：12345678910111213let node = &#123; type: "Identifier", name: "foo"&#125;let type = "Literal", name = 5;function outputInfo(value) &#123; console.log(value === node)&#125;outputInfo(&#123; type, name &#125; = node) // trueconsole.log(type, name) // Identifier foo 默认值123456789let node = &#123; type: "Identifier", name: "foo"&#125;let &#123; type, name, value &#125; = nodeconsole.log(type, name, value)// Identifier foo undefined 123456789let node = &#123; type: "Identifier", name: "foo"&#125;let &#123; type, name, value = true &#125; = nodeconsole.log(type, name, value)// Identifier foo true 嵌套对象的 destructuring12345678910111213141516171819let node = &#123; type: "Identifier", name: "foo", loc: &#123; start: &#123; line: 1, column: 1 &#125;, end: &#123; line: 1, column: 4 &#125; &#125;&#125;let &#123; loc: &#123; start &#125;&#125; = nodeconsole.log(start.line, start.column)// 1 1 Array Destructuring1234567let colors = ["red", "green", "blue"]let [ firstColor, secondColor ] = colorsconsole.log(firstColor, secondColor) // red greenlet [ , , thirdColor ] = colorsconsole.log(thirdColor) // blue ECMAScript 5 中的变量互换：1234567let a = 1, b = 2, tmp;tmp = aa = bb = tmpconsole.log(a, b) // 2 1 ECMAScript 6 中的变量互换：12345let a = 1, b = 2;[ a, b ] = [ b, a ]console.log(a, b) // 2 1 嵌套数组：12345let colors = [ "red", [ "green", "lightgreen" ], "blue" ]let [ firstColor, [ secondColor ] ] = colorsconsole.log(firstColor, secondColor) // red green Rest Items：12345678let colors = [ "red", "green", "blue" ]let [ firstColor, ...restColors ] = colorsconsole.log(firstColor) // "red"console.log(restColors.length) // 2console.log(restColors[0]) // greenconsole.log(restColors[1]) // blue 在 ECMAScript 5 中，可以使用 concat() 方法创建某个数组的克隆：12345var colors = [ "red", "green", "blue" ]var clonedColors = colors.concat()console.log(clonedColors)// [ 'red', 'green', 'blue' ] ECMAScript 6 中则可以使用 rest items 语法创建克隆：12345let colors = [ "red", "green", "blue" ]let [ ...clonedColors ] = colorsconsole.log(clonedColors)// [ 'red', 'green', 'blue' ] 参考资料Understanding ECMAScript 6]]></content>
      <categories>
        <category>Program</category>
      </categories>
      <tags>
        <tag>Program</tag>
        <tag>Development</tag>
        <tag>Web</tag>
        <tag>JavaScript</tag>
        <tag>ECMAScript6</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[理解 JavaScript（ECMAScript 6）—— 扩展的对象]]></title>
    <url>%2F2020%2F08%2F05%2Funderstanding-javascript-ecmascript-6-expanded-object%2F</url>
    <content type="text"><![CDATA[一、对象字面量对象字面量（object literal）是 JavaScript 中最常见的模式之一，JSON 即基于它的语法。 在 ECMAScript 5 及更早的版本中，对象字面量即一系列简单的键值对的集合：123456789function createPerson(name, age) &#123; return &#123; name: name, age: age &#125;;&#125;console.log(createPerson('Jack', 16))// &#123; name: 'Jack', age: 16 &#125; 而在 ECMAScript 6 中，则可以通过 property initializer 的简写语法省略上述代码中的部分重复内容：123456789function createPerson(name, age) &#123; return &#123; name, age &#125;&#125;console.log(createPerson('Jack', 16))// &#123; name: 'Jack', age: 16 &#125; 如果一个对象字面量只给出名字而没有值，则 JavaScript 引擎会在周围的作用域中寻找同名的变量，将其值赋值给对象字面量中对应的名字。 方法简化ECMAScript 6 同样简化了将方法关联给某个对象字面量的语法。 ECMAScript 5 及更早版本中为对象添加方法：123456789var person = &#123; name: "Nicholas", sayName: function() &#123; console.log(this.name) &#125;&#125;;person.sayName()// Nicholas ECMAScript 6 则支持更简单的语法：123456789var person = &#123; name: "Nicholas", sayName() &#123; console.log(this.name) &#125;&#125;;person.sayName()// Nicholas Computed Property Names在 ECMAScript 5 和早期版本中，允许在对象实例上使用由中括号（[]）包围的计算属性。相对于由点（.）指示的属性，由中括号包裹的属性可以使用变量或者有可能导致语法错误的字符串作为属性名。1234567891011var person = &#123;&#125;var lastName = "last name"person["first name"] = "Nicholas"person[lastName] = "Zakas"person.age = 16console.log(person["first name"])console.log(person[lastName])// Nicholas// Zakas ECMAScript 6 中计算属性的语法则更加灵活：1234567891011let lastName = "last name"let person = &#123; "first name": "Nicholas", [lastName]: "Zakas"&#125;console.log(person["first name"])console.log(person[lastName])// Nicholas// Zakas 甚至可以在中括号中使用表达式作为计算属性名：1234567891011var suffix = " name"var person = &#123; ["first" + suffix]: "Nicholas", ["last" + suffix]: "Zakas"&#125;console.log(person["first name"])console.log(person["last name"])// Nicholas// Zakas 二、新方法Object.is()在 JavaScript 中比较两个值，可以使用相等操作符（==）或相同操作符（===）。大部分开发者会倾向于使用后者。但 === 操作符并非完全准确，比如 +0 和 -0 会被认为相同，而 NaN === NaN 会返回 false。 ECMAScript 6 中引入了 Object.is() 方法可以解决 === 中存在的不准确的问题。1234567891011121314console.log(+0 == -0) // trueconsole.log(+0 === -0) // trueconsole.log(Object.is(+0, -0)) // falseconsole.log(NaN == NaN) // falseconsole.log(NaN === NaN) // falseconsole.log(Object.is(NaN, NaN)) // trueconsole.log(5 == 5) // trueconsole.log(5 == "5") // trueconsole.log(5 === 5) // trueconsole.log(5 === "5") // falseconsole.log(Object.is(5, 5)) // trueconsole.log(Object.is(5, "5")) // false Object.assign()Mixins 是 JavaScript 中最流行的构建对象的方式之一。通过 mixin，一个对象可以接收另一个对象中定义的属性和方法。其底层原理类似于如下代码：1234567891011121314151617181920function mixin(receiver, supplier) &#123; Object.keys(supplier).forEach(function(key) &#123; receiver[key] = supplier[key] &#125;) return receiver&#125;let person1 = &#123; name: "Jack", age: 16&#125;let person2 = &#123; name: "Rose", gender: "F"&#125;mixin(person1, person2)console.log(person1)// &#123; name: 'Rose', age: 16, gender: 'F' &#125; mixin() 函数遍历 supplier 对象的属性，将它们复制给 receiver（注意是浅复制，即当属性值是对象时，复制的是对象的引用）。这允许 receiver 不通过继承就能获取到新的属性。 鉴于 mixin 模式非常常用，ECMAScript 6 中添加了同样行为的 Object.assign() 方法。1234567891011function EventTarget() &#123; /*...*/ &#125;EventTarget.prototype = &#123; constructor: EventTarget, emit: function() &#123; /*...*/ &#125;, on: function() &#123; /*...*/ &#125;&#125;var myObject = &#123;&#125;Object.assign(myObject, EventTarget.prototype)myObject.emit("somethingChanged") 三、PrototypesPrototypes 是 JavaScript 中继承的基础。 修改对象的 Prototype通常某个对象的 prototype 由构造器或 Object.create() 方法指定。ECMAScript 5 中添加了 Object.getPrototypeOf() 方法可以获取任意对象的 prototype，但 prototype 在对象实例化之后就无法再变更了。 ECMAScript 6 中添加了 Object.setPrototypeOf() 方法可以用来变更任意对象的 prototype。123456789101112131415161718192021let person = &#123; getGreeting() &#123; return "Hello" &#125;&#125;let dog = &#123; getGreeting() &#123; return "Woof" &#125;&#125;// prototype is personlet friend = Object.create(person)console.log(friend.getGreeting()) // "Hello"console.log(Object.getPrototypeOf(friend) === person) // true// set prototype to dogObject.setPrototypeOf(friend, dog)console.log(friend.getGreeting()) // "Woof"console.log(Object.getPrototypeOf(friend) === dog) // true 通过 super 访问 Prototypesuper 是一个指向当前对象的 prototype 的指针，即 Object.getPrototypeOf(this) 的值。12345678910111213141516171819let person = &#123; getGreeting() &#123; return "Hello" &#125;&#125;let friend = &#123; getGreeting() &#123; return super.getGreeting() + ", hi" &#125;&#125;Object.setPrototypeOf(friend, person)// prototype is friendlet relative = Object.create(friend)console.log(person.getGreeting()) // "Hello"console.log(friend.getGreeting()) // "Hello, hi"console.log(relative.getGreeting()) // "Hello, hi" 参考资料Understanding ECMAScript 6]]></content>
      <categories>
        <category>Program</category>
      </categories>
      <tags>
        <tag>Program</tag>
        <tag>OOP</tag>
        <tag>Development</tag>
        <tag>Web</tag>
        <tag>Frontend</tag>
        <tag>JavaScript</tag>
        <tag>ECMAScript6</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python Tricks —— 使用 pywinrm 远程控制 Windows 主机]]></title>
    <url>%2F2020%2F07%2F20%2Fpython-tricks-remote-control-windows-machines-with-pywinrm%2F</url>
    <content type="text"><![CDATA[WinRM 即 Windows Remote Management，是微软对于 WS-Management 远程管理协议的实现。 一、受控端配置 WinRM 服务方式一：cmd 命令行（管理员） 启用 WinRM 远程服务：winrm quickconfig 查看 WinRM 服务监听状态：winrm e winrm/config/listener 12345678910C:\Windows\system32&gt;winrm e winrm/config/listenerListener [Source="GPO"] Address = * Transport = HTTP Port = 5985 Hostname Enabled = true URLPrefix = wsman CertificateThumbprint ListeningOn = 127.0.0.1, 169.254.52.7, xx.xx.xx.xx, ::1, fe80::3989:dd91:e6b3:6f41%15, fe80::fd01:a9fd:c410:3407%12 允许使用 Basic 认证方式：winrm set winrm/config/service/auth @{Basic=&quot;true&quot;} 12345678winrm set winrm/config/service/auth @&#123;Basic="true"&#125;Auth Basic = true [Source="GPO"] Kerberos = true Negotiate = true Certificate = false CredSSP = false CbtHardeningLevel = Relaxed 允许 WinRM 使用非加密的连接：winrm set winrm/config/service @{AllowUnencrypted=&quot;true&quot;} 方式二：bat 脚本123call winrm quickconfig -quietcall winrm set winrm/config/service/auth @&#123;Basic="true"&#125;call winrm set winrm/config/service @&#123;AllowUnencrypted="true"&#125; 方式三：组策略定位到计算机配置 -&gt; 策略 -&gt; 管理模板 -&gt; Windows 组件 -&gt; Windows 远程管理(WinRM) -&gt; WinRM 服务。启用允许通过 WinRM 进行远程服务器管理、允许基本身份验证、允许未加密通信。 建议同时启用服务与防火墙策略：计算机配置 -&gt; 策略 -&gt; Windows 设置 -&gt; 安全设置 -&gt; 系统服务 -&gt; Windows Remote Management (WS-Management)，启动模式为自动。 计算机配置 -&gt; 策略 -&gt; Windows 设置 -&gt; 安全设置 -&gt; 高级安全 Windows 防火墙 -&gt; 高级安全 Windows 防火墙 - XXX -&gt; 入站规则，开放 5985（HTTP）和 5986（HTTPS）端口。 二、Python 使用 pywinrm 连接 WinRM 服务安装 pywinrm 库：pip install pywinrm 执行 cmd 命令：12345&gt;&gt;&gt; import winrm&gt;&gt;&gt; session = winrm.Session('xx.xx.xx.xx', auth=('Administrator', 'admin_password'))&gt;&gt;&gt; cmd = session.run_cmd('ipconfig')&gt;&gt;&gt; cmd.std_outb'\r\nWindows IP Configuration\r\n\r\n\r\nEthernet adapter \xd2\xd4\xcc\xab\xcd\xf8:\r\n\r\n Connection-specific DNS Suffix . : example.com\r\n Link-local IPv6 Address . . . . . : fe80::3989:dd91:e6b3:6f41%15\r\n IPv4 Address. . . . . . . . . . . : xx.xx.xx.xx\r\n Subnet Mask . . . . . . . . . . . : 255.255.255.0\r\n Default Gateway . . . . . . . . . : 172.20.23.254\r\n\r\nEthernet adapter \xd2\xd4\xcc\xab\xcd\xf8 2:\r\n\r\n Media State . . . . . . . . . . . : Media disconnected\r\n Connection-specific DNS Suffix . : \r\n' 执行 Powershell 命令：12345&gt;&gt;&gt; import winrm&gt;&gt;&gt; session = winrm.Session('xx.xx.xx.xx', auth=('Administrator', 'admin_password'))&gt;&gt;&gt; ps = session.run_ps('Get-Disk')&gt;&gt;&gt; ps.std_outb'\r\nNumber Friendly Name Serial Number HealthStatus OperationalStatus Total Size Partition \r\n Style \r\n------ ------------- ------------- ------------ ----------------- ---------- ----------\r\n0 ST500DM002... Z3TFS1S3 Healthy Online 465.76 GB MBR \r\n\r\n\r\n']]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Tools</tag>
        <tag>Windows</tag>
        <tag>Python</tag>
        <tag>Program</tag>
        <tag>Remote</tag>
        <tag>WinRM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 密码学实践 —— 理解哈希（Hash）算法]]></title>
    <url>%2F2020%2F07%2F03%2Fpractical-cryptography-in-python-hash-algorithm%2F</url>
    <content type="text"><![CDATA[Hash 是密码学安全性的基石，它引入了单向函数（one-way function）和指纹（fingerprint）的概念。即： 对于任意输入，都可以产生相同的、唯一的输出值 输出值中不包含输入值的任何线索 一、保密性（confidentiality）与完整性（integrity）简单来说，信息的保密性确保除授权人员以外的任何人都无法读取该消息，信息的完整性则确保除授权人员以外的任何人都无法修改该消息。很多时候一段加密的消息无法被他人读取和理解（保密性），并不意味着该密文不会在传播过程中被截取和恶意修改（完整性）。 信息摘要（message digest）或指纹（fingerprint）技术即用于验证信息的完整性。 信息摘要需满足的基本条件为： 相同的文档永远会生成相同的摘要（能够作为身份线索） 生成的摘要“感觉”是随机的，即摘要中不包含原始文档的任何信息（无法被破解） 信息摘要也被称作指纹，即可以代表某份文档“身份”的一小段数据，类似于人类的指纹。每个人都可以通过指纹验证其身份，但该指纹并不包含其身体的所有信息。文档的指纹也是如此，可以很方便快速的通过文档内容计算得出一小段唯一的指纹数据作为其身份证明，但是只有指纹数据就几乎不可能得出原始文档的内容。 对于两份文档，只需要比对其信息摘要（指纹）是否一致，就可以确保其内容是否相同，在传播过程中是否被人恶意修改。同时该指纹信息也不会造成原始文档本内容的泄露。 二、MD5MD5 是一种比较古老的哈希算法，其名字中的 MD 即代表 message digest。它可以从任意大小的文档计算出一个唯一的 16 字节长度的摘要数据。 PS：鉴于 MD5 较悠久的历史和不够长的摘要长度，不推荐在安全性很敏感的场景中使用该算法。1234567891011121314151617&gt;&gt;&gt; from hashlib import md5&gt;&gt;&gt; md5(b'alice').hexdigest()'6384e2b2184bcbf58eccf10ca7a6563c'&gt;&gt;&gt; md5(b'bob').hexdigest()'9f9d51bc70ef21ca5c14f307980a29d8'&gt;&gt;&gt; md5(b'balice').hexdigest()'6760742ebf884c998752b4e082b78224'&gt;&gt;&gt; md5(b'cob').hexdigest()'386685f06beecb9f35db2e22da429ec9'&gt;&gt;&gt; md5(b'a').hexdigest()'0cc175b9c0f1b6a831c399e269772661'&gt;&gt;&gt; md5(b'aa').hexdigest()'4124bc0a9335c27f086f24ba207a4912'&gt;&gt;&gt; md5(b'aa' * 100000).hexdigest()'561b1994f6baacd6e5eaf4baaa12849f'&gt;&gt;&gt; md5(b'alice').hexdigest()'6384e2b2184bcbf58eccf10ca7a6563c' 从输出中可以看出，针对不同的输入内容（即便相似度很高，比如 bob 和 cob），摘要算法生成的输出是发散的，彼此之间没有相似性，像是随机生成的结果。但是对于任意相同的输入，生成的摘要数据则都是确定的、唯一的。 三、哈希算法的规则一般我们提到哈希算法，都会关联到密码学、安全性等场景中，实际上我们很早就接触了一种完全“非密码学”的哈希场景。比如小时候跟老师学习判断一个数是奇数还是偶数。。。从本质上看，哈希函数的目的是将巨大（甚至无穷大）数量的事物映射到一个相对较小的数据集中。比如 MD5，不管输入的文档有多大，最终都会生成一个固定长度（16 字节）的十六进制数字作为指纹。这就意味着 MD5 的输入集合，实际上是大于其输出集合的。即只要输入文档的集合足够大（很大很大），就有可能出现重复的指纹信息。 这和判断数字奇偶是相通的。不管某个数字有多大多奇特，我们永远可以将它“压缩”成奇数或偶数，用 1 bit 的 1 或 0 表示就可以。但是只说明某个未知数字是奇数（或偶数），我们就无法猜出该数字的准确值。 上面的逻辑验证了哈希函数共有的 3 个特性： consistency（一致性）：相同的输入只会生成相同的输出信息 compression（压缩）：可以将体量很大的输入压缩成一个固定大小的输出 lossiness（有损的）：只通过检查输出无法反向计算出输入值 但是对于一个满足密码学安全的哈希函数而言，除以上三点以外还需要具有如下属性： Preimage resistance Second-preimage resistance Collision resistance Preimage Resistance哈希函数的 preimage 是指能够生成同一个特定指纹的所有输入的合集。即对于某个哈希函数 H 与摘要 k，所有能够生成 k 的输入值 x （满足 H(x) = k）共同组成了 H 与 k 的 preimage。 preimage resistance 的意义即为，在仅仅只是知晓某个摘要的前提下，通过有限的计算无法获取其 preimage 中的任何一个元素。即只通过结果无法知晓输入。摘要中不包含原始文档的任何信息（lossiness），无法通过逆向运算的方式由摘要反推出原始输入。只能随机地尝试任意输入，以期碰巧得到同样的摘要信息（暴力破解）。 因此前面提到的奇偶函数就不能作为一个安全的哈希函数使用。假设使用奇偶作为哈希函数（奇数输出 1，偶数输出 0），则对于摘要 1，总可以很轻易的在 preimage（此处是全体奇数）中找到任意多个摘要同为 1 的元素。这意味着原始输入可以轻易被修改而不影响指纹数据，则该指纹作为信息完整性的验证条件就失去了意义。 但是对于较安全的哈希算法如 MD5，由 MD5(x) = ca8a0fb205782051bd49f02eae17c9ee 就无法在有限的计算内找到确定的 x 的值。MD5 生成 16 字节（16 * 8 = 128bit）长度的摘要，其中可以包含 2^128 种不同的数字组合。因此使用暴力破解的话，最多需要尝试 2^128 = 340282366920938463463374607431768211456 次！假设每秒钟可以尝试一百万条输入，仍需要 10^26 年完成所有验证操作！ Second-Preimage Resistance 与 Collision Resistancesecond-primage resistance 是指即便知晓某个原始文档以及由该文档生成的摘要数据，仍很难计算可以出生成同样摘要的另一个不同的文档。即在已知 MD5(alice) = 384e2b2184bcbf58eccf10ca7a6563c 的情况下，仍无法找出除 alice 以外的另一个输入生成同样的摘要。为了寻求可以替换掉 alice 的另一个值，同时不影响摘要认证，达到混淆的目的，最终仍需使用暴力破解的方式。 collision resistance 是指很难找出任意两个生成相同摘要（相同而非特定）的输入值。可以参考“生日问题”，即在一个班级中，存在两个生日为同一天的学生的概率远比存在一个生日为特定日期的学生的概率大得多。 collision resistance 的意义在于，无法故意找出两套符合同一指纹的输入以达到混淆的目的。比如 MD5 算法：12345&gt;&gt;&gt; from hashlib import md5&gt;&gt;&gt; md5('bob').hexdigest()'9f9d51bc70ef21ca5c14f307980a29d8'&gt;&gt;&gt; md5('cob').hexdigest()'386685f06beecb9f35db2e22da429ec9' 对于很相似的输入 bob 和 cob，其指纹信息的差异却非常大，没有任何可供预测的规律。这得益于一种称为 avalanche property 的特性：输入的微小变化总可以在输出中产生巨大的无法预测的差异。 由前面提到的生日问题可知，找出两个生成相同指纹的元素远比找出某个可以生成特定指纹的元素要容易的多。以 MD5 算法的暴力破解为例，后者往往需要做 2^128 次尝试，而前者只需要 2^64 次尝试。现实中 MD5 的 collision resistance 远非想象中那么优异，甚至存在一种非暴力破解的方式 能够在一小时以内攻破 MD5 的 collision resistance。所以尽量不要使用 MD5 这个已经不再维护超过 10 年、安全漏洞存在 20 年的古老算法。 参考资料Practical Cryptography in Python: Learning Correct Cryptography by Example]]></content>
      <categories>
        <category>Program</category>
      </categories>
      <tags>
        <tag>Security</tag>
        <tag>Python</tag>
        <tag>Program</tag>
        <tag>Algorithm</tag>
        <tag>Hash</tag>
        <tag>Cryptography</tag>
        <tag>Digest</tag>
        <tag>MD5</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JavaScript 解密 —— 函数进阶（闭包与生成器）]]></title>
    <url>%2F2020%2F06%2F04%2Funderstanding-javascript-closure-and-generator%2F</url>
    <content type="text"><![CDATA[一、闭包简单来说，闭包（closure）允许函数访问和操作位于自身外部的变量。借助闭包的特性，函数可以访问任何变量及其他函数，只要这些数据在该函数定义时位于其作用域内部。1234567891011121314151617var outerValue = "samurai"var laterfunction outerFunction() &#123; var innerValue = "ninja" function innerFunction() &#123; console.log(outerValue) console.log(innerValue) &#125; later = innerFunction&#125;outerFunction()later()// samurai// ninja 参考上面的代码，按照通常的理解： 变量 outerValue 定义在全局作用域中，因此其可以从程序的任意位置访问 outerFunction 执行，将 innerFunction 关联给全局变量 later 当 later（innerFunction）执行时，outerFunction 已经执行完毕，其内部的作用域理应失效，无法被 later 访问 innerValue 由于在 outerFunction 内部定义，则 later 访问 innerValue 时其值应该为 undefined 实际上程序输出的 innerValue 的值为 ninja，即 outerFunction 内部定义的 innerValue 可以被 later 访问。这就是闭包所产生的效果。 当我们在 outerFunction 内部声明 innerFunction 时，一个包含当前作用域（“当前”指的是内部函数定义的时刻）中所有变量的闭包同时被创建。最终 innerFunction 执行时，即便其声明时的原始作用域已经消失，innerFunction 还是可以通过闭包访问其原始作用域。闭包像是使用了一个“保护层”将函数定义时的作用域封闭起来，只要该函数的生命周期未结束，“保护层”内的作用域就一直可以被访问。 二、闭包的现实应用模拟私有变量私有变量即从对象外部不可见的变量，可以向用户隐藏对象内部不必要的实现细节。JavaScript 没有对私有变量的原生支持，但是通过闭包可以实现类似的功能。123456789101112131415161718function Ninja() &#123; var feints = 0 this.getFeints = function() &#123; return feints &#125; this.feint = function() &#123; feints++ &#125;&#125;var ninja1 = new Ninja()ninja1.feint()console.log(ninja1.feints) // undefinedconsole.log(ninja1.getFeints()) // 1var ninja2 = new Ninja()console.log(ninja2.getFeints()) // 0 在回调函数中使用闭包12345678910111213141516&lt;button id="box1"&gt;First Button&lt;/button&gt; &lt;script&gt; function animateIt(elementId) &#123; var elem = document.getElementById(elementId) var tick = 100 var timer = setInterval(function() &#123; if (tick &lt; 1000) &#123; elem.style.width = tick + "px" tick += 10 &#125; else &#123; clearInterval(timer) &#125; &#125;, 100) &#125; animateIt("box1") &lt;/script&gt; 在上面的代码中，一个匿名函数作为参数（回调函数）传递给 setInterval，令指定元素的宽度能够随时间增长以形成动画效果。该匿名函数借助闭包能够访问外部定义的 elem、tick、timer 三个参数，控制动画的进度。这三个参数定义在 animateIt 内部通过闭包被回调函数访问，而不是直接在全局作用域中定义。这样可以避免多个 animateIt 函数依次运行时引起冲突。 三、生成器生成器是一种可以生成一系列值的特殊函数，只不过这些值不是同时产生的，需要用户显式地去请求新值（通过 for、next 等）。12345678910111213function* WeaponGenerator() &#123; yield "Katana" yield "Wakizashi" yield "Kusarigama"&#125;for(let weapon of WeaponGenerator()) &#123; console.log(weapon)&#125;// Katana// Wakizashi// Kusarigama 调用生成器并不意味着会逐步执行生成器函数的定义代码，而是会创建一个迭代器（iterator）对象，通过这个迭代器对象与生成器进行交互（如请求新的值）。123456789101112131415161718function* WeaponGenerator() &#123; yield "Katana" yield "Wakizashi"&#125;const weaponsIterator = WeaponGenerator()const result1 = weaponsIterator.next()console.log(typeof result1, result1.value, result1.done)// object Katana falseconst result2 = weaponsIterator.next()console.log(typeof result2, result2.value, result2.done)// object Wakizashi falseconst result3 = weaponsIterator.next()console.log(typeof result3, result3.value, result3.done)// object undefined true 使用 while 遍历生成器：12345678910111213function* WeaponGenerator() &#123; yield "Katana" yield "Wakizashi"&#125;const weaponsIterator = WeaponGenerator()let itemwhile(!(item = weaponsIterator.next()).done) &#123; console.log(item.value)&#125;// Katana// Wakizashi 生成器嵌套：12345678910111213141516171819function* WarriorGenerator() &#123; yield "Sun Tzu" yield* NinjaGenerator() yield "Genghis Khan"&#125;function* NinjaGenerator() &#123; yield "Hattori" yield "Yoshi"&#125;for(let warrior of WarriorGenerator()) &#123; console.log(warrior)&#125;// Sun Tzu// Hattori// Yoshi// Genghis Khan 生成器的应用生成 ID123456789101112131415function* IdGenerator() &#123; let id = 0 while (true) &#123; yield ++id &#125;&#125;const idIterator = IdGenerator()const ninja1 = &#123; id: idIterator.next().value &#125;const ninja2 = &#123; id: idIterator.next().value &#125;const ninja3 = &#123; id: idIterator.next().value &#125;console.log(ninja1.id) // 1console.log(ninja2.id) // 2console.log(ninja3.id) // 3 遍历DOM 使用递归函数：123456789101112131415161718192021&lt;div id="subTree"&gt; &lt;form&gt; &lt;input type="text" /&gt; &lt;/form&gt; &lt;p&gt;Paragraph&lt;/p&gt; &lt;span&gt;Span&lt;/span&gt;&lt;/div&gt; &lt;script&gt; function traverseDOM(element, callback) &#123; callback(element) element = element.firstElementChild while (element) &#123; traverseDOM(element, callback) element = element.nextElementSibling &#125; &#125; const subTree = document.getElementById("subTree") traverseDOM(subTree, function(element) &#123; console.log(element.nodeName) &#125;) &lt;/script&gt; 使用生成器（无需借助 callback）：12345678910111213141516171819202122&lt;div id="subTree"&gt; &lt;form&gt; &lt;input type="text" /&gt; &lt;/form&gt; &lt;p&gt;Paragraph&lt;/p&gt; &lt;span&gt;Span&lt;/span&gt;&lt;/div&gt; &lt;script&gt; function* DomTraversal(element) &#123; yield element element = element.firstElementChild while (element) &#123; yield* DomTraversal(element) element = element.nextElementSibling &#125; &#125; const subTree = document.getElementById("subTree") for(let element of DomTraversal(subTree)) &#123; console.log(element.nodeName) &#125; &lt;/script&gt; 通过 next 方法向生成器发送值生成器不仅可以通过 yield 表达式生成一系列值，还可以接受用户传入数据，形成一种双向的通信。1234567891011function* NinjaGenerator(action) &#123; const imposter = yield ("Hattori " + action) yield ("Yoshi (" + imposter + ") " + action)&#125;const ninjaIterator = NinjaGenerator("skulk")const result1 = ninjaIterator.next()console.log(result1.value) // Hattori skulkconst result2 = ninjaIterator.next("Hanzo")console.log(result2.value) // Yoshi (Hanzo) skulk 具体的执行流程为： 第一个 ninjaIterator.next() 向生成器请求新值，获取到第一个 yield 右侧的值 &quot;Hattori &quot; + action，同时在 yield (&quot;Hattori &quot; + action) 表达式处挂起执行流程 第二个 ninjaIterator.next(&quot;Hanzo&quot;) 继续向生成器请求新值，同时还发送了参数 Hanzo 给生成器，该参数刚好用作前面挂起的 yield (&quot;Hattori &quot; + action) 表达式的结果，使得 imposter 的值成为 Hanzo 最终 ninjaIterator.next(&quot;Hanzo&quot;) 请求获得第二个 yield 右侧 &quot;Yoshi (&quot; + imposter + &quot;) &quot; + action 的值，即 Yoshi (Hanzo) skulk 参考资料Secrets of the JavaScript Ninja, Second Edition]]></content>
      <categories>
        <category>Program</category>
      </categories>
      <tags>
        <tag>Program</tag>
        <tag>Functional</tag>
        <tag>Function</tag>
        <tag>Development</tag>
        <tag>Web</tag>
        <tag>JavaScript</tag>
        <tag>ECMAScript6</tag>
        <tag>Nodejs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JavaScript 解密 —— 函数初步]]></title>
    <url>%2F2020%2F06%2F04%2Funderstanding-javascript-function-as-first-class-object%2F</url>
    <content type="text"><![CDATA[一、First-class objects在理解函数作为一等对象前，先列举下 JavaScript 中对象支持的操作： 可以通过 {} 字面量创建 可以被赋值给变量、数组项，可以作为其他对象的属性 123var ninja = &#123;&#125; // 赋值给变量ninjaArray.push(&#123;&#125;) // 作为数组项ninja.data = &#123;&#125; // 作为其他对象的属性 可以作为函数的参数或返回值 可以拥有支持动态创建与赋值的属性1234567891011function hide(ninja) &#123; ninja.visibility = false&#125;hide(&#123;&#125;) // 对象作为函数参数function returnNewNinja() &#123; return &#123;&#125; // 对象作为函数返回值&#125;var ninja = &#123;&#125;ninja.name = "Hanzo" // 动态创建的属性 函数作为一等对象JavaScript 中的函数拥有作为对象的所有特性，因此可以像对待任何其他对象一样对其进行使用。 通过字面量创建： function ninjaFunction() {} 将函数赋值给变量：var ninjaFunction = function() {} 数组中加入函数作为数据项：ninjaArray.push(function() {}) 函数作为对象的属性：ninja.data = function() {} 函数作为其他函数的参数或返回值 12345678function call(ninjaFunction) &#123; ninjaFunction()&#125;call(function() &#123;&#125;) // 函数作为参数function returnNewNinjaFunction() &#123; return function() &#123;&#125; // 函数作为返回值&#125; 函数可以拥有支持动态创建和赋值的属性 12var ninjaFunction = function () &#123;&#125;ninjaFunction.name = "Hanzo" 函数即对象，只不过拥有一项额外的特性（能够被调用）以完成一些特定的动作。任何可以对对象做出的操作，同样可以应用在函数身上。 Callback functions回调函数即在后续的某个指定的时间节点被其他代码调用（call back）的函数。12345678910111213141516var text = "Domo arigato"function useless(ninjaCallback) &#123; console.log("In useless function") return ninjaCallback()&#125;function getText() &#123; console.log("In getText function") return text&#125;console.log(useless(getText))// In useless function// In getText function// Domo arigato 或者12345678var text = 'Domo arigato'function useless(ninjaCallback) &#123; return ninjaCallback()&#125;console.log(useless(function() &#123; return text &#125;))// Domo arigato 回调函数在数组排序中的使用：1234var values = [0, 3, 2, 5, 7, 4, 8, 1]values.sort(function(value1, value2)&#123; return value1 - value2&#125;) Self-memoizing functionsmemoization 是指构建一个特殊的函数，该函数可以将之前计算过的值缓存在自己内部，之后再做同样的计算时则可以直接读取缓存的值而不必重新计算。12345678910111213141516171819202122function isPrime(value) &#123; if (!isPrime.answers) &#123; isPrime.answers = &#123;&#125; &#125; if (isPrime.answers[value] !== undefined) &#123; return isPrime.answers[value] &#125; var prime = value !== 1 for (var i = 2; i &lt; value; i++) &#123; if (value % i === 0) &#123; prime = false break &#125; &#125; return isPrime.answers[value] = prime&#125;console.log(isPrime(5))console.log(isPrime.answers)// true// &#123; '5': true &#125; 二、函数定义JavaScript 提供以下几种定义函数的方式： 函数声明（表达式）：function myFun() { return 1 } Arrow function：myArg =&gt; myArg * 2 函数构造器：new Function(&#39;a&#39;, &#39;b&#39;, &#39;return a + b&#39;) 生成器函数：function* myGen() { yield 1 } 函数声明是最基础的定义函数的方式，其基本格式如下：1234function myFunctionName(myFirstArg, mySecondArg) &#123; myStatement1 myStatement2&#125; 函数声明代码可以出现在另一个函数内部：123456function ninja() &#123; function hiddenNinja() &#123; return "ninja here" &#125; return hiddenNinja()&#125; 函数表达式作为 JavaScript 中的一等对象，函数可以通过字面量创建，可以赋值给变量和对象属性，可以作为另一个函数的参数或返回值。也因此可以将其作为表达式使用，即成为其他代码语句的一部分（比如放在赋值语句的等号右边、充当参数或返回值等）12345var myFunc = function() &#123;&#125;myFunc(function() &#123; // 作为参数 return function() &#123;&#125; // 作为返回值&#125;) 函数表达式甚至可以放置在通常应该使用函数标识符的地方，在声明的同时立即完成调用，称为 immediate function：1234567myFunctionName(3) // 普通调用(function() &#123;&#125;)(3) // immediate call+function () &#123;&#125; ()-function () &#123;&#125; ()!function () &#123;&#125; ()~function () &#123;&#125; () Arrow function在很多情况下，arrow function 可以看作对普通函数表达式的简化。如之前的排序示例：1234var values = [0, 3, 2, 5, 7, 4, 8, 1]values.sort(function(value1, value2)&#123; return value1 - value2&#125;) 使用 arrow function 则可以改为如下形式：12var values = [0, 3, 2, 5, 7, 4, 8, 1]values.sort((value1, value2) =&gt; value1 - value2) arrow function 最简单的语法形式为 param =&gt; expression，一个基本示例如下：12var greet = name =&gt; "Greetings, " + nameconsole.log(greet('Oishi')) // Greetings, Oishi 更复杂一点的形式如：123456var greet = name =&gt; &#123; var helloString = 'Greetings, ' return helloString + name&#125;console.log(greet('Oishi')) // Greetings, Oishi 参数Rest prarmeters12345678function multiMax(first, ...remainingNumbers)&#123; var sorted = remainingNumbers.sort(function(a, b)&#123; return b - a &#125;) return first * sorted[0]&#125;console.log(multiMax(3, 1, 2, 3)) // 9 Default parameters123456function performAction(ninja, action = "skulking") &#123; return ninja + " " + action&#125;console.log(performAction("Fuma")) // Fuma skulkingconsole.log(performAction("Yagyu", "sneaking")) // Yagyu sneaking 甚至可以这样写：12345function performAction(ninja, action = "skulking", message = ninja + " " + action) &#123; return message&#125;console.log(performAction("Yoshi")) // Yoshi skulking 三、函数调用作为“函数”调用1234567function ninja(name) &#123; console.log(name) &#125;ninja('Hattori') // Hattorivar samurai = function(name) &#123; console.log(name) &#125;samurai('Hattori'); // Hattori(function(name) &#123; console.log(name) &#125;)('Hattori') // Hattori 作为方法调用：123var ninja = &#123;&#125;ninja.skulk = function() &#123;&#125;ninja.skulk() 作为方法调用与作为函数调用的区别：1234567891011121314151617function whatsMyContext() &#123; return this&#125;console.log(whatsMyContext() === global) // truevar getMyThis = whatsMyContextconsole.log(getMyThis() === global) // truevar ninja1 = &#123; getMyThis: whatsMyContext&#125;console.log(ninja1.getMyThis() === ninja1) // truevar ninja2 = &#123; getMyThis: whatsMyContext&#125;console.log(ninja2.getMyThis() === ninja2) // true 作为构造器函数：12function whatsMyContext()&#123; return this &#125;new whatsMyContext() 注意与函数构造器（如 new Function(&#39;a&#39;, &#39;b&#39;, &#39;return a + b&#39;) ）的区别：函数构造器用来从字符串中动态地创建函数，而构造器函数则用来创建对象实例。 构造器函数在调用时一般会执行以下操作： 创建一个空的对象 新创建的对象绑定给 this 参数传递给构造器，成为构造器函数的上下文 新创建的对象作为 new 操作的返回值被返回 1234567891011function Ninja() &#123; this.skulk = function() &#123; return this &#125;&#125;var ninja1 = new Ninja()console.log(ninja1.skulk() === ninja1) // truevar ninja2 = new Ninja()console.log(ninja2.skulk() === ninja2) // true 若构造器函数的定义中本身具有返回值，则分为两种情况： 若构造器的定义中返回了一个非对象值（字符串、数字等），则该返回值被忽略，由构造器创建的 this 对象被返回作为 new 表达式的值 若构造器的定义中返回了一个对象，则该对象作为 new 表达式的值，由构造器创建的 this 对象则被忽略12345678910function Ninja() &#123; this.skulk = function() &#123; return true &#125; return 1&#125;var ninja = new Ninja()console.log(ninja) // Ninja &#123; skulk: [Function] &#125;console.log(ninja.skulk()) // true 123456789101112var puppet = &#123; rules: false&#125;function Emperor() &#123; this.rules = true return puppet&#125;var emperor = new Emperor()console.log(emperor) // &#123; rules: false &#125;console.log(emperor.rules) // false apply 和 call 方法：12345678910111213141516function juggle() &#123; var result = 0 for (var n = 0; n &lt; arguments.length; n++) &#123; result += arguments[n] &#125; this.result = result&#125;var ninja1 = &#123;&#125;var ninja2 = &#123;&#125;juggle.apply(ninja1, [1,2,3,4])juggle.call(ninja2, 5,6,7,8)console.log(ninja1.result) // 10console.log(ninja2.result) // 26 参考资料Secrets of the JavaScript Ninja, Second Edition]]></content>
      <categories>
        <category>Program</category>
      </categories>
      <tags>
        <tag>Program</tag>
        <tag>Functional</tag>
        <tag>Function</tag>
        <tag>Development</tag>
        <tag>Web</tag>
        <tag>JavaScript</tag>
        <tag>ECMAScript6</tag>
        <tag>Nodejs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django 借助 Celery 实现计划任务排期及调度系统（django-celery-beat）]]></title>
    <url>%2F2020%2F05%2F08%2Ftask-schedule-system-with-django-celery-beat%2F</url>
    <content type="text"><![CDATA[一、环境搭建配置运行环境：123$ python -m venv env$ source ./env/bin/activate$ pip install django-celery-beat django-celery-results redis 项目初始化：123$ django-admin startproject schedule_task$ cd schedule_task$ django-admin startapp schedules 修改 schedule_task/settings.py 配置文件，将 ALLOWED_HOSTS = [] 改为 ALLOWED_HOSTS = [&#39;*&#39;]运行 web 服务： $ python manage.py runserver 0.0.0.0:8000 二、启用 schedule-celery-beat 和 schedule-celery-results在 schedule_task/settings.py 文件中的 INSTALLED_APPS 配置项下，添加如下三个应用：123456INSTALLED_APPS = [ ... 'schedules', 'django_celery_results', 'django_celery_beat'] 其中 django_celery_results 用于在数据库中存储 Celery 任务执行的结果。django_celery_beat 则用于在数据库中记录预先定义好的任务执行规则（比如每隔一分钟执行一次），以及与这些规则关联的待执行的具体任务。 数据库迁移，创建超级用户：12$ python manage.py migrate$ python manage.py createsuperuser 三、系统后台启动 web 服务，用上一步中创建的超级用户登录后台管理系统：http://127.0.0.1:8000/admin 。界面如下： 界面中 CELERY RESULTS 为 django_celery_results 创建的用于保存任务结果的数据库表。 PERIODIC TASKS 下面则是由 django_celery_beat 创建的用于保存 Celery 任务及其执行规则的几张数据库表，具体含义如下： Clocked：定义在具体某个时间点触发的执行规则 Crontabs：类似于 Linux 系统下 crontab 的语法 Intervals：定义任务重复执行的时间间隔 Periodic tasks：具体某个待执行的任务，需要与其他表（Clocked、Crontabs、Intervals、Solar events）中定义的执行规则相关联 Solar events：根据日升和日落等太阳运行轨迹确定执行规则 如定义一个每隔 10 秒执行一次的规则，步骤如下： 四、创建 Celery 任务Celery 任务需要在源代码中手动创建，具体可参考官方文档 Using Celery With Django，简要步骤如下： schedule_task/schedule_task/celery.py：123456789101112131415# schedule_task/schedule_task/celery.pyfrom __future__ import absolute_import, unicode_literalsimport osfrom celery import Celery# set the default Django settings module for the 'celery' program.os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'schedule_task.settings')app = Celery('schedule_task')# - namespace='CELERY' means all celery-related configuration keys# should have a `CELERY_` prefix.app.config_from_object('django.conf:settings', namespace='CELERY')# Load task modules from all registered Django app configs.app.autodiscover_tasks() schedule_task/schedule_task/__init__.py：12345678# schedule_task/schedule_task/__init__.pyfrom __future__ import absolute_import, unicode_literals# This will make sure the app is always imported when# Django starts so that shared_task will use this app.from .celery import app as celery_app__all__ = ('celery_app',) schedule_tasks/schedules/tasks.py：1234567# schedule_tasks/schedules/tasks.pyfrom __future__ import absolute_import, unicode_literalsfrom celery import shared_task@shared_task(bind=True)def debug_task(self): return f'Hello Celery, the task id is: &#123;self.request.id&#125;' 使用 Redis 作为 Message Broker，Django 默认配置的数据库作为 Result Backend，DatabaseScheduler 作为 Celery 的任务调度器： schedule_task/schedule_task/settings.py：12345# schedule_task/schedule_task/settings.py# ...CELERY_RESULT_BACKEND = 'django-db'CELERY_BROKER_URL = 'redis://127.0.0.1:6379/0'CELERY_BEAT_SCHEDULER = 'django_celery_beat.schedulers:DatabaseScheduler' 此时可进入系统管理后台，将任务 debug_task 关联给每隔 10s 执行的规则： 只需要填写基本信息，选择相关联的任务和 Schedule 即可。此外，还可以根据需求自行定义计划任务的其他参数，如： 生效时间 是否只执行一次 传递给任务的参数 失效时间 五、运行测试为了使系统正常运行，需要同时开启三个服务： web 服务：python manage.py runserver 0.0.0.0:8000 Celery Worker：celery -A schedule_task worker -l info Celery Beat：celery -A schedule_task beat -l info 服务成功运行后，输出信息如下 Celery Beat 持续监测数据库中存储的计划任务信息，将满足触发条件的任务传递给 Celery Worker 执行： 12345678910111213141516171819$ celery -A schedule_task beat -l infocelery beat v4.4.2 (cliffs) is starting.__ - ... __ - _LocalTime -&gt; 2020-05-08 03:44:41Configuration -&gt; . broker -&gt; redis://127.0.0.1:6379/0 . loader -&gt; celery.loaders.app.AppLoader . scheduler -&gt; django_celery_beat.schedulers.DatabaseScheduler . logfile -&gt; [stderr]@%INFO . maxinterval -&gt; 5.00 seconds (5s)[2020-05-08 03:44:41,578: INFO/MainProcess] beat: Starting...[2020-05-08 03:44:41,578: INFO/MainProcess] Writing entries...[2020-05-08 03:44:46,745: INFO/MainProcess] Writing entries...[2020-05-08 03:44:51,594: INFO/MainProcess] Scheduler: Sending due task debug_task (schedules.tasks.debug_task)[2020-05-08 03:45:01,585: INFO/MainProcess] Scheduler: Sending due task debug_task (schedules.tasks.debug_task)[2020-05-08 03:45:11,587: INFO/MainProcess] Scheduler: Sending due task debug_task (schedules.tasks.debug_task)[2020-05-08 03:45:21,588: INFO/MainProcess] Scheduler: Sending due task debug_task (schedules.tasks.debug_task)[2020-05-08 03:45:31,591: INFO/MainProcess] Scheduler: Sending due task debug_task (schedules.tasks.debug_task) Celery Worker 负责执行由 Beat 传过来的任务，输出执行结果并将结果保存至 result backend（即数据库）： 123456789101112131415$ celery -A schedule_task worker -l info[tasks] . schedules.tasks.debug_task[2020-05-08 03:44:05,521: INFO/MainProcess] Connected to redis://127.0.0.1:6379/0[2020-05-08 03:44:05,529: INFO/MainProcess] mingle: searching for neighbors[2020-05-08 03:44:06,546: INFO/MainProcess] mingle: all alone[2020-05-08 03:44:06,558: INFO/MainProcess] celery@mirrors ready.[2020-05-08 03:44:51,607: INFO/MainProcess] Received task: schedules.tasks.debug_task[3d6b77bb-d4b7-4a5d-b05f-3b85e5dafce7][2020-05-08 03:44:51,687: INFO/ForkPoolWorker-1] Task schedules.tasks.debug_task[3d6b77bb-d4b7-4a5d-b05f-3b85e5dafce7] succeeded in 0.07936301361769438s: 'Hello Celery, the task id is: 3d6b77bb-d4b7-4a5d-b05f-3b85e5dafce7'[2020-05-08 03:45:01,588: INFO/MainProcess] Received task: schedules.tasks.debug_task[a097dc02-71c9-4cab-9871-92ed1a7f2f45][2020-05-08 03:45:01,660: INFO/ForkPoolWorker-1] Task schedules.tasks.debug_task[a097dc02-71c9-4cab-9871-92ed1a7f2f45] succeeded in 0.07120843604207039s: 'Hello Celery, the task id is: a097dc02-71c9-4cab-9871-92ed1a7f2f45'[2020-05-08 03:45:11,590: INFO/MainProcess] Received task: schedules.tasks.debug_task[1b0dfc23-d3cc-495a-b306-9d1defe4b119][2020-05-08 03:45:11,659: INFO/ForkPoolWorker-1] Task schedules.tasks.debug_task[1b0dfc23-d3cc-495a-b306-9d1defe4b119] succeeded in 0.0677587790414691s: 'Hello Celery, the task id is: 1b0dfc23-d3cc-495a-b306-9d1defe4b119' 后台管理系统 task results 界面： task results 里默认显示的是 UTC 时间，可以修改 schedule_task/schedule_task/settings.py 配置文件更改时区设置：1TIME_ZONE = 'Asia/Shanghai' PS：实际测试以后，此处的时区设置只会对网页端 task results 表格中显示的时间起作用，实际保存到 task results 数据库表中的时间依旧是 UTC 时间。如需要二次开发，可以调用返回的 datetime 对象的 astimezone 方法进行时区格式转换。 参考资料Celery 4.4.2 documentation: First steps with Djangodjango-celery-beat - Database-backed Periodic Tasksdjango-celery-results - Celery Result Backends for Django]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Program</tag>
        <tag>Development</tag>
        <tag>Web</tag>
        <tag>Django</tag>
        <tag>Celery</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[理解 JavaScript 编程（ECMAScript 6）（一）]]></title>
    <url>%2F2020%2F05%2F07%2Funderstanding-JavaScript-EMCAScript-6-1%2F</url>
    <content type="text"><![CDATA[一、Block Binding在大多数基于 C 的编程语言中，变量通常会在声明时创建。而对于 JavaScript 语言，变量创建的时间点则取决于具体的声明方式。JavaScript 中经典的使用 var 声明变量的方式容易引起困惑，因此 ECMAScript 6 中引入了块级别的变量绑定（block-level binding）。 var 关键字var 关键字对于变量的声明，会默认该声明位于函数顶部（位于函数外部时为全局作用域），而不去管声明语句实际出现的位置。称为 hoisting 。12345678910111213141516function getValue(condition) &#123; if (condition) &#123; var value = "blue" console.log("condition is true and value is " + value) &#125; else &#123; console.log("condition is false and value is " + value) &#125; console.log("outside if, value is " + value)&#125;getValue(true)// condition is true and value is blue// outside if, value is bluegetValue(false)// condition is false and value is undefined// outside if, value is undefined 习惯上会认为，上述代码中只有 condition 为 True 时变量 value 才会被创建；实际上 value 变量存在于函数的各个部分，只是在 condition 为 False 时未被初始化（undefined）。 上面的代码会被 JavaScript 引擎视作如下形式：12345678910function getValue(condition) &#123; var value; if (condition) &#123; value = "blue"; console.log("condition is true and value is " + value) &#125; else &#123; console.log("condition is false and value is " + value) &#125; console.log("outside if, value is " + value)&#125; 块级声明和 let 语句由块级声明创建的变量无法被该代码块以外的部分访问。 块作用域（Block scopes）一般创建于以下位置： 函数内部 代码块内部（被大括号 {} 包裹的部分） 块作用域符合大部分基于 C 的编程语言的工作方式。 let 关键字会将变量的作用域限制在当前代码块内部。12345678910111213function getValue(condition) &#123; if (condition) &#123; let value = "blue" console.log("condition is true and value is " + value) &#125; else &#123; console.log("condition is false and value is " + value) &#125; console.log("outside if, value is " + value)&#125;getValue(true)// condition is true and value is blue// ReferenceError: value is not defined No Redeclaration如果同一作用域内已有相同名称的变量被声明，则 let 语句会报错。123var count = 30let count = 40// SyntaxError: Identifier 'count' has already been declared 但是在不同作用域中，类似的情况则不会报错：1234var count = 30if (true) &#123; let count = 40&#125; const 关键字用于声明常量，常量的值一旦确定后即不可再变更，因此在声明的同时必须赋值以完成初始化。123const maxItems = 30const name;// SyntaxError: Missing initializer in const declaration 需要注意的是，常量的“不可变”仅针对变量与值的绑定关系，而并不限制值本身的改动。即使用 const 声明某个对象，则对象本身的改动不被禁止。12345678910const person = &#123; name: "Nicholas"&#125;person.name = "Greg"person.name// 'Greg'person = &#123; name: "Grep"&#125;// TypeError: Assignment to constant variable. 循环中的块级绑定var：12345for (var i = 0; i &lt; 10; i++) &#123; // do nothing&#125;console.log(i)// 10 let：12345for (let i = 0; i &lt; 10; i++) &#123; // do nothing&#125;console.log(i)// ReferenceError: i is not defined var 声明语句的特性（loop 变量可以从 loop 外部访问）使得在循环中创建函数的行为会产生问题。123456789101112131415161718192021var funcs = []for (var i = 0; i &lt; 10; i++) &#123; funcs.push(function() &#123; console.log(i) &#125;)&#125;funcs.forEach(function(func) &#123; func()&#125;)// 10// 10// 10// 10// 10// 10// 10// 10// 10// 10 解决的办法是使用如下代码：1234567891011121314151617181920212223var funcs = []for (var i = 0; i &lt; 10; i++) &#123; funcs.push((function(value) &#123; return function() &#123; console.log(value) &#125; &#125;(i)))&#125;funcs.forEach(function(func) &#123; func()&#125;)// 0// 1// 2// 3// 4// 5// 6// 7// 8// 9 有了块级声明以后，上述需求可以被简单地实现（只需要将第一段代码中的 var 关键字改为 let 即可）：123456789101112131415161718192021var funcs = []for (let i = 0; i &lt; 10; i++) &#123; funcs.push(function() &#123; console.log(i) &#125;)&#125;funcs.forEach(function(func) &#123; func()&#125;)// 0// 1// 2// 3// 4// 5// 6// 7// 8// 9 二、函数参数带默认值的函数在 ECMAScript 5 及以前版本的 JavaScript 中，通常使用如下模式创建带默认参数的函数：12345function makeRequest(url, timeout, callback) &#123; timeout = timeout || 2000 callback = callback || function() &#123;&#125; // the rest code&#125; 但上述 || （或）操作符的使用存在一定问题，如传递给 timeout 参数的值为 0 时，timeout || 2000 表达式的值为 2000 而不是 0，导致程序出现意想不到的结果。改进如下：12345function makeRequest(url, timeout, callback) &#123; timeout = (typeof timeout !== "undefined") ? timeout : 2000 callback = (typeof callback !== "undefined") ? callback : function() &#123;&#125; // the rest code&#125; 在 ECMAScript 6 中，为函数的参数提供默认值的方式则非常简单直观：123function makeRequest(url, timeout = 2000, callback = function() &#123;&#125;) &#123; // the rest code&#125; 表达式作为参数默认值：12345678910function getValue() &#123; return 5&#125;function add(first, second = getValue()) &#123; return first + second&#125;console.log(add(1, 1)) // 2console.log(add(1)) // 6 甚至可以使用如下代码：12345678910function getValue(value) &#123; return value + 5&#125;function add(first, second = getValue(first)) &#123; return first + second&#125;console.log(add(1, 1)) // 2console.log(add(1)) // 7 匿名参数ECMAScript 5 中的匿名参数（通过 arguments 对象获取所有参数，包含定义函数时未显式指定的参数）：123456789101112131415161718function pick(object) &#123; let result = Object.create(null) for (let i = 1, len = arguments.length; i &lt; len; i++) &#123; result[arguments[i]] = object[arguments[i]] &#125; return result&#125;let book = &#123; title: "Understanding ECMAScript 6", author: "Nicholas C. Zakas", year: 2016&#125;let bookData = pick(book, "author", "year")console.log(bookData.author) // "Nicholas C. Zakas"console.log(bookData.year) // 2016 注意 for 循环是从 i=1 即第二个参数开始的。 Rest Parameters上述 pick 函数可以利用 ECMAScript 6 支持的 Rest Parameters 特性重写为如下形式：12345678910111213141516171819function pick(object, ...keys) &#123; let result = Object.create(null) for (let i = 0, len = keys.length; i &lt; len; i++) &#123; result[keys[i]] = object[keys[i]] &#125; return result&#125;let book = &#123; title: "Understanding ECMAScript 6", author: "Nicholas C. Zakas", year: 2016&#125;let bookData = pick(book, "author", "year")console.log(bookData.author) // "Nicholas C. Zakas"console.log(bookData.year) // 2016 函数构造器123var add = new Function("first", "second", "return first + second")console.log(add(1, 1)) // 2 ECMAScript 6 使得函数构造器可以支持默认参数和 rest parameters 等特性：12345678var add = new Function("first", "second = first", "return first + second")console.log(add(1, 1)) // 2console.log(add(1)) // 2var pickFirst = new Function("...args", "return args[0]")console.log(pickFirst(1, 2)) // 1 函数的两种调用方式123456789function Person(name) &#123; this.name = name&#125;var person = new Person("Nicholas")var notAPerson = Person("Nicholas")console.log(person) // Person &#123; name: 'Nicholas' &#125;console.log(notAPerson) // undefined JavaScript 有两个针对函数的内部方法：[[Call]] 和 [[Construct]]。 当函数不通过 new 关键字调用时，[[Call]] 方法执行，接着运行函数体中的代码；当函数通过 new 关键字调用时，[[Construct]] 方法执行，创建一个新的对象实例并绑定给 this，之后继续执行函数体中的代码。 ECMAScript 6 中可以使用 new.target 判断当前函数是否由 new 调用：12345678910function Person(name) &#123; if (typeof new.target !== "undefined") &#123; this.name = name &#125; else &#123; throw new Error("You must use new with Person.") &#125;&#125;var person = new Person("Nicholas")var notAPerson = Person("Michael") // error Arrow FunctionArrow Function 是指使用 =&gt; 符号定义的函数。与传统的 JavaScript 函数相比，Arrow Function 主要有以下几个不同点： 没有 this, super, arguments, new.target 的绑定。Arrow Function 中 this, super, arguments, new.target 的值由距离最近的非 Arrow Function 定义 不能被 new 调用。Arrow Function 没有构造方法因此不能作为构造器使用 没有 prototype。Arrow Function 的 prototype 属性不存在（函数本身不能被 new 调用，prototype 没有必要） 函数中的 this 的值不能被修改 基本语法：123456789101112131415161718192021222324252627282930313233343536373839let reflect = value =&gt; value// equivalent to:let reflect = function(value) &#123; return value&#125;let sum = (num1, num2) =&gt; num1 + num2// equivalent to:let sum = function(num1, num2) &#123; return num1 + num2&#125;let getName = () =&gt; "Nicholas"// equivalent to:let getName = function() &#123; return "Nicholas"&#125;let doNothing = () =&gt; &#123;&#125;// equivalent to:let doNothing = function() &#123;&#125;let getTempItem = id =&gt; (&#123; id: id, name: "Temp" &#125;)// equivalent to:let getTempItem = function(id) &#123; return &#123; id: id, name: "Temp" &#125;&#125; 参考资料Understanding ECMAScript 6]]></content>
      <categories>
        <category>Program</category>
      </categories>
      <tags>
        <tag>Program</tag>
        <tag>Advanced</tag>
        <tag>Functional</tag>
        <tag>JavaScript</tag>
        <tag>ECMAScript6</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Vue.js 学习笔记（二）事件与表单元素]]></title>
    <url>%2F2020%2F04%2F03%2Fevents-and-form-elements-in-vue-js%2F</url>
    <content type="text"><![CDATA[接上文 Vue.js 学习笔记（一）数据绑定与指示器，环境搭建与配置等基础内容可前往参考 Events用户与 HTML 元素的交互行为都会触发特定的事件。Vue.js 通过 v-on 指示器创建对事件的绑定。1234567891011121314151617&lt;template&gt; &lt;div class=&quot;container-fluid&quot;&gt; &lt;div class=&quot;bg-primary text-white m-2 p-3 text-center&quot;&gt; &lt;h3 v-on:click=&quot;name = &apos;Clicked&apos;&quot;&gt;&#123;&#123; name &#125;&#125;&lt;/h3&gt; &lt;/div&gt; &lt;/div&gt;&lt;/template&gt;&lt;script&gt;export default &#123; data: function () &#123; return &#123; name: &quot;Lifejacket&quot; &#125; &#125;,&#125;&lt;/script&gt; Methods &amp; Events12345678910111213141516171819202122232425&lt;template&gt; &lt;div class=&quot;container-fluid&quot;&gt; &lt;div class=&quot;bg-primary text-white m-2 p-3 text-center&quot;&gt; &lt;h3 v-on:click=&quot;handleEvent(&apos;Soccer Ball&apos;, $event)&quot;&gt;&#123;&#123; name &#125;&#125;&lt;/h3&gt; &lt;/div&gt; &lt;div class=&quot;bg-primary text-white m-2 p-3 text-center&quot;&gt; &lt;h3 v-on:click=&quot;handleEvent(&apos;Stadium&apos;, $event)&quot;&gt;&#123;&#123; name &#125;&#125;&lt;/h3&gt; &lt;/div&gt; &lt;/div&gt;&lt;/template&gt;&lt;script&gt;export default &#123; data: function () &#123; return &#123; name: &quot;Lifejacket&quot; &#125; &#125;, methods: &#123; handleEvent(name, $event) &#123; this.name = `$&#123;name&#125; - $&#123;$event.type&#125;`; &#125; &#125;&#125;&lt;/script&gt; 综合示例12345678910111213141516171819202122232425262728293031323334&lt;template&gt; &lt;div class=&quot;container-fluid&quot;&gt; &lt;h3 class=&quot;bg=primary text-white text-center mt-2 p-2&quot;&gt;&#123;&#123; message &#125;&#125;&lt;/h3&gt; &lt;table class=&quot;table table-sm table-striped table-bordered&quot;&gt; &lt;tr&gt;&lt;th&gt;Index&lt;/th&gt;&lt;th&gt;Name&lt;/th&gt;&lt;th&gt;Actions&lt;/th&gt;&lt;/tr&gt; &lt;tr v-for=&quot;(name, index) in names&quot; v-bind:key=&quot;name&quot;&gt; &lt;td&gt;&#123;&#123; index &#125;&#125;&lt;/td&gt; &lt;td&gt;&#123;&#123; name &#125;&#125;&lt;/td&gt; &lt;td&gt; &lt;button class=&quot;btn btn-sm bg-primary text-white&quot; v-on:click=&quot;handleClick(name)&quot;&gt; Select &lt;/button&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;/div&gt;&lt;/template&gt;&lt;script&gt;export default &#123; data: function () &#123; return &#123; message: &quot;Ready&quot;, names: [&quot;Kayak&quot;, &quot;Lifejacket&quot;, &quot;Soccer Ball&quot;, &quot;Stadium&quot;] &#125; &#125;, methods: &#123; handleClick(name) &#123; this.message = `Select: $&#123;name&#125;`; &#125; &#125;&#125;&lt;/script&gt; Keyboard Events123456789101112131415161718192021222324&lt;template&gt; &lt;div class=&quot;container-fluid&quot;&gt; &lt;div class=&quot;bg-primary p-4 text-white h3&quot;&gt; &#123;&#123; message &#125;&#125; &lt;/div&gt; &lt;input class=&quot;form-control bg-light&quot; placeholder=&quot;Type here...&quot; v-on:keydown.ctrl=&quot;handleKey&quot; /&gt; &lt;/div&gt;&lt;/template&gt;&lt;script&gt;export default &#123; data: function () &#123; return &#123; message: &quot;Ready&quot;, &#125; &#125;, methods: &#123; handleKey($event) &#123; this.message = $event.key; &#125; &#125;&#125;&lt;/script&gt; Form Elementsv-model 是 Vue.js 中用于 HTML 表单元素（input、select、textarea 等）的内置指示器。它能够在表单元素与数据之间创建双向绑定，使得不管数据怎样变更，元素的行为与数据值总可以保持一致性。 Two-Way Binding1234567891011121314151617181920212223242526272829303132333435363738394041424344454647&lt;template&gt; &lt;div class=&quot;container-fluid&quot;&gt; &lt;div class=&quot;bg-info m-2 p-2 text-white&quot;&gt; &lt;div&gt;Data Value: &#123;&#123; dataValue &#125;&#125;&lt;/div&gt; &lt;div&gt;Other Value: &#123;&#123; otherValue || &quot;(Empty)&quot; &#125;&#125;&lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;bg-primary m-2 p-2 text-white&quot;&gt; &lt;div class=&quot;form-check&quot;&gt; &lt;label class=&quot;form-check-label&quot;&gt; &lt;input class=&quot;form-check-input&quot; type=&quot;checkbox&quot; v-model=&quot;dataValue&quot; /&gt; Data Value &lt;/label&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;bg-primary m-2 p-2&quot;&gt; &lt;input type=&quot;text&quot; class=&quot;form-control&quot; v-model=&quot;otherValue&quot; /&gt; &lt;/div&gt; &lt;div class=&quot;text-center m-2&quot;&gt; &lt;button class=&quot;btn btn-secondary&quot; v-on:click=&quot;reset&quot;&gt; Reset &lt;/button&gt; &lt;/div&gt; &lt;/div&gt;&lt;/template&gt;&lt;script&gt;export default &#123; name: &quot;app&quot;, data: function () &#123; return &#123; dataValue: false, otherValue: &quot;&quot; &#125; &#125;, methods: &#123; reset() &#123; this.dataValue = false; this.otherValue = &quot;&quot;; &#125; &#125;&#125;&lt;/script&gt; 在上面的示例中，选中或取消 checkbox，在 input 中输入任意文本内容，与之关联的数据 dataValue 和 otherValue 的值都会同步发生改变。反过来，在控制台中手动修改 dataValue 和 otherValue 的值，checkbox 和 input 元素也会立即产生相应的变更。 Binding Text Fields123456789101112131415161718192021222324252627282930313233343536373839&lt;template&gt; &lt;div class=&quot;container-fluid&quot;&gt; &lt;div class=&quot;bg-info m-2 p-2 text-white&quot;&gt; &lt;div&gt;Name: &#123;&#123; name &#125;&#125;&lt;/div&gt; &lt;div&gt;Password: &#123;&#123; password &#125;&#125;&lt;/div&gt; &lt;div&gt;Details: &#123;&#123; details &#125;&#125;&lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;bg-primary m-2 p-2 text-white&quot;&gt; &lt;div class=&quot;form-group&quot;&gt; &lt;label&gt;Name&lt;/label&gt; &lt;input class=&quot;form-control&quot; v-model=&quot;name&quot; /&gt; &lt;/div&gt; &lt;div class=&quot;form-group&quot;&gt; &lt;label&gt;Password&lt;/label&gt; &lt;input type=&quot;password&quot; class=&quot;form-control&quot; v-model=&quot;password&quot; /&gt; &lt;/div&gt; &lt;div class=&quot;form-group&quot;&gt; &lt;label&gt;Detials&lt;/label&gt; &lt;textarea class=&quot;form-control&quot; v-model=&quot;details&quot; /&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt;&lt;/template&gt;&lt;script&gt;export default &#123; name: &quot;app&quot;, data: function () &#123; return &#123; name: &quot;Bob&quot;, password: &quot;secret&quot;, details: &quot;Has admin access&quot; &#125; &#125;&#125;&lt;/script&gt; Radio &amp; Checkbox1234567891011121314151617181920212223242526272829303132333435363738394041&lt;template&gt; &lt;div class=&quot;container-fluid&quot;&gt; &lt;div class=&quot;bg-info m-2 p-2 text-white&quot;&gt; &lt;div&gt;Name: &#123;&#123; name &#125;&#125;&lt;/div&gt; &lt;div&gt;Has Admin Access: &#123;&#123; hasAdminAccess &#125;&#125;&lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;bg-primary m-2 p-2 text-white&quot;&gt; &lt;div class=&quot;form-check&quot;&gt; &lt;input class=&quot;form-check-input&quot; type=&quot;radio&quot; v-model=&quot;name&quot; value=&quot;Bob&quot; /&gt; &lt;label class=&quot;form-check-label&quot;&gt;Bob&lt;/label&gt; &lt;/div&gt; &lt;div class=&quot;form-check&quot;&gt; &lt;input class=&quot;form-check-input&quot; type=&quot;radio&quot; v-model=&quot;name&quot; value=&quot;Alice&quot; /&gt; &lt;label class=&quot;form-check-label&quot;&gt;Alice&lt;/label&gt; &lt;/div&gt; &lt;div class=&quot;form-check&quot;&gt; &lt;input class=&quot;form-check-input&quot; type=&quot;checkbox&quot; v-model=&quot;hasAdminAccess&quot; /&gt; &lt;label class=&quot;form-check-label&quot;&gt;Has Admin Access?&lt;/label&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt;&lt;/template&gt;&lt;script&gt;export default &#123; name: &quot;app&quot;, data: function () &#123; return &#123; name: &quot;Bob&quot;, hasAdminAccess: true &#125; &#125;&#125;&lt;/script&gt; 注意每个 radio 元素都需要配置一个 value 属性，它决定了 v-model 指示器怎样修改与之绑定的数据（name）的值。 Bind Select12345678910111213141516171819202122232425262728293031&lt;template&gt; &lt;div class=&quot;container-fluid&quot;&gt; &lt;div class=&quot;bg-info m-2 p-2 text-white&quot;&gt; &lt;div&gt;Name: &#123;&#123; name &#125;&#125;&lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;bg-primary m-2 p-2 text-white&quot;&gt; &lt;div class=&quot;form-group&quot;&gt; &lt;label&gt;Selected Names&lt;/label&gt; &lt;select class=&quot;form-control&quot; v-model=&quot;name&quot;&gt; &lt;option value=&quot;all&quot;&gt;Everyone&lt;/option&gt; &lt;option v-for=&quot;n in allNames&quot; v-bind:key=&quot;n&quot; v-bind:value=&quot;n&quot;&gt;Just &#123;&#123; n &#125;&#125;&lt;/option&gt; &lt;/select&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt;&lt;/template&gt;&lt;script&gt;export default &#123; name: &quot;app&quot;, data: function () &#123; return &#123; allNames: [&quot;Bob&quot;, &quot;Alice&quot;, &quot;Joe&quot;], name: &quot;Bob&quot; &#125; &#125;&#125;&lt;/script&gt; 注意代码中 v-bind 指示器的使用。这里必须使用 v-bind 设置 option 的 value 属性，因为等号后面的 n 是变量而不是某个具体的值。 Bind Array12345678910111213141516171819202122232425262728293031323334353637&lt;template&gt; &lt;div class=&quot;container-fluid&quot;&gt; &lt;div class=&quot;bg-info m-2 p-2 text-white&quot;&gt; &lt;div&gt;Selected Cities: &#123;&#123; cities &#125;&#125;&lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;form-check m-2&quot; v-for=&quot;city in cityNames&quot; v-bind:key=&quot;city&quot;&gt; &lt;label class=&quot;form-check-label&quot;&gt; &lt;input type=&quot;checkbox&quot; class=&quot;form-check-input&quot; v-model=&quot;cities&quot; v-bind:value=&quot;city&quot; /&gt; &#123;&#123; city &#125;&#125; &lt;/label&gt; &lt;/div&gt; &lt;div class=&quot;text-center&quot;&gt; &lt;button v-on:click=&quot;reset&quot; class=&quot;btn btn-info&quot;&gt;Reset&lt;/button&gt; &lt;/div&gt; &lt;/div&gt;&lt;/template&gt;&lt;script&gt;export default &#123; name: &quot;app&quot;, data: function () &#123; return &#123; cityNames: [&quot;London&quot;, &quot;New York&quot;, &quot;Paris&quot;, &quot;Berlin&quot;], cities: [] &#125; &#125;, methods: &#123; reset() &#123; this.cities = []; &#125; &#125;&#125;&lt;/script&gt; Form 元素自定义值123456789101112131415161718192021222324252627282930&lt;template&gt; &lt;div class=&quot;container-fluid&quot;&gt; &lt;div class=&quot;m-2 p-2 text-white&quot; v-bind:class=&quot;elemClass&quot;&gt; &lt;div&gt;Value: &#123;&#123; elemClass &#125;&#125;&lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;form-check m-2&quot;&gt; &lt;label class=&quot;form-check-label&quot;&gt; &lt;input type=&quot;checkbox&quot; class=&quot;form-check-input&quot; v-model=&quot;dataValue&quot; /&gt; Dark Color &lt;/label&gt; &lt;/div&gt; &lt;/div&gt;&lt;/template&gt;&lt;script&gt;export default &#123; name: &quot;app&quot;, data: function () &#123; return &#123; dataValue: false, &#125; &#125;, computed: &#123; elemClass() &#123; return this.dataValue ? &quot;bg-primary&quot; : &quot;bg-info&quot;; &#125; &#125;&#125;&lt;/script&gt; 通过 computed property 将 checkbox 的 true 和 false 值转换为 &lt;div&gt; 元素的 bg-primary 和 bg-info class 属性。 综合示例12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152&lt;template&gt; &lt;div class=&quot;container-fluid&quot;&gt; &lt;div class=&quot;m-2 p-2 text-white&quot; v-bind:class=&quot;dataValue&quot;&gt; &lt;div&gt;Value: &#123;&#123; dataValue &#125;&#125;&lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;form-check m-2&quot;&gt; &lt;label class=&quot;form-check-label&quot;&gt; &lt;input type=&quot;checkbox&quot; class=&quot;form-check-input&quot; v-model=&quot;dataValue&quot; v-bind:true-value=&quot;darkColor&quot; v-bind:false-value=&quot;lightColor&quot; /&gt; Dark Color &lt;/label&gt; &lt;/div&gt; &lt;div class=&quot;form-group m-2 p-2 bg-secondary&quot;&gt; &lt;label&gt;Color&lt;/label&gt; &lt;select v-model=&quot;dataValue&quot; class=&quot;form-control&quot;&gt; &lt;option v-bind:value=&quot;darkColor&quot;&gt;Dark Color&lt;/option&gt; &lt;option v-bind:value=&quot;lightColor&quot;&gt;Light Color&lt;/option&gt; &lt;/select&gt; &lt;/div&gt; &lt;div class=&quot;form-check-inline m-2&quot;&gt; &lt;label class=&quot;form-check-label&quot;&gt; &lt;input type=&quot;radio&quot; class=&quot;form-check-input&quot; v-model=&quot;dataValue&quot; v-bind:value=&quot;darkColor&quot; /&gt; Dark Color &lt;/label&gt; &lt;/div&gt; &lt;div class=&quot;form-check-inline m-2&quot;&gt; &lt;label class=&quot;form-check-lable&quot;&gt; &lt;input type=&quot;radio&quot; class=&quot;form-check-input&quot; v-model=&quot;dataValue&quot; v-bind:value=&quot;lightColor&quot; /&gt; Light Color &lt;/label&gt; &lt;/div&gt; &lt;/div&gt;&lt;/template&gt;&lt;script&gt;export default &#123; name: &quot;app&quot;, data: function () &#123; return &#123; darkColor: &quot;bg-primary&quot;, lightColor: &quot;bg-info&quot;, dataValue: &quot;bg-info&quot; &#125; &#125;,&#125;&lt;/script&gt; 参考资料Pro Vue.js 2]]></content>
      <categories>
        <category>Program</category>
      </categories>
      <tags>
        <tag>Program</tag>
        <tag>Web</tag>
        <tag>Vue</tag>
        <tag>Frontend</tag>
        <tag>Javascript</tag>
        <tag>Vue.js</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[高效 Python 代码 —— 属性与 @property 方法]]></title>
    <url>%2F2020%2F04%2F03%2Feffective-python-using-property%2F</url>
    <content type="text"><![CDATA[一、用属性替代 getter 或 setter 方法以下代码中包含手动实现的 getter（get_ohms) 和 setter（set_ohms） 方法：12345678910111213141516171819class OldResistor(object): def __init__(self, ohms): self._ohms = ohms self.voltage = 0 self.current = 0 def get_ohms(self): return self._ohms def set_ohms(self, ohms): self._ohms = ohmsr0 = OldResistor(50e3)print(f'Before: &#123;r0.get_ohms()&#125;')r0.set_ohms(10e3)print(f'After: &#123;r0.get_ohms()&#125;')# =&gt; Before: 50000.0# =&gt; After: 10000.0 这些工具方法有助于定义类的接口，使得开发者可以方便地封装功能、验证用法并限定取值范围。但是在 Python 语言中，应尽量从简单的 public 属性写起：123456789101112class Resistor(object): def __init__(self, ohms): self.ohms = ohms self.voltage = 0 self.current = 0r1 = Resistor(50e3)print(f'Before: &#123;r1.ohms&#125;')r1.ohms = 10e3print(f'After: &#123;r1.ohms&#125;')# =&gt; Before: 50000.0# =&gt; After: 10000.0 访问实例的属性则可以直接使用 instance.property 这样的格式。 如果想在设置属性的同时实现其他特殊的行为，如在对上述 Resistor 类的 voltage 属性赋值时，需要同时修改其 current 属性。可以借助 @property 装饰器和 setter 方法实现此类需求：1234567891011121314151617181920212223from resistor import Resistorclass VoltageResistor(Resistor): def __init__(self, ohms): super().__init__(ohms) self._voltage = 0 @property def voltage(self): return self._voltage @voltage.setter def voltage(self, voltage): self._voltage = voltage self.current = self._voltage / self.ohmsr2 = VoltageResistor(1e3)print(f'Before: &#123;r2.current&#125; amps')r2.voltage = 10print(f'After: &#123;r2.current&#125; amps')Before: 0 ampsAfter: 0.01 amps 此时设置 voltage 属性会执行名为 voltage 的 setter 方法，更新当前对象的 current 属性，使得最终的电流值与电压和电阻相匹配。 @property 的其他使用场景属性的 setter 方法里可以包含类型验证和数值验证的代码：1234567891011121314151617181920from resistor import Resistorclass BoundedResistor(Resistor): def __init__(self, ohms): super().__init__(ohms) @property def ohms(self): return self._ohms @ohms.setter def ohms(self, ohms): if ohms &lt;= 0: raise ValueError('ohms must be &gt; 0') self._ohms = ohmsr3 = BoundedResistor(1e3)r3.ohms = -5# =&gt; ValueError: ohms must be &gt; 0 甚至可以通过 @property 防止继承自父类的属性被修改：1234567891011121314151617181920from resistor import Resistorclass FixedResistance(Resistor): def __init__(self, ohms): super().__init__(ohms) @property def ohms(self): return self._ohms @ohms.setter def ohms(self, ohms): if hasattr(self, '_ohms'): raise AttributeError("Can't set attribute") self._ohms = ohmsr4 = FixedResistance(1e3)r4.ohms = 2e3# =&gt; AttributeError: Can't set attribute 要点 优先使用 public 属性定义类的接口，不手动实现 getter 或 setter 方法 在访问属性的同时需要表现某些特殊的行为（如类型检查、限定取值）等，使用 @property @property 的使用需遵循 rule of least surprise 原则，避免不必要的副作用 缓慢或复杂的工作，应放在普通方法中 二、需要复用的 @property 方法对于如下需求：编写一个 Homework 类，其成绩属性在被赋值时需要确保该值大于 0 且小于 100。借助 @property 方法实现起来非常简单：12345678910111213141516171819class Homework(object): def __init__(self): self._grade = 0 @property def grade(self): return self._grade @grade.setter def grade(self, value): if not (0 &lt;= value &lt;= 100): raise ValueError('Grade must be between 0 and 100') self._grade = valuegalileo = Homework()galileo.grade = 95print(galileo.grade)# =&gt; 95 假设上述验证逻辑需要用在包含多个科目的考试成绩上，每个科目都需要单独计分。则 @property 方法及验证代码就要重复编写多次，同时这种写法也不够通用。 采用 Python 的描述符可以更好地实现上述功能。在下面的代码中，Exam 类将几个 Grade 实例作为自己的类属性，Grade 类则通过 __get__ 和 __set__ 方法实现了描述符协议。1234567891011121314151617181920212223242526272829303132class Grade(object): def __init__(self): self._value = 0 def __get__(self, instance, instance_type): return self._value def __set__(self, instance, value): if not (0 &lt;= value &lt;= 100): raise ValueError('Grade must be between 0 and 100') self._value = valueclass Exam(object): math_grade = Grade() science_grade = Grade()first_exam = Exam()first_exam.math_grade = 82first_exam.science_grade = 99print('Math', first_exam.math_grade)print('Science', first_exam.science_grade)second_exam = Exam()second_exam.science_grade = 75print('Second exam science grade', second_exam.science_grade, ', right')print('First exam science grade', first_exam.science_grade, ', wrong')# =&gt; Math 82# =&gt; Science 99# =&gt; Second exam science grade 75 , right# =&gt; First exam science grade 75 , wrong 在对 exam 实例的属性进行赋值操作时：12exam = Exam()exam.math_grade = 40 Python 会将其转译为如下代码：1Exam.__dict__['math_grade'].__set__(exam, 40) 而获取属性值的代码：1print(exam.math_grade) 也会做如下转译：1print(Exam.__dict__['math_grade'].__get__(exam, Exam)) 但上述实现方法会导致不符合预期的行为。由于所有的 Exam 实例都会共享同一份 Grade 实例，在多个 Exam 实例上分别操作某一个属性就会出现错误结果。123456second_exam = Exam()second_exam.science_grade = 75print(&apos;Second exam science grade&apos;, second_exam.science_grade, &apos;, right&apos;)print(&apos;First exam science grade&apos;, first_exam.science_grade, &apos;, wrong&apos;)# =&gt; Second exam science grade 75 , right# =&gt; First exam science grade 75 , wrong 可以做出如下改动，将每个 Exam 实例所对应的值依次记录到 Grade 中，用字典结构保存每个实例的状态：1234567891011121314151617181920212223242526272829class Grade(object): def __init__(self): self._values = &#123;&#125; def __get__(self, instance, instance_type): if instance is None: return self return self._values.get(instance, 0) def __set__(self, instance, value): if not (0 &lt;= value &lt;= 100): raise ValueError('Grade must be between 0 and 100') self._values[instance] = valueclass Exam(object): math_grade = Grade() writing_grade = Grade() science_grade = Grade()first_exam = Exam()first_exam.math_grade = 82second_exam = Exam()second_exam.math_grade = 75print('First exam math grade', first_exam.math_grade, ', right')print('Second exam math grade', second_exam.math_grade, ', right')# =&gt; First exam math grade 82 , right# =&gt; Second exam math grade 75 , right 还有另外一个问题是，在程序的生命周期内，对于传给 __set__ 的每个 Exam 实例来说，_values 字典都会保存指向该实例的一份引用，导致该实例的引用计数无法降为 0 从而无法被 GC 回收。解决方法是将普通字典替换为 WeakKeyDictionary：12from weakref import WeakKeyDictionaryself._values = WeakKeyDictionary() 参考资料Effective Python]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Program</tag>
        <tag>OOP</tag>
        <tag>Class</tag>
        <tag>Advanced</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Windows 10 系统添加“从此处打开命令提示符”右键菜单]]></title>
    <url>%2F2020%2F01%2F23%2Fadd-open-cmd-in-right-click-menu%2F</url>
    <content type="text"><![CDATA[之前版本的 Windows 系统中，按住 shift 键的同时在空白处点击右键，可以找到从此处打开命令提示符的菜单项。可惜后面版本的 Win10 系统移除了该菜单项，替换成从此处打开 Powershell 窗口。可能习惯所致，对 Poweshell 不太感冒（启动很慢的感觉[狗头]）。在 cmd 中手动切换目录又过于麻烦，尤其是在跨驱动器的情况下。故尝试在右键菜单中添加“打开命令提示符”选项。 一、添加“从此处打开命令提示符”右键菜单 打开注册表编辑器（CTRL+R -&gt; regedit） 切换到 HKEY_CLASSES_ROOT\Directory\Background\shell\ 新建项 cmd_shell，其中字符串值(默认) 的数据改为“打开命令提示符”（菜单项的名称） 在 cmd_shell 中新建字符串值，名称为 Icon，数据为 C:\Windows\System32\cmd.exe（图标路径） 在 cmd_shell 下新建项 command，修改字符串值(默认) 的数据为 cmd.exe /s /k pushd &quot;%V&quot;（具体执行的命令） 如不想手动添加注册表，以下为可直接双击导入的 open_cmd.reg 文件：12345678Windows Registry Editor Version 5.00[HKEY_CLASSES_ROOT\Directory\Background\shell\cmd_shell]@=&quot;打开命令提示符&quot;&quot;Icon&quot;=&quot;C:\\Windows\\System32\\cmd.exe&quot;[HKEY_CLASSES_ROOT\Directory\Background\shell\cmd_shell\command]@=&quot;cmd.exe /s /k pushd \&quot;%V\&quot;&quot; 二、右键菜单添加“使用 VSCode 编辑文件” 个人习惯问题，感觉右键点击代码源文件，弹出的菜单里包含“使用 VSCode 编辑”会比较方便一点。方法同样是修改注册表。 打开注册表编辑器（CTRL+R -&gt; regedit） 切换到 HKEY_CLASSES_ROOT\*\shell 新建项 vscode，其中字符串值(默认) 的数据改为“Open with VSCode”（菜单项的名称） 在 vscode 中新建字符串值，名称为 Icon，数据类似 &quot;F:\Software\VSCode-win32-x64-1.36.1\Code.exe&quot;（图标路径） 在 vscode 下新建项 command，修改字符串值(默认) 的数据为 &quot;F:\Software\VSCode-win32-x64-1.36.1\Code.exe&quot; &quot;%1&quot;（具体执行的命令） 如不想手动添加注册表，以下为可直接双击导入的 open_with_code.reg 文件：12345678Windows Registry Editor Version 5.00[HKEY_CLASSES_ROOT\*\shell\VSCode]@=&quot;Open with Code&quot;&quot;Icon&quot;=&quot;F:\\Software\\VSCode-win32-x64-1.36.1\\Code.exe&quot;[HKEY_CLASSES_ROOT\*\shell\VSCode\command]@=&quot;\&quot;F:\\Software\\VSCode-win32-x64-1.36.1\\Code.exe\&quot; \&quot;%1\&quot;&quot;]]></content>
      <categories>
        <category>Tools</category>
      </categories>
      <tags>
        <tag>Tools</tag>
        <tag>Shell</tag>
        <tag>Windows</tag>
        <tag>Cmd</tag>
        <tag>VSCode</tag>
        <tag>Tricks</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基本数据结构的 Python 实现及应用]]></title>
    <url>%2F2020%2F01%2F23%2Fbasic-data-structure-and-algorithm-with-python%2F</url>
    <content type="text"><![CDATA[一、内置数据结构的性能ListPython 内置的 List 类型针对不同操作的性能如下： Operation Big-O index O(1) index 赋值 O(1) append O(1) pop() O(1) pop(i) O(n) insert(i, item) O(n) del O(n) iteration O(n) contains (in) O(n) slice [x:y] O(k) del slice O(n) set slice O(n+k) reverse O(n) concatenate O(k) sort O(n log n) multiply O(nk) 几种列表初始化方式的运行时间对比（ concatenate &gt; append &gt; comprehension &gt; list range）：123456789101112131415161718192021222324252627282930313233343536373839from timeit import Timer# concatenatedef test1(): l = [] for i in range(1000): l = l + [i]# appenddef test2(): l = [] for i in range(1000): l.append(i)# comprehensiondef test3(): l = [ i for i in range(1000)]# list rangedef test4(): l = list(range(1000))t1 = Timer("test1()", "from __main__ import test1")print(f"concat &#123;t1.timeit(number=1000)&#125; milliseconds")t2 = Timer("test2()", "from __main__ import test2")print(f"append &#123;t2.timeit(number=1000)&#125; milliseconds")t3 = Timer("test3()", "from __main__ import test3")print(f"comprehension &#123;t3.timeit(number=1000)&#125; milliseconds")t4 = Timer("test4()", "from __main__ import test4")print(f"list range &#123;t4.timeit(number=1000)&#125; milliseconds")# concat 1.3573799000000002 milliseconds# append 0.0650925 milliseconds# comprehension 0.03262469999999995 milliseconds# list range 0.01332690000000003 milliseconds 列表的 pop(0) 方法和 pop() 方法的运行时间对比（pop(0) 为 O(n)，pop() 为 O(1)）：123456789101112131415161718192021from timeit import Timerfrom matplotlib import pyplot as pltpop_zero = Timer("x.pop(0)", "from __main__ import x")pop_end = Timer("x.pop()", "from __main__ import x")X, Ypt, Ypz = [], [], []for i in range(10000, 500001, 10000): x = list(range(i)) pt = pop_end.timeit(number=1000) pz = pop_zero.timeit(number=1000) X.append(i) Ypt.append(pt) Ypz.append(pz)plt.scatter(X, Ypt, c='m', marker='^', label='pop_end')plt.scatter(X, Ypz, c='y', label='pop_zero')plt.legend()plt.show() Dictionary Operation Big-O copy O(n) get item O(1) set item O(1) delete item O(1) contains (in) O(1) iteration O(n) 列表与字典 contains 操作的运行时间对比（列表为 O(n)，字典为 O(1)）：123456789101112131415161718192021222324import timeitimport randomfrom matplotlib import pyplot as pltX, Ylist, Ydict = [], [], []for i in range(10000, 1000001, 20000): t = timeit.Timer(f"random.randrange(&#123;i&#125;) in x", "from __main__ import random, x") x = list(range(i)) list_time = t.timeit(number=1000) x = &#123;j: None for j in range(i)&#125; dict_time = t.timeit(number=1000) X.append(i) Ylist.append(list_time) Ydict.append(dict_time)plt.scatter(X, Ylist, c='m', marker='^', label='list')plt.scatter(X, Ydict, c='y', label='dict')plt.legend()plt.show() 二、StackLIFO（last-in first-out），从顶部添加新项目，移除项目也从顶部开始。即越是最新添加到栈中的项目越是接近栈的顶部位置，同时也最先出栈。类似于在厨房里刷盘子，洗好的盘子摞在顶部，使用时也最先拿顶部（最近洗好）的干净盘子。12345678910111213141516171819202122232425262728293031323334# stack.pyclass Stack: def __init__(self): self.items = [] def is_empty(self): return self.items == [] def push(self, item): self.items.append(item) def pop(self): return self.items.pop() def peek(self): return self.items[len(self.items) - 1] def size(self): return len(self.items)if __name__ == '__main__': s = Stack() print(s.is_empty()) # True s.push(4) s.push('dog') print(s.peek()) # dog s.push(True) print(s.size()) # 3 s.push(8.4) print(s.pop()) # 8.4 print(s.pop()) # True print(s.size()) # 2 进制转换通过短除法将十进制数转换为其他进制：1234567891011121314151617181920from stack import Stackdef base_converter(dec_number, base): digits = "0123456789ABCDEF" rem_stack = Stack() while dec_number &gt; 0: rem = dec_number % base rem_stack.push(rem) dec_number = dec_number // base new_string = "" while not rem_stack.is_empty(): new_string = new_string + digits[rem_stack.pop()] return new_stringprint(base_converter(28, 2)) # 11100print(base_converter(28, 16)) # 1C 短除法每次循环生成的余数从后往前拼接得到最终的目标进制数字。即最先生成的余数为目标进制数字的最后一位，最后生成的余数为目标进制数字的第一位，后进先出。 括号匹配12345678910111213141516171819202122232425262728293031from stack import Stackdef par_checker(symbol_string): s = Stack() balanced = True index = 0 while index &lt; len(symbol_string) and balanced: symbol = symbol_string[index] if symbol in "([&#123;": s.push(symbol) else: if s.is_empty(): balanced = False else: top = s.pop() if not matches(top, symbol): balanced = False index += 1 if balanced and s.is_empty(): return True else: return Falsedef matches(open, close): opens = "([&#123;" closes = ")]&#125;" return opens.index(open) == closes.index(close)print(par_checker('&#123;&#123;([][])&#125;()&#125;')) # Trueprint(par_checker('[&#123;()]')) # False 遇到左半边括号（([{）就 push 到 Stack 中，遇到右半边括号（)]}）就从 Stack 中 pop 出一个左半边括号进行配对。最先遇到的右半边括号肯定需要与最后放入 Stack 中的左半边括号匹配（因此使用 Stack 的 pop 取数据）。最终 Stack 为空，则括号全部匹配。 三、Queues &amp; DequesFIFO（first-in first-out），从队列一端（rear）添加新项目，从另一端（front）弹出项目。类似于生活中的排队点餐，新来的顾客排到队伍最后面，队伍最前面的人优先点餐和付款然后离开队伍。deque 为双向队列。123456789101112131415161718192021222324252627282930313233343536373839# queues.pyclass Queue: def __init__(self): self.items = [] def is_empty(self): return self.items == [] def enqueue(self, item): self.items.insert(0, item) def dequeue(self): return self.items.pop() def size(self): return len(self.items)class Deque: def __init__(self): self.items = [] def is_empty(self): return self.items == [] def add_front(self, item): self.items.append(item) def add_rear(self, item): self.items.insert(0, item) def remove_front(self): return self.items.pop() def remove_rear(self): return self.items.pop(0) def size(self): return len(self.items) Palindrome CheckerPalindrome 是指左右对称的单词，如 radar、toot、madam 等。双向队列 Deque 从队列两端弹出数据，可用于检测目标单词首尾两端的字符是否一一对应，即是否对称。123456789101112131415161718192021from queues import Dequedef pal_checker(a_string): char_deque = Deque() for ch in a_string: char_deque.add_rear(ch) still_equal = True while char_deque.size() &gt; 1 and still_equal: first = char_deque.remove_front() last = char_deque.remove_rear() if first != last: still_equal = False return still_equalprint(pal_checker("lsdkjfskf")) #Falseprint(pal_checker("radar")) # True 四、SortBubble Sort12345678910def bubble_sort(a_list): for pass_num in range(len(a_list) - 1, 0, -1): for i in range(pass_num): if a_list[i] &gt; a_list[i + 1]: a_list[i], a_list[i + 1] = a_list[i + 1], a_list[i]a_list = [54, 26, 93, 17, 77, 31, 44, 55, 20]bubble_sort(a_list)print(a_list)# =&gt; [17, 20, 26, 31, 44, 54, 55, 77, 93] Bubble Sort 会依次比较序列中相邻两个数字的大小关系，必要时交换位置，保证较大的数字排在另一个数字右侧。整个序列执行完一轮完整的两两对比之后，可以保证最大的数字位于序列的最右侧。下一轮则可以将第二大的数字交换到序列中倒数第二的位置，依次类推。 Bubble Sort 通常认为是最低效的排序算法。复杂度为 O(n^2)。 Selection Sort123456789101112def selection_sort(a_list): for fill_slot in range(len(a_list) - 1, 0, -1): pos_of_max = 0 for location in range(1, fill_slot + 1): if a_list[location] &gt; a_list[pos_of_max]: pos_of_max = location a_list[pos_of_max], a_list[location] = a_list[location], a_list[pos_of_max]a_list = [54, 26, 93, 17, 77, 31, 44, 55, 20]selection_sort(a_list)print(a_list)# =&gt; [17, 20, 26, 31, 44, 54, 55, 77, 93] Selection Sort 会在第一轮对所有数字的比较中，记录最大数字的位置，令其与序列中最后位置的数字互换。然后在第二轮比较中找出第二大的数字，与序列中倒数第二位置上的数字互换，依次类推。复杂度为 O(n^2)。相对于 Bubble Sort，Selection Sort 减少了数字位置的交换次数，一轮只需要一次交换。 Insertion Sort123456789101112131415def insertion_sort(a_list): for index in range(1, len(a_list)): current_value = a_list[index] position = index while position &gt; 0 and a_list[position - 1] &gt; current_value: a_list[position] = a_list[position - 1] position = position - 1 a_list[position] = current_valuea_list = [54, 26, 93, 17, 77, 31, 44, 55, 20]insertion_sort(a_list)print(a_list)# =&gt; [17, 20, 26, 31, 44, 54, 55, 77, 93] Insertion Sort 可以理解为在当前序列头部维护着一个长度不断增长的排序好的子序列，每从序列后半段取出一个数字，就根据其与头部序列中数字的大小关系，插入到头部序列的适当位置。复杂度依旧为 O(n^2)。但通常情况下，shift 操作的性能一般要优于 exchange 操作。 Shell SortShell Sort 会从原来的待排序列表中以一定的间隔选取数字组成一系列小的子列表，再依次通过 Insertion Sort 方法进行排序。复杂度大概介于 O(n) 和 O(n^2) 之间。1234567891011121314151617181920212223242526def shell_sort(a_list): sublist_count = len(a_list) // 2 while sublist_count &gt; 0: for start_position in range(sublist_count): gap_insertion_sort(a_list, start_position, sublist_count) print("After increments of size", sublist_count, "The list is", a_list) sublist_count = sublist_count // 2def gap_insertion_sort(a_list, start, gap): for i in range(start + gap, len(a_list), gap): current_value = a_list[i] position = i while position &gt;= gap and a_list[position - gap] &gt; current_value: a_list[position] = a_list[position - gap] position = position - gap a_list[position] = current_valuea_list = [54, 26, 93, 17, 77, 31, 44, 55, 20]shell_sort(a_list)print(a_list)# =&gt; After increments of size 4 The list is [20, 26, 44, 17, 54, 31, 93, 55, 77]# =&gt; After increments of size 2 The list is [20, 17, 44, 26, 54, 31, 77, 55, 93]# =&gt; After increments of size 1 The list is [17, 20, 26, 31, 44, 54, 55, 77, 93]# =&gt; [17, 20, 26, 31, 44, 54, 55, 77, 93] Merge SortMerge Sort 是将原待排序列表递归地一分为二直到成为最小单位，然后在两两合并的过程中进行大小排序。合并操作完成后得到完整的排序好的列表。 Merge Sort 和后面的 Quick Sort 复杂度都为 O(n log n)。Merge Sort 在融合过程中需要使用额外的存储空间，而 Quick Sort 的复杂度有可能提高到 O(n^2) （当分割点不是接近列表的中间位置）。12345678910111213141516171819202122232425262728293031323334353637def merge_sort(a_list): print("Splitting ", a_list) if len(a_list) &gt; 1: mid = len(a_list) // 2 left_half = a_list[:mid] right_half = a_list[mid:] merge_sort(left_half) merge_sort(right_half) i = 0 j = 0 k = 0 while i &lt; len(left_half) and j &lt; len(right_half): if left_half[i] &lt; right_half[j]: a_list[k] = left_half[i] i = i + 1 else: a_list[k] = right_half[j] j = j + 1 k = k + 1 while i &lt; len(left_half): a_list[k] = left_half[i] i = i + 1 k = k + 1 while j &lt; len(right_half): a_list[k] = right_half[j] j = j + 1 k = k + 1 print("Merging ", a_list)a_list = [54, 26, 93, 17, 77, 31, 44, 55, 20]merge_sort(a_list)print(a_list) Quick Sort123456789101112131415161718192021222324252627282930313233343536373839404142def quick_sort(a_list): quick_sort_helper(a_list, 0, len(a_list) - 1)def quick_sort_helper(a_list, first, last): if first &lt; last: split_point = partition(a_list, first, last) quick_sort_helper(a_list, first, split_point - 1) quick_sort_helper(a_list, split_point + 1, last)def partition(a_list, first, last): pivot_value = a_list[first] left_mark = first + 1 right_mark = last done = False while not done: while left_mark &lt;= right_mark and \ a_list[left_mark] &lt;= pivot_value: left_mark = left_mark + 1 while a_list[right_mark] &gt;= pivot_value and \ right_mark &gt;= left_mark: right_mark = right_mark - 1 if right_mark &lt; left_mark: done = True else: temp = a_list[left_mark] a_list[left_mark] = a_list[right_mark] a_list[right_mark] = temp temp = a_list[first] a_list[first] = a_list[right_mark] a_list[right_mark] = temp return right_marka_list = [54, 26, 93, 17, 77, 31, 44, 55, 20]quick_sort(a_list)print(a_list) 参考资料Problem Solving with Algorithms and Data Structures using Python]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Program</tag>
        <tag>Advanced</tag>
        <tag>DataStructure</tag>
        <tag>Algorithm</tag>
        <tag>Sort</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Haskell 基本语法（一）列表与类型系统]]></title>
    <url>%2F2020%2F01%2F20%2Fbasic-haskell-lists-and-typeclass%2F</url>
    <content type="text"><![CDATA[算术与逻辑运算算术运算：12345678910111213Prelude&gt; 2 + 1517Prelude&gt; 5 / 22.5Prelude&gt; 50 * (100 - 4999)-244950Prelude&gt; 5 * -3&lt;interactive&gt;:4:1: error: Precedence parsing error cannot mix ‘*’ [infixl 7] and prefix `-' [infixl 6] in the same infix expressionPrelude&gt; 5 * (-3)-15 逻辑运算：123456Prelude&gt; True &amp;&amp; FalseFalsePrelude&gt; False || TrueTruePrelude&gt; not (True &amp;&amp; False)True 判断是否相等：12345678Prelude&gt; 5 == 5TruePrelude&gt; 5 == 4FalsePrelude&gt; 5 /= 4TruePrelude&gt; "hello" == "hello"True 函数调用在 Haskell 中，+ - * / 等操作符实际上也是函数，只不过调用时函数名位于两个参数之间，叫做 infix 函数。其他常见的函数为 prefix 函数，通过函数名+空格+参数的格式（fun a b ...）调用。123456Prelude&gt; succ 89Prelude&gt; min 9 109Prelude&gt; succ 9 + max 5 4 + 116 接收两个参数的函数也可以在调用时将函数名放在参数中间，如：1234Prelude&gt; div 10 25Prelude&gt; 10 `div` 25 Haskell 中传递给函数的参数不需要像 C 语言中那样放置在 () 中，因此 bar (bar 3) 实际上等同于 C 中的 bar(bar(3))。 函数定义12345Prelude&gt; doubleMe x = x + xPrelude&gt; doubleMe 918Prelude&gt; doubleMe 8.316.6 12345Prelude&gt; doubleSmallNumber x = if x &gt; 100 then x else x * 2Prelude&gt; doubleSmallNumber 123123Prelude&gt; doubleSmallNumber 80160 Haskell 中的 if 语句是一种表达式。表达式是指某一段有返回值的代码片段。比如 5 是表达式，返回数字 5；x + y 也是表达式，返回 x 与 y 的和。因此 Haskell if 语句中的 else 是必需的（保证一定有返回值）。 listHaskell 中的列表只能存放同一类型的数据项。123Prelude&gt; let a = [1,2,3,4]Prelude&gt; a[1,2,3,4] Haskell 中的字符串实际上是数据项类型为 Char 的列表，&quot;hello&quot; 仅仅是 [&#39;h&#39;,&#39;e&#39;,&#39;l&#39;,&#39;l&#39;,&#39;o&#39;] 的一种语法糖。123456Prelude&gt; ['h','e','l','l','o']"hello"Prelude&gt; ['h','e','l','l','o'] == "hello"TruePrelude&gt; :t "hello""hello" :: [Char] 列表通过 ++ 符号执行连接操作。1234Prelude&gt; [1,2,3,4] ++ [5][1,2,3,4,5]Prelude&gt; "hello" ++ " " ++ "world""hello world" PS：使用 ++ 操作符连接两个列表时，即便右边的列表只包含一个数据项，也需要用 [] 括起来。不管右边添加的列表有多少数据项，左边的列表都会在合并时遍历自身的所有项。 可以使用 : 操作符在列表左侧添加一个数据项。1234Prelude&gt; 'A' : " Small Cat""A Small Cat"Prelude&gt; 5 : [1,2,3,4,5][5,1,2,3,4,5] PS：[1,2,3] 实际上是 1:2:3:[] 的语法糖。12Prelude&gt; 1:2:3:[][1,2,3] 使用 !! 操作符根据索引获取列表中的某个数据项。1234Prelude&gt; [1,2,3,4] !! 01Prelude&gt; "hello" !! 1'e' elem 可以判断某个数据项与列表的包含关系。1234Prelude&gt; elem 4 [3,4,5,6]TruePrelude&gt; elem 100 [3,4,5,6]False 比较列表的大小时，会从列表左侧开始逐个数据项进行比对。123456Prelude&gt; [3,2,1] &gt; [3,1,0]TruePrelude&gt; [3,2,1] &gt; [2,10,100]TruePrelude&gt; [3,4,2] &gt; [3,4]True 常见的作用于列表的函数： head：获取列表的首个元素 tail：获取列表的尾部（除首个元素以外的）元素 last：获取列表的最后一个元素 init：获取列表的前几个（除最后一个元素以外）元素 length：返回列表长度 null：判断列表是否为空 reverse：逆序输出源列表 minimum：获取列表中的最小值 maximum：获取列表中的最大值 sum：获取列表中所有元素的加和 product：获取列表中所有元素的乘积 1234567891011121314Prelude&gt; head [5,4,3,2,1]5Prelude&gt; tail [5,4,3,2,1][4,3,2,1]Prelude&gt; last [5,4,3,2,1]1Prelude&gt; init [5,4,3,2,1][5,4,3,2]Prelude&gt; length [5,4,3,2,1]5Prelude&gt; null [5,4,3,2,1]FalsePrelude&gt; reverse [5,4,3,2,1][1,2,3,4,5] 其他如对列表的 subset 操作，take 函数可以获取列表中的前几个数据项（即生成子列表），drop 可以获取列表中除前几项以外的其他数据项。12345678910Prelude&gt; take 3 [5,4,3,2,1][5,4,3]Prelude&gt; drop 3 [5,4,3,2,1][2,1]Prelude&gt; take 0 [5,4,3,2,1][]Prelude&gt; take 10 [5,4,3,2,1][5,4,3,2,1]Prelude&gt; drop 100 [5,4,3,2,1][] range1234Prelude&gt; [1..10][1,2,3,4,5,6,7,8,9,10]Prelude&gt; ['a' .. 'z']"abcdefghijklmnopqrstuvwxyz" 包含步长的 range，比如获取 2 到 20 之间的所有偶数，和获取 3 到 20 之间所有3 的倍数：1234Prelude&gt; [2,4..20][2,4,6,8,10,12,14,16,18,20]Prelude&gt; [3,6..20][3,6,9,12,15,18] 从语法上看，[] 中需包含前两项以及最后一项（的范围）。因此获取 20 到 1 的数字列表则可以使用 [20,19..1]。12Prelude&gt; [20,19..1][20,19,18,17,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1] 此外，获取从 13 开始共 24 个 13 的倍数，可以使用 [13,26..24*13]，也可以使用 take 24 [13,26..]。没有提供最后一项的范围（如 [13,26..]）时，range 方式会生成无穷列表。Haskell 的计算是 lazy 的，因此不用担心无穷列表会吃掉所有内存。 生成无穷列表还可以使用 cycle 或者 repeat：1234Prelude&gt; take 10 (cycle [1,2,3])[1,2,3,1,2,3,1,2,3,1]Prelude&gt; take 10 (repeat 5)[5,5,5,5,5,5,5,5,5,5] 列表推导Haskell 中的列表推导，写法上很像单纯的数学公式 $S = {2 * x | x \in \mathbb{N}, x &lt;= 10}$12Prelude&gt; [x*2 | x &lt;- [1..10]][2,4,6,8,10,12,14,16,18,20] 等同于 Python 中的如下表达式：12&gt;&gt;&gt; [x * 2 for x in range(1, 11)][2, 4, 6, 8, 10, 12, 14, 16, 18, 20] 更复杂的情况如：12Prelude&gt; [x*2 | x &lt;- [1..10], x*2 &gt;= 12][12,14,16,18,20] 甚至可以有如下用法：1234Prelude&gt; [x*y | x &lt;- [2,5,10], y &lt;- [8,10,11]][16,20,22,40,50,55,80,100,110]Prelude&gt; [x*y | x &lt;- [2,5,10], y &lt;- [8,10,11], x*y &gt; 50][55,80,100,110] 借助列表推导可以定义自己的 length 函数：123Prelude&gt; length xs = sum [1 | _ &lt;- xs]Prelude&gt; length [1,2,3,4]4 定义函数去除某个列表中所有的非大写字符：123Prelude&gt; removeNonUppercase st = [c | c &lt;- st, elem c ['A' .. 'Z']]Prelude&gt; removeNonUppercase "Hahaha! Ahahaha!""HA" TupleHaskell 中的元组相对于列表主要有以下特性： 元组的类型由所含元素的长度和每个元素的类型确定 元组中可以包含不同类型的元素 如 (&quot;Christopher&quot;, &quot;Walken&quot;, 55) 这样的元组是合法的，即单个元组中可以包含字符串（列表）、数字等不同类型；[(1,2),(8,11,5),(4,5)] 和 [(1,2),(&quot;One&quot;,2)] 这样的列表则是不合法的，因为不同长度或者元素类型不同的元组，其类型也是不同的，不能作为同一个列表中的元素。123456Prelude&gt; :t (1,2)(1,2) :: (Num t, Num t1) =&gt; (t1, t)Prelude&gt; :t (8,11,5)(8,11,5) :: (Num t, Num t1, Num t2) =&gt; (t2, t1, t)Prelude&gt; :t ("one",2)("one",2) :: Num t =&gt; ([Char], t) fst 可以返回元组的第一个元素，snd 返回元组的第二个元素。这两个函数只作用于长度为 2 的元组。1234Prelude&gt; fst (8,11)8Prelude&gt; snd ("Wow", False)False zip 可以将两个列表中的每一个元素一一组合成长度为二的元组，最终形成新的以元组为元素的列表。123456Prelude&gt; zip [1,2,3,4,5] [5,5,5,5,5][(1,5),(2,5),(3,5),(4,5),(5,5)]Prelude&gt; zip [1..5] ["one", "two", "three", "four", "five"][(1,"one"),(2,"two"),(3,"three"),(4,"four"),(5,"five")]Prelude&gt; zip [1..] ["apple", "orange", "cherry", "mango"][(1,"apple"),(2,"orange"),(3,"cherry"),(4,"mango")] 类型系统Haskell 是静态类型的语言，每一个表达式在编译时其类型便已知。不同于 Java 等语言，Haskell 支持类型推断。它可以自行推断出某个数字属于 Int 类型。123456789101112Prelude&gt; :t 'a''a' :: CharPrelude&gt; :t TrueTrue :: BoolPrelude&gt; :t "HELLO!""HELLO!" :: [Char]Prelude&gt; :t (True, 'a')(True, 'a') :: (Bool, Char)Prelude&gt; :t ('a','b','c')('a','b','c') :: (Char, Char, Char)Prelude&gt; :t 4 == 54 == 5 :: Bool :: 读作 has type of 。元组的类型取决于其中每一个元素的类型以及元组长度，因此表达式 (&#39;a&#39;,&#39;b&#39;,&#39;c&#39;) 的类型为 (Char, Char, Char)。表达式 4 == 5 总是返回 False，因此其类型为 Bool。 Haskell 中的函数同样有类型。123Prelude&gt; removeNonUppercase st = [ c | c &lt;- st, c `elem` ['A'..'Z']]Prelude&gt; :t removeNonUppercaseremoveNonUppercase :: [Char] -&gt; [Char] removeNoneUppercase 函数的类型为 [Char] -&gt; [Char]，说明该函数的参数类型为字符串，返回值类型为字符串。即函数的类型通过由 -&gt; 符号分隔的参数与返回值的类型表示。 类型变量函数的类型由参数和返回值表示，但是有些函数的参数与返回值的类型并不会固定为某一种。比如 head 函数可以获取列表中的第一个元素，而列表中元素的类型可能由很多种。123456Prelude&gt; head [1,2,3,4]1Prelude&gt; head "hello"'h'Prelude&gt; :t headhead :: [a] -&gt; a head :: [a] -&gt; a 中的 a 即为类型变量，表示该参数或返回值可以是任意类型。包含类型变量的函数叫做多态函数。除了 a 以外，其他如 b、c、d 等也可作为类型变量使用。像前面的 [a] -&gt; a， a 可以表示任意类型，但两个 a 必定是同一类型。1234Prelude&gt; fst ("hello", True)"hello"Prelude&gt; :t fstfst :: (a, b) -&gt; a TypeclassTypeclass 是一种定义了某些行为的接口。如果某个类型属于特定的 typeclass，则意味着该类型实现了由 typeclass 描述的行为，类似于 Java 中的 interface。12Prelude&gt; :t (==)(==) :: Eq a =&gt; a -&gt; a -&gt; Bool 其中 =&gt; 符号前面的部分叫做类约束，可以这样理解：== 函数接收任意两个相同类型（a）的数值，根据其是否相等返回 Bool 值。两个输入参数的类型必须是 Eq 类的成员（因此叫类约束）。 Eq typeclass 为其成员类型提供了测试相等性的接口，任何可以用来比较是否相等的类型都应该是 Eq 类的成员。所有 Haskell 基本类型（除 IO 外）和函数都是 Eq typeclass 的一部分。 以下是一些基本的 typeclass：Eq 用于类型之间的相等性测试，实现的函数有 == 和 /= 。12345678910Prelude&gt; 5 == 5TruePrelude&gt; 5 /= 5FalsePrelude&gt; 'a' == 'a'TruePrelude&gt; "Ho Ho" == "Ho Ho"TruePrelude&gt; 3.432 == 3.432True Ord 用于拥有顺序的类型，包含所有基本的比较函数如 &gt;、&lt; 和 &gt;= 等。123456Prelude&gt; :t (&gt;)(&gt;) :: Ord a =&gt; a -&gt; a -&gt; BoolPrelude&gt; "Abrakadabra" &lt; "Zebra"TruePrelude&gt; 5 &gt;= 2True Show 的成员可以表示为字符串，最常用的用于处理 Show 成员的函数是 show，可以将某个类型的值转换为字符串表示：123456Prelude&gt; show 3"3"Prelude&gt; show 5.334"5.334"Prelude&gt; show True"True" Read 是和 Show 相反的一类 typeclass。read 函数可以接收字符串并返回属于 Read 的某个类型（自行推断或显示指定）：12345678910Prelude&gt; read "True" || FalseTruePrelude&gt; read "8.2" + 3.812.0Prelude&gt; read "[1,2,3,4]" ++ [3][1,2,3,4,3]Prelude&gt; read "[1,2,3,4]"*** Exception: Prelude.read: no parsePrelude&gt; read "[1,2,3,4]" :: [Int][1,2,3,4] Enum 的成员是有顺序的序列类型，可以被枚举。Enum 中的成员都可以使用 range 的方式生成列表，也都可以被自增或自减函数调用。12345678Prelude&gt; ['a'..'e']"abcde"Prelude&gt; [3..5][3,4,5]Prelude&gt; succ 'B''C'Prelude&gt; pred 'C''B' Num 是一个数字类型的 typeclass。它的成员都具有数字类型的属性。12345678Prelude&gt; :t 2020 :: Num t =&gt; tPrelude&gt; 20 :: Float20.0Prelude&gt; 20 :: Double20.0Prelude&gt; :t (*)(*) :: Num a =&gt; a -&gt; a -&gt; a 函数 * 的类型为 Num a =&gt; a -&gt; a -&gt; a，因此其参数必须是 Num typelcass 的成员，且必须是同一类型。123456789Prelude&gt; 5 * (6 :: Float)30.0Prelude&gt; (5 :: Int) * (6 :: Float)&lt;interactive&gt;:56:15: error: • Couldn't match expected type ‘Int’ with actual type ‘Float’ • In the second argument of ‘(*)’, namely ‘(6 :: Float)’ In the expression: (5 :: Int) * (6 :: Float) In an equation for ‘it’: it = (5 :: Int) * (6 :: Float) 参考资料Learn You a Haskell for Great Good!]]></content>
      <categories>
        <category>Program</category>
      </categories>
      <tags>
        <tag>Program</tag>
        <tag>Advanced</tag>
        <tag>Haskell</tag>
        <tag>Functional</tag>
        <tag>Type</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Vue.js 学习笔记（一）数据绑定与指示器]]></title>
    <url>%2F2020%2F01%2F18%2Fdata-binding-and-directives-in-vue-js%2F</url>
    <content type="text"><![CDATA[一、安装与项目初始化安装 @vue/cli：$ npm install -g @vue/cli 安装 git：sudo apt-get install git 创建项目：$ vue create todo --default 项目结构12345678910111213141516$ tree todo -I node_modulestodo├── babel.config.js├── package.json├── package-lock.json├── public│ ├── favicon.ico│ └── index.html├── README.md└── src ├── App.vue ├── assets │ └── logo.png ├── components │ └── HelloWorld.vue └── main.js 文件 功能 public/index.html 浏览器加载的 HTML 文件。其中包含用于显示 Vue 应用的标签和加载应用文件的 &lt;script&gt; src/main.js 负责 Vue 应用的基本配置，通常还用于注册应用依赖的第三方组件 src/App.vue Vue 组件，即 Vue 应用的主要构成部分。如需要显示给用户的 HTML 页面、Javascript 代码和 CSS 等 public/index.html：12345678910&lt;!DOCTYPE html&gt;&lt;html lang="en"&gt; &lt;head&gt; &lt;title&gt;todo&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;div id="app"&gt;&lt;/div&gt; &lt;!-- built files will be auto injected --&gt; &lt;/body&gt;&lt;/html&gt; src/main.js：12345678import Vue from 'vue'import App from './App.vue'Vue.config.productionTip = falsenew Vue(&#123; render: h =&gt; h(App),&#125;).$mount('#app') App.vue：12345678910111213141516171819202122232425262728&lt;template&gt; &lt;div id=&quot;app&quot;&gt; &lt;img alt=&quot;Vue logo&quot; src=&quot;./assets/logo.png&quot;&gt; &lt;HelloWorld msg=&quot;Welcome to Your Vue.js App&quot;/&gt; &lt;/div&gt;&lt;/template&gt;&lt;script&gt;import HelloWorld from &apos;./components/HelloWorld.vue&apos;export default &#123; name: &apos;app&apos;, components: &#123; HelloWorld &#125;&#125;&lt;/script&gt;&lt;style&gt;#app &#123; font-family: &apos;Avenir&apos;, Helvetica, Arial, sans-serif; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: grayscale; text-align: center; color: #2c3e50; margin-top: 60px;&#125;&lt;/style&gt; 开启测试服务器：$ npm run serve 二、数据绑定与指示器准备工作，添加 CSS 库：$ npm install bootstrap@4.0.0 修改 src/main.js 导入 Bootstrap：1234567891011// src/main.jsimport Vue from 'vue'import App from './App.vue'import "bootstrap/dist/css/bootstrap.min.css";Vue.config.productionTip = falsenew Vue(&#123; render: h =&gt; h(App),&#125;).$mount('#app') 1. 数据绑定展示数据给用户是 web 组件最重要的工作之一，数据绑定可以将 &lt;script&gt; 下定义的数据对象关联给 &lt;template&gt; 中的特定元素进行显示。 显示数据修改 src/App.vue 如下：1234567891011121314151617// src/App.vue&lt;template&gt; &lt;div id=&quot;app&quot; class=&quot;bg-primary text-white text-center m-2 p-3&quot;&gt; &lt;h3&gt;Product: &#123;&#123; name &#125;&#125;&lt;/h3&gt; &lt;/div&gt;&lt;/template&gt;&lt;script&gt;export default &#123; name: &quot;app&quot;, data: function () &#123; return &#123; name: &quot;Kayak&quot; &#125; &#125;&#125;&lt;/script&gt; 通过 JavaScript 模块中的 data 属性定义了可以被绑定的数据值：12345data: function () &#123; return &#123; name: "Kayak" &#125;&#125; 再借助数据绑定机制，使用 等语法将 &lt;script&gt; 中定义的数据关联到 template 模块的 HTML 元素中。这种形式的绑定也称为文本注入（text interpolation binding）。 绑定中使用表达式实际上数据绑定不仅仅可以通过预先定义的变量替换 HTML 元素中的文本，还可以直接嵌入复杂的表达式，具体代码如下：1234567891011121314151617181920// src/App.vue&lt;template&gt; &lt;div id=&quot;app&quot; class=&quot;bg-primary text-white text-center m-2 p-3&quot;&gt; &lt;h3&gt;Product: &#123;&#123; name &#125;&#125;&lt;/h3&gt; &lt;h3&gt;Price: $&#123;&#123; (price + (price * (taxRate / 100))).toFixed(2) &#125;&#125;&lt;/h3&gt; &lt;/div&gt;&lt;/template&gt;&lt;script&gt;export default &#123; name: &quot;app&quot;, data: function () &#123; return &#123; name: &quot;Kayak&quot;, price: 275, taxRate: 12 &#125; &#125;&#125;&lt;/script&gt; Computed Properties在数据绑定中使用过于复杂的表达式并不利于代码的阅读、维护和复用。为了使 template 组件可以足够简单，Vue 提供了计算属性模块，可以从 data 属性提供的数据中生成需要的值，从而减少数据绑定中复杂表达式的使用。12345678910111213141516171819202122232425// src/App.vue&lt;template&gt; &lt;div id=&quot;app&quot; class=&quot;bg-primary text-white text-center m-2 p-3&quot;&gt; &lt;h3&gt;Product: &#123;&#123; name &#125;&#125;&lt;/h3&gt; &lt;h3&gt;Price: $&#123;&#123; totalPrice.toFixed(2) &#125;&#125;&lt;/h3&gt; &lt;/div&gt;&lt;/template&gt;&lt;script&gt;export default &#123; name: &quot;app&quot;, data: function () &#123; return &#123; name: &quot;Kayak&quot;, price: 275, taxRate: 12 &#125; &#125;, computed: &#123; totalPrice: function() &#123; return this.price + (this.price * (this.taxRate / 100)); &#125; &#125;&#125;&lt;/script&gt; 关键代码：1234567&lt;h3&gt;Price: $&#123;&#123; totalPrice.toFixed(2) &#125;&#125;&lt;/h3&gt;...computed: &#123; totalPrice: function() &#123; return this.price + (this.price * (this.taxRate / 100)); &#125;&#125; Methods方法相对于计算属性则更加灵活，可以定义自己的参数。1234567891011121314151617181920212223242526272829303132333435// src/App.vue&lt;template&gt; &lt;div id=&quot;app&quot; class=&quot;bg-primary text-white text-center m-2 p-3&quot;&gt; &lt;h3&gt;Product: &#123;&#123; name &#125;&#125;&lt;/h3&gt; &lt;h4&gt;Price: $&#123;&#123; lowTotalPrice.toFixed(2) &#125;&#125; (Low Rate)&lt;/h4&gt; &lt;h4&gt;Price: $&#123;&#123; highTotalPrice.toFixed(2) &#125;&#125; (High Rate)&lt;/h4&gt; &lt;/div&gt;&lt;/template&gt;&lt;script&gt;export default &#123; name: &quot;app&quot;, data: function () &#123; return &#123; name: &quot;Kayak&quot;, price: 275, lowTaxRate: 12, highTaxRate: 20 &#125; &#125;, computed: &#123; lowTotalPrice: function () &#123; return this.getTotalPrice(this.lowTaxRate); &#125;, highTotalPrice: function () &#123; return this.getTotalPrice(this.highTaxRate); &#125;&#125;, methods: &#123; getTotalPrice(taxRate) &#123; return this.price + (this.price * (taxRate / 100)); &#125; &#125;&#125;&lt;/script&gt; 也可以省略掉上面代码中对计算属性的定义，直接在数据绑定时调用方法。这显示了各组件之间组织的灵活性，当然此举也会增加 template 中代码的复杂度：12345678910111213141516171819202122232425262728// src/App.vue&lt;template&gt; &lt;div id=&quot;app&quot; class=&quot;bg-primary text-white text-center m-2 p-3&quot;&gt; &lt;h3&gt;Product: &#123;&#123; name &#125;&#125;&lt;/h3&gt; &lt;h4&gt;Price: $&#123;&#123; getTotalPrice(lowTaxRate).toFixed(2) &#125;&#125; (Low Rate)&lt;/h4&gt; &lt;h4&gt;Price: $&#123;&#123; getTotalPrice(highTaxRate).toFixed(2) &#125;&#125; (High Rate)&lt;/h4&gt; &lt;/div&gt;&lt;/template&gt;&lt;script&gt;export default &#123; name: &quot;app&quot;, data: function () &#123; return &#123; name: &quot;Kayak&quot;, price: 275, lowTaxRate: 12, highTaxRate: 20 &#125; &#125;, methods: &#123; getTotalPrice(taxRate) &#123; return this.price + (this.price * (taxRate / 100)); &#125; &#125;&#125;&lt;/script&gt; Filters过滤器是一种在 filter 模块下定义的函数，可以对需要显示的表达式进行格式化操作：12345678910111213141516171819202122232425262728293031323334// src/App.vue&lt;template&gt; &lt;div id=&quot;app&quot; class=&quot;bg-primary text-white text-center m-2 p-3&quot;&gt; &lt;h3&gt;Product: &#123;&#123; name &#125;&#125;&lt;/h3&gt; &lt;h4&gt;Price: &#123;&#123; getTotalPrice(lowTaxRate) | currency &#125;&#125; (Low Rate)&lt;/h4&gt; &lt;h4&gt;Price: &#123;&#123; getTotalPrice(highTaxRate) | currency &#125;&#125; (High Rate)&lt;/h4&gt; &lt;/div&gt;&lt;/template&gt;&lt;script&gt;export default &#123; name: &quot;app&quot;, data: function () &#123; return &#123; name: &quot;Kayak&quot;, price: 275, lowTaxRate: 12, highTaxRate: 20 &#125; &#125;, methods: &#123; getTotalPrice(taxRate) &#123; return this.price + (this.price * (taxRate / 100)); &#125; &#125;, filters: &#123; currency(value) &#123; return new Intl.NumberFormat(&quot;en-US&quot;, &#123; style: &quot;currency&quot;, currency: &quot;USD&quot; &#125;).format(value); &#125; &#125;&#125;&lt;/script&gt; 复杂 Filter1234567891011121314151617181920212223242526272829303132333435363738394041424344// src/App.vue&lt;template&gt; &lt;div id=&quot;app&quot; class=&quot;bg-primary text-white text-center m-2 p-3&quot;&gt; &lt;h3&gt;Product: &#123;&#123; name | reverse | capitalize &#125;&#125;&lt;/h3&gt; &lt;h4&gt;Price: &#123;&#123; getTotalPrice(lowTaxRate) | currency(3) &#125;&#125; (Low Rate)&lt;/h4&gt; &lt;h4&gt;Price: &#123;&#123; getTotalPrice(highTaxRate) | currency &#125;&#125; (High Rate)&lt;/h4&gt; &lt;/div&gt;&lt;/template&gt;&lt;script&gt;export default &#123; name: &quot;app&quot;, data: function () &#123; return &#123; name: &quot;Lifejacket&quot;, price: 48.95, lowTaxRate: 12, highTaxRate: 20 &#125; &#125;, methods: &#123; getTotalPrice(taxRate) &#123; return this.price + (this.price * (taxRate / 100)); &#125; &#125;, filters: &#123; currency(value, places) &#123; return new Intl.NumberFormat(&quot;en-US&quot;, &#123; style: &quot;currency&quot;, currency: &quot;USD&quot;, minimumFractionDigits: places || 2, maximumFractionDigits: places || 2 &#125;).format(value); &#125;, capitalize(value) &#123; return value[0].toUpperCase() + value.slice(1); &#125;, reverse(value) &#123; return value.split(&quot;&quot;).reverse().join(&quot;&quot;); &#125; &#125;&#125;&lt;/script&gt; 2. Directives指示器 是 template 中可以为 HTML 元素添加 Vue.js 功能的一些特殊属性。比如 v-on:click 指示器可以为 &lt;button&gt; 元素添加对鼠标单击事件的监听，v-text 指示器可以设置某个 HTML 元素的文字内容。 有选择地显示元素使用 v-if 指示器控制 HTML 元素的显示与隐藏：12345678910111213141516171819202122232425262728293031// src/App.vue&lt;template&gt; &lt;div id=&quot;app&quot; class=&quot;container-fluid text-center&quot;&gt; &lt;div class=&quot;bg-primary text-white m-2 p-3&quot;&gt; &lt;h3&gt;Product: &lt;span v-text=&quot;name&quot;&gt;&lt;/span&gt;&lt;/h3&gt; &lt;h4 v-if=&quot;showElements&quot;&gt;&#123;&#123; price &#125;&#125;&lt;/h4&gt; &lt;/div&gt; &lt;button v-on:click=&quot;handleClick&quot; class=&quot;btn btn-primary&quot;&gt; Press Me &lt;/button&gt; &lt;/div&gt;&lt;/template&gt;&lt;script&gt;export default &#123; name: &quot;app&quot;, data: function () &#123; return &#123; name: &quot;Lifejacket&quot;, price: 275, showElements: true &#125; &#125;, methods: &#123; handleClick() &#123; this.showElements = !this.showElements; &#125; &#125;,&#125;&lt;/script&gt; 关键代码：12345678&lt;h4 v-if="showElements"&gt;&#123;&#123; price &#125;&#125;&lt;/h4&gt;&lt;button v-on:click="handleClick" class="btn btn-primary"&gt;...methods: &#123; handleClick() &#123; this.showElements = !this.showElements; &#125; &#125; v-if-else123456789101112131415161718192021222324252627282930313233// src/App.vue&lt;template&gt; &lt;div id=&quot;app&quot; class=&quot;container-fluid text-center&quot;&gt; &lt;div class=&quot;bg-primary text-white m-2 p-3&quot;&gt; &lt;h3 v-if=&quot;counter % 3 == 0&quot;&gt;Product: &#123;&#123;name&#125;&#125;&lt;/h3&gt; &lt;h3 v-else-if=&quot;counter % 3 == 1&quot;&gt;Price: &#123;&#123;price&#125;&#125;&lt;/h3&gt; &lt;h3 v-else&gt;Category: &#123;&#123;category&#125;&#125;&lt;/h3&gt; &lt;/div&gt; &lt;button v-on:click=&quot;handleClick&quot; class=&quot;btn btn-primary&quot;&gt; Press Me &lt;/button&gt; &lt;/div&gt;&lt;/template&gt;&lt;script&gt;export default &#123; name: &quot;app&quot;, data: function () &#123; return &#123; name: &quot;Lifejacket&quot;, price: 275, category: &quot;Waterspots&quot;, counter: 0 &#125; &#125;, methods: &#123; handleClick() &#123; this.counter++; &#125; &#125;,&#125;&lt;/script&gt; 设置元素的 Class 属性 通过 v-bind:class 设置 HTML 元素的 Class 属性：1234567891011121314151617181920212223242526272829303132333435// src/App.vue&lt;template&gt; &lt;div id=&quot;app&quot; class=&quot;container-fluid text-center&quot;&gt; &lt;div class=&quot;bg-primary text-white m-2 p-3&quot;&gt; &lt;h3 v-bind:class=&quot;elemClasses&quot;&gt;Product: &#123;&#123;name&#125;&#125;&lt;/h3&gt; &lt;/div&gt; &lt;button v-on:click=&quot;handleClick&quot; class=&quot;btn btn-primary&quot;&gt; Press Me &lt;/button&gt; &lt;/div&gt;&lt;/template&gt;&lt;script&gt;export default &#123; name: &quot;app&quot;, data: function () &#123; return &#123; name: &quot;Lifejacket&quot;, highlight: false &#125; &#125;, computed: &#123; elemClasses() &#123; return this.highlight ? [&quot;bg-light&quot;, &quot;text-dark&quot;, &quot;display-4&quot;] : [&quot;bg-dark&quot;, &quot;text-light&quot;, &quot;p-2&quot;]; &#125; &#125;, methods: &#123; handleClick() &#123; this.highlight = !this.highlight; &#125; &#125;,&#125;&lt;/script&gt; 设置多个属性v-bind 指示器可以同时设置 HTML 元素的 class、style 等属性，甚至还可以包括原本不存在的由用户自行定义的属性（如下面代码中的 data-size）。12345678910111213141516171819202122232425262728293031323334353637383940414243// src/App.vue&lt;template&gt; &lt;div id=&quot;app&quot; class=&quot;container-fluid text-center&quot;&gt; &lt;div class=&quot;bg-primary text-white m-2 p-3&quot;&gt; &lt;h3 v-bind=&quot;attrValues&quot;&gt;Product: &#123;&#123;name&#125;&#125;&lt;/h3&gt; &lt;/div&gt; &lt;button v-on:click=&quot;handleClick&quot; class=&quot;btn btn-primary&quot;&gt; Press Me &lt;/button&gt; &lt;/div&gt;&lt;/template&gt;&lt;script&gt;export default &#123; name: &quot;app&quot;, data: function () &#123; return &#123; name: &quot;Lifejacket&quot;, highlight: false &#125; &#125;, computed: &#123; attrValues() &#123; return &#123; class: this.highlight ? [&quot;bg-light&quot;, &quot;text-dark&quot;] : [], style: &#123; border: this.highlight ? &quot;5px solid red&quot; : &quot;&quot; &#125;, &quot;data-size&quot;: this.highlight ? &quot;big&quot; : &quot;small&quot; &#125; &#125; &#125;, methods: &#123; handleClick() &#123; this.highlight = !this.highlight; &#125; &#125;,&#125;&lt;/script&gt;&lt;style&gt; [data-size=big] &#123; font-size: 40pt; &#125; [data-size=small] &#123; font-size: 20pt; &#125;&lt;/style&gt; 设置 HTMLElement 属性（不常用）123456789101112131415161718192021222324252627282930313233// src/App.vue&lt;template&gt; &lt;div id=&quot;app&quot; class=&quot;container-fluid text-center&quot;&gt; &lt;div class=&quot;bg-primary text-white m-2 p-3&quot;&gt; &lt;h3 v-bind:text-content.prop=&quot;textContent&quot;&gt;&lt;/h3&gt; &lt;/div&gt; &lt;button v-on:click=&quot;handleClick&quot; class=&quot;btn btn-primary&quot;&gt; Press Me &lt;/button&gt; &lt;/div&gt;&lt;/template&gt;&lt;script&gt;export default &#123; name: &quot;app&quot;, data: function () &#123; return &#123; name: &quot;Lifejacket&quot;, highlight: false &#125; &#125;, computed: &#123; textContent() &#123; return this.highlight ? &quot;Highlight!&quot; : `Product: $&#123;this.name&#125;`; &#125; &#125;, methods: &#123; handleClick() &#123; this.highlight = !this.highlight; &#125; &#125;,&#125;&lt;/script&gt; 3. Repeater DirectiveVue.js 中的 v-for 指示器可以用来操作数组格式的数据、生成表格和 Grid 布局等。 遍历数组v-for 指示器可以遍历数组结构中的数据对象并以循环的方式绑定给多个 HTML 元素。1234567891011121314151617181920212223242526272829303132333435363738394041// src/App.vue&lt;template&gt; &lt;div id=&quot;app&quot; class=&quot;container-fluid text-center&quot;&gt; &lt;h2 class=&quot;bg-primary text-while text-center p-3&quot;&gt;Products&lt;/h2&gt; &lt;table class=&quot;table table-sm table-bordered table-striped text-left&quot;&gt; &lt;tr&gt;&lt;th&gt;Index&lt;/th&gt;&lt;th&gt;Name&lt;/th&gt;&lt;th&gt;Price&lt;/th&gt;&lt;/tr&gt; &lt;tbody&gt; &lt;tr v-for=&quot;(p, i) in products&quot; v-bind:key=&quot;p.name&quot;&gt; &lt;td&gt;&#123;&#123; i + 1 &#125;&#125;&lt;/td&gt; &lt;td&gt;&#123;&#123; p.name &#125;&#125;&lt;/td&gt; &lt;td&gt;&#123;&#123; p.price &#125;&#125;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;div&gt; &lt;button v-on:click=&quot;handleClick&quot; class=&quot;btn btn-primary&quot;&gt; Press Me &lt;/button&gt; &lt;/div&gt; &lt;/div&gt;&lt;/template&gt;&lt;script&gt;export default &#123; name: &quot;app&quot;, data: function () &#123; return &#123; products: [ &#123; name: &quot;Kayak&quot;, price: 275 &#125;, &#123; name: &quot;Lifejacket&quot;, price: 48.95 &#125;, &#123; name: &quot;Soccer Ball&quot;, price: 19.50 &#125;, ] &#125; &#125;, methods: &#123; handleClick() &#123; this.products.push(this.products.shift()); &#125; &#125;,&#125;&lt;/script&gt; 关键代码：12345&lt;tr v-for=&quot;(p, i) in products&quot; v-bind:key=&quot;p.name&quot;&gt; &lt;td&gt;&#123;&#123; i + 1 &#125;&#125;&lt;/td&gt; &lt;td&gt;&#123;&#123; p.name &#125;&#125;&lt;/td&gt; &lt;td&gt;&#123;&#123; p.price &#125;&#125;&lt;/td&gt;&lt;/tr&gt; 遍历对象注意此处遍历的数据结构是 JavaScript 对象而不是上面代码中的数组。123456789101112131415161718192021222324252627282930313233343536373839404142434445// src/App.vue&lt;template&gt; &lt;div id=&quot;app&quot; class=&quot;container-fluid text-center&quot;&gt; &lt;h2 class=&quot;bg-primary text-while text-center p-3&quot;&gt;Products&lt;/h2&gt; &lt;table class=&quot;table table-sm table-bordered table-striped text-left&quot;&gt; &lt;tr&gt;&lt;th&gt;Index&lt;/th&gt;&lt;th&gt;Key&lt;/th&gt;&lt;th&gt;Name&lt;/th&gt;&lt;th&gt;Price&lt;/th&gt;&lt;/tr&gt; &lt;tbody&gt; &lt;tr v-for=&quot;(p, key, i) in products&quot; v-bind:key=&quot;p.name&quot;&gt; &lt;td&gt;&#123;&#123; i + 1 &#125;&#125;&lt;/td&gt; &lt;td&gt;&#123;&#123; key &#125;&#125;&lt;/td&gt; &lt;td&gt;&#123;&#123; p.name &#125;&#125;&lt;/td&gt; &lt;td&gt;&#123;&#123; p.price &#125;&#125;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;div&gt; &lt;button v-on:click=&quot;handleClick&quot; class=&quot;btn btn-primary&quot;&gt; Press Me &lt;/button&gt; &lt;/div&gt; &lt;/div&gt;&lt;/template&gt;&lt;script&gt;import Vue from &quot;vue&quot;;export default &#123; name: &quot;app&quot;, data: function () &#123; return &#123; products: &#123; &quot;kayak&quot;: &#123; name: &quot;Kayak&quot;, price: 275 &#125;, 22: &#123; name: &quot;Lifejacket&quot;, price: 48.95 &#125;, 3: &#123; name: &quot;Soccer Ball&quot;, price: 19.50 &#125;, 4: &#123; name: &quot;Corner Flags&quot;, price: 39.95 &#125; &#125; &#125; &#125;, methods: &#123; handleClick() &#123; Vue.set(this.products, 5, &#123; name: &quot;Running Shoes&quot;, price: 100 &#125;); &#125; &#125;,&#125;&lt;/script&gt; PS：更新表格数据应使用 Vue.set()，不要使用类似 this.products[index] = xx 这样的形式。1Vue.set(this.products, 5, &#123; name: &quot;Running Shoes&quot;, price: 100 &#125;); v-for 与 Computed Propertiesv-for 指示器可以搭配计算属性和方法等一起使用，参考（细品）下面的分页示例：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859// src/App.vue&lt;template&gt; &lt;div id=&quot;app&quot; class=&quot;container-fluid text-center&quot;&gt; &lt;h2 class=&quot;bg-primary text-while text-center p-3&quot;&gt;Products&lt;/h2&gt; &lt;table class=&quot;table table-sm table-bordered table-striped text-left&quot;&gt; &lt;tr&gt;&lt;th&gt;Name&lt;/th&gt;&lt;th&gt;Price&lt;/th&gt;&lt;/tr&gt; &lt;tbody&gt; &lt;tr v-for=&quot;p in pageItems&quot; v-bind:key=&quot;p.name&quot;&gt; &lt;td&gt;&#123;&#123; p.name &#125;&#125;&lt;/td&gt; &lt;td&gt;&#123;&#123; p.price &#125;&#125;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;div&gt; &lt;!-- eslint-disable-next-line vue/require-v-for-key --&gt; &lt;button v-for=&quot;i in pageCount&quot; v-on:click=&quot;selectPage(i)&quot; class=&quot;btn btn-secondary m-1&quot; v-bind:class=&quot;&#123;&apos;bg-primary&apos;: currentPage == i&#125;&quot;&gt; &#123;&#123; i &#125;&#125; &lt;/button&gt; &lt;/div&gt; &lt;/div&gt;&lt;/template&gt;&lt;script&gt;export default &#123; data: function () &#123; return &#123; pageSize: 3, currentPage: 1, products: [ &#123; name: &quot;Kayak&quot;, price: 275 &#125;, &#123; name: &quot;Lifejacket&quot;, price: 48.95 &#125;, &#123; name: &quot;Soccer Ball&quot;, price: 19.50 &#125;, &#123; name: &quot;Corner Flags&quot;, price: 39.95 &#125;, &#123; name: &quot;Stadium&quot;, price: 79500 &#125;, &#123; name: &quot;Thinking Cap&quot;, price: 16 &#125;, &#123; name: &quot;Unsteady Chair&quot;, price: 29.95 &#125;, &#123; name: &quot;Human Chess Board&quot;, price: 75 &#125;, &#123; name: &quot;Bling Bling King&quot;, price: 1200 &#125; ] &#125; &#125;, computed: &#123; pageCount() &#123; return Math.ceil(this.products.length / this.pageSize); &#125;, pageItems() &#123; let start = (this.currentPage - 1) * this.pageSize; return this.products.slice(start, start + this.pageSize); &#125; &#125;, methods: &#123; selectPage(page) &#123; this.currentPage = page; &#125; &#125;&#125;&lt;/script&gt; 参考资料Pro Vue.js 2]]></content>
      <categories>
        <category>Program</category>
      </categories>
      <tags>
        <tag>Program</tag>
        <tag>Development</tag>
        <tag>Web</tag>
        <tag>Vue</tag>
        <tag>Frontend</tag>
        <tag>JavaScript</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python3 中的字符串格式化语法]]></title>
    <url>%2F2020%2F01%2F14%2Fstring-format-in-python3%2F</url>
    <content type="text"><![CDATA[一、旧式的字符串格式化% 操作符参考以下示例：123&gt;&gt;&gt; name = "Eric"&gt;&gt;&gt; "Hello, %s." % name'Hello, Eric.' 当有多个变量需要插入到字符串中时：1234&gt;&gt;&gt; name = "Eric"&gt;&gt;&gt; age = 74&gt;&gt;&gt; "Hello, %s. You are %s." % (name, age)'Hello, Eric. You are 74.' 当需要替换的变量进一步增多时，使用 % 操作符格式化字符串会导致代码可读性变得很差：1234567&gt;&gt;&gt; first_name = "Eric"&gt;&gt;&gt; last_name = "Idle"&gt;&gt;&gt; age = 74&gt;&gt;&gt; profession = "comedian"&gt;&gt;&gt; affiliation = "Monty Python"&gt;&gt;&gt; "Hello, %s %s. You are %s. You are a %s. You were a member of %s." % (first_name, last_name, age, profession, affiliation)'Hello, Eric Idle. You are 74. You are a comedian. You were a member of Monty Python.' str.format()str.format() 是对 % 方式的改进，它使用常见的函数调用的语法，并且可以通过定义对象本身的 __format__() 方法控制字符串格式化的具体行为。 基本用法：1234&gt;&gt;&gt; name = "Eric"&gt;&gt;&gt; age = 74&gt;&gt;&gt; "Hello, &#123;&#125;. You are &#123;&#125;.".format(name, age)'Hello, Eric. You are 74.' str.format() 相对于 % 操作符有着更强的灵活性。比如可以通过数字索引来关联替换到字符串中的变量：1234&gt;&gt;&gt; name = "Eric"&gt;&gt;&gt; age = 74&gt;&gt;&gt; "Hello, &#123;1&#125;. You are &#123;0&#125;.".format(age, name)'Hello, Eric. You are 74.' 为了提高代码可读性，{} 中也可以使用有具体含义的参数名：1234&gt;&gt;&gt; name = "Eric"&gt;&gt;&gt; age = 74&gt;&gt;&gt; "Hello, &#123;name&#125;. You are &#123;age&#125;".format(name=name, age=age)'Hello, Eric. You are 74' 针对字典结构的数据：123&gt;&gt;&gt; person = &#123;'name': 'Eric', 'age': 74&#125;&gt;&gt;&gt; "Hello, &#123;name&#125;. You are &#123;age&#125;.".format(name=person['name'], age=person['age'])'Hello, Eric. You are 74.' 或者更简洁的方式：123&gt;&gt;&gt; person = &#123;'name': 'Eric', 'age': 74&#125;&gt;&gt;&gt; "Hello, &#123;name&#125;. You are &#123;age&#125;.".format(**person)'Hello, Eric. You are 74.' 问题在于当需要替换的变量很多时，str.format() 方式依然会导致代码变得过于冗长：12345678910&gt;&gt;&gt; first_name = "Eric"&gt;&gt;&gt; last_name = "Idle"&gt;&gt;&gt; age = 74&gt;&gt;&gt; profession = "comedian"&gt;&gt;&gt; affiliation = "Monty Python"&gt;&gt;&gt; "Hello, &#123;first_name&#125; &#123;last_name&#125;. You are &#123;age&#125;. \ You are a &#123;profession&#125;. You were a member of &#123;affiliation&#125;."\ .format(first_name=first_name, last_name=last_name, age=age, \ profession=profession, affiliation=affiliation)'Hello, Eric Idle. You are 74. You are a comedian. You were a member of Monty Python.' 二、f-string基本用法1234&gt;&gt;&gt; name = "Eric"&gt;&gt;&gt; age = 74&gt;&gt;&gt; f"Hello, &#123;name&#125;. You are &#123;age&#125;."'Hello, Eric. You are 74.' 嵌入表达式123456789101112&gt;&gt;&gt; f"&#123;2 * 37&#125;"'74'&gt;&gt;&gt; def to_lowercase(input):... return input.lower() &gt;&gt;&gt; name = "Eric Idle"&gt;&gt;&gt; f"&#123;to_lowercase(name)&#125; is funny"'eric idle is funny'&gt;&gt;&gt; f"&#123;name.lower()&#125; is funny"'eric idle is funny' f-string 中还可以直接嵌入某个对象实例，只要其内部实现了 __str__ 或者 __repr__ 方法：12345678910111213class Comedian: def __init__(self, first_name, last_name, age): self.first_name = first_name self.last_name = last_name self.age = age def __str__(self): return f"&#123;self.first_name&#125; &#123;self.last_name&#125; is &#123;self.age&#125;"new_comedian = Comedian("Eric", "Idle", 74)print(f"&#123;new_comedian&#125;")# Eric Idle is 74 多行 f-string12345678910&gt;&gt;&gt; name = "Eric"&gt;&gt;&gt; profession = "comedian"&gt;&gt;&gt; affiliation = "Monty Python"&gt;&gt;&gt; message = (... f"Hi &#123;name&#125;. "... f"You are a &#123;profession&#125;. "... f"You were in &#123;affiliation&#125;."... )&gt;&gt;&gt; message'Hi Eric. You are a comedian. You were in Monty Python.' 参考资料Python 3’s f-Strings: An Improved String Formatting Syntax (Guide)]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Program</tag>
        <tag>Basic</tag>
        <tag>String</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 通过 ldap3 操作 Windows 域账号]]></title>
    <url>%2F2020%2F01%2F14%2Fmanipulate-windows-ad-account-with-python-ldap3%2F</url>
    <content type="text"><![CDATA[ldap3 是一个严格遵守 RFC 4510 规范，完全由纯 Python 代码实现的 LDAPv3 客户端。ldap3 只依赖于 Python 标准库和 pyasn1，无需使用 C 编译器编译或者安装其他二进制程序，直接使用 pip 命令安装即可（pip install ldap3）。 一、连接服务器12345&gt;&gt;&gt; from ldap3 import Server, Connection, ALL&gt;&gt;&gt; server = Server('&lt;hostname_or_ip&gt;')&gt;&gt;&gt; conn = Connection(server)&gt;&gt;&gt; conn.bind()True 或者也可以使用更简短的形式：123&gt;&gt;&gt; conn = Connection('&lt;hostname_or_ip&gt;', auto_bind=True)&gt;&gt;&gt; connConnection(server=Server(host='xx.xx.xx.xx', port=389, use_ssl=False, allowed_referral_hosts=[('*', True)], get_info='SCHEMA', mode='IP_V6_PREFERRED'), auto_bind='NO_TLS', version=3, authentication='ANONYMOUS', client_strategy='SYNC', auto_referrals=True, check_names=True, read_only=False, lazy=False, raise_exceptions=False, fast_decoder=True, auto_range=True, return_empty_attributes=True, auto_encode=True, auto_escape=True, use_referral_cache=False) 获取服务器信息：12345678910&gt;&gt;&gt; server = Server('&lt;hostname_or_ip&gt;', get_info=ALL)&gt;&gt;&gt; conn = Connection(server, auto_bind=True)&gt;&gt;&gt; server.infoDSA info (from DSE): Supported LDAP versions: 3, 2 Naming contexts: DC=example,DC=com CN=Configuration,DC=example,DC=com CN=Schema,CN=Configuration,DC=example,DC=com... 用户绑定12345&gt;&gt;&gt; from ldap3 import Server, Connection, ALL&gt;&gt;&gt; server = Server('&lt;hostname_or_ip&gt;', get_info=ALL)&gt;&gt;&gt; conn = Connection(server, user='admin@example.com', password='admin@123', auto_bind=True)&gt;&gt;&gt; conn.extend.standard.who_am_i()'u:EXAMPLE\\admin' PS：通常在对域账号进行修改等操作时，需要启用 SSL 连接以符合安全规范。在构造 Server 对象时传入 use_ssl=True 即可：1server = Server(&apos;&lt;hostname_or_ip&gt;&apos;, port=636, use_ssl=True, get_info=ALL) 如有需要，TLS 的启用可以参考官方文档 https://ldap3.readthedocs.io/ssltls.html 二、对象检索使用 FreeIPA 开放的 LDAP 示例服务测试搜索操作：1234567891011&gt;&gt;&gt; from ldap3 import Server, Connection, ALL&gt;&gt;&gt; server = Server('ipa.demo1.freeipa.org', get_info=ALL)&gt;&gt;&gt; conn = Connection(server, 'uid=admin,cn=users,cn=accounts,dc=demo1,dc=freeipa,dc=org', 'Secret123', auto_bind=True)&gt;&gt;&gt; conn.search('dc=demo1,dc=freeipa,dc=org', '(objectclass=person)')True&gt;&gt;&gt; conn.entries[DN: uid=admin,cn=users,cn=accounts,dc=demo1,dc=freeipa,dc=org - STATUS: Read - READ TIME: 2020-01-13T18:57:39.837356, DN: uid=manager,cn=users,cn=accounts,dc=demo1,dc=freeipa,dc=org - STATUS: Read - READ TIME: 2020-01-13T18:57:39.837594, DN: uid=employee,cn=users,cn=accounts,dc=demo1,dc=freeipa,dc=org - STATUS: Read - READ TIME: 2020-01-13T18:57:39.837722, DN: uid=helpdesk,cn=users,cn=accounts,dc=demo1,dc=freeipa,dc=org - STATUS: Read - READ TIME: 2020-01-13T18:57:39.837845] search 方法有两个参数是必需的： search_base 用于指定搜索的起始位置 search_filter 用于指定搜索的筛选条件 filter 的定义语法支持 =、&lt;=、&gt;= 等比较运算符和 &amp;、|、! 等逻辑运算符。如：(&amp;(givenName=John)(mail=*@example.com)) 表示寻找名字为 John 且邮箱以 ``@example.com 结尾的域账号。 以下搜索条件则表示寻找名字为 Fred 或 John，并且邮箱以 ``@example.com 结尾的域账号：1234567(&amp; (| (givenName=Fred) (givenName=John) ) (mail=*@example.com)) attributes参考如下代码：12345678910111213141516&gt;&gt;&gt; conn.search('dc=demo1,dc=freeipa,dc=org', '(&amp;(objectclass=person)(uid=admin))', attributes=['sn', 'krbLastPwdChange', 'objectclass'])True&gt;&gt;&gt; conn.entries[0]DN: uid=admin,cn=users,cn=accounts,dc=demo1,dc=freeipa,dc=org - STATUS: Read - READ TIME: 2020-01-13T19:14:20.657761 krbLastPwdChange: 2019-01-25 15:16:11+00:00 objectclass: top person posixaccount krbprincipalaux krbticketpolicyaux inetuser ipaobject ipasshuser ipaSshGroupOfPubKeys ipaNTUserAttrs sn: Administrator 其中 (&amp;(objectclass=person)(uid=admin) 用于指定查找对象类型为 person 且 uid 为 admin 的用户账号； attributes 参数则用于指定搜索结果中额外包含该账户的 sn、krbLastPwdChange 和 objectclass 属性。 PS：设置 attributes = [&#39;*&#39;] 可以在搜索结果中显示对象的所有属性。 可以使用 Entry 对象的 entry_to_json 方法将该对象的所有属性以 JSON 格式输出：123456789101112131415161718192021222324&gt;&gt;&gt; print(conn.entries[0].entry_to_json())&#123; "attributes": &#123; "krbLastPwdChange": [ "2019-01-25 15:16:11+00:00" ], "objectclass": [ "top", "person", "posixaccount", "krbprincipalaux", "krbticketpolicyaux", "inetuser", "ipaobject", "ipasshuser", "ipaSshGroupOfPubKeys", "ipaNTUserAttrs" ], "sn": [ "Administrator" ] &#125;, "dn": "uid=admin,cn=users,cn=accounts,dc=demo1,dc=freeipa,dc=org"&#125; 此外还可以使用 paged_size 参数控制每页显示的结果数量。综合实例如下：12345678910111213141516&gt;&gt;&gt; searchParameters = &#123;'search_base': 'dc=demo1,dc=freeipa,dc=org',... 'search_filter': '(objectClass=Person)',... 'attributes': ['cn', 'givenName'],... 'paged_size': 3&#125;&gt;&gt;&gt; conn.search(**searchParameters)True&gt;&gt;&gt; conn.entries[DN: uid=admin,cn=users,cn=accounts,dc=demo1,dc=freeipa,dc=org - STATUS: Read - READ TIME: 2020-01-13T19:23:50.396926 cn: Administrator, DN: uid=manager,cn=users,cn=accounts,dc=demo1,dc=freeipa,dc=org - STATUS: Read - READ TIME: 2020-01-13T19:23:50.397214 cn: Test Manager givenName: Test, DN: uid=employee,cn=users,cn=accounts,dc=demo1,dc=freeipa,dc=org - STATUS: Read - READ TIME: 2020-01-13T19:23:50.397509 cn: Test Employee givenName: Test] 三、数据库操作创建条目新建 OU：12&gt;&gt;&gt; conn.add('ou=ldap3-tutorial,dc=demo1,dc=freeipa,dc=org', 'organizationalUnit')True 新建用户：12&gt;&gt;&gt; conn.add('cn=b.young,ou=ldap3-tutorial,dc=demo1,dc=freeipa,dc=org', 'inetOrgPerson', &#123;'givenName': 'Beatrix', 'sn': 'Young', 'departmentNumber': 'DEV', 'telephoneNumber': 1111&#125;)True 查看 objectClass 结构：12345678910&gt;&gt;&gt; server.schema.object_classes['person']Object class: 2.5.6.6 Short name: person Type: Structural Superior: top Must contain attributes: sn, cn May contain attributes: userPassword, telephoneNumber, seeAlso, description Extensions: X-ORIGIN: RFC 4519 OidInfo: ('2.5.6.6', 'OBJECT_CLASS', 'person', 'RFC4519') 重命名条目12&gt;&gt;&gt; conn.modify_dn('cn=b.young,ou=ldap3-tutorial,dc=demo1,dc=freeipa,dc=org', 'cn=b.smith')True 移动条目1234&gt;&gt;&gt; conn.add('ou=moved,ou=ldap3-tutorial,dc=demo1,dc=freeipa,dc=org','organizationalUnit')True&gt;&gt;&gt; conn.modify_dn('cn=b.smith,ou=ldap3-tutorial,dc=demo1,dc=freeipa,dc=org', 'cn=b.smith', new_superior='ou=moved, ou=ldap3-tutorial,dc=demo1,dc=freeipa,dc=org')True 更新条目添加属性值：12345678910&gt;&gt;&gt; from ldap3 import MODIFY_ADD, MODIFY_REPLACE, MODIFY_DELETE&gt;&gt;&gt; conn.modify('cn=b.smith,ou=moved,ou=ldap3-tutorial,dc=demo1,dc=freeipa,dc=org', &#123;'sn': [(MODIFY_ADD, ['Smyth'])]&#125;)True&gt;&gt;&gt; conn.search('ou=moved,ou=ldap3-tutorial,dc=demo1,dc=freeipa,dc=org', '(cn=b.smith)', attributes=['cn', 'sn'])True&gt;&gt;&gt; conn.entries[0]DN: cn=b.smith,ou=moved,ou=ldap3-tutorial,dc=demo1,dc=freeipa,dc=org - STATUS: Read - READ TIME: 2020-01-14T13:50:45.461241 cn: b.smith sn: Young Smyth 移除属性值：12345678&gt;&gt;&gt; conn.modify('cn=b.smith,ou=moved,ou=ldap3-tutorial,dc=demo1,dc=freeipa,dc=org', &#123;'sn': [(MODIFY_DELETE, ['Young'])]&#125;)True&gt;&gt;&gt; conn.search('ou=moved,ou=ldap3-tutorial,dc=demo1,dc=freeipa,dc=org', '(cn=b.smith)', attributes=['cn', 'sn'])True&gt;&gt;&gt; conn.entries[0]DN: cn=b.smith,ou=moved,ou=ldap3-tutorial,dc=demo1,dc=freeipa,dc=org - STATUS: Read - READ TIME: 2020-01-14T13:52:43.586339 cn: b.smith sn: Smyth 替换属性值：12345678&gt;&gt;&gt; conn.modify('cn=b.smith,ou=moved,ou=ldap3-tutorial,dc=demo1,dc=freeipa,dc=org', &#123;'sn': [(MODIFY_REPLACE, ['Smith'])]&#125;)True&gt;&gt;&gt; conn.search('ou=moved,ou=ldap3-tutorial,dc=demo1,dc=freeipa,dc=org', '(cn=b.smith)', attributes=['cn', 'sn'])True&gt;&gt;&gt; conn.entries[0]DN: cn=b.smith,ou=moved,ou=ldap3-tutorial,dc=demo1,dc=freeipa,dc=org - STATUS: Read - READ TIME: 2020-01-14T13:53:28.834019 cn: b.smith sn: Smith 判断属性是否为某个特定值：1234&gt;&gt;&gt; conn.compare('cn=b.smith,ou=moved,ou=ldap3-tutorial,dc=demo1,dc=freeipa,dc=org', 'departmentNumber', 'DEV')True&gt;&gt;&gt; conn.compare('cn=b.smith,ou=moved,ou=ldap3-tutorial,dc=demo1,dc=freeipa,dc=org', 'departmentNumber', 'QA')False 四、域账号操作修改账户密码：12345678910&gt;&gt;&gt; from ldap3 import Server, Connection&gt;&gt;&gt; server = Server('&lt;hostname_or_ip', use_ssl=True)&gt;&gt;&gt; conn = Connection(server, user='admin@example.com', password='admin@123', auto_bind=True)&gt;&gt;&gt; conn.search('ou=Domain Users,dc=example,dc=com', '(&amp;(objectClass=person)(sAMAccountName=admin))')True&gt;&gt;&gt; dn = conn.entries[0].entry_dn&gt;&gt;&gt; dn'CN=admin,OU=Domain Users,DC=example,DC=com'&gt;&gt;&gt; conn.extend.microsoft.modify_password(dn, 'new_password')True 解锁账户：12&gt;&gt;&gt; conn.extend.microsoft.unlock_account(dn)True 此外该模块下还有 addMembersToGroups 和 removeMembersFromGroups 等函数。 参考资料ldap3 官方文档ldap3 项目主页]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Program</tag>
        <tag>LDAP</tag>
        <tag>ActiveDirectory</tag>
        <tag>Practical</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 绘图库 Matplotlib 代码示例]]></title>
    <url>%2F2020%2F01%2F08%2Fsample-plots-in-matplotlib%2F</url>
    <content type="text"><![CDATA[Line Plot1234567import matplotlib.pyplot as pltX = range(100)Y = [value ** 2 for value in X]plt.plot(X, Y)plt.show() Using Numpy12345678import numpy as npimport matplotlib.pyplot as pltX = np.linspace(0, 2 * np.pi, 100)Y = np.sin(X)plt.plot(X, Y)plt.show() 等同于如下代码：123456789import mathimport matplotlib.pyplot as pltT = range(100)X = [(2 * math.pi * t) / len(T) for t in T]Y = [math.sin(value) for value in X]plt.plot(X, Y)plt.show() Multiple Line Plot12345678910import numpy as npimport matplotlib.pyplot as pltX = np.linspace(0, 2 * np.pi, 100)Ya = np.sin(X)Yb = np.cos(X)plt.plot(X, Ya)plt.plot(X, Yb)plt.show() 从文本文件中读取数据1234567# my_data.txt0 01 12 44 165 256 36 1234567import numpy as npimport matplotlib.pyplot as pltdata = np.loadtxt('my_data.txt')plt.plot(data[:, 0], data[:, 1])plt.show() 等同于如下代码：12345678910import matplotlib.pyplot as pltX, Y = [], []for line in open('my_data.txt', 'r'): values = [float(s) for s in line.split()] X.append(values[0]) Y.append(values[1])plt.plot(X, Y)plt.show() 其他类型的图形Scatter Plot123456789101112131415import numpy as npimport matplotlib.pyplot as pltdata = np.random.rand(1024, 2)print(data)# [[0.74566428 0.27225566]# [0.49387305 0.22290731]# [0.78644733 0.44918945]# ...# [0.42446667 0.31317443]# [0.06518628 0.21378513]# [0.85117629 0.83458943]]plt.scatter(data[:, 0], data[:, 1])plt.show() Bar Charts123456import matplotlib.pyplot as pltdata = [5., 25., 50., 20.]plt.bar(range(len(data)), data)plt.show() Multiple Bar Charts12345678910111213import numpy as npimport matplotlib.pyplot as pltdata = [[5., 25., 50., 20.], [4., 23., 51., 17.], [6., 22., 52., 19.]]X = np.arange(4)plt.bar(X + 0.00, data[0], color='b', width=0.25)plt.bar(X + 0.25, data[1], color='g', width=0.25)plt.bar(X + 0.50, data[2], color='r', width=0.25)plt.show() Histogram1234567import numpy as npimport matplotlib.pyplot as pltX = np.random.randn(1000)plt.hist(X, bins=20)plt.show() Pie Charts12345678910111213import matplotlib.pyplot as plt# Pie chart, where the slices will be ordered and plotted counter-clockwise:labels = 'Frogs', 'Hogs', 'Dogs', 'Logs'sizes = [15, 30, 45, 10]explode = (0, 0.1, 0, 0) # only "explode" the 2nd slice (i.e. 'Hogs')fig1, ax1 = plt.subplots()ax1.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%', shadow=True, startangle=90)ax1.axis('equal') # Equal aspect ratio ensures that pie is drawn as a circle.plt.show() Colormatplotlib 中定义颜色的方式有以下几种： (R, G, B, A)，如 (1.0, 0.0, 0.0) 表示红色，第四项数字 A （可省略）表示透明度 字符 b、g、r、c、m、y、k、w，分别表示蓝、绿、红、青、洋红、黄、黑、白 HTML 颜色字符串 #RRGGBB，如 #FFFFFF 表示纯白色 灰度字符串，介于 0 和 1 之间的浮点数，如 0.75 表示中度浅灰 示例一：123456789101112import numpy as npimport matplotlib.pyplot as pltA = np.random.standard_normal((100, 2))A += np.array((-1, -1))B = np.random.standard_normal((100, 2))B += np.array((1, 1))plt.scatter(A[:, 0], A[:, 1], color='.75')plt.scatter(B[:, 0], B[:, 1], color='y')plt.show() 示例二：Iris 文本数据下载自 http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data，格式如下：123454.6,3.2,1.4,0.2,Iris-setosa5.3,3.7,1.5,0.2,Iris-setosa5.0,3.3,1.4,0.2,Iris-setosa7.0,3.2,4.7,1.4,Iris-versicolor6.4,3.2,4.5,1.5,Iris-versicolor 12345678910111213141516171819202122import numpy as npimport matplotlib.pyplot as pltlabel_set = ( b'Iris-setosa', b'Iris-versicolor', b'Iris-virginica')# convert label to index number.# Iris-setosa -&gt; 0, Iris-versicolor -&gt; 1, Iris-virginica -&gt; 2.def read_label(label): return label_set.index(label)data = np.loadtxt('iris.data', delimiter=',', converters=&#123;4: read_label&#125;)color_set = ('b', 'r', 'm')color_list = [color_set[int(label)] for label in data[:, 4]]plt.scatter(data[:, 0], data[:, 1], color=color_list)plt.show() 上面的示例中只用到前两列和最后一列数据，将最后一列数据 label 替换为对应的 index 数字，以便在作图时根据 index 施以不同的着色。 Line Pattern123456789101112131415import numpy as npimport matplotlib.pyplot as pltdef pdf(X, mu, sigma): a = 1. / (sigma * np.sqrt(2. * np.pi)) b = -1. / (2. * sigma ** 2) return a * np.exp(b * (X - mu) ** 2)X = np.linspace(-6, 6, 1024)plt.plot(X, pdf(X, 0., 1.), color='r', linestyle='solid')plt.plot(X, pdf(X, 0., .5), color='g', linestyle='dashed')plt.plot(X, pdf(X, 0., .25), color='m', linestyle='dashdot')plt.show() Marker Style123456789101112import numpy as npimport matplotlib.pyplot as pltA = np.random.standard_normal((100, 2))A += np.array((-1, -1))B = np.random.standard_normal((100, 2))B += np.array((1, 1))plt.scatter(A[:, 0], A[:, 1], color='m', marker='x', size=100)plt.scatter(B[:, 0], B[:, 1], color='g', marker='^')plt.show() Title and Label12345678910111213import numpy as npimport matplotlib.pyplot as pltX = np.linspace(-4, 4, 1024)Y = .25 * (X + 4.) * (X + 1.) * (X - 2.)plt.title('Power curve for airfoil KV873')plt.xlabel('Air speed')plt.ylabel('Total drag')plt.text(-0.5, -0.25, 'Brackmard minimum')plt.plot(X, Y, c='k')plt.show() Legend123456789101112131415import numpy as npimport matplotlib.pyplot as pltX = np.linspace(0, 6, 1024)Y1 = np.sin(X)Y2 = np.cos(X)plt.xlabel('X')plt.ylabel('Y')plt.plot(X, Y1, c='k', lw=3., label='sin(X)')plt.plot(X, Y2, c='.5', lw=3., ls='--', label='cos(X)')plt.legend()plt.show() Figures1234567891011121314151617import numpy as npfrom matplotlib import pyplot as pltT = np.linspace(-np.pi, np.pi, 1024)grid_size = (4, 2)plt.subplot2grid(grid_size, (0, 0), rowspan=3, colspan=1)plt.plot(np.sin(2 * T), np.cos(0.5 * T), c='k')plt.subplot2grid(grid_size, (0, 1), rowspan=3, colspan=1)plt.plot(np.cos(3 * T), np.sin(T), c='k')plt.subplot2grid(grid_size, (3, 0), rowspan=1, colspan=3)plt.plot(np.cos(5 * T), np.sin(7 * T), c='k')plt.tight_layout()plt.show() Subplots12345678910111213import matplotlib.pyplot as pltimport numpy as npnp.random.seed(19680801)data = np.random.randn(2, 100)fig, axs = plt.subplots(2, 2, figsize=(5, 5))axs[0, 0].hist(data[0])axs[1, 0].scatter(data[0], data[1])axs[0, 1].plot(data[0], data[1])axs[1, 1].hist2d(data[0], data[1])plt.show() User Interface123456789101112131415161718192021222324252627282930313233343536373839404142434445import numpy as npfrom matplotlib import pyplot as pltfrom matplotlib.widgets import Sliderdef supershape_radius(phi, a, b, m, n1, n2, n3): theta = .25 * m * phi cos = np.fabs(np.cos(theta) / a) ** n2 sin = np.fabs(np.sin(theta) / b) ** n3 r = (cos + sin) ** (-1. / n1) r /= np.max(r) return rphi = np.linspace(0, 2 * np.pi, 1024)m_init = 3n1_init = 2n2_init = 18n3_init = 18fig = plt.figure()ax = fig.add_subplot(111, polar=True)ax_m = plt.axes([0.05, 0.05, 0.25, 0.025])ax_n1 = plt.axes([0.05, 0.10, 0.25, 0.025])ax_n2 = plt.axes([0.7, 0.05, 0.25, 0.025])ax_n3 = plt.axes([0.7, 0.10, 0.25, 0.025])slider_m = Slider(ax_m, 'm', 1, 20, valinit=m_init)slider_n1 = Slider(ax_n1, 'n1', .1, 10, valinit=n1_init)slider_n2 = Slider(ax_n2, 'n2', .1, 20, valinit=n2_init)slider_n3 = Slider(ax_n3, 'n3', .1, 20, valinit=n3_init)r = supershape_radius(phi, 1, 1, m_init, n1_init, n2_init, n3_init)lines, = ax.plot(phi, r, lw=3.)def update(val): r = supershape_radius(phi, 1, 1, np.floor(slider_m.val), slider_n1.val, slider_n2.val, slider_n3.val) lines.set_ydata(r) fig.canvas.draw_idle()slider_n1.on_changed(update)slider_n2.on_changed(update)slider_n3.on_changed(update)slider_m.on_changed(update)plt.show() 参考资料matplotlib Plotting CookbookSample plots in Matplotlib]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Program</tag>
        <tag>Data</tag>
        <tag>Plot</tag>
        <tag>Matplotlib</tag>
        <tag>Graphics</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 设计模式——MVC模式]]></title>
    <url>%2F2020%2F01%2F06%2Fpython-design-patterns-mvc-pattern%2F</url>
    <content type="text"><![CDATA[模型 - 视图 - 控制器模式MVC 不仅仅是一种实现用户界面的软件模式，同时也是一种易于修改和维护的架构。通常 MVC 模式将应用程序分为 3 个基本部分：模型（Model）、视图（View）和控制器（Controller）。这 3 个部分相互关联，有助于将信息的处理与信息的呈现分开。 MVC 模式的工作机制为：模型提供数据和业务逻辑（如何存储和查询信息），视图负责数据的展示（如何呈现），而控制器则是两者之间的粘合剂，根据用户要求的呈现方式协调模型和视图。视图和控制器依赖于模型，但模型是可以独立工作的。 模型：定义针对数据的所有操作（如创建、修改和删除等），并提供与数据使用有关的方法 视图：提供相应的方法，帮助根据上下文和应用程序的需要构建 Web 或 GUI 界面 控制器：从请求接收数据，并将其发送到系统的其他部分。需要提供用于路由请求的方法 MVC 模式的主要意图： 将数据和数据的展示隔离开 使类的维护和实现更加简单 灵活地改变数据的存储和显示方式，两者相互独立 模型是应用程序的基石，提供客户端请求的数据，必须在多个操作中保持一致。视图用来将数据展示在接口上供用户查看。可以独立开发，但不应包含复杂的逻辑；需要足够灵活，适应多种平台；应避免与数据库直接交互。控制器应该作为模型和视图之间的粘合剂，要尽可能薄；不应该进行数据库调用或参与数据的展示。 示例代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849class Model: services = &#123; 'email': &#123;'number': 1000, 'price': 2&#125;, 'sms': &#123;'number': 1000, 'price': 10&#125;, 'voice': &#123;'number': 1000, 'price': 15&#125; &#125;class View: def list_services(self, services): for svc in services: print(svc, ' ') def list_pricing(self, services): for svc in services: print("For", Model.services[svc]['number'], svc, 'message you pay $', Model.services[svc]['price'])class Controller: def __init__(self): self.model = Model() self.view = View() def get_services(self): services = self.model.services.keys() return (self.view.list_services(services)) def get_pricing(self): services = self.model.services.keys() return (self.view.list_pricing(services))if __name__ == '__main__': controller = Controller() print("Services Provided:") controller.get_services() print("Pricing for Services:") controller.get_pricing()# =&gt; Services Provided:# =&gt; email# =&gt; sms# =&gt; voice# =&gt; Pricing for Services:# =&gt; For 1000 email message you pay $ 2# =&gt; For 1000 sms message you pay $ 10# =&gt; For 1000 voice message you pay $ 15 现实世界中的 MVC 模式目录结构：123456mvc├── server.py└── templates ├── base.html ├── index.html └── new.html server.py 源代码：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374import tornadoimport tornado.webimport tornado.ioloopimport tornado.httpserverimport sqlite3def _execute(sql): db = sqlite3.connect('sqlite3.db') cursor = db.cursor() cursor.execute(sql) res = cursor.fetchall() db.commit() db.close() return resclass IndexHandler(tornado.web.RequestHandler): def get(self): query = "select * from task;" todos = _execute(query) print(todos) self.render('index.html', todos=todos)class NewHandler(tornado.web.RequestHandler): def post(self): name = self.get_argument('name', None) query = "create table if not exists task (id INTEGER \ PRIMARY KEY, name TEXT, status NUMERIC);" _execute(query) query = "insert into task (name, status) values ('%s', %d);" % (name, 1) _execute(query) self.redirect('/') def get(self): self.render('new.html')class UpdateHandler(tornado.web.RequestHandler): def get(self, id, status): query = "update task set status=%d where \ id=%s;" % (int(status), id) _execute(query) print(query) self.redirect('/')class DeleteHandler(tornado.web.RequestHandler): def get(self, id): query = "delete from task where id=%s;" % id _execute(query) self.redirect('/')class RunApp(tornado.web.Application): def __init__(self): Handlers = [ (r'/', IndexHandler), (r'/todo/new', NewHandler), (r'/todo/update/(\d+)/(\d+)', UpdateHandler), (r'/todo/delete/(\d+)', DeleteHandler), ] settings = dict( debug=True, template_path='templates', static_path='static', ) tornado.web.Application.__init__(self, Handlers, **settings)if __name__ == '__main__': http_server = tornado.httpserver.HTTPServer(RunApp()) http_server.listen(5000) tornado.ioloop.IOLoop.instance().start() templates/base.html 源代码：123456789&lt;!DOCTYPE&gt;&lt;html&gt; &lt;head&gt; &#123;% block header %&#125;&#123;% end%&#125; &lt;/head&gt; &lt;body&gt; &#123;% block body %&#125;&#123;% end %&#125; &lt;/body&gt;&lt;/html&gt; templates/index.html 源代码：1234567891011121314151617181920212223242526272829303132333435&#123;% extends 'base.html' %&#125;&lt;title&gt;ToDo&lt;/title&gt;&#123;% block body %&#125;&lt;h3&gt;Your Tasks&lt;/h3&gt;&lt;table border="1"&gt; &lt;tr align="center"&gt; &lt;td&gt;Id&lt;/td&gt; &lt;td&gt;Name&lt;/td&gt; &lt;td&gt;Status&lt;/td&gt; &lt;td&gt;Update&lt;/td&gt; &lt;td&gt;Delete&lt;/td&gt; &lt;/tr&gt; &#123;% for todo in todos %&#125; &lt;tr align="center"&gt; &lt;td&gt;&#123;&#123; todo[0] &#125;&#125;&lt;/td&gt; &lt;td&gt;&#123;&#123; todo[1] &#125;&#125;&lt;/td&gt; &#123;% if todo[2] %&#125; &lt;td&gt;Open&lt;/td&gt; &#123;% else %&#125; &lt;td&gt;Closed&lt;/td&gt; &#123;% end %&#125; &#123;% if todo[2] %&#125; &lt;td&gt;&lt;a href="/todo/update/&#123;&#123;todo[0]&#125;&#125;/0"&gt;Close Task&lt;/a&gt;&lt;/td&gt; &#123;% else %&#125; &lt;td&gt;&lt;a href="/todo/update/&#123;&#123;todo[0]&#125;&#125;/1"&gt;Open Task&lt;/a&gt;&lt;/td&gt; &#123;% end %&#125; &lt;td&gt;&lt;a href="/todo/delete/&#123;&#123;todo[0]&#125;&#125;"&gt;X&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &#123;% end %&#125;&lt;/table&gt;&lt;div&gt; &lt;h3&gt;&lt;a href="/todo/new"&gt;Add Task&lt;/a&gt;&lt;/h3&gt;&lt;/div&gt;&#123;% end %&#125; templates/new.html 源代码：1234567891011&#123;% extends 'base.html' %&#125;&lt;title&gt;ToDo&lt;/title&gt;&#123;% block body %&#125;&lt;div&gt; &lt;h3&gt;Add Task to your List&lt;/h3&gt; &lt;form action="/todo/new" method="post" id="new"&gt; &lt;p&gt;&lt;input type="text" name="name" placeholder="Enter task" /&gt; &lt;input type="submit" class="submit" value="add" /&gt;&lt;/p&gt; &lt;/form&gt;&lt;/div&gt;&#123;% end %&#125; 运行 python server.py 命令，浏览 http://localhost:5000/ ，效果如下： PS：需安装 tornado 模块（pip install tornado）]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Program</tag>
        <tag>OOP</tag>
        <tag>Design</tag>
        <tag>Pattern</tag>
        <tag>Development</tag>
        <tag>MVC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 设计模式——理解面向对象编程]]></title>
    <url>%2F2020%2F01%2F06%2Fpython-design-patterns-understanding-OOP%2F</url>
    <content type="text"><![CDATA[面向对象编程面向对象的编程范式引入了对象的概念，对象具有属性和用来处理属性的成员函数。比如对象 Car 拥有多种属性，如 fuel_level（油位）、is_sedan（是否为轿车）、speed（速度）、steering_wheel（方向盘）和 coordinates（坐标）等。同时还拥有一些方法，比如 accelerate() 方法用来提升速度，takeleft() 方法让车左转等。 在 Python 中，一切皆对象。每个类的实例或变量都有它自己的内存地址或身份。对象就是类的实例，应用开发就是通过对象交互来实现目的的过程。 对象： 表示所开发的应用程序内的实体 实体之间可以通过交互来解决现实世界的问题 如 Person 是实体，Car 也是实体。Person 可以驾驶 Car，从一个地方开到另一个地方 类： 类可以定义对象的属性和行为。属性是数据成员，行为由成员函数表示 类包含了构造函数，为对象提供初始状态 类就像模板，非常易于重复使用 方法： 表示对象的行为 可以对属性进行处理，实现所需的功能 12345678910111213class Person: def __init__(self, name, age): self.name = name self.age = age def get_person(self,): return f"&lt;Person (&#123;self.name&#125;, &#123;self.age&#125;&gt;)"p = Person("John", 32)print(f"Type of Object: &#123;type(p)&#125;, Memory Address: &#123;id(p)&#125;")print(p.get_person())# =&gt; Type of Object: &lt;class '__main__.Person'&gt;, Memory Address: 140586467650144# =&gt; &lt;Person (John, 32&gt;) 主要概念封装 对象的行为对于外部是不可见的，或者说对象的状态信息是私密的 客户端不能通过直接操作来改变对象的内部状态。需要通过发送消息来请求对象改变其内部状态，对象则根据请求通过特定的成员函数完成内部状态的修改 在 Python 中，封装的概念不是隐式的，它没有提供封装所需的诸如 public、private 和 protected 等关键字 多态 多态有两种类型。对象根据输入的参数提供方法的不同实现；不同类型的对象可以使用相同的接口 对于 Python 而言，多态是内置功能。如操作符 + 既可以应用于两个整数进行加法运算，也可以应用于字符串用来连接它们 12345&gt;&gt;&gt; a = "John"&gt;&gt;&gt; b = (1, 2, 3)&gt;&gt;&gt; c = [3, 4, 6, 8, 9]&gt;&gt;&gt; print(a[1], b[0], c[2])o 1 6 继承 表示一个类可以继承父类的（大部分）功能 可以重用基类中定义的功能并允许子类独立地进行扩展 可以利用不同类对象之间的关系建立层次结构。Python 支持多重继承（继承多个基类） 设计原则开放/封闭原则类或对象及其方法对于扩展来说应该是开放的，对于修改来说应该是封闭的。即在开发软件的时候，一定确保以通用的方式来编写类或模块，以便需要扩展类或对象行为的时候不必修改类本身。 优点： 现有的类不会被修改，退化的可能性较小 有助于保持以前代码的向后兼容性 控制反转原则高层级的模块不应该依赖于低层级的模块，应该都依赖于抽象。细节依赖于抽象，而不是抽象依赖于细节。该原则建议任何两个模块都不应以紧密方式相互依赖。基本模块和从属模块应当在它们之间提供一个抽象层来耦合。类的细节应描绘抽象。 优点： 削弱了模块间的紧耦合，消除了系统中的复杂性和刚性 在依赖模块之间有一个明确的抽象层，便于通过更好的方式处理模块之间的依赖关系 接口隔离原则客户端不应依赖于它们不需要使用的接口。 优点： 强制开发人员编写“瘦身型”接口，并使方法与接口紧密相关 防止向接口中随意添加方法 单一职责原则类的职责单一，引起类变化的原因单一。当我们在开发类时，它应该为特定的功能服务。若一个类实现了两个功能，则最好将它们分开。 优点： 每当一个功能发生变化时，除了特定的类需要改变外，其他类无需变动 若一个类有多种功能，那么依赖它的类必定会由于多种原因经历多次修改。这是应该避免的 替换原则派生类必须能够完全取代基类。当应用程序开发人员编写派生类时，该原则的含义就是他们应该扩展基类。此外，派生类应该尽可能对基类封闭，以致于派生类本身可以替换基类而无需修改任何代码。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Program</tag>
        <tag>OOP</tag>
        <tag>Design</tag>
        <tag>Pattern</tag>
        <tag>Development</tag>
        <tag>Object</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基本算法问题的 Python 解法（递归与搜索）]]></title>
    <url>%2F2020%2F01%2F05%2Fclassic-compute-problems-with-python%2F</url>
    <content type="text"><![CDATA[一、斐波那契数列递归函数123456789def fib(n: int) -&gt; int: if n &lt; 2: return n return fib(n - 2) + fib(n - 1)if __name__ == '__main__': for i in range(10): print(fib(i), end=' ')# =&gt; 0 1 1 2 3 5 8 13 21 34 不要尝试调用 fib(50)（这个我试过了，因为等得不耐烦 Ctrl+C 掉了）。在上面版本的程序中，每次对 fib() 函数的调用都会导致额外的两次对 fib() 自身的调用（即 fib(n -2) 和 fib(n - 1)）。这个行为模式会一直传递下去，直到每一个新的调用分支都满足 n &lt; 2。比如 fib(4) 最终会执行以下函数：123456789fib(4) -&gt; fib(3), fib(2)fib(3) -&gt; fib(2), fib(1)fib(2) -&gt; fib(1), fib(0)fib(2) -&gt; fib(1), fib(0)fib(1) -&gt; 1fib(1) -&gt; 1fib(1) -&gt; 1fib(0) -&gt; 0fib(0) -&gt; 0 计算 fib(4) 最终会调用 9 次 fib() 函数，fib(5) 调用 15 次，fib(10) 调用 177 次，计算 fib(20) 则需要执行整整 21891 次。换句话说，函数的调用树会以指数级的速度扩展。 MemoizationMemoization 是指将计算任务执行后得到的结果保存在某个地方，后面需要用到时就可以直接取出使用，而不必再次计算。1234567891011from typing import Dictmemo: Dict[int, int] = &#123;0: 0, 1: 1&#125; # base casesdef fib2(n: int) -&gt; int: if n not in memo: memo[n] = fib2(n -1) + fib2(n - 2) # memoization return memo[n]if __name__ == '__main__': print(fib2(50))# =&gt; 12586269025 lru_cachePython 提供了一个内置的装饰器 ``@functools.lru_cache()用来自动记录（缓存）某个函数的执行结果。因此上面的fib2() 可以改为如下形式：1234567891011from functools import lru_cache@lru_cache(maxsize=None)def fib3(n: int) -&gt; int: if n &lt; 2: return n return fib3(n - 2) + fib3(n - 1)if __name__ == '__main__': print(fib3(50))# =&gt; 12586269025 迭代式方案1234567891011def fib4(n: int) -&gt; int: if n == 0: return n last: int = 0 next: int = 1 for _ in range(1, n): last, next = next, last + next return nextif __name__== '__main__': print(fib4(50))# =&gt; 12586269025 上面的程序算是这几个方案中性能最好的一个，for 循环最多只执行 n-1 次。递归式方案借助逆向思维，而迭代式方案则是正向的逻辑。在某些情况下，原始的递归计算方式会带来过多的性能消耗。但是任何可以通过递归计算解决的问题，同样可以用迭代的方式解决。 生成器123456789101112131415from typing import Generatordef fib5(n: int) -&gt; Generator[int, None, None]: yield 0 if n &gt; 0: yield 1 last: int = 0 next: int = 1 for _ in range(1, n): last, next = next, last + next yield nextif __name__ == '__main__': for i in fib5(50): print(i, end=' ')# =&gt; 0 1 1 2 3 5 8 13 21 34 55 89 144 233 377 610 987 1597 2584 4181 6765 10946 17711 28657 46368 75025 121393 196418 317811 514229 832040 1346269 2178309 3524578 5702887 9227465 14930352 24157817 39088169 63245986 102334155 165580141 267914296 433494437 701408733 1134903170 1836311903 2971215073 4807526976 7778742049 12586269025 二、汉诺塔问题关于汉诺塔问题的简单描述： 有三根柱子，柱子 A 上摞有 n 个不同大小的圆盘，需要将所有圆盘借助柱子 B 转移到柱子 C 每次只能转移一个圆盘 大的圆盘必须放置在小的圆盘下面 递归解法的逻辑如下： 利用 C 作中转，移动 A 顶部的 n-1 个圆盘到柱子 B 移动 A 底部剩下的最大号圆盘到柱子 C 利用 A 作中转，移动 B 上的 n-1 个圆盘到柱子 C（此时问题由最初的将 n 个圆盘从 A 转移到 C，变成将 n-1 个圆盘从 B 转移到 C） 重复以上步骤 实现代码如下：1234567891011121314151617181920212223242526272829303132333435363738from typing import TypeVar, Generic, ListT = TypeVar('T')class Stack(Generic[T]): def __init__(self) -&gt; None: self._container: List[T] = [] def push(self, item: T) -&gt; None: self._container.append(item) def pop(self) -&gt; T: return self._container.pop() def __repr__(self) -&gt; str: return repr(self._container)def hanobi(begin: Stack[int], end: Stack[int], temp: Stack[int], n: int) -&gt; None: if n == 1: end.push(begin.pop()) else: hanobi(begin, temp, end, n - 1) hanobi(begin, end, temp, 1) hanobi(temp, end, begin, n - 1)if __name__ == '__main__': num_discs: int = 10 tower_a: Stack[int] = Stack() tower_b: Stack[int] = Stack() tower_c: Stack[int] = Stack() for i in range(1, num_discs + 1): tower_a.push(i) hanobi(tower_a, tower_c, tower_b, num_discs) print(tower_a) # [] print(tower_b) # [] print(tower_c) # [1, 2, 3] 这只是一个简单的演示程序，Stack 类抽象出单根柱子与圆盘的模型，hanobi 函数则以递归的方式定义了圆盘移动的过程。如果是真人来操作的话，每次移动一个圆盘花费 1 秒钟时间。解决由 50 个圆盘组成的汉诺塔问题，共需要耗费多长时间？约等于 3570.2 万年。 三、DNA 检索在计算机软件中，基因（Gene）一般通过由 A、C、G、T 组成的字符序列表示。其中每个字符代表一个核苷酸（nucleotide），每相邻的三个核苷酸（碱基）组成一个密码子（codon）。 基因的数据模型如下：12345678910111213141516171819202122232425262728293031323334353637383940from enum import IntEnumfrom typing import Tuple, ListNucleotide: IntEnum = IntEnum('Nucleotide', ('A', 'C', 'G', 'T'))Codon = Tuple[Nucleotide, Nucleotide, Nucleotide]Gene = List[Codon]def string_to_gene(s: str) -&gt; Gene: gene: Gene = [] for i in range(0, len(s), 3): if (i + 2) &gt;= len(s): return gene codon: Codon = (Nucleotide[s[i]], Nucleotide[s[i + 1]], Nucleotide[s[i + 2]]) gene.append(codon) return genegene_str: str = "ACGTGGCTCTCTAACGTACGTACGTACGGGGTTTATATATACCCTAGGACTCCCTTT"my_gene: Gene = string_to_gene(gene_str)for conda in my_gene: print(conda)# =&gt; (&lt;Nucleotide.A: 1&gt;, &lt;Nucleotide.C: 2&gt;, &lt;Nucleotide.G: 3&gt;)# =&gt; (&lt;Nucleotide.T: 4&gt;, &lt;Nucleotide.G: 3&gt;, &lt;Nucleotide.G: 3&gt;)# =&gt; (&lt;Nucleotide.C: 2&gt;, &lt;Nucleotide.T: 4&gt;, &lt;Nucleotide.C: 2&gt;)# =&gt; (&lt;Nucleotide.T: 4&gt;, &lt;Nucleotide.C: 2&gt;, &lt;Nucleotide.T: 4&gt;)# =&gt; (&lt;Nucleotide.A: 1&gt;, &lt;Nucleotide.A: 1&gt;, &lt;Nucleotide.C: 2&gt;)# =&gt; (&lt;Nucleotide.G: 3&gt;, &lt;Nucleotide.T: 4&gt;, &lt;Nucleotide.A: 1&gt;)# =&gt; (&lt;Nucleotide.C: 2&gt;, &lt;Nucleotide.G: 3&gt;, &lt;Nucleotide.T: 4&gt;)# =&gt; (&lt;Nucleotide.A: 1&gt;, &lt;Nucleotide.C: 2&gt;, &lt;Nucleotide.G: 3&gt;)# =&gt; (&lt;Nucleotide.T: 4&gt;, &lt;Nucleotide.A: 1&gt;, &lt;Nucleotide.C: 2&gt;)# =&gt; (&lt;Nucleotide.G: 3&gt;, &lt;Nucleotide.G: 3&gt;, &lt;Nucleotide.G: 3&gt;)# =&gt; (&lt;Nucleotide.G: 3&gt;, &lt;Nucleotide.T: 4&gt;, &lt;Nucleotide.T: 4&gt;)# =&gt; (&lt;Nucleotide.T: 4&gt;, &lt;Nucleotide.A: 1&gt;, &lt;Nucleotide.T: 4&gt;)# =&gt; (&lt;Nucleotide.A: 1&gt;, &lt;Nucleotide.T: 4&gt;, &lt;Nucleotide.A: 1&gt;)# =&gt; (&lt;Nucleotide.T: 4&gt;, &lt;Nucleotide.A: 1&gt;, &lt;Nucleotide.C: 2&gt;)# =&gt; (&lt;Nucleotide.C: 2&gt;, &lt;Nucleotide.C: 2&gt;, &lt;Nucleotide.T: 4&gt;)# =&gt; (&lt;Nucleotide.A: 1&gt;, &lt;Nucleotide.G: 3&gt;, &lt;Nucleotide.G: 3&gt;)# =&gt; (&lt;Nucleotide.A: 1&gt;, &lt;Nucleotide.C: 2&gt;, &lt;Nucleotide.T: 4&gt;)# =&gt; (&lt;Nucleotide.C: 2&gt;, &lt;Nucleotide.C: 2&gt;, &lt;Nucleotide.C: 2&gt;)# =&gt; (&lt;Nucleotide.T: 4&gt;, &lt;Nucleotide.T: 4&gt;, &lt;Nucleotide.T: 4&gt;) Linear search线性搜索会按照原始数据结构中的元素排列顺序，逐个检查搜索空间中的每一个元素，是最简单直观的搜索方式，复杂度为 O(n)。12345678910def linear_contains(gene: Gene, key_codon: Codon) -&gt; bool: for codon in gene: if codon == key_codon: return True return Falseacg: Codon = (Nucleotide.A, Nucleotide.C, Nucleotide.G)gat: Codon = (Nucleotide.G, Nucleotide.A, Nucleotide.T)print(linear_contains(my_gene, acg)) # Trueprint(linear_contains(my_gene, gat)) # False Binary search12345678910111213141516def binary_contains(gene: Gene, key_codon: Codon) -&gt; bool: low: int = 0 high: int = len(gene) - 1 while low &lt;= high: mid: int = (low + high) if gene[mid] &lt; key_codon: low = mid + 1 elif gene[mid] &gt; key_codon: high = mid - 1 else: return True return Falsemy_sorted_gene: Gene = sorted(my_gene)print(binary_contains(my_sorted_gene, acg))print(binary_contains(my_sorted_gene, gat)) Binary Search 将搜索对象与有序列表中间位置的值进行比对，根据大小关系确定下一次搜索在序列的前半或者后半部分继续进行。依照此规则持续进行检索，每一次比较都会将下一次的搜索空间减小一半（类似猜数字游戏）。 Binary Search 在最坏情况下的复杂度为 O(lg n)，前提是序列中的所有元素已经根据大小排序。最好的排序算法复杂度为 O(n lg n)。因此在需要对无序列表执行多次搜索的场景下，可以先对列表元素排序再执行 Binary Search。 通用示例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748# generic_search.pyfrom typing import TypeVar, Iterable, Sequence, Generic, List, Callable, Set, Deque, Dict, Any, Optionalfrom typing import Protocolfrom heapq import heappush, heappopT = TypeVar('T')def linear_contains(iterable: Iterable[T], key: T) -&gt; bool: for item in iterable: if item == key: return True return FalseC = TypeVar("C", bound="Comparable")class Comparable(Protocol): def __eq__(self, other: Any) -&gt; bool: return self == other def __lt__(self, other: C) -&gt; bool: return self &lt; other def __gt__(self: C, other: C) -&gt; bool: return (not self &lt; other) and self != other def __le__(self: C, other: C) -&gt; bool: return self &lt; other or self == other def __ge__(self: C, other: C) -&gt; bool: return not self &lt; otherdef binary_contains(sequence: Sequence[C], key: C) -&gt; bool: low: int = 0 high: int = len(sequence) - 1 while low &lt;= high: mid: int = (low + high) // 2 if sequence[mid] &lt; key: low = mid + 1 elif sequence[mid] &gt; key: high = mid - 1 else: return True return Falseif __name__ == '__main__': print(linear_contains([1, 5, 15, 15, 15, 15, 20], 5)) print(binary_contains(["a", "d", "e", "f", "z"], "f")) print(binary_contains(["join", "mark", "ronald", "sarah"], "sheila")) 四、迷宫问题迷宫建模：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748# maze.pyfrom enum import Enumfrom typing import List, NamedTuple, Callable, Optionalimport randomfrom math import sqrtclass Cell(str, Enum): EMPTY = " " BLOCKED = "X" START = "S" GOAL = "G" PATH = "*"class MazeLocation(NamedTuple): row: int column: intclass Maze: def __init__(self, rows: int = 10, columns: int = 10, sparseness: float = 0.2, start: MazeLocation = MazeLocation(0, 0), goal: MazeLocation = MazeLocation(9, 9)) -&gt; None: self._rows: int = rows self._columns: int = columns self.start: MazeLocation = start self.goal: MazeLocation = goal self._grid: List[List[Cell]] = [[Cell.EMPTY for c in range(columns)] for r in range(rows)] self._randomly_fill(rows, columns, sparseness) self._grid[start.row][start.column] = Cell.START self._grid[goal.row][goal.column] = Cell.GOAL def _randomly_fill(self, rows: int, columns: int, sparseness: float): for row in range(rows): for column in range(columns): if random.uniform(0, 1.0) &lt; sparseness: self._grid[row][column] = Cell.BLOCKED def __str__(self) -&gt; str: output: str = "" for row in self._grid: output += "".join([c.value for c in row]) + "\n" return outputmaze: Maze = Maze()print(maze) 12345678910SX X X XX X X X X XXX X X X X XXX X X G 为 Maze 类创建如下两个方法，goal_test 用于确定当前 Cell 是否为目标地点；successors 用于判断与当前 Cell 相邻的哪些 Cell 可以作为下一步的落脚点，返回其列表：123456789101112131415# maze.py continued def goal_test(self, ml: MazeLocation) -&gt; bool: return ml == self.goal def successors(self, ml: MazeLocation) -&gt; List[MazeLocation]: locations: List[MazeLocation] = [] if ml.row + 1 &lt; self._rows and self._grid[ml.row + 1][ml.column] != Cell.BLOCKED: locations.append(MazeLocation(ml.row + 1, ml.column)) if ml.row - 1 &gt;= 0 and self._grid[ml.row - 1][ml.column] != Cell.BLOCKED: locations.append(MazeLocation(ml.row - 1, ml.column)) if ml.column + 1 &lt; self._columns and self._grid[ml.row][ml.column + 1] != Cell.BLOCKED: locations.append(MazeLocation(ml.row, ml.column + 1)) if ml.column - 1 &gt;= 0 and self._grid[ml.row][ml.column - 1] != Cell.BLOCKED: locations.append(MazeLocation(ml.row, ml.column - 1)) return locations DFS（depth-first search）算法我理解的 DFS 算法，就是“一条道儿走到黑”。只要当前的路径能走通就一直走下去，不去考虑途中其他可能的方案。一旦遇到障碍走入“死胡同”，则回退到上一个节点尝试之前未选择的路径。最终到达目标地点后停止，或者一路回退到起点，则证明不存在可行的方案。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061# generic_search.py continuedclass Stack(Generic[T]): def __init__(self) -&gt; None: self._container: List[T] = [] @property def empty(self) -&gt; bool: return not self._container # not is true for empty container def push(self, item: T) -&gt; None: self._container.append(item) def pop(self) -&gt; T: return self._container.pop() # LIFO def __repr__(self) -&gt; str: return repr(self._container)class Node(Generic[T]): def __init__(self, state: T, parent: Optional[Node], cost: float = 0.0, heuristic: float = 0.0) -&gt; None: self.state: T = state self.parent: Optional[Node] = parent self.cost: float = cost self.heuristic: float = heuristic def __lt__(self, other: Node) -&gt; bool: return (self.cost + self.heuristic) &lt; (other.cost + other.heuristic)def dfs(initial: T, goal_test: Callable[[T], bool], successors: Callable[[T], List[T]]) -&gt; Optional[Node[T]]: # frontier is where we've yet to go frontier: Stack[Node[T]] = Stack() frontier.push(Node(initial, None)) # explored is where we've been explored: Set[T] = &#123;initial&#125; # keep going while there is more to explore while not frontier.empty: current_node: Node[T] = frontier.pop() current_state: T = current_node.state # if we found the goal, we're done if goal_test(current_state): return current_node # check where we can go next and haven't explored for child in successors(current_state): if child in explored: # skip children we already explored continue explored.add(child) frontier.push(Node(child, current_node)) return None # went through everything and never found goaldef node_to_path(node: Node[T]) -&gt; List[T]: path: List[T] = [node.state] # work backwards from end to front while node.parent is not None: node = node.parent path.append(node.state) path.reverse() return path 其中 Node 类可以算作 MazeLocation 上的又一层封装，为其添加 parent-child 关系方便后续连接为路径。其 cost 和 heuristic 属性会在后面的 A* 算法中用到。 dfs 函数中的 frontier 使用 Stack 数据结构记录每一步选择中出现的可行路径节点，其 pop 方法用于回溯到上一个节点。若 frontier 为空则说明已回溯到起点，未找到合适路径。explored 使用 Set 数据结构记录所有已经尝试过的路径选择，避免回溯时重复之前的路径。 node_to_path 函数则可以从终点开始反向查找，将 Node 节点（即带有 parent-child 关系的 MazeLocation）逐步扩展连接成完整路径存放在列表中。 补充 maze.py 代码：1234567891011121314151617181920212223242526272829# maze.py continuedfrom generic_search import dfs, node_to_path, Node def mark(self, path: List[MazeLocation]): for maze_location in path: self._grid[maze_location.row][maze_location.column] = Cell.PATH self._grid[self.start.row][self.start.column] = Cell.START self._grid[self.goal.row][self.goal.column] = Cell.GOAL def clear(self, path: List[MazeLocation]): for maze_location in path: self._grid[maze_location.row][maze_location.column] = Cell.EMPTY self._grid[self.start.row][self.start.column] = Cell.START self._grid[self.goal.row][self.goal.column] = Cell.GOALif __name__ == '__main__': m: Maze = Maze() print(m)# test DFS solution1: Optional[Node[MazeLocation]] = dfs(m.start, m.goal_test, m.successors) if solution1 is None: print("No solution found using depth-first search") else: path1: List[MazeLocation] = node_to_path(solution1) m.mark(path1) print(m) m.clear(path1) 结果如下：123456789101112131415161718192021SX XX X X XX X X X X XX XX X XX GSX XX X**X *XX*X** X X* *****X****XX *X ***X X* XX****G BFS（breadth-first search）算法寻找最短路径BFS 算法更倾向于一步一个脚印，每一步都要尝试过所有可能的方案，层层递进直到其中任何一种方案走通。假如共有 100 种路径都可以到达迷宫终点，各条路径或长或短。BFS 每次都会在所有 100 种路径上各走一步，直到其中某一条路径刚好到达终点后停止。由于对所有可行路径的尝试是同步进行的，因此该算法找到的路径总是最短的。12345678910111213141516171819202122232425262728293031323334353637383940# generic_search.py continuedclass Queue(Generic[T]): def __init__(self) -&gt; None: self._container: Deque[T] = Deque() @property def empty(self) -&gt; bool: return not self._container # not is true for empty container def push(self, item: T) -&gt; None: self._container.append(item) def pop(self) -&gt; T: return self._container.popleft() # FIFO def __repr__(self) -&gt; str: return repr(self._container)def bfs(initial: T, goal_test: Callable[[T], bool], successors: Callable[[T], List[T]]) -&gt; Optional[Node[T]]: # frontier is where we've yet to go frontier: Queue[Node[T]] = Queue() frontier.push(Node(initial, None)) # explored is where we've been explored: Set[T] = &#123;initial&#125; # keep going while there is more to explore while not frontier.empty: current_node: Node[T] = frontier.pop() current_state: T = current_node.state # if we found the goal, we're done if goal_test(current_state): return current_node # check where we can go next and haven't explored for child in successors(current_state): if child in explored: # skip children we already explored continue explored.add(child) frontier.push(Node(child, current_node)) return None # went through everything and never found goal 其中 Queue 数据结构底层使用 Deque 而不是 List，目的是提升 popleft 方法的效率。相对于 DFS 中的 Stack 数据结构，Queue 的 pop 方法是从序列的最左侧（Stack 是从最右侧）弹出项目。由此使得 BFS 可以从起点开始最大程度地尝试所有可行方案，即广度优先原则。 12345678910# maze.py continued# test BFS solution2: Optional[Node[MazeLocation]] = bfs(m.start, m.goal_test, m.successors) if solution2 is None: print("No solution found using breadth-first search!") else: path2: List[MazeLocation] = node_to_path(solution2) m.mark(path2) print(m) m.clear(path2) 执行结果：123456789101112131415161718192021S******* X X X *XX********* X X********** X *XX ******* * X XXXX*X **** X****XXGS*X X X XX** X X**** XXX***** X* XXXX X **** X XXG A* 算法寻找最优解粗略地说，DFS 可以较快地找到合适路径，BFS 则可以寻求最短路径（但时间花费较高）。 A* 算法则引入了 heuristic 概念，大概就是在做出决策前先根据一定的指导原则确定路径选择的优先级。即优先选择可能最短（离终点最近）的路径作为下一步的节点，而不是随机尝试所有可行的节点，以此来降低 BFS 导致的时间损耗。1234567891011121314151617181920212223242526272829303132333435363738394041# generic_search.py continuedclass PriorityQueue(Generic[T]): def __init__(self) -&gt; None: self._container: List[T] = [] @property def empty(self) -&gt; bool: return not self._container # not is true for empty container def push(self, item: T) -&gt; None: heappush(self._container, item) # in by priority def pop(self) -&gt; T: return heappop(self._container) # out by priority def __repr__(self) -&gt; str: return repr(self._container)def astar(initial: T, goal_test: Callable[[T], bool], successors: Callable[[T], List[T]], heuristic: Callable[[T], float]) -&gt; Optional[Node[T]]: # frontier is where we've yet to go frontier: PriorityQueue[Node[T]] = PriorityQueue() frontier.push(Node(initial, None, 0.0, heuristic(initial))) # explored is where we've been explored: Dict[T, float] = &#123;initial: 0.0&#125; # keep going while there is more to explore while not frontier.empty: current_node: Node[T] = frontier.pop() current_state: T = current_node.state # if we found the goal, we're done if goal_test(current_state): return current_node # check where we can go next and haven't explored for child in successors(current_state): new_cost: float = current_node.cost + 1 # 1 assumes a grid, need a cost function for more sophisticated apps if child not in explored or explored[child] &gt; new_cost: explored[child] = new_cost frontier.push(Node(child, current_node, new_cost, heuristic(child))) return None # went through everything and never found goal 其中 PriorityQueue 数据结构使用 heappop 方法确保每次从队列中取出的数据项都是（对算法而言）优先级最高的。 下面 maze.py 中添加的 manhattan_distance 函数则以意向节点与目标节点的路线距离作为下一步路径选择的依据。123456789101112131415161718# maze.py continueddef manhattan_distance(goal: MazeLocation) -&gt; Callable[[MazeLocation], float]: def distance(ml: MazeLocation) -&gt; float: xdist: int = abs(ml.column - goal.column) ydist: int = abs(ml.row - goal.row) return (xdist + ydist) return distance# ... # Test A* distance: Callable[[MazeLocation], float] = manhattan_distance(m.goal) solution3: Optional[Node[MazeLocation]] = astar(m.start, m.goal_test, m.successors, distance) if solution3 is None: print("No solution found using A*!") else: path3: List[MazeLocation] = node_to_path(solution3) m.mark(path3) print(m) 执行结果：1234567891011121314151617181920212223242526272829303132S******X X ********* **X X********** X XX*X X** XX***X * ****X*XX***XX * X GS X*X**X X**** X XX*X X * XXX * XXX * XX X******GS X*X**X X*******X XX X ** X XX***X X*XX XX * X G 参考资料Classic Computer Science Problems in Pythondavecom/ClassicComputerScienceProblemsInPython]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Program</tag>
        <tag>Advanced</tag>
        <tag>DataStructure</tag>
        <tag>Algorithm</tag>
        <tag>Search</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python Cookbook —— 类与对象]]></title>
    <url>%2F2020%2F01%2F02%2Fpython-cookbook-class-and-object%2F</url>
    <content type="text"><![CDATA[一、实例对象的字符串表示修改实例对象的 __str__() 和 __repr__() 方法可以改变该实例生成的字符串输出。123456789101112131415class Pair: def __init__(self, x, y): self.x = x self.y = y def __repr__(self): return 'Pair(&#123;0.x!r&#125;, &#123;0.y!r&#125;)'.format(self) def __str__(self): return '(&#123;0.x!s&#125;, &#123;0.y!s&#125;)'.format(self)# test&gt;&gt;&gt; p = Pair(3, 4)&gt;&gt;&gt; pPair(3, 4) # __repr__() output&gt;&gt;&gt; print(p) # __str__() output(3, 4) 二、自定义字符串格式化通过定义实例的 __format__ 方法，可以配置实例对象接受字符串格式化操作时的表现（即定义被 format 函数调用的方式与调用后的输出结果）。123456789101112131415161718192021222324252627_formats = &#123; 'ymd' : '&#123;d.year&#125;-&#123;d.month&#125;-&#123;d.day&#125;', 'mdy' : '&#123;d.month&#125;/&#123;d.day&#125;/&#123;d.year&#125;', 'dmy' : '&#123;d.day&#125;/&#123;d.month&#125;/&#123;d.year&#125;'&#125;class Date: def __init__(self, year, month, day): self.year = year self.month = month self.day = day def __format__(self, code): if code == '': code = 'ymd' fmt = _formats[code] return fmt.format(d=self)d = Date(2012, 12, 21)print(format(d))print(format(d, 'mdy'))print('The date is &#123;:ymd&#125;'.format(d))# =&gt; 2012-12-21# =&gt; 12/21/2012# =&gt; The date is 2012-12-21 三、使对象支持 Context-Management 协议为了使某个对象支持 Context-Management（即 with 语句），需要在类中实现 __enter__() 和 __exit__() 方法。12345678910111213141516171819202122232425262728293031from socket import socket, AF_INET, SOCK_STREAMclass LazyConnection: def __init__(self, address, family=AF_INET, type=SOCK_STREAM): self.address = address self.family = family self.type = type self.sock = None def __enter__(self): if self.sock is not None: raise RuntimeError('Already connected') self.sock = socket(self.family, self.type) self.sock.connect(self.address) return self.sock def __exit__(self, exc_ty, exc_val, tb): self.sock.close() self.sock = None# test codefrom functools import partialconn = LazyConnection(('www.python.org', 80))with conn as s: s.send(b'GET /index.htm HTTP/1.0\r\n') s.send(b'Host: www.python.org\r\n') s.send(b'\r\n') resp = b''.join(iter(partial(s.recv, 8192), b'')) print(resp) 四、创建自定义属性有些时候，在设置或者获取某个实例属性的值时，需要对其添加额外的操作，比如类型检查或者属性值的合法性检查等。这类需求可以使用 @property 装饰器来完成。 12345678910111213141516171819202122232425262728class Person: def __init__(self, first_name): self._first_name = first_name # Getter function @property def first_name(self): return self._first_name # Setter function @first_name.setter def first_name(self, value): if not isinstance(value, str): raise TypeError('Expected a string') self._first_name = value # Deleter function(optional) @first_name.deleter def first_name(self): raise AttributeError("Can't delete attribute")# test codea = Person('Guido')print(a.first_name) # =&gt; Guidoa.first_name = 'Linus'print(a.first_name) # =&gt; Linusa.first_name = 42 # =&gt; TypeError: Expected a stringdel a.first_name # =&gt; AttributeError: can't delete attribute PS：和 Java 等语言的使用习惯不同，Python 的 Property 属性应该只用在访问属性时需要对其进行额外处理的情况下。并不是所有对属性的访问都需要由 getter 或 setter 处理。 Property 还可以用来创建按需计算的属性。即属性值并非预先存放在某个地方，而是在访问它时实时地计算并返回。12345678910111213141516171819import mathclass Circle: def __init__(self, radius): self.radius = radius @property def area(self): return math.pi * self.radius ** 2 @property def perimeter(self): return 2 * math.pi * self.radiusc = Circle(4.0) print(c.radius) # =&gt; 4.0print(c.area) # =&gt; 50.26548245743669print(c.perimeter) # =&gt; 25.132741228718345 五、简化数据结构的初始化有些时候，程序中会包含很多需要用类去定义的数据结构，导致出现类似下面的代码：12345678910111213141516171819class Stock: def __init__(self, name, shares, price): self.name = name self.shares = shares self.price = priceclass Point: def __init__(self, x, y): self.x = x self.y = yclass Circle: def __init__(self, radius): self.radius = radius def area(self): return math.pi * self.radius ** 2 上述多个数据结构（Stock、Point、Circle）都是通过 __init__ 方法进行初始化的。实际上可以先定义一个基类，在其中抽象出 __init__ 方法的逻辑，使其可以作为一个通用的方法简化重复的初始化操作。1234567891011121314151617181920212223242526272829class Structure: # Class variable that specifies expected fields _fields = [] def __init__(self, *args, **kwargs): if len(args) != len(self._fields): raise TypeError('Expected &#123;&#125; arguments'.format(len(self._fields))) # Set the arguments for name, value in zip(self._fields, args): setattr(self, name, value) # Set the additional arguments (if any) extra_args = kwargs.keys() - self._fields for name in extra_args: setattr(self, name, kwargs.pop(name)) if kwargs: raise TypeError('Duplicate values for &#123;&#125;'.format(','.join(kwargs)))# Example useif __name__ == '__main__': class Stock(Structure): _fields = ['name', 'shares', 'price'] s1 = Stock('ACME', 50, 91.1) s2 = Stock('ACME', 50, 91.1, date='8/2/2012') print(s1.shares) # =&gt; 50 print(s2.date) # =&gt; 8/2/2012 六、定义额外的构造器类方法可以作为除 __init__() 方法以外的，另一个用来创建类实例的构造方法。123456789101112131415161718192021222324import timeclass Date: # Primary constructor def __init__(self, year, month, day): self.year = year self.month = month self.day = day # Alternate constructor @classmethod def today(cls): t = time.localtime() return cls(t.tm_year, t.tm_mon, t.tm_mday) def __repr__(self): return f'&#123;self.year&#125;-&#123;self.month&#125;-&#123;self.day&#125;'a = Date(2019, 12, 23)print(a) # =&gt; 2019-12-23b = Date.today()print(b) # =&gt; 2019-12-23 七、通过 Mixin 扩展类123456789101112131415161718192021222324252627282930313233343536373839404142# mixins.pyclass LoggedMappingMixin: ''' Add logging to get/set/delete operations for debugging. ''' __slots__ = () def __getitem__(self, key): print(f'Getting &#123;key&#125;') return super().__getitem__(key) def __setitem__(self, key, value): print(f'Setting &#123;key&#125; = &#123;value&#125;') return super().__setitem__(key, value) def __delitem__(self, key): print(f'Deleting &#123;key&#125;') return super().__delitem__(key)class SetOnceMappingMixin: ''' Only allow a key to be set once. ''' __slots__ = () def __setitem__(self, key, value): if key in self: raise KeyError(f'&#123;key&#125; already set') return super().__setitem__(key, value)class StringKeysMappingMixin: ''' Restrict keys to strings only. ''' __slots__ = () def __setitem__(self, key, value): if not isinstance(key, str): raise TypeError('keys must be strings') return super().__setitem__(key, value) 测试代码：12345678910111213141516171819202122&gt;&gt;&gt; from mixins import *&gt;&gt;&gt; class LoggedDict(LoggedMappingMixin, dict):... pass...&gt;&gt;&gt; d = LoggedDict()&gt;&gt;&gt; d['x'] = 23Setting x = 23&gt;&gt;&gt; d['x']Getting x23&gt;&gt;&gt; del d['x']Deleting x&gt;&gt;&gt;&gt;&gt;&gt; class SetOnceStringDict(StringKeysMappingMixin, SetOnceMappingMixin, dict):... pass...&gt;&gt;&gt; d2 = SetOnceStringDict()&gt;&gt;&gt; d2['x'] = 23&gt;&gt;&gt; d2['x'] = 10KeyError: 'x already set'&gt;&gt;&gt; d2[3] = 42TypeError: keys must be strings 八、使类支持比较运算符1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859from functools import total_orderingclass Room: def __init__(self, name, length, width): self.name = name self.length = length self.width = width self.square_feet = self.length * self.width@total_orderingclass House: def __init__(self, name, style): self.name = name self.style = style self.rooms = list() @property def living_space_footage(self): return sum(r.square_feet for r in self.rooms) def add_room(self, room): self.rooms.append(room) def __str__(self): return f'&#123;self.name&#125;: &#123;self.living_space_footage&#125; square foot &#123;self.style&#125;' def __eq__(self, other): return self.living_space_footage == other.living_space_footage def __lt__(self, other): return self.living_space_footage &lt; other.living_space_footage# Build a few houses, and add rooms to themh1 = House('h1', 'Cape')h1.add_room(Room('Master Bedroom', 14, 21))h1.add_room(Room('Living Room', 18, 20))h1.add_room(Room('Kitchen', 12, 16))h1.add_room(Room('Office', 12, 12))h2 = House('h2', 'Ranch')h2.add_room(Room('Master Bedroom', 14, 21))h2.add_room(Room('Living Room', 18, 20))h2.add_room(Room('Kitchen', 12, 16))h3 = House('h3', 'Split')h3.add_room(Room('Master Bedroom', 14, 21))h3.add_room(Room('Living Room', 18, 20))h3.add_room(Room('Office', 12, 16))h3.add_room(Room('Kitchen', 15, 17))houses = [h1, h2, h3]print('Is h1 bigger than h2?', h1 &gt; h2) # =&gt; Trueprint('Is h2 smaller than h3?', h2 &lt; h3) # =&gt; Trueprint('Is h2 greater than or equal to h1?', h2 &gt;= h1) # =&gt; Falseprint('Which one is biggest?', max(houses)) # =&gt; h3: 1101 square foot Splitprint('Which is smallest?', min(houses)) # =&gt; h2: 846 square foot Ranch 参考资料Python Cookbook, 3rd Edition]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Tricks</tag>
        <tag>Python</tag>
        <tag>Program</tag>
        <tag>OOP</tag>
        <tag>Class</tag>
        <tag>Advanced</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 服务器上的大文件查找及清理实践]]></title>
    <url>%2F2019%2F12%2F29%2Flinux-server-big-file-find-and-clean%2F</url>
    <content type="text"><![CDATA[生产上的 Linux 服务器磁盘空间不足，后面排查得知是某个应用频繁写 log 导致。于是加了一条自动清理过期日志的 crontab 。具体的排查过程记录如下，都是很基础的命令。 一、dfdf -h 命令查看当前磁盘空间的使用情况：12345678# df -hFilesystem Size Used Avail Use% Mounted on/dev/vda1 50G 50G 0G 100% /devtmpfs 3.9G 0 3.9G 0% /devtmpfs 3.9G 24K 3.9G 1% /dev/shmtmpfs 3.9G 476K 3.9G 1% /runtmpfs 3.9G 0 3.9G 0% /sys/fs/cgrouptmpfs 783M 0 783M 0% /run/user/0 系统只有一个磁盘分区 /dev/vda1，大小为 50G，已挂载到根目录下，用量为 100%，确实没有剩余空间。 以此可断定并非存在分区划分不合理的情况。比如磁盘大部分容量分配给了其他分区，挂载到诸如 /home、/usr 等目录下导致 / 路径下没有足够的空间。 二、lsblklsblk 命令查看硬盘的分区与挂载点：12345# lsblkNAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINTsr0 11:0 1 37M 0 romvda 253:0 0 50G 0 disk`-vda1 253:1 0 50G 0 part / 当前只有一块硬盘 vda，大小为 50G，全部分配给了唯一的分区 vda1，不存在剩余空间。 此处可以确定硬盘的所有容量都已被分配使用。不存在剩余空间（未分配区域）或者因为 LVM 卷导致仍有空闲存储未被使用等情况。 三、dudu 命令统计文件和目录占用的磁盘空间大小。 du 命令默认会以递归的方式输出当前路径中包含的所有文件的大小（以目录为单位显示），信息量有时会比较庞大。可以使用 -s 选项获取当前目录下所有文件的大小总和。或者使用 -d 选项指定遍历的深度，即只统计到某一层目录而无需展开到更深层的子目录。 当前 Linux 服务器上的应用都部署在某个特定的路径下，因此切换到该目录并统计其中文件占用的磁盘空间大小总和：12# du -sh12G . 通过 -d 选项指定遍历的层数为 1，显示当前路径下包含的每一个子目录各自占用的磁盘空间总和：12345678910# du -hd180K ./work7.6M ./lib12G ./logs236K ./conf4.0K ./temp221M ./webapps15M ./backup860K ./bin12G . 可以看到 logs 子目录下的文件总共占用了 12G 存储空间，几乎与整个目录大小相当。因此基本可以确定 logs 目录为需要进一步排查的对象。 PS：如当前路径下子目录众多，也可以使用 sort 命令对输出结果按大小进行排序。12345678910# du -d1 | sort -nr12448764 .12199320 ./logs225988 ./webapps14432 ./backup7736 ./lib860 ./bin236 ./conf80 ./work4 ./temp sort 命令的 -n 选项表示以数字大小为排序依据，-r 则表示逆序输出排序结果。du 命令去掉 -h 选项则避免将文件大小（bytes）自动转换为 KB、MB、GB 等导致单位不一致。可以使用 -k 或 -m 等选项手动指定 du 命令的单位。 四、lsls 命令获取指定目录下包含的文件列表（及详细信息）。 123456789# ls -Slh logs | head -8total 12G-rw-r--r-- 1 tomcat Devops 3.5G Dec 29 01:40 catalina.out-rw-r----- 1 root root 108M Nov 15 00:00 localhost_access.2019-11-14.log-rw-r----- 1 root root 107M Nov 22 00:00 localhost_access.2019-11-21.log-rw-r----- 1 root root 106M Nov 14 00:00 localhost_access.2019-11-13.log-rw-r----- 1 root root 104M Nov 17 00:00 localhost_access.2019-11-16.log-rw-r----- 1 root root 104M Nov 23 00:00 localhost_access.2019-11-22.log-rw-r----- 1 root root 104M Nov 13 00:00 localhost_access.2019-11-12.log 其中 -S 选项用于将输出结果按文件大小排序，-l 选项指定输出各文件的详细信息。由于 logs 目录下文件众多，使用 head -8 筛选前 8 条输出进行显示。 此时即可根据对文件大小和功能的判断手动执行删除操作。 五、findfind 命令筛选指定时期内创建的文件 logs 目录下每天都会创建新的日志文件，导致占用的磁盘空间与日俱增。因此需要定期删除指定日期以前的旧文件，释放不必要的空间占用。 如删除当前目录下只在 60 天以前修改过的文件，保留最近两个月的日志记录：1# find . -mtime +60 -type f -exec rm &#123;&#125; \; 将该命令添加到 crontab 中，设置好定时规则，即可定期执行清理任务，避免过高的磁盘占用。 命令总结 du -h：查看当前系统中磁盘空间的使用情况 lsblk：查看当前系统中磁盘的分区和挂载情况 du -hd1：查看当前目录下各子目录分别占用的磁盘空间大小 ls -Slh | head -8：列出当前目录下所有文件的详细信息，结果由大到小排序，输出前 8 条 find . -mtime +60 -type f -exec rm {} \;：查找当前目录下所有 60 天之前修改过的文件并删除]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Admin</tag>
        <tag>Tools</tag>
        <tag>Server</tag>
        <tag>Tricks</tag>
        <tag>Disk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ansible Playbooks 基本功能介绍及示例代码]]></title>
    <url>%2F2019%2F12%2F29%2Fansible-playbooks-basic-code-examples%2F</url>
    <content type="text"><![CDATA[一、基本结构用于安装或升级 apache 服务的 playbook 完整示例：1234567891011121314151617181920212223242526---- hosts: webservers remote_user: root vars: http_port: 80 max_clients: 200 tasks: - name: ensure apache is at the latest version yum: name: httpd state: latest - name: write the apache config file template: src: /srv/httpd.j2 dest: /etc/httpd.conf notify: - restart apache - name: ensure apache is running service: name: httpd state: started handlers: - name: restart apache service: name: httpd state: restarted targettarget 部分用于指定执行 playbook 任务时面向的目标主机和远程用户。123---- hosts: webservers remote_user: root 远程用户也可以在特定的 task 中额外进行定义：1234567---- hosts: webservers remote_user: root tasks: - name: test connection ping: remote_user: yourname 执行任务时允许远程用户通过 sudo 提升权限：12345---- hosts: webservers remote_user: yourname become: yes become_method: sudo 执行任务时切换到其他用户身份：12345---- hosts: webservers remote_user: yourname become: yes become_user: postgres variablevariable 部分用于定义变量，作用域为当前 play。123vars: http_port: 80 max_clients: 200 通过 vars_files 定义变量：1234vars_files: conf/country-AU.yml conf/datacenter-SYD.yml conf/cluster-mysql.yml 交互式地从用户输入中获取变量的值。如将用户输入赋值给变量 https_passphrase：1234vars_prompt: - name: https_passphrase prompt: Key Passphrase private: yes tasktask 部分包含了一系列我们希望在目标机器上执行的动作。12345tasks:- name: ensure apache is at the latest version yum: name: httpd state: latest handlerhandler 与 task 有着相同的语法和类似的功能。但是 handler 只可以在特定的条件下由 task 调用后执行。 如配置文件替换成功后触发重启服务的动作：123456789101112tasks: - name: copy the DHCP config copy: src: dhcp/dhcpd.conf dest: /etc/dhcp/dhcpd.conf notify: restart dhcphandlers:- name: restart dhcp service: name: dhcpd state: restarted 可以在单个 task 中同时调用多个 handler：123456789101112131415161718192021222324252627---- hosts: qroud tasks: - name: checkout Qroud git: repo:git@github.com:smarthall/Qroud.git dest: /opt/apps/Qroud force=no notify: - migrate db - generate static - restart httpdhandlers:- name: migrate db command: ./manage.py migrate –all args: chdir: /opt/apps/Qroud- name: generate static command: ./manage.py collectstatic -c –noinput args: chdir: /opt/apps/Qroud- name: restart httpd service: name: httpd state: restarted 二、playbook 模块templatetemplate 模块一般用于定义配置文件的主体框架，并为 Ansible 中的变量预留好位置以便在需要时渲染。类似于 Flask 应用依据模板文件动态地生成 HTML 文档。其模板功能由 Jinja2 提供，支持条件语句、for 循环和宏等高级语法。123&#123;% for ip in ansible_all_ipv4_addresses %&#125; &#123;&#123; ip &#125;&#125;;&#123;% endfor %&#125; 12345tasks:- name: write the apache config file template: src: /srv/httpd.j2 dest: /etc/httpd.conf pausepause 模块会将执行中的 playbook 暂停一段时间。比如部署了一个新版本的 web 应用，需要用户手动确认一切正常才能继续执行后面的任务。123456789---- hosts: localhost tasks: - name: wait on user input pause: prompt: "Warning! Press ENTER to continue or CTRL-C to quit." - name: timed wait pause: seconds: 30 wait_for等待某个特定的 TCP 端口可以被远程主机访问连通。1234567891011121314151617---- hosts: webapps tasks: - name: Install Tomcat yum: name: tomcat7 state: latest - name: Start Tomcat service: name: tomcat7 state: started - name: Wait for Tomcat to start wait_for: port: 8080 state: started group_bygroup_by 模块可以在 task 中基于收集到的 facts 信息动态地对 hosts 进行分组。1234567891011121314151617---- name: Create operating system group hosts: all tasks: - group_by: key=os_&#123;&#123; ansible_distribution &#125;&#125;- name: Run on CentOS hosts only hosts: os_CentOS tasks: - name: Install Apache yum: name=httpd state=latest- name: Run on Ubuntu hosts only hosts: os_Ubuntu tasks: - name: Install Apache apt: pkg=apache2 state=latest 三、Playbooks 进阶Looping1234567891011tasks:- name: Secure config files file file: path: "/etc/&#123;&#123; item &#125;&#125;" mode: 0600 owner: root group: root with_items: - my.cnf - shadow - fstab 条件语句123456789101112131415161718192021---- name: Install VIM hosts: all tasks: - name: Install VIM via yum yum: name: vim-enhanced state: latest when: ansible_os_family == "RedHat" - name: Install VIM via apt apt: name: vim state: latest when: ansible_os_family == "Debian" - name: Unexpected OS family debug: msg: "OS Family &#123;&#123; ansible_os_family &#125;&#125; is not supported" fail: yes when: ansible_os_family != "RedHat" and ansible_os_family != "Debian" 123456# Copy a file to the remote server if the hosts file doesn't existtasks:- stat: path=/etc/hosts register: hosts_file- copy: src=path/to/local/file dest=/path/to/remote/file when: not hosts_file.stat.exists 123456# Downgrade PHP version if the current version contains '7.0'.tasks:- shell: php --version register: php_version- shell: yum -y downgrade php* when: "'7.0' in php_version.stdout" 注册变量123456789101112131415- name: Using register hosts: ansibletest remote_user: root tasks: - name: Get /tmp info file: dest: /tmp state: directory register: tmp - name: Set mode on /tmp/subtmp file: dest: /tmp/subtmp mode: "&#123;&#123; tmp.mode &#125;&#125;" state: directory debug 查看变量的值：1234- name: capture output of id command command: id -un register: login- debug: msg="Logged in as user &#123;&#123; login.stdout &#125;&#125;" filterJinja2 模板提供的 filter 功能可以将原始数据转换为用户需要的格式123456789101112---- name: Create user accounts hosts: all vars: users: tasks: - name: Create accounts user: name=&#123;&#123; item|lower &#125;&#125; state=present with_items: - Fred - John - DanielH 常用 filter： min：参数为列表，返回列表中最小的项目 max：参数为列表，返回列表中最大的项目 random：参数为列表，随机返回列表中的某个值 default(x)：指定变量不存在时，以 x 为该变量的默认值 unique：参数为列表，返回该列表去除重复项后的版本 replace(x, y)：将字符串中出现的所有 x 替换为 y join(x)：参数为列表，返回由 x 拼接所有列表项后形成的字符串 针对文件路径的常用 filter： basename：返回路径中包含的文件名 dirname：返回文件所在的目录 expanduser：将路径中包含的 ~ 替换为 home 目录 realpath：文件的真实路径 12345vars: homepage: /usr/share/nginx/html/index.htmltasks:- name: copy home page copy: src=files/&#123;&#123; homepage | basename &#125;&#125; dest=&#123;&#123; homepage &#125;&#125; Lookuplookups 允许 Ansible 从多种类型的源头（如文本文档、CSV 文件等）读取配置数据。 file：读取文本文件内容作为参数12- name: Add my public key as an EC2 key ec2_key: name=mykey key_material="&#123;&#123; lookup('file', '/Users/lorin/.ssh/id_rsa.pub') &#125;&#125;" pipe：在远程机器上执行某个外部程序并关联其标准输出12- name: get SHA of most recent commit debug: msg="&#123;&#123; lookup('pipe', 'git rev-parse HEAD') &#125;&#125;" env：获取远程机器上的某个环境变量12- name: get the current shell debug: msg="&#123;&#123; lookup('env', 'SHELL') &#125;&#125;" password：生成随机密码，并将该密码写入指定文件1234- name: create deploy postgres user postgresql_user: name: deploy password: "&#123;&#123; lookup('password', 'deploy-password.txt') &#125;&#125;" redis-kv：获取 redis 服务器上某个 key 的值（需要在远程机器上安装 redis Python 库）12- name: look up value in Redis debug: msg="&#123;&#123; lookup('redis_kv', 'redis://localhost:6379,weather') &#125;&#125;" 复杂循环 名称 输入 循环规则 with_items 列表 遍历所有列表项 with_lines 命令 遍历命令输出的所有行 with_fileglob Glob 遍历文件（文件名可使用通配符） with_first_found 路径列表 路径中检索到的第一个文件目标 with_dict 字典 遍历字典中的数据项 with_flattened 多层列表 遍历展开后的多层列表 with_inventory Host 模式 遍历匹配的主机 with_lines：12345# files/turing.txtLeslie LamportSilvio MicaliShafi GoldwasserJudea Pearl 1234567- name: Send out a slack message slack: domain: example.slack.com token: "&#123;&#123; slack_token &#125;&#125;" msg: "&#123;&#123; item &#125;&#125; was in the list" with_lines: - cat files/turing.txt with_fileglob：12345- name: add public keys to account authorized_key: user=deploy key="&#123;&#123; lookup('file', item) &#125;&#125;" with_fileglob: - /var/keys/*.pub - keys/*.pub with_dict：123- name: iterate over ansible_eth0 debug: msg=&#123;&#123; item.key &#125;&#125;=&#123;&#123; item.value &#125;&#125; with_dict: "&#123;&#123; ansible_eth0.ipv4 &#125;&#125;" 参考资料Ansible Configuration Management - Second Edition]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Admin</tag>
        <tag>Ansible</tag>
        <tag>DevOps</tag>
        <tag>Playbooks</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python3 中的时间与日期模块详解]]></title>
    <url>%2F2019%2F12%2F22%2Fpython3-date-and-time-manual%2F</url>
    <content type="text"><![CDATA[Python 中不包含表示日期和时间的基本数据类型，但是提供了如下三个模块用于操作时间日期类型的数据： time：提供与时间相关的功能。包含时钟时间、处理器运行时间、基本的格式化工具等 datetime：提供处理时间日期的高级接口。如时间日期之间的算术计算和大小比较等 calendar：提供类似日历的年、月、日、周的格式化输出和其他日历相关的功能 一、timetime 模块包含多种类型的“时钟”时间，方便处理多种不同需求的任务。如： time() 返回自纪年以来的秒数。纪年指 time.gmtime(0) 函数的返回值，大多数系统中是 1970年1月1日00:00:00（UTC） monotonic() 可以用来测量长时间运行的进程，它不会因为系统时间被修改而发生变化 perf_counter() 提供了精度最高的时间量度，因此多用于获取短时间段的高精度结果 process_time() 返回当前进程中系统和用户 CPU 时间的总和。不包括 sleep() 时间 测试程序：1234567891011121314151617181920212223import textwrapimport timeavailable_clocks = [ ('time', time.time), ('monotonic', time.monotonic), ('perf_counter', time.perf_counter), ('process_time', time.process_time),]for clock_name, func in available_clocks: print(textwrap.dedent('''\ &#123;name&#125;: adjustable : &#123;info.adjustable&#125; implementation: &#123;info.implementation&#125; monotonic : &#123;info.monotonic&#125; resolution : &#123;info.resolution&#125; result : &#123;result&#125; ''').format( name=clock_name, info=time.get_clock_info(clock_name), result=func()) ) 输出中包含了各时钟函数的实现方式、精度和执行结果等：123456789101112131415161718192021222324252627time: adjustable : True implementation: clock_gettime(CLOCK_REALTIME) monotonic : False resolution : 1e-09 result : 1576935284.1307783monotonic: adjustable : False implementation: clock_gettime(CLOCK_MONOTONIC) monotonic : True resolution : 1e-09 result : 1653915.765674168perf_counter: adjustable : False implementation: clock_gettime(CLOCK_MONOTONIC) monotonic : True resolution : 1e-09 result : 1653915.765707337process_time: adjustable : False implementation: clock_gettime(CLOCK_PROCESS_CPUTIME_ID) monotonic : True resolution : 1e-09 result : 0.017164144 上述几个时钟函数中，只有 time.time() 在计算时间时有明确的参考点（time.gmtime(0)），另外三个函数都没有定义绝对的起点作为基准，因此常用来计算某个过程耗费的时间总和（多次执行并计算时间差），而无法获取一个标准的绝对时间。1234&gt;&gt;&gt; start = time.perf_counter()&gt;&gt;&gt; end = time.perf_counter()&gt;&gt;&gt; end - start8.590425299999993 上述函数返回的都是以秒为单位精度较高的浮点数。对于需要记录或者打印方便阅读的时间日期时，可以使用 ctime() 函数：12345&gt;&gt;&gt; time.ctime()'Sat Dec 21 21:56:41 2019'&gt;&gt;&gt; later = time.time() + 30&gt;&gt;&gt; time.ctime(later)'Sat Dec 21 21:57:19 2019' time 模块中还定义了 struct_time 结构用来表示日期和时间中的每一项独立的值（年月日、时分秒等）。1234567&gt;&gt;&gt; loc_time = time.localtime()&gt;&gt;&gt; loc_timetime.struct_time(tm_year=2019, tm_mon=12, tm_mday=21, tm_hour=22, tm_min=20, tm_sec=32, tm_wday=5, tm_yday=355, tm_isdst=0)&gt;&gt;&gt; loc_time.tm_year2019&gt;&gt;&gt; loc_time.tm_hour22 时间的格式化输出strftime() 函数可以用来将 struct_time 格式的时间值转换为字符串的形式表示。strptime() 函数的行为则相反。如：1234567891011&gt;&gt;&gt; import time&gt;&gt;&gt; loc_time = time.localtime()&gt;&gt;&gt; loc_timetime.struct_time(tm_year=2019, tm_mon=12, tm_mday=21, tm_hour=23, tm_min=21, tm_sec=33, tm_wday=5, tm_yday=355, tm_isdst=0)&gt;&gt;&gt; time.strftime('%Y-%m-%d %H:%M:%S', loc_time)'2019-12-21 23:21:33'&gt;&gt;&gt; now = time.ctime()&gt;&gt;&gt; now'Sat Dec 21 23:23:31 2019'&gt;&gt;&gt; time.strptime(now)time.struct_time(tm_year=2019, tm_mon=12, tm_mday=21, tm_hour=23, tm_min=23, tm_sec=31, tm_wday=5, tm_yday=355, tm_isdst=-1) 常见的字符串形式的时间日期格式如下：1234&gt;&gt;&gt; time.strftime('%Y-%m-%d %H:%M:%S')'2019-12-21 23:26:40'&gt;&gt;&gt; time.strftime('%a %b %d %H:%M:%S %Y')'Sat Dec 21 23:27:15 2019' strftime() 函数支持的更多格式定义可参考 help(time.strftime) 。 二、datetimedatetime 模块中定义的函数和类可以对日期和时间进行转换、格式化输出、算术计算等操作。 timetime 类可以表示包含时、分、秒、时区等信息的时间对象。123456789101112&gt;&gt;&gt; import datetime&gt;&gt;&gt; t = datetime.time(1, 2, 3)&gt;&gt;&gt; print(t)01:02:03&gt;&gt;&gt; t.hour1&gt;&gt;&gt; t.minute2&gt;&gt;&gt; t.second3&gt;&gt;&gt; t.tzinfo&gt;&gt;&gt; datedate 类可以表示包含年、月、日等信息的日期值。其 today() 方法可以获取当天的日期。12345678910&gt;&gt;&gt; from datetime import date&gt;&gt;&gt; today = date.today()&gt;&gt;&gt; todaydatetime.date(2019, 12, 21)&gt;&gt;&gt; today.year2019&gt;&gt;&gt; today.month12&gt;&gt;&gt; today.day21 datetimedatetime 模块中的 datetime 类可以表示完整的日期和时间，类似 time 类和 date 类的结合。1234567891011121314151617&gt;&gt;&gt; import datetime&gt;&gt;&gt; now = datetime.datetime.now()&gt;&gt;&gt; nowdatetime.datetime(2019, 12, 22, 0, 30, 21, 372631)&gt;&gt;&gt; print(now)2019-12-22 00:30:21.372631&gt;&gt;&gt; t = datetime.time(1, 2, 3)&gt;&gt;&gt; print(t)01:02:03&gt;&gt;&gt; d = datetime.date.today()&gt;&gt;&gt; print(d)2019-12-22&gt;&gt;&gt; dt = datetime.datetime.combine(d, t)&gt;&gt;&gt; dtdatetime.datetime(2019, 12, 22, 1, 2, 3)&gt;&gt;&gt; print(dt)2019-12-22 01:02:03 timedeltatimedelta 类可以表示某个特定长度的时间段（以年月日或时分秒等为单位）。两个 datetime 相减会得到 timedelta 对象，也可以用 datetime 加上或者减去 timedelta 得到新的时间值。 日期加减：123456789&gt;&gt;&gt; import datetime&gt;&gt;&gt; today = datetime.date.today()&gt;&gt;&gt; todaydatetime.date(2019, 12, 22)&gt;&gt;&gt; twodays = datetime.timedelta(days=2)&gt;&gt;&gt; print(twodays)2 days, 0:00:00&gt;&gt;&gt; today - twodaysdatetime.date(2019, 12, 20) 时间加减：1234567891011&gt;&gt;&gt; import datetime&gt;&gt;&gt; now = datetime.datetime.now()&gt;&gt;&gt; print(now)2019-12-22 00:35:30.298198&gt;&gt;&gt; delta = datetime.timedelta(days=1, minutes=30)&gt;&gt;&gt; print(delta)1 day, 0:30:00&gt;&gt;&gt; now - deltadatetime.datetime(2019, 12, 21, 0, 5, 30, 298198)&gt;&gt;&gt; print(now - delta)2019-12-21 00:05:30.298198 比较大小12345678910111213141516&gt;&gt;&gt; t1 = datetime.time(12, 30, 20)&gt;&gt;&gt; print(t1)12:30:20&gt;&gt;&gt; t2 = datetime.time(11, 20, 40)&gt;&gt;&gt; print(t2)11:20:40&gt;&gt;&gt; t2 &gt; t1False&gt;&gt;&gt; today = datetime.date.today()&gt;&gt;&gt; print(today)2019-12-22&gt;&gt;&gt; tomorrow = today + datetime.timedelta(days=1)&gt;&gt;&gt; print(tomorrow)2019-12-23&gt;&gt;&gt; today &lt; tomorrowTrue 格式化输出datetime 类中也提供了 strftime 和 strptime 方法，使用示例如下：12345678910&gt;&gt;&gt; import datetime&gt;&gt;&gt; format = "%a %b %d %H:%M:%S %Y"&gt;&gt;&gt; now = datetime.datetime.now()&gt;&gt;&gt; nowdatetime.datetime(2019, 12, 22, 1, 7, 53, 505983)&gt;&gt;&gt; s = now.strftime(format)&gt;&gt;&gt; s'Sun Dec 22 01:07:53 2019'&gt;&gt;&gt; datetime.datetime.strptime(s, format)datetime.datetime(2019, 12, 22, 1, 7, 53) 三、calendar这部分的功能目前还没用到，简单示例如下：12345678910111213141516171819202122232425262728293031&gt;&gt;&gt; import calendar&gt;&gt;&gt; c = calendar.TextCalendar(calendar.SUNDAY)&gt;&gt;&gt; c.prmonth(2019, 12) December 2019Su Mo Tu We Th Fr Sa 1 2 3 4 5 6 7 8 9 10 11 12 13 1415 16 17 18 19 20 2122 23 24 25 26 27 2829 30 31&gt;&gt;&gt; cal = calendar.TextCalendar(calendar.SUNDAY)&gt;&gt;&gt; print(cal.formatyear(2020, 2, 1, 1, 3)) 2020 January February MarchSu Mo Tu We Th Fr Sa Su Mo Tu We Th Fr Sa Su Mo Tu We Th Fr Sa 1 2 3 4 1 1 2 3 4 5 6 7 5 6 7 8 9 10 11 2 3 4 5 6 7 8 8 9 10 11 12 13 1412 13 14 15 16 17 18 9 10 11 12 13 14 15 15 16 17 18 19 20 2119 20 21 22 23 24 25 16 17 18 19 20 21 22 22 23 24 25 26 27 2826 27 28 29 30 31 23 24 25 26 27 28 29 29 30 31 April May JuneSu Mo Tu We Th Fr Sa Su Mo Tu We Th Fr Sa Su Mo Tu We Th Fr Sa 1 2 3 4 1 2 1 2 3 4 5 6 5 6 7 8 9 10 11 3 4 5 6 7 8 9 7 8 9 10 11 12 1312 13 14 15 16 17 18 10 11 12 13 14 15 16 14 15 16 17 18 19 2019 20 21 22 23 24 25 17 18 19 20 21 22 23 21 22 23 24 25 26 2726 27 28 29 30 24 25 26 27 28 29 30 28 29 30 31... 参考资料THE PYTHON 3 STANDARD LIBRARY BY EXAMPLE]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Program</tag>
        <tag>Basic</tag>
        <tag>Datastructure</tag>
        <tag>Datetime</tag>
        <tag>Date</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django REST framework 实现文件上传的两种方式]]></title>
    <url>%2F2019%2F12%2F21%2Ffile-upload-by-django-rest-framework-two-ways%2F</url>
    <content type="text"><![CDATA[一、基于 Django modelsDjango 框架的数据模型（models 类）中定义了 ImageField 和 FileField 等类型的字段，可以用来存储图片或者文件对象。ImageField 和 FileField 针对文件对象的属性和行为封装了易于使用的 API，配合 Django REST framework 提供的一系列组件，可以在编写很少量代码的情况下完成初步的文件上传功能。 各组件代码Models12345from django.db import modelsclass FileModel(models.Model): name = models.CharField(max_length=50) file = models.FileField(upload_to='upload') Serializers12345678from .models import FileModelfrom rest_framework import serializersclass FileSerializer(serializers.ModelSerializer): class Meta: model = FileModel fields = '__all__' Views1234567from rest_framework import viewsetsfrom .models import FileModelfrom .serializers import FileSerializerclass FileViewSet(viewsets.ModelViewSet): queryset = FileModel.objects.all() serializer_class = FileSerializer Urls12345678910from django.urls import path, includefrom &lt;appname&gt;.views import FileViewSetfrom rest_framework import routersrouter = routers.DefaultRouter()router.register(r'upload', FileViewSet)urlpatterns = [ path('', include(router.urls)),] 功能测试使用 HTTPie 工具利用 POST 方法以 Form 表单的形式（-f）提交上传的文件：12345678$ http -f POST http://127.0.0.1:8000/upload/ name="test" file@test.txtHTTP/1.1 201 Created&#123; "file": "http://127.0.0.1:8000/upload/upload/test.txt", "id": 4, "name": "test"&#125; 同时也可以直接访问 http://127.0.0.1:8000/upload/ ，通过 Django REST framework 提供的前端界面手动上传文件。 或者也可以自定义前端界面，HTML 上传页面示例代码：1234567891011121314151617&lt;!DOCTYPE html&gt;&lt;html lang="en"&gt;&lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;Title&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;form action="http://xx.xx.xx.xx:8000/upload/" method="post" enctype="multipart/form-data"&gt; &lt;input type="text" name="name" placeholder="name"&gt;&lt;br&gt; &lt;input type="file" name="file"&gt;&lt;br&gt; &lt;input type="submit" value="submit"&gt; &lt;/form&gt;&lt;/body&gt;&lt;/html&gt; 二、FileField对于 FileField 类型的文件字段，后台的视图代码可以通过 requests.FILES 获取上传的文件数据。如 requests.FILES[&#39;file&#39;]。只有当请求方法为 POST 且前端的 &lt;form&gt; 带有属性 enctype=&quot;multipart/form-data&quot; 时，request.FILES 才能接收到数据，否则为空。 FileField 字段的 upload_to 属性用于指定上传文件的保存位置，以 settings.py 中定义的 MEDIA_ROOT 为路径前缀。upload_to 属性可以接收包含 strftime() 格式的日期字符串（/%Y/%m/%d），用来定义类似 /year/month/day 格式的路径。如：12# 文件上传至类似 MEDIA_ROOT/uploads/2019/12/20 的路径下upload = models.FileField(upload_to='uploads/%Y/%m/%d/') 可以使用 models 提供的查询接口获取已上传文件的相关信息：1234567891011121314$ python manage.py shell(InteractiveConsole)&gt;&gt;&gt; from &lt;appname&gt;.models import FileModel&gt;&gt;&gt; test = FileModel.objects.get(name='test') # 获取某一条数据记录&gt;&gt;&gt; test.file # 数据记录关联的文件对象&lt;FieldFile: upload/test.txt&gt;&gt;&gt;&gt; test.file.name # 文件名'upload/test.txt'&gt;&gt;&gt; test.file.url # 文件 URL'upload/test.txt'&gt;&gt;&gt; test.file.size # 文件大小22&gt;&gt;&gt; test.file.path # 文件路径'/home/starky/program/python/web/django/filestorage/media/upload/test.txt' 通过数据模型检索到的文件（test.file）为 FieldFile 对象。FieldFile 类封装了一些便捷的 API 可以用来操作关联的底层文件，以下是一些简单的示例。 读取文件内容1234567&gt;&gt;&gt; test = FileModel.objects.get(name='test')&gt;&gt;&gt; test.file&lt;FieldFile: upload/test.txt&gt;&gt;&gt;&gt; with test.file.open('rb') as f:... f.read()...b'sdfsdfsdfweofssdnvdvs\n' 写入新的内容123456&gt;&gt;&gt; with test.file.open('wb') as f:... f.write(b'Hello, World')...12&gt;&gt;&gt; test.file.open().read()b'Hello, World' 删除关联的底层文件123&gt;&gt;&gt; test.file.delete()&gt;&gt;&gt; test.file&lt;FieldFile: None&gt; 新建文件语法格式为 FieldFile.save(name, content, save=True)其中 content 参数必须接收 django.core.files.File 类或者其子类的实例，比如 ContentFile，不能直接使用 Python 内置的 file 对象。 不管是删除（FieldFile.delete()）还是新建（FieldFile.save()）文件，save 参数默认都为 True。即自动调用模型实例的 save() 方法提交对数据库的改动。 12345678910&gt;&gt;&gt; testnew = FileModel(name='testnew', file=None)&gt;&gt;&gt; testnew.file&lt;FieldFile: None&gt;&gt;&gt;&gt; from django.core.files.base import ContentFile&gt;&gt;&gt; content = ContentFile('Hello, Django!')&gt;&gt;&gt; testnew.file.save('testnew.txt', content)&gt;&gt;&gt; testnew.file&lt;FieldFile: upload/testnew.txt&gt;&gt;&gt;&gt; testnew.file.open().read()b'Hello, Django!' 三、FileUploadParserDjango REST framework 提供了 parsers.FileUploadParser 类，可以用来处理原始格式的文件内容的上传。后端获取到的 request.data 为字典结构，其中包含的 &#39;file&#39; 键对应的值即为上传的文件对象。 如果 FileUploadParser 类被包含 filename 参数的 URL 调用，则该参数会作为文件保存到服务端后的文件名。若 URL 中不包含 filename 参数，则客户端发起的请求必须包含 Content-Disposition 请求头及 filename 参数。如 Content-Disposition: attachment; filename=upload.jpg。 示例代码12345678910111213141516# views.pyfrom rest_framework.views import APIViewfrom rest_framework.response import Responsefrom rest_framework.parsers import FileUploadParserclass FileUploadView(APIView): parser_classes = [FileUploadParser, ] def put(self, request, filename, format=None): file_obj = request.data['file'] with open(filename, 'wb') as f: for chunk in file_obj.chunks(): f.write(chunk) return Response(f'&#123;filename&#125; uploaded',status=204) 123456# urls.pyfrom django.urls import re_pathurlpatterns = [ re_path(r'^files/(?P&lt;filename&gt;[^/]+)$', views.FileUploadView.as_view()),] 上传接口的 URL 为 http://xx.xx.xx.xx/files/&lt;filename&gt; ，其中 &lt;filenmae&gt; 用于指定上传成功后在服务器端的文件名。客户端使用 PUT 请求上传文件。 使用 postman 测试文件上传，截图如下： 前端上传代码示例如下（使用 jQuery，有可能出现跨域问题，可参考网上资料解决）：12345678910111213141516171819202122232425262728293031323334&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;文件上传&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;form&gt; &lt;p&gt;上传文件： &lt;input type="file" name="files" id='files' /&gt;&lt;/p&gt; &lt;input type="button" value="上传" onclick="doUpload()" /&gt; &lt;/form&gt; &lt;script src="https://cdn.bootcss.com/jquery/3.4.1/jquery.js"&gt;&lt;/script&gt; &lt;script type="text/javascript"&gt; function doUpload() &#123; $.ajax(&#123; url: 'http://xx.xx.xx.xx:8000/files/test.jpg', type: 'PUT', data: $('#files')[0].files[0], cache: false, processData: false, contentType: false, async: false &#125;).done(function (res) &#123; alert("上传成功") &#125;).fail(function (res) &#123; alert("上传失败：" + res) &#125;); &#125; &lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 参考资料Managing filesmodels.FileFieldparsers]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Program</tag>
        <tag>Development</tag>
        <tag>Web</tag>
        <tag>Django</tag>
        <tag>REST</tag>
        <tag>API</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[高效 Python 代码——类与继承]]></title>
    <url>%2F2019%2F12%2F14%2Feffective-python-class-and-inherit%2F</url>
    <content type="text"><![CDATA[一、用辅助类（而不是字典）来维护程序的状态Python 内置的字典类型可以很好地保存某个对象在其生命周期中的（动态）内部状态。如下面的成绩单类：12345678910111213141516171819202122class SimpleGradebook(object): def __init__(self): self._grades = &#123;&#125; def add_student(self, name): self._grades[name] = [] def report_grade(self, name, score): self._grades[name].append(score) def average_grade(self, name): grades = self._grades[name] return sum(grades) / len(grades)book = SimpleGradebook()book.add_student('Isaac Newton')book.report_grade('Isaac Newton', 90)book.report_grade('Isaac Newton', 80)print(book.average_grade('Isaac Newton'))# =&gt; 85.0 在上面的 SimpleGradebook 类中，学生名字及其对应的成绩都保存在 _grades 字典结构中，这样就不必把每个学生都表示成对象并预设一个用于存放名字的属性了。 字典类型用起来方便，但也容易因为过度使用导致一些问题。如果需要扩充上面成绩单类的功能，把学生成绩按照科目保存。则 _grades 字典中需要嵌入另一个字典存储科目与多次成绩的键值对。即类似这样的结构：{&#39;Einstein&#39;: {&#39;Math&#39;: [80, 90]}}123456789101112131415161718192021222324252627282930class BySubjectGradebook(object): def __init__(self): self._grades = &#123;&#125; def add_student(self, name): self._grades[name] = &#123;&#125; def report_grade(self, name, subject, grade): by_subject = self._grades[name] grade_list = by_subject.setdefault(subject, []) grade_list.append(grade) def average_grade(self, name): by_subject = self._grades[name] total, count = 0, 0 for grades in by_subject.values(): total += sum(grades) count += len(grades) return total / countbook = BySubjectGradebook()book.add_student('Albert Einstein')book.report_grade('Albert Einstein', 'Math', 80)book.report_grade('Albert Einstein', 'Math', 90)book.report_grade('Albert Einstein', 'Gym', 70)book.report_grade('Albert Einstein', 'Gym', 80)print(book.average_grade('Albert Einstein'))# =&gt; 80.0 假设需求再次改变，在记录某个分数的同时，还需要记录该次成绩占该科目历次成绩的权重。。。此时用于保存成绩的数据结构可以改成这样：{&#39;Einstein&#39;: {&#39;Math&#39;: [(80, 0.4), (90, 0.6)]}}但是对于新的 average_grade 方法来说，处理上述数据记录的代码就比较难以理解了。 把嵌套结构重构为类用来保存程序状态的数据结构一旦过于复杂（如包含多层嵌套），则应该将其拆解为类，提供更为明确的接口，同时更好的封装数据。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657from collections import namedtupleGrade = namedtuple('Grade', ('score', 'weight'))# 科目类。_grades 属性用于保存带权重的分数（Grade()）对象# average_grade 方法用于按权重计算本科成绩的平均分class Subject(object): def __init__(self): self._grades = [] def report_grade(self, score, weight): self._grades.append(Grade(score, weight)) def average_grade(self): total, total_weight = 0, 0 for grade in self._grades: total += grade.score * grade.weight total_weight += grade.weight return total / total_weight# 学生类。_subjects 属性用于保存该学生的所有科目（Subject()）对象# average_grade 方法用于计算该学生所有科目成绩的平均分class Student(object): def __init__(self): self._subjects = &#123;&#125; def subject(self, name): if name not in self._subjects: self._subjects[name] = Subject() return self._subjects[name] def average_grade(self): total, count = 0, 0 for subject in self._subjects.values(): total += subject.average_grade() count += 1 return total / count# 成绩单类。_students 属性保存所有的学生（Student()）对象class Gradebook(object): def __init__(self): self._students = &#123;&#125; def student(self, name): if name not in self._students: self._students[name] = Student() return self._students[name]book = Gradebook()albert = book.student('Albert Einstein')math = albert.subject('Math')math.report_grade(80, 0.4)math.report_grade(90, 0.6)print(albert.average_grade())# =&gt; 86.0 要点 尽量不使用多层嵌套的字典（如包含其他字典的字典）存储程序状态，也不要使用过长的元组 容器中包含简单又不可变的数据，可以先使用 namedtuple 表示，有需要时再改为完整的类 用于保存程序内部状态的字典若过于复杂，应将其拆解为多个辅助类 二、用 super 初始化父类初始化父类的传统方式，是在子类里直接调用父类的 __init__ 方法：1234567class MyBaseClass(object): def __init__(self, value): self.value = valueclass MyChildClass(MyBaseClass): def __init__(self): MyBaseClass.__init__(self, 5) 上述方法对于简单的继承行为是可行的，但是很多情况下仍会出现问题。首先若子类受到多重继承的影响，则直接调用父类的 __init__ 方法会产生无法预知的行为（调用顺序不确定）。1234567891011121314151617181920212223242526272829class MyBaseClass(object): def __init__(self, value): self.value = valueclass TimesTwo(object): def __init__(self): self.value *= 2class PlusFive(object): def __init__(self): self.value += 5class OneWay(MyBaseClass, TimesTwo, PlusFive): def __init__(self, value): MyBaseClass.__init__(self, value) TimesTwo.__init__(self) PlusFive.__init__(self)class AnotherWay(MyBaseClass, TimesTwo, PlusFive): def __init__(self, value): MyBaseClass.__init__(self, value) PlusFive.__init__(self) TimesTwo.__init__(self)foo = OneWay(5)print(f"First ordering is (5 * 2) + 5 = &#123;foo.value&#125;")bar = AnotherWay(5)print(f"Second ordering still is &#123;foo.value&#125;") OneWay 与 AnotherWay 中定义了两种完全不同的调用父类 __init__ 方法的顺序，但实际执行的结果却是相同的，导致子类代码中定义的调用顺序与子类实际产生的行为不一致。 此外在菱形继承中，直接调用父类的构造器也会出现问题。菱形继承是指子类继承自两个单独的超类，而这两个父类又都继承自同一个公共基类。这种继承方式会使菱形顶部的公共基类多次执行其 __init__ 方法，产生意想不到的行为。12345678910111213141516171819202122class MyBaseClass(object): def __init__(self, value): self.value = valueclass TimesFive(MyBaseClass): def __init__(self, value): MyBaseClass.__init__(self, value) self.value *= 5class PlusTwo(MyBaseClass): def __init__(self, value): MyBaseClass.__init__(self, value) self.value += 2class ThisWay(TimesFive, PlusTwo): def __init__(self, value): TimesFive.__init__(self, value) PlusTwo.__init__(self, value)foo = ThisWay(5)print(f"Should be (5 * 5) + 2 = 27 but actully is &#123;foo.value&#125;")# =&gt; Should be (5 * 5) + 2 = 27 but actully is 7 最终结果为 7，原因是在调用第二个父类的构造器（即 PlusTwo.__init__）时，公共基类的构造器（即 MyBaseClass.__init__）会再次被调用导致 value 重新变成 5，而不能保持 TimesFive.__init__ 之后的 25。 通过 super 调用父类的构造器：123456789101112131415161718192021class MyBaseClass(object): def __init__(self, value): self.value = valueclass TimesFive(MyBaseClass): def __init__(self, value): super(__class__, self).__init__(value) self.value *= 5class PlusTwo(MyBaseClass): def __init__(self, value): super(__class__, self).__init__(value) self.value += 2class ThisWay(TimesFive, PlusTwo): def __init__(self, value): super(__class__, self).__init__(value)foo = ThisWay(5)print(f"Should be (5 * 5) + 2 = 27 but actully is &#123;foo.value&#125;")# =&gt; Should be (5 * 5) + 2 = 27 but actully is 35 此时程序的行为可以说和预期相符合了，注意后三个类中 super 的使用。其中传入 super 的第一个参数 __class__ 用来指代当前类本身。 三、只在使用 Mix-in 制作工具类时进行多重继承应在 Python 编程中尽量避开多重继承。若一定要利用多重继承带来的便捷及封装性，应考虑编写 mix-in 类。mix-in 是一种“小型”类，其中只定义了其他类可能需要提供的一套附加方法，但是不定义自身的实例属性，也不要求继承者调用自己的 __init__ 构造器。 可以在 mix-in 里面通过动态检测机制编写一套通用的功能代码，根据对各类对象当前状态的判定，确定代码实际的行为。从而将 mix-in 应用到多个不同的类上面。分层地组合 mix-in 类可以减少重复代码并提升代码复用程度。如需要将内存中的 Python 对象转换为字典结构（即通常所说的序列化操作），可以创建下面的 mix-in 类实现此功能并添加 public 方法供其他类继承。重点在于通过 hasattr 动态地访问实例的属性、用 isinstance 动态地检测对象类型、用 __dict__ 访问实例内部的字典。12345678910111213141516171819202122232425262728293031323334353637# todict.pyclass ToDictMixin(object): def to_dict(self): return self._traverse_dict(self.__dict__) def _traverse_dict(self, instance_dict): output = &#123;&#125; for key, value in instance_dict.items(): output[key] = self._traverse(key, value) return output def _traverse(self, key, value): if isinstance(value, ToDictMixin): return value.to_dict() elif isinstance(value, dict): return self._traverse_dict(value) elif isinstance(value, list): return [self._traverse(key, i) for i in value] elif hasattr(value, '__dict__'): return self._traverse_dict(value.__dict__) else: return value# 利用 mix-in 将二叉树表示为字典class BinaryTree(ToDictMixin): def __init__(self, value, left=None, right=None): self.value = value self.left = left self.right = rightif __name__ == '__main__': tree = BinaryTree(10, left=BinaryTree(7, right=BinaryTree(9)), right=BinaryTree(13, left=BinaryTree(11))) print(tree.to_dict())# =&gt; &#123;'value': 10, 'left': &#123;'value': 7, 'left': None, 'right': &#123;'value': 9, 'left': None, 'right': None&#125;&#125;, 'right': &#123;'value': 13, 'left': &#123;'value': 11, 'left': None, 'right': None&#125;, 'right': None&#125;&#125; mix-in 最大的优势在于，可以随时向基类中添加额外的通用功能，并且在必要时覆盖重写某些方法。多个 mix-in 之间也可以相互组合。在前面 todict.py 代码的基础上，可以再定义一个 JsonMixin 用来为任意类提供通用的 JSON 序列化功能。JsonMixin 的定义代码决定了继承自它的类需要包含 to_dict 方法（比如可以从 ToDictMixin 中继承），且其 __init__ 方法接受关键字参数：123456789101112131415161718192021222324252627282930313233343536373839404142import jsonfrom todict import ToDictMixinclass JsonMixin(object): @classmethod def from_json(cls, data): kwargs = json.loads(data) return cls(**kwargs) def to_json(self): return json.dumps(self.to_dict())class DatacenterRack(ToDictMixin, JsonMixin): def __init__(self, switch=None, machines=None): self.switch = Switch(**switch) self.machines = [ Machine(**kwargs) for kwargs in machines ]class Switch(ToDictMixin, JsonMixin): def __init__(self, ports=None, speed=None): self.ports = ports self.speed = speedclass Machine(ToDictMixin, JsonMixin): def __init__(self, cores=None, ram=None, disk=None): self.cores = cores self.ram = ram self.disk = diskserialized = """&#123; "switch": &#123;"ports": 5, "speed": 1e9&#125;, "machines": [ &#123;"cores": 8, "ram": 32e9, "disk": 5e12&#125;, &#123;"cores": 4, "ram": 16e9, "disk": 1e12&#125;, &#123;"cores": 2, "ram": 4e9, "disk": 500e9&#125; ]&#125;"""deserialized = DatacenterRack.from_json(serialized)roundtrip = deserialized.to_json()assert json.loads(serialized) == json.loads(roundtrip) ToDictMixin 和 ToJsonMixin 两个 mix-in 中分别定义了不同的通用功能组件，符合规范的多重继承下的子类则可以直接使用这两者提供的 to_dict 和 to_json 方法，达到功能整合的效果。 要点 能用 mix-in 组件实现的效果，就不用多重继承来做 将各功能实现为可插拔的 mix-in 组件，让子类选择继承需要的组件 简单行为封装到 mix-in 组件里，多个 mix-in 组合成复杂行为 四、多使用 public 属性Python 类的属性有 public 和 private 两种，任何人都可以在对象上通过 dot 操作符（.）访问 public 属性。private 属性是名称中以两个下划线开头的属性，可以被当前类中的方法访问。但从类外部直接访问 private 属性会报 AttributeError 异常。12345678910111213141516class MyObject(object): def __init__(self): self.public_field = 5 self.__private_field = 10 def get_private_field(self): return self.__private_fieldfoo = MyObject()print(foo.public_field)# =&gt; 5print(foo.get_private_field())# =&gt; 10print(foo.__private_field)# =&gt; AttributeError: 'MyObject' object has no attribute '__private_field' 类方法可以访问当前类的私有属性。子类无法访问父类的私有字段。1234567891011121314151617181920class MyObject(object): def __init__(self): self.__private_field = 71 @classmethod def get_private_field(cls, instance): return instance.__private_fieldclass MyChildObject(MyObject): def get_private_field(self): return self.__private_fieldbar = MyObject()print(MyObject.get_private_field(bar))# =&gt; 71bar_child = MyChildObject()print(bar_child.get_private_field())# =&gt; AttributeError: 'MyChildObject' object has no attribute '_MyChildObject__private_field' Python 会对私有属性的名称做一些简单的变换，这种变换导致了私有属性对类外部不可见，同时子类也无法访问父类的私有属性。也就是说，Python 并没有从语法上严格保证 private 字段的私密性。在子类的继承体系发生变化时，对 private 字段的引用很容易失效，从而导致子类出现错误。 为了尽量减少无意义的访问内部属性导致的意外，Python 习惯用单下划线开头的字段表示受保护（protected）字段，当前类之外的代码使用这些字段时应格外注意。一般来说，宁可让子类更多地访问父类的 protected 属性，也尽量不要把这些属性设置成 private。并且应该在文档中说明每个 protected 字段的含义，在扩展代码时如何保证数据安全。123456class MyClass(object): def __init__(self, value): # This stores the user-supplied value for the object. # It should be coercible to a string. Once assigned for # the object it should be treated as immutable. self._value = value 要点 Python 编译器无法严格保证 private 字段的私密性 应多使用 protected 属性，并将合理用法在文档中说明 只有子类不受自己控制时，为避免命名冲突才考虑使用 private 属性 五、继承 collections.abc 以实现自定义的容器类型大部分的 Python 编程都是在定义类，类可以包含数据且能够描述数据与对象之间的交互方式。Python 中的类从某种程度上说都是封装了属性与功能容器。 如果要设计功能比较简单的序列，可以直接继承 Python 内置的 list 类型。如创建一种可以统计各元素出现频率的自定义列表：12345678910111213141516class FrequencyList(list): def __init__(self, members): super().__init__(members) def frequency(self): counts = &#123;&#125; for item in self: counts.setdefault(item, 0) counts[item] += 1 return countsfoo = FrequencyList(['a', 'b', 'a', 'c', 'b', 'a', 'd'])print(f'Length is &#123;len(foo)&#125;')print(f'Frequency: ', foo.frequency())# =&gt; Length is 7# =&gt; Frequency: &#123;'a': 3, 'b': 2, 'c': 1, 'd': 1&#125; 假设某对象本身并不是 list 类型的子类，但是需要它表现得像 list 一样，可以通过下标访问其元素。Python 用一些名称比较特殊的实例方法来实现与容器有关的行为。如需要用下标访问序列中的元素，可以考虑实现序列对象的 __getitem__ 方法：12345&gt;&gt;&gt; bar = [1, 2, 3]&gt;&gt;&gt; bar[0]1&gt;&gt;&gt; bar.__getitem__(0)1 如下面的二叉树类实现了 __getitem__ 方法，使得不仅可以按深度优先的次序遍历（_traverse()）二叉树中的对象，还可以通过下标访问：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253# binarynode.pyclass BinaryNode(object): def __init__(self, value, left=None, right=None): self.value = value self.left = left self.right = rightclass IndexableNode(BinaryNode): def _traverse(self): if self.left is not None: yield from self.left._traverse() yield self if self.right is not None: yield from self.right._traverse() def __getitem__(self, index): for i, item in enumerate(self._traverse()): if i == index: return item.value raise IndexError(f'Index &#123;index&#125; is out of range')class SequenceNode(IndexableNode): def __len__(self): for count, _ in enumerate(self._traverse(), 1): pass return countif __name__ == '__main__': tree = SequenceNode( 10, left=SequenceNode( 5, left=SequenceNode(2), right=SequenceNode( 6, right=SequenceNode(7))), right=SequenceNode( 15, left=SequenceNode(11))) print('LRR is', tree.left.right.right.value) # =&gt; LRR is 7 print('Index 0 is', tree[0]) # =&gt; Index 0 is 2 print('11 in the tree?', 11 in tree) # =&gt; 11 in the tree? True print('Tree is', list(tree)) # =&gt; Tree is [2, 5, 6, 7, 10, 11, 15] print('Tree length is', len(tree)) # =&gt; Tree length is 7 为了使序列可以通过内置的 len() 函数获取长度，SequenceNode 类中还实现了 __len__ 方法。而更多的功能就意味着需要额外实现更多的特殊方法。为了避免这些麻烦，可以使用内置的 collections.abc 模块。此模块定义了一系列抽象基类，提供了每一种容器类型所应具备的常用方法。继承这样的基类，如果忘记实现某个方法，collections.abc 模块会报错；如果实现了抽象基类要求的每一个方法，则基类会自动实现剩下的所有方法。1234567891011121314151617181920212223from binarynode import SequenceNodefrom collections.abc import Sequenceclass BetterNode(SequenceNode, Sequence): passtree = BetterNode( 10, left=BetterNode( 5, left=BetterNode(2), right=BetterNode( 6, right=BetterNode(7))), right=BetterNode( 15, left=BetterNode(11)))print('Index of 7 is', tree.index(7))# =&gt; Index of 7 is 3print('Count of 10 is', tree.count(10))# =&gt; Count of 10 is 1 SequenceNode 中已经实现了 Sequence 要求的 __getitem__ 和 __len__ 方法，因此 Sequence 基类为继承的 BetterNode 子类自动实现了 index() 和 count() 方法。 要点 如自定义的容器子类比较简单，可直接继承 Python 内置的容器类型（如 list、dict 等） 正确实现自定义容器类型可能需要编写大量的特殊方法 编写自定义容器类型时，可以从 collections.abc 模块中的抽象基类继承，这些基类能保证子类具备统一的接口及行为 参考资料Effective Python]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>OOP</tag>
        <tag>Class</tag>
        <tag>Advanced</tag>
        <tag>Inherit</tag>
        <tag>Effective</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Vim 进阶——按键映射与 VimScript 脚本编程]]></title>
    <url>%2F2019%2F12%2F10%2Fvim-key-binding-and-vimscript-programming%2F</url>
    <content type="text"><![CDATA[一、按键映射Vim 中的快捷键绑定可以通过以下命令配置： :imap：只在 Insert 模式下生效的快捷键 :cmap：只在 Command-line 模式下生效 :nmap：只在 Normal 模式下生效 :vmap：只在 Visual 模式下生效 :map：在以上所有模式下生效 :noremap：包含 :inoremap、:nnoremap 等，非递归映射 PS：关于递归映射，如 :map a b，:map b c。根据按键映射之间的传递，则有 a -&gt; c 的关系。而 nore 则用于禁止这种递归行为。 做快捷键映射时，各功能按键在 Vim 中的名称如下： 名称 对应按键 &lt;BS&gt; 退格键 &lt;Tab&gt; 制表键 &lt;CR&gt;或&lt;Enter&gt;或&lt;Return&gt; 回车 &lt;Esc&gt; Escape &lt;Space&gt; 空格键 &lt;Up&gt; 上方向键 &lt;Down&gt; 下方向键 &lt;Left&gt; 左方向键 &lt;Right&gt; 右方向键 &lt;F1&gt; - &lt;F12&gt; 功能键 F1 到 F12 #1, #2..#9,#0 F1 到 F10 &lt;Insert&gt; Insert &lt;Del&gt; Delete &lt;Home&gt; Home &lt;End&gt; End &lt;PageUp&gt; 上翻页 &lt;PageDown&gt; 下翻页 示例12345678" save file (ctrl-s):map &lt;C-s&gt; :w&lt;cr&gt;" copy selected text (ctrl-c):vmap &lt;C-c&gt; y" Paste clipboard contents (ctrl-v):imap &lt;C-v&gt; &lt;esc&gt;P" cut selected text (ctrl-x):vmap &lt;C-x&gt; x :map &lt;C-s&gt; :w&lt;cr&gt; 表示将 Ctrl+S 组合键映射为 :w&lt;cr&gt;（保存修改，写入文件）其中 &lt;C-s&gt; 在 Vim 中即表示 Ctrl+S，类似的用法还有 &lt;A-s&gt; （Alt+S）、&lt;M-s&gt; （Meta+S）、&lt;C-S-s&gt;（Ctrl+Shift+S）等。:w 后面的 &lt;cr&gt; 表示按下回车键。如命令最后没有加上 &lt;cr&gt;，则按下组合键 &lt;C-s&gt; 后只会将对应的 :w 输入到命令栏而不执行。 上面映射的保存文件功能还可以更加细化一些：:imap &lt;C-s&gt; &lt;esc&gt;:w&lt;cr&gt;a即该组合键适用于插入模式，按下 Ctrl+S 意味着会依照如下顺序执行命令： &lt;esc&gt;：退出插入模式 :w&lt;cr&gt;：将之前的修改写入文件 a：回到插入模式继续编辑文件 对于 Gvim 下的按键映射，还可以调出对话框完成 Open 和 save-as 功能：1234"Open new file dialog (ctrl-n):map &lt;C-n&gt; :browse confirm e&lt;cr&gt;"Open save-as dialog (ctrl-shift-s):map &lt;C-S-s&gt; :browse confirm saveas&lt;cr&gt; 对于依次按下多个按键的组合键，如：:map $1 :MyFunction1()&lt;cr&gt;当按下 $ 键后，Vim 会等待一秒钟，若一秒钟之内又按下了 1 键，则执行映射的 MyFunction1() 函数；若一秒钟之内没有任何按键按下，则执行 $ 键原本的功能（移动光标到行尾）。 二、Vim 脚本Vim 脚本是指用 VimScript 编写的为 Vim 添加自定义功能的单独的文件。如下面的 hello.vim 脚本：12345678" hello.vimfunction! SayHello() echo 'Hello, World!'endfunctioncommand! Hello call SayHello()nnoremap Q :Hello&lt;CR&gt; 其中 function! 用于定义 SayHello() 函数，command! 用于将调用该函数的行为绑定给 Hello 命令，nnoremap Q 则用于将 :Hello 命令的执行绑定给键盘上的 Q 按键。 在 Vim 中使用 :source 命令导入刚刚创建的脚本：:source hello.vim之后执行 :Hello 命令或者按下 Q 按键即可在命令栏输出 Hello, World! 字符串。 变量变量的赋值使用 :let 命令：1234:let mystring = "a string":let mynumber = 123:let mylist = [1, 2, "three", 0x04 ,["five", "six"]]:let mydict = &#123;1: "one", 2: "two", "others": &#123;3: "three", 4: "four"&#125;&#125; Vim 脚本中支持以下类型的变量： String：字符串，如 &quot;this is a string&quot; Number：数字，包含十进制（123）、十六进制（0x8A）和八进制（012） List：有序列表，列表项支持多种数据类型混合 Dictionary：字典，无序键值对 Funcref：函数引用 PS：不同进制的数字之间可以直接进行算术运算由单引号（&#39;）包裹的字符串不会转义 \n 等转义字符 关于字符串和数字之间的自动类型转换，参考以下规则： Input (type) Result (type) “hello” . “world” “hello world” (string) “number” . 123 “number 123” (string) “123” + 10 133 (number) “123” - 10 . “hits” “113 hits” (string) “123” - 10 + “hits” 113 (number) 变量作用域Vim 中变量的作用域通过不同的前缀来指定： v：Vim 预定义的全局作用域 g：全局变量前缀 b：Buffer 范围内生效的变量 t：Tab 范围内生效的变量 w：Window 范围内生效的变量 l：即 local，Function 范围内生效 s：即 source，通过 :source 载入的当前脚本内生效 a：即 argument，用来修饰函数的参数 当变量没有任何作用域前缀修饰时，默认为全局变量（除非该变量在函数内部定义）。 变量作用域前缀的使用参考如下脚本：1234567891011let g:sum = 10function SumNumbers(num1, num2) let l:sum = a:num1 + a:num2 if g:sum &lt; l:sum let g:sum = l:sum endif return l:sumendfunctionecho SumNumbers(3, 4)echo g:sum 条件语句Vim 脚本中的 if-else 语句语法格式如下：12345if condition1 code-to-execute-if-condition1-is-trueelseif condition2 code-to-execute-if-condition2-is-trueendif 其中判断条件 condition 可以有以下几种形式： val1 == val2 val1 != val2 val1 &gt; val1 val1 &lt; val2 val1 &lt;= val2 val1 &gt;= val2 str1 =~ str2 str1 !~ str2 字符串比较中的 str2 可以是某个模式，支持正则表达式。 if-else 语句的使用可以参考如下脚本，根据当前时间切换不同的配色：1234567891011121314151617" note addition of zero" this guarantees return from function is numericlet currentHour = strftime ("%H")echo "currentHour is " currentHourif currentHour &lt; 6 + 0 colorscheme darkblue echo "setting colorscheme to darkblue"elseif currentHour &lt; 12 + 0 colorscheme morning echo "setting colorscheme to morning"elseif currentHour &lt; 18 + 0 colorscheme shine echo "setting colorscheme to shine"else colorscheme evening echo "setting colorscheme to evening"endif 循环For 循环的语法格式如下：123for var in range do-somethingendfor 或123for var in list do-somthingendfor 代码示例12345678910111213141516" rangefor myvar in range(1,10) echo myvarendfor" listlet mylist = ['a','b','c','d','e','f','g','h','i','j','k']for itemvar in mylist echo itemvarendfor" dictionarylet mydict = &#123;"a": "apple", "b":"banana", "c": "citrus" &#125;for keyvar in keys(mydict) echo mydict[keyvar]endfor While 循环的语法格式：123while condition execute-this-codeendwhile 代码示例：12345let x=0while x &lt;= 5 echo "x is now " x let x+=1endwhile 列表与字典关于 Vim 中列表与字典的操作，可以参考如下代码：123456789101112131415161718192021222324252627282930let mylist = [1, 2, "three"]echo mylist[2]" =&gt; threelet mylist2 = [[1, 2, 3], ["four", "five", "six"]]echo mylist2[1][0]" =&gt; fourecho mylist2[0][-1]" =&gt; 3let mylist3 = [1, 2, 3, 4]call add(mylist3, [5, 6])echo mylist3" =&gt; [1, 2, 3, 4, [5, 6]]let mylist4 = [1, 2, 3, 4]let mylist4 = mylist4 + [5, 6]echo mylist4" =&gt; [1, 2, 3, 4, 5, 6]let mylist5 = [1, 2, 3, 4]call remove(mylist5, 3)echo mylist5" =&gt; [1, 2, 3]let mydict = &#123;'banana': 'yellow', 'apple': 'green'&#125;echo mydict['banana']" =&gt; yellowecho mydict.apple" =&gt; green get map split join 可以对列表或字典中的值应用某个函数（如 join 和 map 等）以完成特定的需求，常见示例如下：12345678910111213141516171819let a = split("one two")echo a" =&gt; onelet mylist = ["one", "two", "three"]call map(mylist, '"&lt;" . v:val . "&gt;"')echo mylist" =&gt; ['&lt;one&gt;', '&lt;two&gt;', '&lt;three&gt;']let mylist2 = ["one", "two", "three"]echo get(mylist2, 2, "none")" =&gt; threeecho get(mylist2, 3, "none")" =&gt; nonelet mylist3 = ["one", "two", "three"]let mystring = join(mylist3, "+")echo mystring" =&gt; one+two+three 更复杂的示例（好神奇，没看懂。。。）：12345678let mynumbers = &#123;0:'zero', 1:'one', 2:'two', 3:'three', 4:'four', 5:'five', 6:'six', 7:'seven', 8:'eight', 9:'nine'&#125;function mynumbers.convert(numb) dict return join(map(split(a:numb, '\zs'), 'get(self, v:val, "unknown")'))endfunctionecho mynumbers.convert(12345)" =&gt; one two three four five 函数Vim 中定义函数的语法如下：123function Name(arg1, arg2,...argN) keyword code-to-execute-when-function-is-calledendfunction 所有在函数体中定义的变量只在该函数内部可见，即作用域为 local。如果需要使用函数外部的变量，可以将其作为参数传递给函数，或者直接调用（需要在该变量名前加上全局作用域前缀 g:）。函数定义代码中使用由参数传递的变量时，需要加上 a: 作用域前缀。 参数列表 参考如下代码：12345678910111213function PrintSum(num1, num2, ...) let sum = a:num1 + a:num2 let argnum = 1 while argnum &lt;= a:0 let sum += a:&#123;argnum&#125; let argnum += 1 endwhile echo "The sum is" sum return sumendfunctionlet sum = PrintSum(1, 2, 3, 4)" =&gt; The sum is 10 注意代码中 a:0（不定参数的长度）和 a:{argnum}（第 argnum 个额外参数）的用法。 此外还可以通过 a:000 以列表的方式获取所有额外的参数：1234567891011function PrintSum(num1, num2, ...) let sum = a:num1 + a:num2 for arg in a:000 let sum += arg endfor echo "The sum is" sum return sumendfunctionlet sum = PrintSum(1, 2, 3, 4)" =&gt; The sum is 10 综合示例根据当前时间自动切换 Vim 配色的脚本：12345678910let g:Favcolorschemes = ["darkblue", "morning", "shine", "evening"]function SetTimeOfDayColors() " currentHour will be 0, 1, 2 or 3 let CurrentHour = (strftime("%H") + 0) / 6 execute "colorscheme " . g:Favcolorschemes[CurrentHour] echo "set color scheme to " . g:Favcolorschemes[CurrentHour]endfunctioncall SetTimeOfDayColors() 三、AutocommandsAutocommands 即在特定条件下自动执行的命令，这些命令包括所有合法的 Vim 命令。Vim 定义了一些事件（event）作为触发命令自动执行的开关，常见的 event 如下： BufNewFile：在开始编辑一个新的文件时触发 BufReadPre：在 Vim 移动到一个新的 buffer 前触发 BufRead, BufReadPost：开始编辑新的 buffer 时，读取文件之后触发 BufWrite, BufWritePre：在将 buffer 内容写入文件之前触发 FileType：在确定了文件类型（filetype）之后触发 VimResized：更改 Vim 的窗口大小后触发 WinEnter, WinLeave：进入或离开某个 Vim 窗口时触发 CursorMoved, CursorMovedI：Normal 或 Insert 模式下，光标移动后触发 Autocommands 代码示例12345augroup demo autocmd! autocmd BufReadPost * echo 'Reading: ' . expand('&lt;afile&gt;') autocmd BufWritePost * echo 'Writing: ' . expand('&lt;afile&gt;')augroup END 上述脚本会添加如下功能：打开文件时 Vim 命令栏输出 “Reading: “；在使用 :w 等命令保存文件时，命令栏输出 “Writing: “。 又如根据源文件类型设置不同的缩进风格：123filetype onautocmd FileType ruby setlocal tabstop=2 softtabstop=2 shiftwidth=2 expandtabautocmd FileType javascript setlocal ts=4 sts=4 sw=4 noet 参考资料Hacking Vim 7.2Modern Vim]]></content>
      <categories>
        <category>Tools</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Tools</tag>
        <tag>Program</tag>
        <tag>Vim</tag>
        <tag>Script</tag>
        <tag>VimScript</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python3 中作为一等对象的函数]]></title>
    <url>%2F2019%2F12%2F10%2Ffirst-class-function-python-3-functional-programming%2F</url>
    <content type="text"><![CDATA[在 Python 语言中，函数与整数、字符串、字典等基本数据类型一样，都是一等对象。所谓一等对象，即满足如下三个条件： 在运行时创建 能赋值给变量 能作为函数的参数或返回值 以下 IDLE 中的代码即在运行时创建了函数 factorial：1234567891011121314151617&gt;&gt;&gt; def factorial(n):... '''calculates n!'''... return 1 if n &lt; 2 else n * factorial(n-1)...&gt;&gt;&gt; factorial(5)120&gt;&gt;&gt; factorial.__doc__'calculates n!'&gt;&gt;&gt; type(factorial)&lt;class 'function'&gt;&gt;&gt;&gt; fact = factorial&gt;&gt;&gt; fact&lt;function factorial at 0x7f55bc771c10&gt;&gt;&gt;&gt; fact(5)120&gt;&gt;&gt; list(map(fact, range(10)))[1, 1, 2, 6, 24, 120, 720, 5040, 40320, 362880] 从输出中可以看出，factorial 是 function 类的实例对象，__doc__ 是 factorial 对象众多属性中的一个。可以把 factorial 函数赋值给变量 fact，通过 fact 变量调用 factorial 函数。还可以把 factorial 作为参数传递给 map 函数。这些行为表现了函数作为一等对象的特性。 一、高阶函数接受函数作为参数，或者把函数作为返回值的函数即为高阶函数。如内置用于排序的 sorted 函数，它的 key 参数用于传入一个函数，在需要排序的每个元素上执行特定的操作。如根据单词长度对多个字符串进行排序：123&gt;&gt;&gt; fruits = ['strawberry', 'fig', 'apple', 'cherry', 'raspberry', 'banana']&gt;&gt;&gt; sorted(fruits, key=len)['fig', 'apple', 'cherry', 'banana', 'raspberry', 'strawberry'] 任何单参数的函数都可以作为 key 的值传给 sorted 函数，如把单词反向拼写作为排序条件：1234567&gt;&gt;&gt; def reverse(word):... return word[::-1]...&gt;&gt;&gt; reverse('test')'tset'&gt;&gt;&gt; sorted(fruits, key=reverse)['banana', 'apple', 'fig', 'raspberry', 'strawberry', 'cherry'] map、filter 与 reduce函数式编程语言通常会提供 map、filter 和 reduce 三个高阶函数或者实现了类似功能的函数。Python3 中的列表推导和生成器即具有 map 和 filter 函数的功能。参考如下示例：1234567891011&gt;&gt;&gt; def fact(n):... return 1 if n &lt; 2 else n * fact(n-1)...&gt;&gt;&gt; list(map(fact, range(6)))[1, 1, 2, 6, 24, 120]&gt;&gt;&gt; [fact(n) for n in range(6)][1, 1, 2, 6, 24, 120]&gt;&gt;&gt; list(map(fact, filter(lambda n: n % 2, range(6))))[1, 6, 120]&gt;&gt;&gt; [fact(n) for n in range(6) if n % 2][1, 6, 120] 通过列表推导可以完成与 map 或 filter 函数类似的工作，且可读性更高，也避免了使用 lambda 表达式。 reduce 在 Python2 中是内置函数，但在 Python3 中被移到了 functools 模块中。reduce 可以把某个操作连续地应用到某个序列上，累计所有的结果，把产生的一系列值规约成一个值。因此常用于求和计算，但内置的 sum 函数在可读性和性能方面更优。123456&gt;&gt;&gt; from functools import reduce&gt;&gt;&gt; from operator import add&gt;&gt;&gt; reduce(add, range(101))5050&gt;&gt;&gt; sum(range(101))5050 二、匿名函数可以使用 lambda 关键字在 Python 表达式内创建匿名函数。在函数的参数列表中最适合使用匿名函数。如前面的根据字符串反序后的结果对单词列表进行排序，可以使用 lambda 匿名函数替代传入 sorted 的 reverse 函数：123&gt;&gt;&gt; fruits = ['strawberry', 'fig', 'apple', 'cherry', 'raspberry', 'banana']&gt;&gt;&gt; sorted(fruits, key=lambda word: word[::-1])['banana', 'apple', 'fig', 'raspberry', 'strawberry', 'cherry'] lambda 表达式 lambda words: words[::-1] 即等同于之前的 reverse 函数：12def reverse(word): return word[::-1] 除了作为参数传给某个高阶函数外，Python 很少使用匿名函数。 三、可调用对象除了用户自定义的函数，其他可调用对象也可以使用调用运算符（即 ()）。Python 的数据模型中共包含 7 种可调用对象： 用户自定义函数：使用 def 语句或 lambda 表达式创建的函数 内置函数：由 C 语言（CPython）实现的函数，如 len 或 time.strftime 等 内置方法：使用 C 语言实现的方法，如 dict.get 方法：在类的定义体中定义的函数 类：类在调用时会使用 __new__ 方法创建实例，然后运行 __init__ 初始化实例，最后将实例返回给调用方。调用类相当于调用函数。 类的实例：如果类的定义中实现了 __call__ 方法，则其实例可以作为函数调用 生成器：使用 yield 关键字的函数或方法。可以返回生成器对象。 使用内置的 callable() 函数可以确认对象是否可调用。 任何 Python 对象都可以表现得像函数，只需实现该实例的 __call__ 方法。如下面的 bingocall.py，从列表中随机取出一个元素：123456789101112131415161718192021222324import randomclass BingoCage: def __init__(self, items): self._items = list(items) random.shuffle(self._items) def pick(self): try: return self._items.pop() except IndexError: raise LookupError('pick from empty BingoCage') def __call__(self): return self.pick()bingo = BingoCage(range(50))print(bingo.pick())# =&gt; 38print(bingo())# =&gt; 22print(callable(bingo))# =&gt; True bingo 是 BingoCage 类的一个实例，由于 BingoCage 类中实现了 __call__ 方法，则 bingo 对象是可调用的（bingo()）。 四、支持函数式编程的模块operator在函数式编程中，经常需要将算术运算符当作函数使用。如不使用递归计算阶乘。 使用 reduce 和 lambda 表达式计算阶乘：123456&gt;&gt;&gt; from functools import reduce&gt;&gt;&gt; def fact(n):... return reduce(lambda a, b: a*b, range(1, n+1))...&gt;&gt;&gt; fact(5)120 Python 中的 operator 为多个运算符提供了对应的函数，可以避免写 lambda a, b: a*b 这种匿名函数。使用 reduce 和 operator.mul 计算阶乘：1234567&gt;&gt;&gt; from operator import mul&gt;&gt;&gt; from functools import reduce&gt;&gt;&gt; def fact(n):... return reduce(mul, range(1, n+1))...&gt;&gt;&gt; fact(5)120 operator 模块中还有一类 itemgetter 和 attrgetter 函数，可以替代从序列中取出元素或读取属性的 lambda 表达式。如根据元组中的第二个元素对多个元组进行排序：12345678910111213141516&gt;&gt;&gt; metro_data = [... ('Tokyo', 'JP', 36.933, (35.689722, 139.691667)),... ('Delhi NCR', 'IN', 21.935, (28.613889, 77.208889)),... ('Mexico City', 'MX', 20.142, (19.433333, -99.133333)),... ('New York-Newark', 'US', 20.104, (40.808611, -74.020386)),... ('Sao Paulo', 'BR', 19.649, (-23.547778, -46.635833)),... ]&gt;&gt;&gt; from operator import itemgetter&gt;&gt;&gt; for city in sorted(metro_data, key=itemgetter(1)):... print(city)...('Sao Paulo', 'BR', 19.649, (-23.547778, -46.635833))('Delhi NCR', 'IN', 21.935, (28.613889, 77.208889))('Tokyo', 'JP', 36.933, (35.689722, 139.691667))('Mexico City', 'MX', 20.142, (19.433333, -99.133333))('New York-Newark', 'US', 20.104, (40.808611, -74.020386)) 如果把多个参数传递给 itemgetter，则该函数会返回由提取的值构成的元组：123456789&gt;&gt;&gt; cc_name = itemgetter(1, 0)&gt;&gt;&gt; for city in metro_data:... print(cc_name(city))...('JP', 'Tokyo')('IN', 'Delhi NCR')('MX', 'Mexico City')('US', 'New York-Newark')('BR', 'Sao Paulo') attrgetter 与 itemgetter 作用类似，可以根据名称提取对象的属性。 operator 模块中还有一个 methodcaller 函数，可以用来在某个对象上调用由参数指定的方法。12345678&gt;&gt;&gt; from operator import methodcaller&gt;&gt;&gt; s = 'The time has come'&gt;&gt;&gt; upcase = methodcaller('upper')&gt;&gt;&gt; upcase(s)'THE TIME HAS COME'&gt;&gt;&gt; hiphenate = methodcaller('replace', ' ', '-')&gt;&gt;&gt; hiphenate(s)'The-time-has-come' functools.partial高阶函数 functools.partial 用来部分应用某个函数。即基于某个函数创建一个新的可调用对象，并把原函数的某些参数固定。 如使用 partial 把一个接受双参数的函数改编成单参数的可调用对象：1234567&gt;&gt;&gt; from operator import mul&gt;&gt;&gt; from functools import partial&gt;&gt;&gt; triple = partial(mul, 3)&gt;&gt;&gt; triple(7)21&gt;&gt;&gt; list(map(triple, range(1, 10)))[3, 6, 9, 12, 15, 18, 21, 24, 27] partial() 函数返回一个 functools.partial 对象，该对象提供对原函数的访问和固定原函数参数的行为。123456789&gt;&gt;&gt; def greeting(words, name):... return f'&#123;words&#125;, &#123;name&#125;!'...&gt;&gt;&gt; from functools import partial&gt;&gt;&gt; greeting2 = partial(greeting, name='skitar')&gt;&gt;&gt; greeting2("what's up")"what's up, skitar!"&gt;&gt;&gt; greeting2functools.partial(&lt;function greeting at 0x7f70f31788b0&gt;, name='skitar') 参考资料Fluent Python]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Program</tag>
        <tag>Advanced</tag>
        <tag>Functional</tag>
        <tag>Function</tag>
        <tag>Lambda</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Golang 基本语法速查——变量、控制结构与函数]]></title>
    <url>%2F2019%2F12%2F10%2Fgo-basic-syntax-variable-control-structure-function%2F</url>
    <content type="text"><![CDATA[一、基本结构hello world 程序：1234567package mainimport "fmt"func main()&#123; fmt.Println("你好，世界")&#125; 直接运行：12$ go run hello.go你好，世界 构建可执行程序：123$ go build hello.go$ ./hello你好，世界 PS Package 用于组织代码架构，类似于其他语言中的库或命名空间 每个 go 源文件都必须属于（且只属于一个）特定的 package，一个 package 可以包含多个 go 代码文件 import 导入某个 package ，意味着获取了该 package 中可见对象的访问权限 import 导入的 package 未在后面的代码中使用会报错，no unnecessary code! 原则 main 函数没有任何参数且不返回任何值，它通常作为程序执行的入口 导入多个包：1234import ( "fmt" "os") 二、变量变量定义和赋值：var identifier [type] = value12345var a intvar b boolvar str string = "hello"var i = 5GOROOT := os.Getenv("GOROOT") PS b := false 等同于 var b = false 。借助 Go 语言的类型推断可以省略类型声明 := 或者 var 形式的变量赋值，包含变量初始化动作。即 a = 1 表示将 1 赋值给变量 a（a 必须已经创建），a := 1 表示新建变量 a，并将 1 赋值给 a。 使用未定义的变量，定义的本地变量未在后续代码中使用，新建变量的名称已存在等都会报错 推荐使用 := 的形式，但只应该在函数内部使用 三、字符串连接字符串，+：123s := "hel" + "lo"s += " world"fmt.Println(s) // prints out "hello world" 前缀匹配：strings.HasPrefix(s, prefix string) bool 后缀匹配：strings.HasSuffix(s, suffix string) bool 包含关系：strings.Contains(s, substr string) bool 字符串替换：strings.Replace(str, old, new, n) string 计算子字符串出现次数：strings.Count(s, str string) int 分割字符串：strings.Split(s, sep string) []string join 字符串：strings.join(sl []string, sep string) string 示例代码：123456789101112131415161718192021222324package mainimport ( "fmt" "strings")func main() &#123; str := "The quick brown fox jumps over the lazy dog" sl := strings.Split(str, " ") fmt.Printf("Splitted in slice: %v\n", sl) // 字符串分割后的返回值 sl 是数组 for _, val := range sl &#123; fmt.Printf("%s,", val) &#125; fmt.Println() str2 := strings.Join(sl, "/") fmt.Printf("sl joined by / is: %s\n", str2)&#125;// output// Splitted in slice: [The quick brown fox jumps over the lazy dog]// The,quick,brown,fox,jumps,over,the,lazy,dog,// sl joined by / is: The/quick/brown/fox/jumps/over/the/lazy/dog 四、控制结构if-elseif-else 结构语法如下：1234567if condition1 &#123; // do something&#125; else if condition2 &#123; // do something&#125; else &#123; // catch-all or default&#125; PS：else 旁边的 } 和 { 不能隔行，否则无法编译通过。 switch12345678switch var1 &#123;case val1: // do somethingcase val2: // do somethingdefault: // catch-all or default&#125; 12345678switch &#123;case condition1: // do somethingcase condition2: // do somethingdefault: // catch-all or default&#125; 12345678switch initialization &#123;case val1: // do somethingcase val2: // do somethingdefault: // catch-all or default&#125; 示例代码：123456789101112131415161718192021222324package mainimport ( "fmt")func Abs(x int) int &#123; if x &lt; 0 &#123; return -x &#125; return x&#125;func main() &#123; switch num := Abs(-5); &#123; case num &gt; 0: fmt.Println("Result is positive") case num &lt; 0: fmt.Println("Never gonna happen") default: fmt.Println("Exact 0") &#125;&#125;// Result is positive for参考如下示例：1234567891011121314151617package mainimport ( "fmt")func main() &#123; str := "Go is a beautiful language" for ix :=0; ix &lt; len(str); ix ++ &#123; fmt.Printf("Character on position %d is: %c \n", ix, str[ix]) &#125; for pos, char := range str &#123; fmt.Printf("Character on position %d is: %c \n", pos, char) &#125;&#125; 五、函数定义函数语法：12func g() &#123;&#125; 注意不能写成如下形式：123func g()&#123;&#125; 函数调用语法：pack1.Function(arg1,arg2, ... ,argn) Go 中的函数可以接收另一个函数作为参数，比如有下面两个函数：f1(a, b, c int) 接收三个参数f2(a, b int) (int, int, int) 有两个参数和三个返回值则可以这样调用：f1(f2(a, b)) 函数本身可以赋值给变量，如 add := binOp 参数与返回值Go 中的函数可以接收 0 个或多个参数，返回 0 个或多个结果。传给函数的参数通常会有名称，但是也可以没有名称只包含类型：func f(int, int, float64) Go 函数的传参默认是按值传递，即实际传给函数的是参数值的复制。如调用 Function(arg1)，函数 Function 收到的是 arg1 指向的值的副本，而不是 arg1 本身。如果需要函数操作传入的参数本身，即 Function(arg1) 操作 arg1 本体的值，则需要传入参数的指针（内存地址）。使用如下形式：Function(&amp;arg1)。 大部分函数都需要返回值，可以是命名的也可以不命名。 数量未知的参数函数的最后一个参数可以是 ...type 这样的形式，即 func myFunc(a, b, arg ...int) {} 。这表示函数可以接收相同类型的不定数量的参数。参考如下示例：1234567891011121314151617181920212223242526272829package mainimport ( "fmt")func main() &#123; x := Min(1, 3, 2, 0) fmt.Printf("The minimum is: %d\n", x) arr := []int&#123;7, 9, 3, 5, 1&#125; x = Min(arr...) fmt.Printf("The minimum in the array arr is: %d", x)&#125;func Min(a ...int) int &#123; if len(a) == 0 &#123; return 0 &#125; min := a[0] for _, v := range a &#123; if v &lt; min &#123; min = v &#125; &#125; return min&#125;// The minimum is: 0// The minimum in the array arr is: 1 Deferdefer 关键字用于“延迟”执行某个函数或命令，由 defer 修饰的语句会在外部函数返回结果之后再执行。123456789101112131415161718package mainimport ( "fmt")func main() &#123; fmt.Printf("In Main function at the top\n") defer Function2() fmt.Printf("In Main function at the bottom\n")&#125;func Function2() &#123; fmt.Printf("Function2: Deferred until the end of the calling function")&#125;// In Main function at the top// In Main function at the bottom// Function2: Deferred until the end of the calling function% 如果 defer 语句中包含变量参数，则该参数的值在 defer 的位置已经确定。参考如下代码：1234567891011121314package mainimport ( "fmt")func main() &#123; i := 0 defer fmt.Println(i) i++ fmt.Println("The number is:")&#125;// The number is:// 0 由 defer 修饰的 Println 函数虽然在程序执行流的最后输出 i 值。但变量 i 在 defer 处的值为 0，因此最后输出 0 而不是执行 i++ 后的 1 。 当代码中包含有多个 defer 语句时，这些语句会以 LIFO（后进先出，类似 stack）的顺序执行。123456789101112package mainimport ( "fmt")func main() &#123; for i := 0; i &lt; 5; i++ &#123; defer fmt.Printf("%d", i) &#125;&#125;// 43210 递归函数12345678910111213141516171819202122package mainimport ( "fmt")func main() &#123; result := 0 for i := 0; i &lt;= 10; i++ &#123; result = fibonacci(i) fmt.Printf("fibonacci(%d) is: %d\n", i, result) &#125;&#125;func fibonacci(n int) (res int) &#123; if n &lt;= 1 &#123; res = 1 &#125; else &#123; res = fibonacci(n-1) + fibonacci(n-2) &#125; return&#125; 高阶函数函数作为参数12345678910111213141516package mainimport "fmt"func main() &#123; callback(1, Add)&#125;func Add(a, b int) &#123; fmt.Printf("The sum of %d and %d is: %d\n", a, b, a+b)&#125;func callback(y int, f func(int, int)) &#123; f(y, 2)&#125;// =&gt; The sum of 1 and 2 is: 3 （匿名）函数赋值给变量123456789101112131415package mainimport "fmt"func main() &#123; for i := 0; i &lt; 4; i++ &#123; g := func(i int) &#123; fmt.Printf("%d", i) &#125; g(i) fmt.Printf(" - g is of type %T and has value %v\n", g, g) &#125;&#125;// =&gt; 0 - g is of type func(int) and has value 0x4839d0// =&gt; 1 - g is of type func(int) and has value 0x4839d0// =&gt; 2 - g is of type func(int) and has value 0x4839d0// =&gt; 3 - g is of type func(int) and has value 0x4839d0 函数作为返回值12345678910111213141516171819202122232425package mainimport "fmt"func main() &#123; p2 := Add2() fmt.Printf("Call Add2 for 3 returns: %v\n", p2(3)) TwoAdder := Adder(2) fmt.Printf("The result is: %v\n", TwoAdder(3))&#125;func Add2() func(b int) int &#123; return func(b int) int &#123; return b + 2 &#125;&#125;func Adder(a int) func(b int) int &#123; return func(b int) int &#123; return a + b &#125;&#125;// =&gt; Call Add2 for 3 returns: 5// =&gt; The result is: 5 另一个版本的累加器，涉及到闭包：12345678910111213141516171819package mainimport "fmt"func main() &#123; f := Adder() fmt.Print(f(1), " - ") fmt.Print(f(20), " - ") fmt.Print(f(300))&#125;func Adder() func(int) int &#123; var x int return func(delta int) int &#123; x += delta return x &#125;&#125;// =&gt; 1 - 21 - 321 参考资料The Way To Go: A Thorough Introduction To The Go Programming Language]]></content>
      <categories>
        <category>Golang</category>
      </categories>
      <tags>
        <tag>Program</tag>
        <tag>Golang</tag>
        <tag>Go</tag>
        <tag>Basic</tag>
        <tag>Code</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu 18.04 搭建私有软件镜像源（支持 Ubuntu 和 CentOS）]]></title>
    <url>%2F2019%2F12%2F04%2Fubuntu-18-04-create-ubuntu-and-centos-software-mirrors%2F</url>
    <content type="text"><![CDATA[系统环境为 Ubuntu 18.04，搭建支持 Ubuntu 系和 CentOS 系的双私有软件仓库。Ubuntu 本地软件镜像源使用 apt-mirror 工具与远程仓库同步，CentOS 本地镜像源使用 reposync 工具与远程仓库同步。上述两个工具都可以通过 Ubuntu 的包管理器 apt-get 命令直接安装使用。 一、环境准备我这里搭建了 Ubuntu 18.04、Ubuntu 16.04 和 CentOS7 三个镜像源，从远程仓库拉取的软件包总共耗费了约 300G 硬盘空间，其中两个 Ubuntu 占用了约 261G，CentOS7（包含 epel 源）约 29G 。创建镜像源前需保证本地有足够的存储空间。 建议给磁盘分区时使用 LVM（逻辑卷管理） 的方式，便于之后对存储空间进行扩容等操作。关于 Linux 系统磁盘分区和 LVM 的介绍可以参考 Linux 磁盘设备和 LVM 管理命令详解。此处不作赘述。 安装同步工具：$ sudo apt-get install apt-mirror reposync createrepo 安装 web 服务器（nginx）：$ sudo apt-get install nginx 二、Ubuntu 镜像源搭建编辑 /etc/apt/mirror.list 配置文件，修改远程仓库的地址为国内的阿里镜像站，本地的 Ubuntu 软件仓库将定期从阿里源同步软件包。1234567891011121314151617181920set nthreads 20set _tilde 0set base_path /opt/mirrors/ubuntuset defaultarch amd64#Ubuntu 16.04deb http://mirrors.aliyun.com/ubuntu/ xenial main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ xenial-updates main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ xenial-backports main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ xenial-security main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ xenial-proposed main restricted universe multiverse#Ubuntu 18.04deb http://mirrors.aliyun.com/ubuntu/ bionic main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ bionic-security main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ bionic-proposed main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiverseclean http://mirrors.aliyun.com/ubuntu 上述配置文件中 base_path 指定的目录，即本地仓库存放软件包的路径。$ sudo mkdir -p /opt/mirrors/ubuntu 运行 apt-mirror 命令拉取软件包，同步完成花费的时间视网速而定：$ sudo apt-mirror 配置 web 服务镜像源同步完成之后，需配置 web 服务使得本地仓库可以被其他 Linux 机器使用。编辑 /etc/nginx/sites-available/default 配置文件，开启目录浏览（auto_index）功能：1234567891011121314server &#123; listen 80 default_server; listen [::]:80 default_server; root /var/www/html; index index.html index.htm index.nginx-debian.html; server_name _; location / &#123; try_files $uri $uri/ =404; autoindex on; autoindex_exact_size off; autoindex_localtime on; &#125;&#125; 创建软链接，将软件包存储路径指向到 web 目录下：$ sudo ln -s /opt/mirrors/ubuntu/mirror/mirrors.aliyun.com/ubuntu /var/www/html/ubuntu 重新载入 nginx 服务：$ sudo service nginx reload 此时访问 http://127.0.0.1/ubuntu ，应该可以在 web 界面中浏览本地仓库中的软件包。 使用本地镜像源前面的配置完成后，即可进入任意一台 Ubuntu 主机，配置其镜像源为刚刚创建的本地软件仓库。编辑 /etc/apt/sources.list 文件，修改镜像源（Ubuntu 18.04）：12345deb http://127.0.0.1/ubuntu/ bionic main restricted universe multiversedeb http://127.0.0.1/ubuntu/ bionic-updates main restricted universe multiversedeb http://127.0.0.1/ubuntu/ bionic-backports main restricted universe multiversedeb http://127.0.0.1/ubuntu/ bionic-security main restricted universe multiversedeb http://127.0.0.1/ubuntu/ bionic-proposed main restricted universe multiverse 运行 sudo apt-get update 命令更新索引文件，之后即可使用 sudo apt-get install 命令从本地仓库中安装软件包了。 效果如下：123456789101112131415$ sudo apt-get updateHit:1 http://127.0.0.1/ubuntu bionic InReleaseHit:2 http://127.0.0.1/ubuntu bionic-updates InReleaseHit:3 http://127.0.0.1/ubuntu bionic-backports InReleaseHit:4 http://127.0.0.1/ubuntu bionic-security InReleaseHit:5 http://127.0.0.1/ubuntu bionic-proposed InReleaseReading package lists... Done$ sudo apt-get install nmap...Get:1 http://127.0.0.1/ubuntu bionic/main amd64 libblas3 amd64 3.7.1-4ubuntu1 [140 kB]Get:2 http://127.0.0.1/ubuntu bionic/main amd64 liblinear3 amd64 2.1.0+dfsg-2 [39.3 kB]Get:3 http://127.0.0.1/ubuntu bionic-updates/main amd64 liblua5.3-0 amd64 5.3.3-1ubuntu0.18.04.1 [115 kB]Get:4 http://127.0.0.1/ubuntu bionic/main amd64 nmap amd64 7.60-1ubuntu5 [5174 kB]Fetched 5467 kB in 1s (9200 kB/s)... 三、CentOS 镜像源搭建创建 /opt/mirrors/CentOS-7.repo 配置文件，添加远程软件仓库的地址。这里使用清华大学开源软件镜像站。12345678910111213141516171819202122232425262728293031323334353637383940[base]name=CentOS-7 - Basebaseurl=https://mirrors.tuna.tsinghua.edu.cn/centos/7/os/x86_64/#mirrorlist=http://mirrorlist.centos.org/?release=$releasever&amp;arch=$basearch&amp;repo=osgpgcheck=0#gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7#released updates[updates]name=CentOS-7 - Updatesbaseurl=https://mirrors.tuna.tsinghua.edu.cn/centos/7/updates/x86_64/#mirrorlist=http://mirrorlist.centos.org/?release=$releasever&amp;arch=$basearch&amp;repo=updatesgpgcheck=0#gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7#additional packages that may be useful[extras]name=CentOS-7 - Extrasbaseurl=https://mirrors.tuna.tsinghua.edu.cn/centos/7/extras/x86_64/#mirrorlist=http://mirrorlist.centos.org/?release=$releasever&amp;arch=$basearch&amp;repo=extrasgpgcheck=0#gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7#additional packages that extend functionality of existing packages[centosplus]name=CentOS-7 - Plusbaseurl=https://mirrors.tuna.tsinghua.edu.cn/centos/7/centosplus/x86_64/#mirrorlist=http://mirrorlist.centos.org/?release=$releasever&amp;arch=$basearch&amp;repo=centosplusgpgcheck=0enabled=0#gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7[epel]name=Extra Packages for Enterprise Linux 7baseurl=https://mirrors.tuna.tsinghua.edu.cn/epel/7/x86_64#mirrorlist=https://mirrors.fedoraproject.org/metalink?repo=epel-7&amp;arch=$basearchfailovermethod=priorityenabled=1gpgcheck=0# gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-7 此处为了简单起见，gpgcheck 都设置为 0 关闭了 GPG KEY 检查。 创建软件仓库本地路径：$ sudo mkdir -p /opt/mirrors/centos 拉取远程仓库到本地：$ sudo reposync -np /opt/mirrors/centos -c /opt/mirrors/CentOS-7.repo 同步完成后，/opt/mirrors/centos 路径下会生成如下 4 个文件夹：12$ ls /opt/mirrors/centosbase epel extras updates 切换到 root 用户，使用 createrepo 命令在上述 4 个文件夹中分别创建私有仓库的索引文件：1234$ cd /opt/mirrors/centos$ for i in base extras updates epel; docreaterepo $idone 索引创建完成后，上述四个路径中都会多出 repodata 和 .repodata 两个文件夹：12$ ls -a base. .. Packages repodata .repodata 配置 web 服务创建软链接，将 CentOS 软件包存储路径指向到 web 目录下：$ sudo ln -s /opt/mirrors/centos /var/www/html/centos 重新载入 nginx 服务：$ sudo service nginx reload 此时访问 http://127.0.0.1/centos ，应该可以在 web 界面中浏览本地 CentOS 仓库中的软件包。 使用本地镜像源登录任意一台 CentOS 主机，备份 /etc/yum.repos.d 目录下的所有 .repo 配置文件到其他位置。创建软件仓库配置文件 /etc/yum.repos.d/CentOS-7.repo，编辑远程仓库地址（baseurl）为前面搭建的私有仓库：12345678910111213141516171819202122232425262728293031323334353637383940[base]name=CentOS-7 - Basebaseurl=http://127.0.0.1/centos/base#mirrorlist=http://mirrorlist.centos.org/?release=$releasever&amp;arch=$basearch&amp;repo=osgpgcheck=0#gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7#released updates[updates]name=CentOS-7 - Updatesbaseurl=http://127.0.0.1/centos/updates#mirrorlist=http://mirrorlist.centos.org/?release=$releasever&amp;arch=$basearch&amp;repo=updatesgpgcheck=0#gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7#additional packages that may be useful[extras]name=CentOS-7 - Extrasbaseurl=http://127.0.0.1/centos/extras#mirrorlist=http://mirrorlist.centos.org/?release=$releasever&amp;arch=$basearch&amp;repo=extrasgpgcheck=0#gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7#additional packages that extend functionality of existing packages[centosplus]name=CentOS-7 - Plusbaseurl=http://127.0.0.1/centos/centosplus#mirrorlist=http://mirrorlist.centos.org/?release=$releasever&amp;arch=$basearch&amp;repo=centosplusgpgcheck=0enabled=0#gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7[epel]name=Extra Packages for Enterprise Linux 7baseurl=http://127.0.0.1/centos/epel#mirrorlist=https://mirrors.fedoraproject.org/metalink?repo=epel-7&amp;arch=$basearchfailovermethod=priorityenabled=1gpgcheck=0# gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-7 运行 sudo yum clean all &amp;&amp; yum makecache 命令重建软件包索引。之后即可通过 sudo yum install 命令从本地的私有仓库中安装软件了。 四、crontab创建 crontab 计划任务，让本地仓库定期从远程仓库拉取有更新的软件包：$ sudo crontab -e 参考配置：120 3 * * 0 reposync -np /opt/mirrors/centos -c /opt/mirrors/CentOS-7.repo0 22 * * 6 apt-mirror 每周日凌晨 3 点运行 reposync 命令同步 CentOS 仓库。每周六晚上 10 点运行 apt-mirror 命令同步 Ubuntu 仓库。 参考文章centos 下创建本地镜像源，结合 nginx]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Server</tag>
        <tag>Software</tag>
        <tag>Ubuntu</tag>
        <tag>CentOS</tag>
        <tag>Mirror</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HTTPie 命令语法速查]]></title>
    <url>%2F2019%2F12%2F01%2Fhttpie-as-powerful-http-client%2F</url>
    <content type="text"><![CDATA[HTTPie 是一个基于命令行的 HTTP 客户端，类似于 Linux 系统中的 curl 工具。它凭借非常简单直观的语法和着色的格式化输出等特性，提供了一种友好的与 Web 服务交互的方式。 一、安装HTTPie 的安装方法有很多种，可以通过 pip 命令以 Python 模块的方式安装，相对简单且通用：$ pip install httpie 语法格式：$ http [flags] [METHOD] URL [ITEM [ITEM]] 基本示例 $ http httpie.org 以 GET 方法访问 Web 服务，获取 HTML 文档或其他类型的响应 $ http PUT example.org X-API-TOKEN:123 name=John 自定义 HTTP 请求头、HTTP 方法及上传的 JSON 数据在上面的示例中，PUT 为自定义的请求方法，X-API-Token 为请求头选项，name=John 即通过 PUT 方法提交的 JSON 数据。 $ http -f POST example.org hello=world 提交 form 表单 $ http example.org &lt; file.json 通过重定向上传文件 http --download exmaple.org/file 下载文件（类似 wget 命令） 二、语法详解HTTP 方法http 命令默认使用 GET 方法（无请求数据时）或 POST 方法（有提供请求数据时）访问 Web 服务，也支持 PUT、DELETE 等标准的 HTTP 动词：http &lt;method&gt; &lt;url&gt;。 如：$ http DELETE example.org/todos/7，实际向服务器发送了类似这样的请求：DELETE /todos/7 HTTP/1.1。 Request 选项http 允许在请求命令中添加 HTTP 请求头、JSON 或表单数据、文件和 URL 参数等请求选项。参考下表： Item 类型 描述 HTTP 请求头 Name:Value 如 X-API-Token:123 URL 参数 name==value 通过 == 符号向 URL 中附加 query string 参数 Data Fields field=value, field=@file.txt 此类数据在提交时会序列化为 JSON 对象，或者编码为 form 格式上传（--form, -f） Raw JSON field:=json, field:=@file.json 用于提交 Boolean、Number、Array 等原始格式的 JSON 数据。如 meals:=&#39;[&quot;ham&quot;,&quot;spam&quot;]&#39; 表单文件 file@/dir/file 配合 --form 或 -f 使用，以 multipart/form-data 的形式提交文件 Querystring对于 URL 链接中包含的 querystring 参数（如 /?param1=value1&amp;param2=value2），http 命令可以通过简单的 param==value 语法来指定。 如 $ http www.google.com search==&#39;HTTPie logo&#39; tbm==isch效果类似于 GET /?search=HTTPie+logo&amp;tbm=isch HTTP/1.1 Non-string JSONhttp 命令中包含的请求数据默认都会序列化为 JSON 格式再提交给服务器，即请求头中 Content-Type 和 Accept 选项的默认值都为 application/json 。 可以使用 := 操作符提交非字符串形式的 JSON 数据；Text 和 JSON 文件则可以分别使用 =@ 和 :=@ 嵌入到请求中。如：12345$ http PUT api.example.com/person/1 \ name=John \ # Text JSON age:=29 married:=false hobbies:=&apos;[&quot;http&quot;, &quot;pies&quot;]&apos; \ # Raw JSON description=@about-john.txt \ # Embed text file bookmarks:=@bookmarks.json # Embed JSON file 表单常规表单提交表单和发送 JSON 请求的方式非常相似，只是需要在命令中添加 --form 或 -f 选项，以确保请求数据的 Content-Type 被设置为 application/x-www-form-urlencoded; charset=utf-8 。 $ http -f POST api.example.org/person/1 name=&#39;John Smith&#39;1234POST /person/1 HTTP/1.1Content-Type: application/x-www-form-urlencoded; charset=utf-8name=John+Smith 文件上传表单如果表单请求中有包含文件项，则该请求的 Content-Type 会被设置成 multipart/form-data 。 $ http -f POST example.com/jobs name=&#39;John Smith&#39; cv@~/Documents/cv.pdf上述命令的效果等同于如下 HTML 表单：1234&lt;form enctype="multipart/form-data" method="post" action="http://example.com/jobs"&gt; &lt;input type="text" name="name" /&gt; &lt;input type="file" name="cv" /&gt;&lt;/form&gt; 其他常用语法Cookies$ http example.org &#39;Cookie:sessionid=foo;another-cookie=bar&#39; 基本认证$ http -a username:password example.org Digest 认证$ http -A digest -a username:password example.org 提示手动输入密码$ http -a username example.org 代理$ http --proxy=http:http://10.10.1.10:3128 --proxy=https:https://10.10.1.10:1080 example.org 带认证的代理$ http --proxy=http:http://user:pass@10.10.1.10:3128 example.org SOCKS 代理需安装依赖库：$ pip install requests[socks]$ http --proxy=http:socks5://user:pass@host:port --proxy=https:socks5://user:pass@host:port example.org 跳过 SSL 证书认证$ http --verify=no https://example.org 自定义 CA$ http --verify=/ssl/custom_ca_bundle https://example.org 客户端 SSL 证书$ http --cert=client.pem https://example.org 或$ http --cert=client.crt --cert-key=client.key https://example.org 三、命令输出默认情况下，http 命令会输出最终的完整响应信息（包含 headers 和 body），可以通过如下选项控制显示的结果： 选项 含义 --headers, -h 只输出响应头信息 --body, -b 只输出响应中的 body 信息 --verbose, -b 输出所有的 HTTP 交互信息（包含请求和响应） --print, -p 自由组合需要显示的部分 --print 或 -p 选项可以携带多个不同的字符对应特定的内容进行显示： 字符 含义 H 请求头 B 请求体 h 响应头 b 响应体 如：http --print=Hh PUT httpbin.org/put hello=world 输出重定向保存文件：$ http example.org/Movie.mov &gt; Movie.mov 保存图片后使用 ImageMagick 改变大小再上传：$ http octodex.github.com/images/original.jpg | convert - -resize 25% - | http example.org/Octocats 下载模式（响应头定向至 stderr，传输过程中显示进度条）：http --download https://github.com/jakubroztocil/httpie/archive/master.tar.gz 下载过程中使用管道重定向：$ http -d https://github.com/jakubroztocil/httpie/archive/master.tar.gz | tar zxf - 参考资料HTTPie Documentation]]></content>
      <categories>
        <category>Tools</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Tools</tag>
        <tag>Tricks</tag>
        <tag>Python</tag>
        <tag>Efficiency</tag>
        <tag>HTTP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 通过 Flask 框架构建 REST API（二）——优化项目架构]]></title>
    <url>%2F2019%2F11%2F30%2Fpython-build-rest-api-with-flask-2-project-structure%2F</url>
    <content type="text"><![CDATA[接上文 Python 通过 Flask 框架构建 REST API（一）——数据库建模。前面介绍了如何通过 Flask 和 marshmallow 框架写一个完整的单页 Web 应用，作为 REST API 实现基本的增删改查功能。本篇主要介绍在前文的基础上，如何将单页应用合理地组织到一个架构清晰的项目中。标准化的同时也方便日后的维护。 一、环境搭建见如下代码：12345678910# 创建项目文件夹$ mkdir author-manager &amp;&amp; cd author-manager# 创建 Python 虚拟环境$ pip install virtualenv$ virtualenv venv$ source venv/bin/activate# 安装依赖库(venv) $ pip install flask marshmallow-sqlalchemy flask-sqlalchemy# 创建文件夹存放源代码$ mkdir src &amp;&amp; cd src 二、初始化应用创建 main.py 源代码文件初始化 Flask 应用：123456789101112131415# main.pyimport osfrom flask import Flaskfrom flask import jsonifyapp = Flask(__name__)if os.environ.get('WORK_ENV') == 'PROD': app_config = ProductionConfigelif os.environ.get('WORK_ENV') == 'TEST': app_config = TestingConfigelse: app_config = DevelopmentConfigapp.config.from_object(app_config) 创建 run.py 文件作为运行 Web 应用的入口：1234# run.pyfrom main import app as applicationif __name__ == "__main__": application.run(port=5000, host="0.0.0.0", use_reloader=False) Config创建 api/config 目录，在其中创建空的 __init__.py 和 config.py 文件，编辑 config.py 加入 config 对象：$ mkdir -p api/config &amp;&amp; touch api/config/__init__.py$ vim api/config/config.py 12345678910111213141516171819# src/config/config.pyclass Config(object): DEBUG = False TESTING = False SQLALCHEMY_TRACK_MODIFICATIONS = Falseclass ProductionConfig(Config):# SQLALCHEMY_DATABASE_URI = &lt;Production DB RUL&gt; passclass DevelopmentConfig(Config): DEBUG = True SQLALCHEMY_DATABASE_URI = "sqlite:///../authors.db" SQLALCHEMY_ECHO = Falseclass TestingConfig(Config): TESTING = True# SQLALCHEMY_DATABASE_URI = &lt;Testing DB URL&gt; SQLALCHEMY_ECHO = False PS：每个目录中包含的 __init__.py 空文件用来指示该目录中包含有可供其他代码文件导入的 Python 模块 Database创建 api/utils 文件夹并编辑 database.py 文件：$ mkdir -p api/utils &amp;&amp; touch api/utils/__init__.py$ vim api/utils/database.py 1234# src/api/utils/database.pyfrom flask_sqlalchemy import SQLAlchemydb = SQLAlchemy() 将 main.py 改为如下版本以导入 config 和 db 对象：1234567891011121314151617181920# main.pyimport osfrom flask import Flaskfrom flask import jsonifyfrom api.config.config import *from api.utils.database import dbif os.environ.get('WORK_ENV') == 'PROD': app_config = ProductionConfigelif os.environ.Get('WORK_ENV') == 'TEST': app_config = TestingConfigelse: app_config = DevelopmentConfigapp = Flask(__name__)app.config.from_object(app_config)db.init_app(app)with app.app_context(): db.create_all() 此时整个项目的目录结构如下：1234567891011121314author-manager├── src│ ├── api│ │ ├── __init__.py│ │ ├── config│ │ │ ├── __init__.py│ │ │ └── config.py│ │ └── utils│ │ ├── __init__.py│ │ └── database.py│ ├── main.py│ ├── requirements.txt│ └── run.py└── venv 三、数据库关系模型创建 api/models 文件夹，在其中编辑 books.py 文件作为数据库模型：$ mkdir -p api/models &amp;&amp; touch api/models/__init__.py books 数据表模型：123456789101112131415161718192021222324252627282930313233# src/api/models/books.pyfrom api.utils.database import dbfrom marshmallow_sqlalchemy import ModelSchemafrom marshmallow import fieldsclass Book(db.Model): __talbename__ = 'books' id = db.Column(db.Integer, primary_key=True, autoincrement=True) title = db.Column(db.String(50)) year = db.Column(db.Integer) author_id = db.Column(db.Integer, db.ForeignKey('authors.id')) def __init__(self, title, year, author_id=None): self.title = title self.year = year self.author_id = author_id def create(self): db.session.add(self) db.session.commit() return selfclass BookSchema(ModelSchema): class Meta(ModelSchema.Meta): model = Book sqla_session = db.session id = fields.Number(dump_only=True) title = fields.String(required=True) year = fields.Integer(required=True) author_id = fields.Integer() authors 数据表模型：12345678910111213141516171819202122232425262728293031323334353637# src/api/models/authors.pyfrom api.utils.database import dbfrom marshmallow_sqlalchemy import ModelSchemafrom marshmallow import fieldsfrom api.models.books import BookSchemaclass Author(db.Model): __tablename__ = 'authors' id = db.Column(db.Integer, primary_key=True, autoincrement=True) first_name = db.Column(db.String(20)) last_name = db.Column(db.String(20)) created = db.Column(db.DateTime, server_default=db.func.now()) books = db.relationship('Book', backref='Author', cascade="all, delete-orphan") def __init__(self, first_name, last_name, books=[]): self.first_name = first_name self.last_name = last_name self.books = books def create(self): db.session.add(self) db.session.commit() return selfclass AuthorSchema(ModelSchema): class Meta(ModelSchema.Meta): model = Author sqla_session = db.session id = fields.Number(dump_only=True) first_name = fields.String(required=True) last_name = fields.String(required=True) created = fields.String(dump_only=True) books = fields.Nested(BookSchema, many=True, only=['title', 'year', 'id']) 四、HTTP 标准响应在 api/utils 目录下创建 responses.py ，作为 REST API 通用的响应格式：123456789101112131415161718192021222324# src/api/utils/responses.pyfrom flask import make_response, jsonifydef response_with(response, value=None, message=None, error=None, headers=&#123;&#125;, pagination=None): result = &#123;&#125; if value is not None: result.update(value) if response.get('message', None) is not None: result.update(&#123;'message': response['message']&#125;) result.update(&#123;'code': response['code']&#125;) if error is not None: result.update(&#123;'errors': error&#125;) if pagination is not None: result.update(&#123;'pagination': pagination&#125;) headers.update(&#123;'Access-Control-Allow-Origin': '*'&#125;) headers.update(&#123;'server': 'Flask REST API'&#125;) return make_response(jsonify(result), response['http_code'], headers) 在 responses.py 文件的 response_with 函数前面插入如下代码，定义 http_code：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950# src/api/utils/responses.pyINVALID_FIELD_NAME_SENT_422 = &#123; "http_code": 422, "code": "invalidField", "message": "Invalid fields found" &#125;INVALID_INPUT_422 = &#123; "http_code": 422, "code": "missingParameter", "message": "Missing parameters" &#125;BAD_REQUEST_400 = &#123; "http_code": 400, "code": "badRequest", "message": "Bad request" &#125;SERVER_ERROR_500 = &#123; "http_code": 500, "code": "serverError", "message": "Server error" &#125;SERVER_ERROR_404 = &#123; "http_code": 404, "code": "notFound", "message": "Resource not found" &#125;UNAUTHORIZED_403 = &#123; "http_code": 403, "code": "notAuthorized", "message": "You are not authorized" &#125;SUCCESS_200 = &#123; 'http_code': 200, 'code': 'success', &#125;SUCCESS_201 = &#123; 'http_code': 201, 'code': 'success', &#125;SUCCESS_204 = &#123; 'http_code': 204, 'code': 'success' &#125; 在 main.py 中添加如下代码，导入 status code 定义和 response_with 函数：1234# src/main.pyimport api.utils.responses as respfrom api.utils.responses import response_withimport logging 在 main.py 中 db.init_app(app) 前面添加如下代码，引入错误场景下的标准响应格式：12345678910111213141516171819# src/main.py@app.after_requestdef add_header(response): return response@app.errorhandler(400)def bad_request(e): logging.error(e) return response_with(resp.BAD_REQUEST_400)@app.errorhandler(500)def server_error(e): logging.error(e) return response_with(resp.SERVER_ERROR_500)@app.errorhandler(404)def not_found(e): logging.error(e) return response_with(resp.SERVER_ERROR_404) 五、API endpoints接下来创建 REST API 的访问端点及其对应的路由。编辑 api/routes/authors.py 文件，加入对 POST、GET 等方法的响应逻辑。$ mkdir -p api/routes &amp;&amp; touch api/routes/__init__.py 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374# src/api/routes/authors.pyfrom flask import Blueprintfrom flask import requestfrom api.utils.responses import response_withfrom api.utils import responses as respfrom api.models.authors import Author, AuthorSchemafrom api.utils.database import dbauthor_routes = Blueprint("author_routes", __name__)@author_routes.route('/', methods=['POST'])def create_author(): try: data = request.get_json() author_schema = AuthorSchema() author = author_schema.load(data) result = author_schema.dump(author.create()) return response_with(resp.SUCCESS_201, value=&#123;"author": result&#125;) except Exception as e: print(e) return response_with(resp.INVALID_INPUT_422)@author_routes.route('/', methods=['GET'])def get_author_list(): fetched = Author.query.all() author_schema = AuthorSchema(many=True, only=['first_name', 'last_name','id']) authors = author_schema.dump(fetched) return response_with(resp.SUCCESS_200, value=&#123;"authors": authors&#125;)@author_routes.route('/&lt;int:author_id&gt;', methods=['GET'])def get_author_detail(author_id): fetched = Author.query.get_or_404(author_id) author_schema = AuthorSchema() author = author_schema.dump(fetched) return response_with(resp.SUCCESS_200, value=&#123;"author": author&#125;)@author_routes.route('/&lt;int:id&gt;', methods=['PUT'])def update_author_detail(id): data = request.get_json() get_author = Author.query.get_or_404(id) get_author.first_name = data['first_name'] get_author.last_name = data['last_name'] db.session.add(get_author) db.session.commit() author_schema = AuthorSchema() author = author_schema.dump(get_author) return response_with(resp.SUCCESS_200, value=&#123;"author": author&#125;)@author_routes.route('/&lt;int:id&gt;', methods=['PATCH'])def modify_author_detail(id): data = request.get_json() get_author = Author.query.get(id) if data.get('first_name'): get_author.first_name = data['first_name'] if data.get('last_name'): get_author.last_name = data['last_name'] db.session.add(get_author) db.session.commit() author_schema = AuthorSchema() author = author_schema.dump(get_author) return response_with(resp.SUCCESS_200, value=&#123;"author": author&#125;)@author_routes.route('/&lt;int:id&gt;', methods=['DELETE'])def delete_author(id): get_author = Author.query.get_or_404(id) db.session.delete(get_author) db.session.commit() return response_with(resp.SUCCESS_204) 然后在 main.py 文件中导入上面创建的 author_routes：from api.routes.authors import author_routes 并加入以下代码（在 ``@app.after_request之前）以完成路由的注册：app.register_blueprint(author_routes, url_prefix=’/api/authors’) 测试运行 python run.py 启动 Web 服务，使用 httpie 工具进行测试：1234567891011$ http POST 127.0.0.1:5000/api/authors/ first_name=Jack last_name=Sparrow&#123; "author": &#123; "books": [], "created": "2019-11-29 04:41:46", "first_name": "Jack", "id": 2.0, "last_name": "Sparrow" &#125;, "code": "success"&#125; 1234567891011$ http 127.0.0.1:5000/api/authors/&#123; "authors": [ &#123; "first_name": "Jack", "id": 1.0, "last_name": "Sparrow" &#125;, ], "code": "success"&#125; 1234567891011$ http 127.0.0.1:5000/api/authors/1&#123; "author": &#123; "books": [], "created": "2019-11-29 03:22:22", "first_name": "Jack", "id": 1.0, "last_name": "Sparrow" &#125;, "code": "success"&#125; 同样的方式，创建 api/routes/books.py 文件，添加 book_routes 的响应逻辑：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869# src/api/routes/books.pyfrom flask import Blueprint, requestfrom api.utils.responses import response_withfrom api.utils import responses as respfrom api.models.books import Book, BookSchemafrom api.utils.database import dbbook_routes = Blueprint("book_routes", __name__)@book_routes.route('/', methods=['POST'])def create_book(): try: data = request.get_json() book_schema = BookSchema() book = book_schema.load(data) result = book_schema.dump(book.create()) return response_with(resp.SUCCESS_201, value=&#123;"book": result&#125;) except Exception as e: print(e) return response_with(resp.INVALID_INPUT_422)@book_routes.route('/', methods=['GET'])def get_book_list(): fetched = Book.query.all() book_schema = BookSchema(many=True, only=['author_id', 'title', 'year']) books = book_schema.dump(fetched) return response_with(resp.SUCCESS_200, value=&#123;"books": books&#125;)@book_routes.route('/&lt;int:id&gt;', methods=['GET'])def get_book_detail(id): fetched = Book.query.get_or_404(id) book_schema = BookSchema() books = book_schema.dump(fetched) return response_with(resp.SUCCESS_200, value=&#123;"books": books&#125;)@book_routes.route('/&lt;int:id&gt;', methods=['PUT'])def update_book_detail(id): data = request.get_json() get_book = Book.query.get_or_404(id) get_book.title = data['title'] get_book.year = data['year'] db.session.add(get_book) db.session.commit() book_schema = BookSchema() book = book_schema.dump(get_book) return response_with(resp.SUCCESS_200, value=&#123;"book": book&#125;)@book_routes.route('/&lt;int:id&gt;', methods=['PATCH'])def modify_book_detail(id): data = request.get_json() get_book = Book.query.get_or_404(id) if data.get('title'): get_book.title = data['title'] if data.get('year'): get_book.year = data['year'] db.session.add(get_book) db.session.commit() book_schema = BookSchema() book = book_schema.dump(get_book) return response_with(resp.SUCCESS_200, value=&#123;"book": book&#125;)@book_routes.route('/&lt;int:id&gt;', methods=['DELETE'])def delete_book(id): get_book = Book.query.get_or_404(id) db.session.delete(get_book) db.session.commit() return response_with(resp.SUCCESS_204) 在 main.py 中导入 book_routes 并使用 app.register_blueprint 方法注册。同时在 main.py 末尾加入 logging 配置代码，最终效果如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354# src/main.pyimport osimport sysimport loggingfrom flask import Flaskfrom flask import jsonifyfrom api.config.config import *from api.utils.database import dbfrom api.utils.responses import response_withimport api.utils.responses as respfrom api.routes.authors import author_routesfrom api.routes.books import book_routesif os.environ.get('WORK_ENV') == 'PROD': app_config = ProductionConfigelif os.environ.get('WORK_ENV') == 'TEST': app_config = TestingConfigelse: app_config = DevelopmentConfigapp = Flask(__name__)app.config.from_object(app_config)app.register_blueprint(author_routes, url_prefix='/api/authors')app.register_blueprint(book_routes, url_prefix='/api/books')@app.after_requestdef add_header(response): return response@app.errorhandler(400)def bad_request(e): logging.error(e) return response_with(resp.BAD_REQUEST_400)@app.errorhandler(500)def server_error(e): logging.error(e) return response_with(resp.SERVER_ERROR_500)@app.errorhandler(404)def not_found(e): logging.error(e) return response_with(resp.SERVER_ERROR_404)db.init_app(app)with app.app_context(): db.create_all()logging.basicConfig( stream=sys.stdout, format='%(asctime)s|%(levelname)s|%(filename)s:%(lineno)s|%(message)s', level=logging.DEBUG) 参考资料Building REST APIs with Flask]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Program</tag>
        <tag>Development</tag>
        <tag>Web</tag>
        <tag>REST</tag>
        <tag>API</tag>
        <tag>Flask</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 通过 Flask 框架构建 REST API（三）——基于 Token 的身份认证]]></title>
    <url>%2F2019%2F11%2F30%2Fpython-build-rest-api-with-flask-3-authorization-by-token%2F</url>
    <content type="text"><![CDATA[接上文Python 通过 Flask 框架构建 REST API（二）——优化项目架构。前面介绍了如何通过 Flask 和 marshmallow 框架写一个完整的架构清晰的项目，作为 REST API 实现基本的增删改查功能。本篇主要介绍在前文的基础上，借助 JWT（JSON Web Tokens）创建基于 Token 的身份认证机制。 一、安装依赖在前文创建的 Python 虚拟环境中，额外安装如下两个 Python 库：$ pip install passlib flask-jwt-extended 其中 passlib 用来提供对明文密码的哈希处理及验证，flask-jwt-extended 则引入了对 JWT 认证的支持。 users 数据库模型编辑 src/api/models/users.py 文件，创建 User 数据库模型和 UserSchema 序列化对象：123456789101112131415161718192021222324252627282930313233343536373839# src/api/models/users.pyfrom api.utils.database import dbfrom passlib.hash import pbkdf2_sha256 as sha256from marshmallow_sqlalchemy import ModelSchemafrom marshmallow import fieldsclass User(db.Model): __tablename__ = 'users' id = db.Column(db.Integer, primary_key=True) username = db.Column(db.String(120), unique=True, nullable=False) password = db.Column(db.String(120), nullable=False) def create(self): db.session.add(self) db.session.commit() return self @classmethod def find_by_username(cls, username): return cls.query.filter_by(username=username).first() @staticmethod def generate_hash(password): return sha256.hash(password) @staticmethod def verify_hash(password, hash): return sha256.verify(password, hash)class UserSchema(ModelSchema): class Meta(ModelSchema.Meta): model = User sqla_session = db.session id = fields.Number(dump_only=True) username = fields.String(required=True) 三、路由和响应逻辑编辑 src/api/routes/users.py 文件，加入 users 路由和对应的 POST 响应逻辑。其中 create_user() 用于创建用户并将用户信息存入数据库表，authenticate_user() 用于完成用户认证并返回 Token 字符串作为之后的访问令牌。123456789101112131415161718192021222324252627282930313233343536373839404142from flask import Blueprintfrom flask import requestfrom flask import url_for, render_template_stringfrom api.utils.responses import response_withfrom api.utils import responses as respfrom api.models.users import User, UserSchemafrom api.utils.database import dbfrom flask_jwt_extended import create_access_tokenimport datetimeuser_routes = Blueprint("user_routes", __name__)@user_routes.route('/', methods=['POST'])def create_user(): try: data = request.get_json() data['password'] = User.generate_hash(data['password']) user_schmea = UserSchema() user = user_schmea.load(data) result = user_schmea.dump(user.create()) return response_with(resp.SUCCESS_201) except Exception as e: print(e) return response_with(resp.INVALID_INPUT_422)@user_routes.route('/login', methods=['POST'])def authenticate_user(): try: data = request.get_json() current_user = User.find_by_username(data['username']) if not current_user: return response_with(resp.SERVER_ERROR_404) if User.verify_hash(data['password'], current_user.password): access_token = create_access_token(identity=data['username']) return response_with(resp.SUCCESS_201, value=&#123;'message': 'Logged in as &#123;&#125;'.format(current_user.username), "access_token": access_token&#125;) else: return response_with(resp.UNAUTHORIZED_401) except Exception as e: print(e) return response_with(resp.INVALID_INPUT_422) 四、配置编辑 src/main.py 文件，添加如下两行代码注册 users 路由：12from api.routes.users import user_routesapp.register_blueprint(user_routes, url_prefix='/api/users') 在 main.py 的 db.init_app(app) 代码前添加如下内容初始化 JWT 模块：12from flask_jwt_extended import JWTManagerjwt = JWTManager(app) 编辑 src/api/config/config.py 文件，在 ProductionConfig、DevelopmentConfig、TestingConfig 添加 JWT_SECRET_KEY 定义：1JWT_SECRET_KEY = &apos;SOME-RADOM-JWT-SECRET&apos; 编辑 src/api/utils/responses.py 文件，添加“未认证”和“禁止访问”的标准响应格式：1234567891011FORBIDDEN_403 = &#123; "http_code": 403, "code": "notAuthorized", "message": "You are not authorised to execute this."&#125;UNAUTHORIZED_401 = &#123; "http_code": 401, "code": "notAuthorized", "message": "Invalid authentication."&#125; 在 src/api/routes/authors.py 和 src/api/routes/books.py 两个路由文件的响应逻辑中，对 POST、PATCH、PUT 和 DELETE 请求加入验证 Token 的访问条件，以限制匿名用户对后台数据的修改。只需要在响应函数前加入 @jwt_required 装饰器即可，如：123456789from flask_jwt_extended import jwt_required@author_routes.route('/&lt;int:id&gt;', methods=['DELETE'])@jwt_requireddef delete_author(id): get_author = Author.query.get_or_404(id) db.session.delete(get_author) db.session.commit() return response_with(resp.SUCCESS_204) 五、测试运行 python run.py，使用 httpie 工具进行测试。 创建用户：1234$ http POST 127.0.0.1:5000/api/users/ username=admin password=flask&#123; "code": "success"&#125; 用户登录获取 Token：123456$ http POST 127.0.0.1:5000/api/users/login username=admin password=flask&#123; "access_token": "eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpYXQiOjE1NzUwODkyMTQsIm5iZiI6MTU3NTA4OTIxNCwianRpIjoiMTdiZjViZTEtMmJjMS00OWY2LWI2ZDgtZDRmZGZkZGE2MDI1IiwiZXhwIjoxNTc1MDkwMTE0LCJpZGVudGl0eSI6ImFkbWluIiwiZnJlc2giOmZhbHNlLCJ0eXBlIjoiYWNjZXNzIn0.iipDvS-BFwOQ9IfybEZxn2adrBu7sC7MznDoXfnWEsI", "code": "success", "message": "Logged in as admin"&#125; 使用 POST 方法向 authors 数据表中插入作者信息：12345678910$ http POST 127.0.0.1:5000/api/authors/ first_name=Jack last_name=SparrowHTTP/1.0 401 UNAUTHORIZEDContent-Length: 44Content-Type: application/jsonDate: Sat, 30 Nov 2019 04:53:15 GMTServer: Werkzeug/0.16.0 Python/3.7.3&#123; "msg": "Missing Authorization Header"&#125; 提示缺少 Authorization 请求头，即 POST 请求需要添加 Token 认证信息。 添加 Authorization 请求头（Token），重新发起 POST 请求：123456789101112131415161718$ http POST 127.0.0.1:5000/api/authors/ Authorization:"Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpYXQiOjE1NzUwODkyMTQsIm5iZiI6MTU3NTA4OTIxNCwianRpIjoiMTdiZjViZTEtMmJjMS00OWY2LWI2ZDgtZDRmZGZkZGE2MDI1IiwiZXhwIjoxNTc1MDkwMTE0LCJpZGVudGl0eSI6ImFkbWluIiwiZnJlc2giOmZhbHNlLCJ0eXBlIjoiYWNjZXNzIn0.iipDvS-BFwOQ9IfybEZxn2adrBu7sC7MznDoXfnWEsI" first_name=Jack last_name=SparrowHTTP/1.0 201 CREATEDAccess-Control-Allow-Origin: *Content-Length: 171Content-Type: application/jsonDate: Sat, 30 Nov 2019 04:57:05 GMTserver: Flask REST API&#123; "author": &#123; "books": [], "created": "2019-11-30 04:57:05", "first_name": "Jack", "id": 1.0, "last_name": "Sparrow" &#125;, "code": "success"&#125; 成功创建数据。 注意 Authorization 请求头的格式为 Authorization:&quot;Bearer &lt;token&gt;&quot;。 参考资料Building REST APIs with Flask]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Program</tag>
        <tag>Development</tag>
        <tag>Web</tag>
        <tag>REST</tag>
        <tag>API</tag>
        <tag>Flask</tag>
        <tag>Token</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 通过 Flask 框架构建 REST API（一）——数据库建模]]></title>
    <url>%2F2019%2F11%2F28%2Fpython-build-rest-api-with-flask-1-modeling%2F</url>
    <content type="text"><![CDATA[一、REST 简介REST（Representational State Transfer）是一种软件架构风格或者开发模式，主要面向可以在多种系统之间提供标准的数据通信功能的 Web 服务。而 REST 风格的 Web 服务则允许请求端通过统一的、预先定义好的无状态行为来访问和操作服务端数据。 下面简要说明 REST 风格的特点。 Uniform Interface：统一接口即使用 HTTP 提供的一系列方法（或动词，GET、POST 等）操作基于名词的 URI 表示的资源 Representations：RESTful 服务关注资源以及资源的访问，representation 即机器可读的对资源当前状态的定义。推荐使用 JSON Messages：客户端与服务器之间的请求/响应，以及其中包含的元数据（请求头、响应码等） Links Between Resources：REST 架构的核心概念即资源，资源可以包含 link 用于驱动多个资源之间的连接和跳转 Caching：缓存，REST API 中的缓存由 HTTP 头控制 Stateless：每个请求都必须是独立的，服务器不保存客户端的状态信息；客户端发送的请求必须包含能够让服务器理解请求的所有信息，因此客户端请求可以被任何可用的服务器应答。无状态原则为 REST 服务的可伸缩性（横向扩展）和 Caching 等特性提供了便利。 二、搭建开发环境virtualenv 创建 Python 虚拟环境：1234$ mkdir flask-mysql &amp;&amp; cd flask-mysql$ pip install virtualenv$ virtualenv venv$ source venv/bin/activate 虚拟环境中安装 Flask：$ pip install flask 最简单 Flask 应用代码：123456from flask import Flaskapp = Flask(__name__)@app.route(&apos;/&apos;)def hello_world(): return &apos;Hello, From Flask!&apos; 使用 $ FLASK_APP=app.py flask run 命令运行上面创建的 app.py 源文件。 SQLAlchemyFlask 是一个灵活的轻量级 Web 开发框架，它以插件的方式提供了针对各种数据源的交互支持。这里使用 ORM（Object Relational Mapper）类型的框架 sqlalchemy 作为与数据库交互的工具。安装命令如下：$ pip install flask-sqlalchemy pymysql 数据库配置代码：123app = Flask(__name__)app.config['SQLALCHEMY_DATABASE_URI'] = 'mysql+pymysql://&lt;mysql_username&gt;:&lt;mysql_password&gt;@&lt;mysql_host&gt;:&lt;mysql_port&gt;/&lt;mysql_db&gt;'db = SQLAlchemy(app) 如未安装 Mysql 服务，可以改为使用 Sqlite3 数据库引擎：app.config[&#39;SQLALCHEMY_DATABASE_URI&#39;] = &#39;sqlite:///./&lt;dbname&gt;.db&#39; 三、数据库建模创建 Authors 模型的代码如下：123456789101112131415161718class Authors(db.Model): id = db.Column(db.Integer, primary_key=True) name = db.Column(db.String(20)) specialisation = db.Column(db.String(50)) def create(self): db.session.add(self) db.session.commit() return self def __init__(self, name, specialisation): self.name = name self.specialisation = specialisation def __repr__(self): return '&lt;Author %d&gt;' % self.iddb.create_all() 上述模型代码会在关联的数据库中创建一张名为 authors 的表，同时 sqlalchemy 框架提供的基于模型的 API 也会用来与该数据表进行交互。 模型中的字段定义等同于如下的 SQL 语句：12345678910mysql&gt; show create table authors\G*************************** 1. row *************************** Table: authorsCreate Table: CREATE TABLE `authors` ( `id` int(11) NOT NULL AUTO_INCREMENT, `name` varchar(20) DEFAULT NULL, `specialisation` varchar(50) DEFAULT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB AUTO_INCREMENT=2 DEFAULT CHARSET=latin11 row in set (0.00 sec) 序列化序列化是指将 Web API 提供的后台数据以特定的形式（如 json 格式）展示给用户，方便前端程序调用。反序列化则是此过程的逆向操作，将前端发送给 API 的 json 格式的数据持久化到后端数据库中。 这里需要安装 marshmallow 框架将 SQLAlchemy 返回的数据对象转换成 JSON 格式。$ pip install marshmallow-sqlalchemy 添加如下代码创建 AuthorSchema 序列化器：1234567891011from marshmallow_sqlalchemy import ModelSchemafrom marshmallow import fieldsclass AuthorsSchema(ModelSchema): class Meta(ModelSchema.Meta): model = Authors sqla_session = db.session id = fields.Number(dump_only=True) name = fields.String(required=True) specialisation = fields.String(required=True) 四、Entrypoint创建 /authors entrypoint，响应 GET 方法获取 authors 模型中的所有数据对象并以 JSON 格式返回给用户。12345678from flask import Flask, request, jsonify, make_response@app.route('/authors', methods=['GET'])def index(): get_authors = Authors.query.all() author_schema = AuthorsSchema(many=True) authors = author_schema.dump(get_authors) return make_response(jsonify(&#123;"authors": authors&#125;)) 以 id 为筛选条件获取某条特定的数据纪录：123456@app.route('/authors/&lt;id&gt;', methods=['GET'])def get_author_by_id(id): get_author = Authors.query.get(id) author_schema = AuthorsSchema() author = author_schema.dump(get_author) return make_response(jsonify(&#123;"author": author&#125;)) 依次添加 POST、PUT、DELETE 等方法的响应逻辑，最终代码如下：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192from flask import Flask, request, jsonify, make_responsefrom flask_sqlalchemy import SQLAlchemyfrom marshmallow_sqlalchemy import ModelSchemafrom marshmallow import fieldsapp = Flask(__name__)app.config['SQLALCHEMY_DATABASE_URI'] = 'sqlite:///./test.db'db = SQLAlchemy(app)class Authors(db.Model): id = db.Column(db.Integer, primary_key=True) name = db.Column(db.String(20)) specialisation = db.Column(db.String(50)) def create(self): db.session.add(self) db.session.commit() return self def __init__(self, name, specialisation): self.name = name self.specialisation = specialisation def __repr__(self): return '&lt;Author %d&gt;' % self.iddb.create_all()class AuthorsSchema(ModelSchema): class Meta(ModelSchema.Meta): model = Authors sqla_session = db.session id = fields.Number(dump_only=True) name = fields.String(required=True) specialisation = fields.String(required=True)@app.route('/authors', methods=['GET'])def index(): get_authors = Authors.query.all() author_schema = AuthorsSchema(many=True) authors = author_schema.dump(get_authors) return make_response(jsonify(&#123;"authors": authors&#125;))@app.route('/authors/&lt;id&gt;', methods=['GET'])def get_author_by_id(id): get_author = Authors.query.get(id) author_schema = AuthorsSchema() author = author_schema.dump(get_author) return make_response(jsonify(&#123;"author": author&#125;))@app.route('/authors', methods=['POST'])def create_author(): data = request.get_json() author_schema = AuthorsSchema() author = author_schema.load(data) result = author_schema.dump(author.create()) return make_response(jsonify(&#123;"author": result&#125;), 200)@app.route('/authors/&lt;id&gt;', methods=['PUT'])def update_author_by_id(id): data = request.get_json() get_author = Authors.query.get(id) if data.get('specialisation'): get_author.specialisation = data['specialisation'] if data.get('name'): get_author.name = data['name'] db.session.add(get_author) db.session.commit() author_schema = AuthorsSchema(only=['id', 'name', 'specialisation']) author = author_schema.dump(get_author) return make_response(jsonify(&#123;"author": author&#125;))@app.route('/authors/&lt;id&gt;', methods=['DELETE'])def delete_author_by_id(id): get_author = Authors.query.get(id) db.session.delete(get_author) db.session.commit() return make_response("", 204)if __name__ == "__main__": app.run(host='0.0.0.0', debug=True) 五、测试运行 python app.py 命令开启 Web 服务，使用 httpie 工具测试 REST API。 POST 方法添加新的数据纪录：1234567891011121314$ http POST 127.0.0.1:5000/authors name="starky" specialisation="Python"HTTP/1.0 200 OKContent-Length: 92Content-Type: application/jsonDate: Wed, 27 Nov 2019 02:12:41 GMTServer: Werkzeug/0.16.0 Python/3.7.4&#123; "author": &#123; "id": 1.0, "name": "starky", "specialisation": "Python" &#125;&#125; GET 方法获取数据纪录：12345678910$ http GET 127.0.0.1:5000/authors&#123; "authors": [ &#123; "id": 1.0, "name": "starky", "specialisation": "Python" &#125; ]&#125; PUT 方法更新已有的数据纪录：12345678$ http PUT 127.0.0.1:5000/authors/1 name="skitar" specialisation="Go"&#123; "author": &#123; "id": 1.0, "name": "skitar", "specialisation": "Go" &#125;&#125; DELETE 方法删除某条数据：123456$ http DELETE 127.0.0.1:5000/authors/1$ http GET 127.0.0.1:5000/authors&#123; "authors": []&#125; 参考资料Building REST APIs with Flask]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Program</tag>
        <tag>Development</tag>
        <tag>Web</tag>
        <tag>REST</tag>
        <tag>API</tag>
        <tag>Flask</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 3.7 通过 asyncio 实现异步编程]]></title>
    <url>%2F2019%2F11%2F07%2Fpython-37-async-programming-with-asyncio%2F</url>
    <content type="text"><![CDATA[Python 中通过 asyncio 实现的异步编程主要包含如下三个模块： 事件循环（event loop）：每一个需要异步执行的任务都会在事件循环中注册，事件循环负责管理这些任务之间的执行流程 协程（Coroutine）：指用于执行具体某个异步任务的函数。函数体中的 await 关键字可以将协程的控制权释放给事件循环 Future：表示已经执行或者尚未执行的任务的结果 在异步程序的世界里，所有代码都运行在事件循环中，可以同时执行多个协程。这些协程异步地执行，直到遇到 await 关键字，此时该协程会让出程序控制权给事件循环，使得其他协程有机会发挥作用。需要注意的是，不能在同一个函数中同时包含异步和同步代码。即在同步函数中无法使用 await 关键字。 一、Hello World以下是一段简单的使用了 async 关键字的 Hello World 程序：12345678910import asyncioasync def hello(first_print, second_print): print(first_print) await asyncio.sleep(1) print(second_print)asyncio.run(hello("Welcome", "Good-bye"))# =&gt; Welcome# =&gt; Good-bye 上述代码的行为看上去更像是同步代码，先输出 Welcome，等待一秒钟之后，再输出 Good-bye。在进一步探究之前，先看下上述异步代码中出现的几个基本概念： Python 语言中，任何由 async def 定义的函数（即上面的 hello()）都可以称之为协程。调用协程函数所返回的对象称为协程对象。 函数 asyncio.run 是所有异步代码的主入口，只应该被调用一次。它负责组织传入的协程对象，同时管理 asyncio 的事件循环。 await 关键字用于将协程运行时获取的程序控制权移交给事件循环，并中断该协程的执行流程。 一个更现实的异步程序的示例如下：1234567891011121314151617181920212223242526import asyncioimport timeasync def say_something(delay, words): print(f"Started: &#123;words&#125;") await asyncio.sleep(delay) print(f"Finished: &#123;words&#125;")async def main(): print(f"Starting Tasks: &#123;time.strftime('%X')&#125;") task1 = asyncio.create_task(say_something(1, "First task")) task2 = asyncio.create_task(say_something(2, "Second task")) await task1 await task2 print(f"Finished Tasks: &#123;time.strftime('%X')&#125;")asyncio.run(main())# =&gt; Starting Tasks: 20:32:28# =&gt; Started: First task# =&gt; Started: Second task# =&gt; Finished: First task# =&gt; Finished: Second task# =&gt; Finished Tasks: 20:32:30 从同步执行的逻辑来看，应该是 task1 开始，等待一秒钟，结束；task2 开始，等待两秒钟，结束。共耗时 3 秒以上。异步程序实际的执行流程为，task1 和 task2 同时开始，各自等待一段时间后，先后结束。共耗时 2 秒。具体如下： task1 中的 say_something 协程开始执行 say_something 遇到 await 关键字时（await asyncio.sleep(delay)），协程暂停执行并等待 1 秒钟，在暂停的同时将程序控制权转移给事件循环 task2 从事件循环获取控制权开始执行，同样遇到 await 关键字时暂停协程并等待 2 秒钟，在暂停的同时将程序控制权转移给事件循环 task1 等待时间结束后，事件循环将控制权移交给 task1，恢复其协程的运行直至结束 task1 运行结束，task2 等待时间完成，task2 获取程序控制权并恢复运行直至结束。两个任务执行完成。 二、Awaitable 对象await 关键字用于将程序控制权移交给事件循环并中断当前协程的执行。它有以下几个使用规则： 只能用在由 async def 修饰的函数中，在普通函数中使用会抛出异常 调用一个协程函数后，就必须等待其执行完成并返回结果 await func() 中的 func() 必须是一个 awaitable 对象。即一个协程函数或者一个在内部实现了 __await__() 方法的对象，该方法会返回一个生成器 Awaitable 对象包含协程、Task 和 Future 等。 协程关于被 await 调用的协程，即上面的第二条规则，可以参考如下代码：12345678910111213141516171819202122232425import asyncioasync def mult(first, second): print(f"Calculating multiply of &#123;first&#125; and &#123;second&#125;") await asyncio.sleep(1) num_mul = first * second print(f"Multiply is &#123;num_mul&#125;") return num_mulasync def sum(first, second): print(f"Calculating sum of &#123;first&#125; and &#123;second&#125;") await asyncio.sleep(1) num_sum = first + second print(f"Sum is &#123;num_sum&#125;") return num_sumasync def main(first, second): await sum(first, second) await mult(first, second)asyncio.run(main(7, 8))# =&gt; Calculating sum of 7 and 8# =&gt; Sum is 15# =&gt; Calculating multiply of 7 and 8# =&gt; Multiply is 56 上述代码中由 await 修饰的两个协程函数 sum 和 mult 即为 awaitable 对象，从输出结果中可以看出，sum 函数先执行完毕并输出结果，随后 mult 函数执行并输出结果。即 await 调用的协程函数必须执行完毕后才能继续执行另外的 await 协程，这看上去并不符合异步程序的定义。 Tasks协程异步执行的关键在于 Tasks。当任意一个协程函数被类似于 asyncio.create_task() 的函数调用时，该协程就会自动排进由事件循环管理的执行流程里。在 asyncio 的定义中，由事件循环控制运行的协程即被称为任务。绝大多数情况下，编写异步代码即意味着需要使用 create_task() 方法将协程放进事件循环。 参考如下代码：12345678910111213141516171819202122232425262728import asyncioasync def mul(first, second): print(f"Calculating multiply of &#123;first&#125; and &#123;second&#125;") await asyncio.sleep(1) num_mul = first * second print(f"Multiply is &#123;num_mul&#125;") return num_mulasync def sum(first, second): print(f"Calculating sum of &#123;first&#125; and &#123;second&#125;") await asyncio.sleep(1) num_sum = first + second print(f"Sum is &#123;num_sum&#125;") return num_sumasync def main(first, second): sum_task = asyncio.create_task(sum(first, second)) mul_task = asyncio.create_task(mul(first, second)) await sum_task await mul_taskasyncio.run(main(7, 8))# =&gt; Calculating sum of 7 and 8# =&gt; Calculating multiply of 7 and 8# =&gt; Sum is 15# =&gt; Multiply is 56 对比上一段代码示例，从输出中可以看出，sum_task 和 mul_task 两个任务的执行流程符合异步程序的逻辑。sum_task 遇到 await asyncio.sleep(1) 语句后并没有让整个程序等待自己返回计算结果，而是中断执行并把控制权通过事件循环移交给 mul_task。两个任务先后执行并进入等待，最后在各自的等待时间结束后输出结果。 除 create_task() 函数以外，还可以使用 asyncio.gather() 函数创建异步任务：123456789101112131415161718192021222324import asyncioimport timeasync def greetings(): print("Welcome") await asyncio.sleep(1) print("Good by")async def main(): await asyncio.gather(greetings(), greetings())def say_greet(): start = time.perf_counter() asyncio.run(main()) elasped = time.perf_counter() - start print(f"Total time elasped: &#123;elasped&#125;")say_greet()# =&gt; Welcome# =&gt; Welcome# =&gt; Good by# =&gt; Good by# =&gt; Total time elasped: 1.0213364 实际两个任务完成的时间略大于 1 秒而不是 2 秒。 FuturesFutures 代表异步操作的预期结果，即该异步操作可能已经执行也可能尚未执行完毕。通常情况下并不需要在代码中显式地管理 Future 对象，这些工作一般由 asyncio 库隐式地处理。 当一个 Future 实例被创建成功以后，即代表该实例关联的异步操作还没有完成，但是会在未来的某个时间返回结果。asyncio 有一个 asyncio.wait_for(aws, timeout, *) 方法可以为异步任务设置超时时间。如果超过指定时间后异步操作仍未执行完毕，则该任务被取消并抛出 asyncio.TimeoutError 异常。timeout 的默认值为 None，即程序会阻塞并一直等待直到 Future 对象关联的操作返回结果。123456789101112131415import asyncioasync def long_time_taking_method(): await asyncio.sleep(4000) print("Completed the work")async def main(): try: await asyncio.wait_for(long_time_taking_method(), timeout=2) except asyncio.TimeoutError: print("Timeout occurred")asyncio.run(main())# =&gt; Timeout occurred 三、Async 实例代码通过创建子进程异步执行 Shell 命令：12345678910111213141516171819202122232425262728293031323334353637import asyncioasync def run(cmd): proc = await asyncio.create_subprocess_shell( cmd, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE) stdout, stderr = await proc.communicate() print(f'[&#123;cmd!r&#125; exited with &#123;proc.returncode&#125;]') if stdout: print(f'[stdout]\n&#123;stdout.decode()&#125;') if stderr: print(f'[stderr]\n&#123;stderr.decode()&#125;')async def main(): await asyncio.gather( run('sleep 2; echo "world"'), run('sleep 1; echo "hello"'), run('ls /zzz'))asyncio.run(main())# =&gt; ['ls /zzz' exited with 2]# =&gt; [stderr]# =&gt; ls: cannot access '/zzz': No such file or directory# =&gt; ['sleep 1; echo "hello"' exited with 0]# =&gt; [stdout]# =&gt; hello# =&gt; ['sleep 2; echo "world"' exited with 0]# =&gt; [stdout]# =&gt; world 通过 Queue 将工作负载分发给多个异步执行的 Task 处理：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576import asyncioimport randomimport timeasync def worker(name, queue): while True: # Get a "work item" out of the queue. sleep_for = await queue.get() # Sleep for the "sleep_for" seconds. await asyncio.sleep(sleep_for) # Notify the queue that the "work item" has been processed. queue.task_done() print(f'&#123;name&#125; has slept for &#123;sleep_for:.2f&#125; seconds')async def main(): # Create a queue that we will use to store our "workload". queue = asyncio.Queue() # Generate random timings and put them into the queue. total_sleep_time = 0 for _ in range(20): sleep_for = random.uniform(0.05, 1.0) total_sleep_time += sleep_for queue.put_nowait(sleep_for) # Create three worker tasks to process the queue concurrently. tasks = [] for i in range(3): task = asyncio.create_task(worker(f'worker-&#123;i&#125;', queue)) tasks.append(task) # Wait until the queue is fully processed. started_at = time.monotonic() await queue.join() total_slept_for = time.monotonic() - started_at # Cancel our worker tasks. for task in tasks: task.cancel() # Wait until all worker tasks are cancelled. await asyncio.gather(*tasks, return_exceptions=True) print('====') print(f'3 workers slept in parallel for &#123;total_slept_for:.2f&#125; seconds') print(f'total expected sleep time: &#123;total_sleep_time:.2f&#125; seconds')asyncio.run(main())# =&gt; worker-2 has slept for 0.12 seconds# =&gt; worker-1 has slept for 0.28 seconds# =&gt; worker-1 has slept for 0.12 seconds# =&gt; worker-0 has slept for 0.46 seconds# =&gt; worker-0 has slept for 0.49 seconds# =&gt; worker-2 has slept for 0.90 seconds# =&gt; worker-1 has slept for 0.62 seconds# =&gt; worker-1 has slept for 0.67 seconds# =&gt; worker-0 has slept for 0.85 seconds# =&gt; worker-2 has slept for 0.94 seconds# =&gt; worker-1 has slept for 0.45 seconds# =&gt; worker-2 has slept for 0.19 seconds# =&gt; worker-0 has slept for 0.99 seconds# =&gt; worker-2 has slept for 0.86 seconds# =&gt; worker-1 has slept for 0.97 seconds# =&gt; worker-0 has slept for 0.74 seconds# =&gt; worker-1 has slept for 0.58 seconds# =&gt; worker-2 has slept for 0.73 seconds# =&gt; worker-1 has slept for 0.27 seconds# =&gt; worker-0 has slept for 0.57 seconds# =&gt; ====# =&gt; 3 workers slept in parallel for 4.10 seconds# =&gt; total expected sleep time: 11.80 seconds 参考资料asyncio — Asynchronous I/O]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Program</tag>
        <tag>Advanced</tag>
        <tag>Concurrency</tag>
        <tag>Async</tag>
        <tag>Performance</tag>
        <tag>Eventloop</tag>
        <tag>Coroutine</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 通过 Celery 框架实现分布式任务队列]]></title>
    <url>%2F2019%2F11%2F06%2Fpython-celery-distributed-message-queue%2F</url>
    <content type="text"><![CDATA[Celery 是一个简单、灵活且可靠的分布式消息处理系统，主要用来作为任务队列对海量消息数据进行实时的处理，在多个程序线程或者主机之间传递和分发工作任务。同时也支持计划任务等需求。 一、环境配置Celery 框架自身并不对传入的消息进行存储，因此在使用前需要先安装第三方的 Message Broker。如 RabbitMQ 和 Redis 等。 安装 RabbitMQ对于 Linux 系统，执行以下命令：12345$ sudo apt-get install rabbitmq-server # 安装 RabbitMQ$ sudo rabbitmqctl add_user myuser mypassword # 添加用户 myuser/mypassword$ sudo rabbitmqctl add_vhost myvhost # 添加 vhost$ sudo rabbitmqctl set_user_tags myuser mytag$ sudo rabbitmqctl set_permissions -p myvhost myuser ".*" ".*" ".*" # 为用户 myuser 设置访问 myvhost 的权限 通过 Docker 安装的步骤如下：123456$ docker pull rabbitmq:3.8-management # 拉取 docker 镜像（包含 web 管理）# 启动 rabbitmq 容器$ docker run -d --name rabbitmq -p 5672:5672 -p 15672:15672 --hostname myRabbit \-e RABBITMQ_DEFAULT_VHOST=myvhost \-e RABBITMQ_DEFAULT_USER=myuser \-e RABBITMQ_DEFAULT_PASS=mypassword rabbitmq:3.8-management 安装 Redis$ sudo apt-get install redis-server 安装 Celery$ pip install celery 二、创建 Celery 应用Celery 应用是该框架所能提供的所有功能（如管理 tasks 和 workers 等）的入口，须确保它可以被其他模块导入。以下是一段简单的 Celery app 代码 tasks.py：12345678910# tasks.pyfrom celery import Celeryapp = Celery('tasks', broker='pyamqp://myuser:mypassword@localhost:5672/myvhost', backend='redis://localhost:6379/0')@app.taskdef add(x, y): return x + y 使用 RabbitMQ 作为 broker 接收和发送任务消息，使用 Redis 作为 backend 存储计算结果。 运行 Celery worker 服务$ celery -A tasks worker --loglevel=info 123456789101112131415161718192021222324$ celery -A tasks worker --loglevel=info -------------- celery@skitarniu-ubuntu18 v4.3.0 (rhubarb)---- **** -------- * *** * -- Linux-4.15.0-60-generic-x86_64-with-debian-buster-sid 2019-11-01 07:21:34-- * - **** ---- ** ---------- [config]- ** ---------- .&gt; app: tasks:0x7f4f30b84a90- ** ---------- .&gt; transport: amqp://myuser:**@localhost:5672/myvhost- ** ---------- .&gt; results: redis://localhost:6379/0- *** --- * --- .&gt; concurrency: 2 (prefork)-- ******* ---- .&gt; task events: OFF (enable -E to monitor tasks in this worker)--- ***** ----- -------------- [queues] .&gt; celery exchange=celery(direct) key=celery[tasks] . tasks.add[2019-11-01 07:21:35,316: INFO/MainProcess] Connected to amqp://myuser:**@127.0.0.1:5672/myvhost[2019-11-01 07:21:35,367: INFO/MainProcess] mingle: searching for neighbors[2019-11-01 07:21:36,535: INFO/MainProcess] mingle: all alone[2019-11-01 07:21:36,782: INFO/MainProcess] celery@skitarniu-ubuntu18 ready. 任务测试进入 Python Shell，执行以下命令发布任务并获取结果：12345678910&gt;&gt;&gt; from tasks import add&gt;&gt;&gt; result = add.delay(4, 4)&gt;&gt;&gt; result&lt;AsyncResult: 6f435bc7-f194-469c-837f-54d77f880ace&gt;&gt;&gt;&gt; result.ready()True&gt;&gt;&gt; result.get()8&gt;&gt;&gt; result.traceback&gt;&gt;&gt; delay() 方法用于发布任务消息，它是 apply_async() 方法的简写，即以异步的方式将任务需求提交给前面启动好的 worker 去处理。delay() 方法返回一个 AsyncResult 对象。result.ready() 方法可以用来检查提交的任务是否已经完成，返回布尔值。 result.get() 方法则用于获取执行完成后的结果。如任务未完成，则程序会一直等待直到有结果返回。因此该方法是阻塞的，并不常用。可以传入 timeout 参数指定等待的时间上限。如 result.get(timeout=1)，尝试获取任务执行后的结果，等待 1 秒。若 1 秒之后结果仍未返回，抛出 celery.exceptions.TimeoutError: The operation timed out. 异常。 如果任务执行过程中有抛出异常，则使用 get() 方法获取结果时会重新抛出该异常导致程序中断。可以通过修改 propagate 参数避免此情况：result.get(propagate=False)result.traceback 则用于获取任务的 traceback 信息。 三、Calling TasksCelery 定义了一些可供 task 实例调用的通用的 Calling API，包括三个方法和一些标准的执行选项： apply_async(args[, kwargs[, ...]])：发送任务消息给 worker delay(*args, **kwargs)：发送任务消息的简写形式，不支持执行选项 calling (__call__)：即在本地进程中直接执行任务函数，不通过 worker 异步执行 以下是一些常见的调用示例： T.delay(arg, kwarg=value) T.apply_async((arg,), {&#39;kwarg&#39;: value}) T.apply_async(countdown=10)10 秒之后开始执行某个任务 T.apply_async(eta=now + timedelta(seconds=10))10 秒之后开始执行某个任务 T.apply_async(countdown=60, expires=120)预计 1 分钟后开始执行，但 2 分钟后还未执行则失效 T.apply_async(expires=now + timedelta(days=2))2 天后失效 通过 countdown 设置任务的延迟执行：123456789101112131415&gt;&gt;&gt; from tasks import add&gt;&gt;&gt; result = add.apply_async((2, 3))&gt;&gt;&gt; result.get()5&gt;&gt;&gt; delay_result = add.apply_async((2, 3), countdown=15)&gt;&gt;&gt; delay_result.ready()False&gt;&gt;&gt; delay_result.ready()False&gt;&gt;&gt; delay_result.ready()False&gt;&gt;&gt; delay_result.ready()True&gt;&gt;&gt; delay_result.get()5 还可以通过 eta（estimated time of arrival） 设置延迟执行的时间：1234&gt;&gt;&gt; from datetime import datetime, timedelta&gt;&gt;&gt; tomorrow = datetime.utcnow() + timedelta(days=1)&gt;&gt;&gt; add.apply_async((2, 3), eta=tomorrow)&lt;AsyncResult: c7dc6d7f-8b87-49d1-8077-73d7f046d709&gt; 此时 worker 在命令行的日志输出如下：12[2019-11-06 05:16:21,362: INFO/MainProcess] Received task: tasks.add[c7dc6d7f-8b87-49d1-8077-73d7f046d709]ETA:[2019-11-07 05:16:06.652736+00:00] 四、计划任务Celery 允许像使用 crontab 那样按计划地定时执行某个任务。参考代码如下：12345678910111213141516171819# tasks.pyfrom celery import Celeryapp = Celery('tasks', broker='pyamqp://myuser:mypassword@localhost:5672/myvhost', backend='redis://localhost:6379/1')app.conf.beat_schedule = &#123; 'add-every-60-seconds': &#123; 'task': 'tasks.add', 'schedule': 60.0, 'args': (16, 16) &#125;,&#125;app.conf.timezone = 'UTC'@app.taskdef add(x, y): print(x + y) 运行 celery -A tasks worker -B 启动 worker 服务。-B 选项表示 beat，即 celery beat 服务，负责执行计划任务。 输出如下（每隔一分钟执行一次）：12345678$ celery -A tasks worker -B...[2019-11-06 05:41:34,057: WARNING/ForkPoolWorker-3] 32[2019-11-06 05:42:33,998: WARNING/ForkPoolWorker-3] 32[2019-11-06 05:43:34,056: WARNING/ForkPoolWorker-3] 32[2019-11-06 05:44:34,105: WARNING/ForkPoolWorker-3] 32[2019-11-06 05:45:34,157: WARNING/ForkPoolWorker-3] 32... 同时 Celery 也支持更复杂的 crontab 类型的时间规划：12345678910from celery.schedules import crontabapp.conf.beat_schedule = &#123; # Executes every Monday morning at 7:30 a.m. 'add-every-monday-morning': &#123; 'task': 'tasks.add', 'schedule': crontab(hour=7, minute=30, day_of_week=1), 'args': (16, 16), &#125;,&#125; Crontab 表达式支持的语法如下： Example Meaning crontab() 每分钟执行一次 crontab(minute=0, hour=0) 每天半夜 0 点执行 crontab(minute=0, hour=&#39;*/3&#39;) 每隔 3 小时执行一次（从 0 时开始） crontab(minute=0, hour=&#39;0,3,6,9,12,15,18,21&#39;) 同上一条 crontab(day_of_week=&#39;sunday&#39;) 只在周日执行，每隔一分钟执行一次 crontab(minute=&#39;*&#39;, hour=&#39;*&#39;, day_of_week=&#39;sun&#39;) 同上一条 crontab(minute=&#39;*/10&#39;, hour=&#39;3,17,22&#39;, day_of_week=&#39;thu,fri&#39;) 只在周四、周五的 3、17、22 时执行，每隔 10 分钟执行一次 crontab(minute=0, hour=&#39;*/2,*/3&#39;) 只在能被 2 或者 3 整除的整点执行 crontab(minute=0, hour=&#39;*/3,8-17&#39;) 在能被 3 整除的整点，和 8-17 点之间的整点执行 crontab(0, 0, day_of_month=&#39;2&#39;) 在每个月的第二天的 0 时执行 crontab(0, 0, day_of_month=&#39;11&#39;, month_of_year=&#39;5&#39;) 在每年的 5 月 11 号 0 点执行 参考资料Celery 4.3.0 documentation]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Program</tag>
        <tag>Advanced</tag>
        <tag>Concurrency</tag>
        <tag>Message</tag>
        <tag>Async</tag>
        <tag>Celery</tag>
        <tag>Distributed</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 中的协程（coroutine）简介]]></title>
    <url>%2F2019%2F11%2F05%2Fpython-coroutines-with-generators%2F</url>
    <content type="text"><![CDATA[Python 中的协程（Coroutine）是一种比线程（Thread）更加轻量的代码执行机构。与线程不同的是，协程完全是由程序本身控制，不需要操作系统内核对其进行调度，因而没有线程切换的开销。同时也不需要多线程中数据同步所依赖的锁机制，执行效率与多线程相比要高出很多。从句法上看，协程可以看作对生成器（Generator）的一种扩展，都是定义体中包含 yield 关键字的函数。启动生成器和协程所需的开销，与调用函数的开销相差无几。 英文中的 yield 有两个意思：产出和让步。 Python 生成器中的 yield 刚好符合了上述两个释义。yield item 会“产出”一个值提供给 next() 的调用方，同时做出“让步”，暂停生成器函数的执行，将程序控制权移交给调用方。直到调用方再次执行 next() 函数，生成器则继续“产出”下一个值。而协程中的 yield 通常出现在表达式右边（如 data = yield），可以产出值，也可以不产出。调用方可以通过 .send(data) 方法向协程提供数据。不管数据如何流动，yield 都是一种用来实现协作式多任务的流程控制工具。协程通过 yield 把控制器让步给中心调度程序，再由调度程序激活其他协程。 一、用作协程的生成器一个最简单的协程实现代码如下：12345# coroutine.pydef simple_coroutine(): print('-&gt; coroutine started') x = yield print('-&gt; coroutine received: ', x) 在 Python Shell 中进行测试，结果如下：123456789101112&gt;&gt;&gt; from coroutine import simple_coroutine&gt;&gt;&gt; my_coro = simple_coroutine()&gt;&gt;&gt; my_coro&lt;generator object simple_coroutine at 0x00000000021B4DC8&gt;&gt;&gt;&gt; next(my_coro)-&gt; coroutine started&gt;&gt;&gt; my_coro.send(42)-&gt; coroutine received: 42Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;StopIteration&gt;&gt;&gt; 上述代码的执行流程为： 主函数调用 next() 函数启动生成器。生成器在 yield 语句处暂停，没有产出值（None） my_coro.send(42) 向协程发送数据 42，协程恢复运行，抛出 StopIteration 异常 协程的状态可以使用 inspect.getgeneratorstate 获取协程的运行状态，共包含以下四种： GEN_CREATED：等待开始执行 GEN_RUNNING：协程正在执行 GEN_SUSPENDED：在 yield 表达式处暂停 GEN_CLOSE：协程执行结束 参考如下代码：123456def simple_coro2(a): print('-&gt; Started: a=', a) b = yield a print('-&gt; Received: b=', b) c = yield a + b print('-&gt; Received: c=', c) 1234567891011121314151617181920&gt;&gt;&gt; my_coro2 = simple_coro2(14)&gt;&gt;&gt; from inspect import getgeneratorstate&gt;&gt;&gt; getgeneratorstate(my_coro2)'GEN_CREATED'&gt;&gt;&gt; next(my_coro2)-&gt; Started: a= 1414&gt;&gt;&gt; getgeneratorstate(my_coro2)'GEN_SUSPENDED'&gt;&gt;&gt; my_coro2.send(28)-&gt; Received: b= 2842&gt;&gt;&gt; my_coro2.send(99)-&gt; Received: c= 99Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;StopIteration&gt;&gt;&gt; getgeneratorstate(my_coro2)'GEN_CLOSED'&gt;&gt;&gt; 协程 simple_coro2 的执行流程分为如下三个阶段： 调用 next(my_coro2)，协程启动，打印消息 Started: a= 14，执行 yield a，产出数字 14 调用 my_coro2.send(28)，把 28 赋值给 b，打印 Received: b= 28，执行 yield a + b，产出数字 42 调用 my_coro2.send(99)，把 99 赋值给 c，打印 Received: c= 99，协程终止 参考如下示意图： 二、使用协程计算移动平均值12345678910# coroaverager.pydef averager(): total = 0.0 count = 0 average = None while True: term = yield average total += term count += 1 average = total/count 在 Python Shell 中进行测试：12345678910&gt;&gt;&gt; from coroaverager import averager&gt;&gt;&gt; coro_avg = averager()&gt;&gt;&gt; next(coro_avg)&gt;&gt;&gt; coro_avg.send(10)10.0&gt;&gt;&gt; coro_avg.send(30)20.0&gt;&gt;&gt; coro_avg.send(5)15.0&gt;&gt;&gt; 从执行结果看，只要调用方不断把数据发送给协程 averager()，协程就会一直接收值并返回平均值的计算结果。其中 yield 表达式用于暂停协程的执行，返回计算结果给调用方，同时等待调用方继续发送数据给协程以恢复循环。 使用装饰器预激协程使用协程之前必须预激（即通过 next() 调用启动协程），可以创建如下的用于预激协程的装饰器：12345678910# coroutil.pyfrom functools import wrapsdef coroutine(func): @wraps(func) def primer(*args, **kwargs): gen = func(*args, **kwargs) next(gen) return gen return primer 此时 coroaverager.py 则可以改为如下版本：123456789101112from coroutil import coroutine@coroutinedef averager(): total = 0.0 count = 0 average = None while True: term = yield average total += term count += 1 average = total/count 123456789&gt;&gt;&gt; from coroaverager import averager&gt;&gt;&gt; coro_avg = averager()&gt;&gt;&gt; coro_avg.send(10)10.0&gt;&gt;&gt; coro_avg.send(20)15.0&gt;&gt;&gt; coro_avg.send(15)15.0&gt;&gt;&gt; 三、协程返回值1234567891011121314151617# coroaverager2.pyfrom collections import namedtupleResult = namedtuple('Result', 'count average')def averager(): total = 0.0 count = 0 average = None while True: term = yield if term is None: break total += term count += 1 average = total/count return Result(count, average) 1234567891011&gt;&gt;&gt; from coroaverager2 import averager&gt;&gt;&gt; coro_avg = averager()&gt;&gt;&gt; next(coro_avg)&gt;&gt;&gt; coro_avg.send(10)&gt;&gt;&gt; coro_avg.send(20)&gt;&gt;&gt; coro_avg.send(15)&gt;&gt;&gt; coro_avg.send(None)Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;StopIteration: Result(count=3, average=15.0)&gt;&gt;&gt; 此处的 yield 表达式只接收数据而不返回任何结果，直到协程收到 None，循环终止协程结束，返回最终结果 Result(...)。return 表达式返回的值会传递给调用方，赋值给 StopIteration 异常的一个属性。因此最终结果需要通过 try...except 语句来捕获。好在 yield from 结构会在内部自动捕获 StopIteration 异常，并把 value 属性的值变成 yield from 表达式的值。 四、yield fromyield from 可以用来简化 for 循环中的 yield 表达式。12345678910111213141516def gen(): for c in 'AB': yield c for i in range(1, 3): yield iprint(list(gen()))# =&gt; ['A', 'B', 1, 2]def gen2(): yield from 'AB' yield from range(1, 3)print(list(gen2()))# =&gt; ['A', 'B', 1, 2] yield from 表示的含义为，在生成器 gen 中使用 yield from subgen() 时，subgen 会获得控制权，其产出的值传递给 gen 的调用方。同时 gen 会阻塞，等待 subgen 终止。 使用 yield from 可以连接多个可迭代对象：123456789def chain(*iterables): for it in iterables: yield from its = 'ABC't = tuple(range(3))print(list(chain(s, t)))# =&gt; ['A', 'B', 'C', 0, 1, 2] yield from 的主要功能是打开双向通道，把最外层的调用方与最内层的子生成器连接起来，使得两者可以直接发送和产出值，而不必在位于中间的协程中添加大量处理异常（StopIteration）的代码。 使用 yield from 计算移动平均值：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566from collections import namedtupleResult = namedtuple('Result', 'count average')# the subgeneratordef averager(): # &lt;1&gt; total = 0.0 count = 0 average = None while True: term = yield # &lt;2&gt; if term is None: # &lt;3&gt; break total += term count += 1 average = total/count return Result(count, average) # &lt;4&gt;# the delegating generatordef grouper(results, key): # &lt;5&gt; while True: # &lt;6&gt; results[key] = yield from averager() # &lt;7&gt;# the client code, a.k.a. the callerdef main(data): # &lt;8&gt; results = &#123;&#125; for key, values in data.items(): group = grouper(results, key) next(group) # &lt;9&gt; for value in values: group.send(value) # &lt;10&gt; group.send(None) # &lt;11&gt; report(results)# output reportdef report(results): for key, result in sorted(results.items()): group, unit = key.split(';') print('&#123;:2&#125; &#123;:5&#125; averaging &#123;:.2f&#125;&#123;&#125;'.format( result.count, group, result.average, unit))data = &#123; 'girls;kg': [40.9, 38.5, 44.3, 42.2, 45.2, 41.7, 44.5, 38.0, 40.6, 44.5], 'girls;m': [1.6, 1.51, 1.4, 1.3, 1.41, 1.39, 1.33, 1.46, 1.45, 1.43], 'boys;kg': [39.0, 40.8, 43.2, 40.8, 43.1, 38.6, 41.4, 40.6, 36.3], 'boys;m': [1.38, 1.5, 1.32, 1.25, 1.37, 1.48, 1.25, 1.49, 1.46],&#125;if __name__ == '__main__': main(data)# =&gt; 9 boys averaging 40.42kg# =&gt; 9 boys averaging 1.39m# =&gt; 10 girls averaging 42.04kg# =&gt; 10 girls averaging 1.43m 注释： ：averager 协程作为被调用的子生成器，计算移动平均值 ：调用方函数 main 发送的值都会绑定给 term 变量 ：终止条件，若无此句代码，子生成器永不终止，yield from 也会一直阻塞 ：返回的 Result 对象将作为 grouper 函数中 yield from 表达式的值 ：grouper 函数作为委派生成器，相当于子生成器和调用方之间的“管道” ：这里 while 循环的每次遍历都会创建一个 averager 协程实例 ：grouper 通过 .send 发送的每一个值都会被 yield from 导向给 averager 实例，待 averager 处理完所有 grouper 发送的值之后，最终的计算结果绑定给 results[key]。while 循环则继续创建另一个 averager 实例用来处理更多的值 ：main 即客户端代码，子生成器的调用方 ：预激协程对象 ：发送数据给 grouper，该数据实际上对 grouper 并不可见，而是通过 yield from 传递给了 averager 中的 term = yield。 我要吐了。再见参考资料Fluent Python]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Program</tag>
        <tag>Advanced</tag>
        <tag>Concurrency</tag>
        <tag>Performance</tag>
        <tag>Coroutine</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 通过 concurrent.futures 模块以异步方式处理并发需求]]></title>
    <url>%2F2019%2F10%2F30%2Fpython-concurrent-futures-async-programming%2F</url>
    <content type="text"><![CDATA[对于计算机程序的执行流而言，I/O 操作通常是时间占比非常大的一块。在当前的硬件设备中，绝大多数 I/O 操作要比 CPU 慢上几个数量级。比如大约花费 1 毫秒写入一个网络 socket，对应到 2.4GHz 的处理器上，同样的时间则可以执行 24000000 条指令。 在一般的同步执行的程序中，当代码遇到 I/O 操作时（如读取一个文件或者写入一个网络 socket），必须暂时中止和内核的交互，去请求 I/O 并等待传输完成。这种因 I/O 阻塞而产生的等待在某些情况下往往导致执行效率的低下和响应的延迟。 而在异步执行的流程中，当一个程序进入 I/O 等待时，其控制权会被移交给程序的其他部分，直到 I/O 操作完成时才可以重新获取（这称为上下文切换）。异步程序中一般会有一个事件循环用来监听事件并分派任务。比如用一个异步程序做一次网络写操作，该请求会立即返回（程序控制权移交给事件循环），即便写操作实际上并未发生。此时程序允许执行另外的函数和运算。当写操作完成时，会触发一个特定的事件，由事件循环响应该事件并执行关联的操作。 同步程序下载网络资源以下代码为使用同步的方式下载网络中存放的多张国旗图片：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849# flags.pyimport osimport timeimport sysimport requestsPOP20_CC = ('CN IN US ID BR PK NG BD RU JP ' 'MX PH VN ET EG DE IR TR CD FR').split()BASE_URL = 'http://flupy.org/data/flags'DEST_DIR = 'downloads/'def save_flag(img, filename): if not os.path.exists(DEST_DIR): os.makedirs(DEST_DIR) path = os.path.join(DEST_DIR, filename) with open(path, 'wb') as fp: fp.write(img)def get_flag(cc): url = '&#123;&#125;/&#123;cc&#125;/&#123;cc&#125;.gif'.format(BASE_URL, cc=cc.lower()) resp = requests.get(url) return resp.contentdef show(text): print(text, end=' ') sys.stdout.flush()def download_many(cc_list): for cc in sorted(cc_list): image = get_flag(cc) show(cc) save_flag(image, cc.lower() + '.gif') return len(cc_list)def main(download_many): t0 = time.time() count = download_many(POP20_CC) elapsed = time.time() - t0 msg = '\n&#123;&#125; flags downloaded in &#123;:.2f&#125;s' print(msg.format(count, elapsed))if __name__ == '__main__': main(download_many)# =&gt; BD BR CD CN DE EG ET FR ID IN IR JP MX NG PH PK RU TR US VN# =&gt; 20 flags downloaded in 218.70s 使用 concurrent.futures 模块下载concurrent.futures 模块的主要特色是包含 ThreadPoolExecutor 和 ProcessPoolExecutor 两个类，它们实现的接口可以分别在不同的线程或进程中执行可调用的对象，并且它们内部都维护着一个工作线程（或进程）池和一个任务队列。 下载代码如下（引用了上一个源文件 flags.py 中的几个功能函数）：12345678910111213141516171819202122232425from concurrent import futuresfrom flags import save_flag, get_flag, show, mainMAX_WORKERS = 20def download_one(cc): image = get_flag(cc) show(cc) save_flag(image, cc.lower() + '.gif') return ccdef download_many(cc_list): workers = min(MAX_WORKERS, len(cc_list)) with futures.ThreadPoolExecutor(workers) as executor: res = executor.map(download_one, sorted(cc_list)) return len(list(res))if __name__ == "__main__": main(download_many)# =&gt; EG BD NG CD IN ET RU ID CN FR US PK PH MX IR VN BR JP DE TR# =&gt; 20 flags downloaded in 83.36s 其中最关键的部分为 download_many 函数。workers 变量用于指定 ThreadPoolExecutor 对象使用的工作线程的数量，取预设的最大线程数（MAX_WORKERS）和实际下载数目（len(cc_list)）中的较小的值；with 语句用于使用指定数量（workers）的工作线程初始化 ThreadPoolExecutor 对象；map 方法类似于内置的 map 函数，目的是使 download_one 函数可以被多个工作线程并行地调用。它会返回一个生成器对象，该生成器可以被遍历以获取每一个 download_one 执行后的结果。 Future 对象Python 标准库中包含两个名为 Future 的类：concurrent.futures.Future 和 asyncio.Future 。这两个类的实例都表示可能已经完成或者尚未完成的延迟计算。Future 对象并不是一个立即产生的实际结果，更像是一种“承诺”，需要等待其执行完毕并被我们期待的值所填充。在等待“承诺”兑现的过程中程序可以同时执行其他运算。 Future 是 concurrent.futures 模块和 asyncio 库的重要组件，但是在上面的代码中并没有直接调用 Future 对象。通常情况下，Future 不应该由用户显式地创建，而只能由并发框架实例化。Future 如同它的名字一样，代表将要发生的事情，而确定某件事未来会发生的唯一方式是其执行时间已经排定。Executor.submit() 方法会接收一个可调用对象作为参数并为其排期，返回一个 Future 对象。 用户代码也不应该改变 Future 对象的状态。并发框架会在 Future 代表的延迟计算结束后自动改变 Future 的状态，没有办法人为地控制延迟计算何时结束。Future 具有非阻塞的 .done() 方法，返回布尔值表明 Future 对应的调用对象是否已经执行完毕。此外还有 .add_done_callback() 方法用于在 Future 运行结束后执行特定的回调函数。concurrency.futures.Future 实例还有 .results() 方法用以获取 Future 执行的可调用对象的结果。该方法会阻塞调用方所在的线程，直到可调用对象运行结束并返回结果。result() 方法可以接收可选的 timeout 参数用于设定超时时间。 从更现实的角度理解 Future 对象，可以参考如下代码：12345678910111213141516171819202122232425262728293031323334from concurrent import futuresfrom flags import save_flag, get_flag, show, mainimport timeMAX_WORKERS = 20def download_one(cc): image = get_flag(cc) show(cc) save_flag(image, cc.lower() + '.gif') return ccdef download_many(cc_list): cc_list = cc_list[:5] with futures.ThreadPoolExecutor(max_workers=3) as executor: to_do = [] for cc in sorted(cc_list): future = executor.submit(download_one, cc) to_do.append(future) msg = 'Scheduled for &#123;&#125;: &#123;&#125;' print(msg.format(cc, future)) results = [] for future in futures.as_completed(to_do): res = future.result() msg = '&#123;&#125; result: &#123;!r&#125;' print(msg.format(future, res)) results.append(res) return len(results)if __name__ == '__main__': main(download_many) 此处的代码只通过 3 个工作线程获取 5 个国家的国旗图片。和之前的代码相比，将 download_many 函数中较抽象的 executor.map 替换成了两个 for 循环： executor.submit 用于排定可调用对象（即 download_one(cc)）给多个工作线程执行，返回一个 Future 对象表示这个待执行的操作。 futures.as_completed 则用于在 Future 运行结束后获取可执行对象（即 download_one(cc)）返回的结果。 本例中的 future.result() 方法绝不会阻塞，因为 future 是由 as_completed 函数返回的。 最终输出如下：123456789101112Scheduled for BR: &lt;Future at 0x4524a48 state=running&gt;Scheduled for CN: &lt;Future at 0x3ca46c8 state=running&gt;Scheduled for ID: &lt;Future at 0x453f7c8 state=running&gt;Scheduled for IN: &lt;Future at 0x2ab1308 state=pending&gt;Scheduled for US: &lt;Future at 0x4530888 state=pending&gt;ID BR CN &lt;Future at 0x453f7c8 state=finished returned str&gt; result: &apos;ID&apos;&lt;Future at 0x3ca46c8 state=finished returned str&gt; result: &apos;CN&apos;&lt;Future at 0x4524a48 state=finished returned str&gt; result: &apos;BR&apos;IN &lt;Future at 0x2ab1308 state=finished returned str&gt; result: &apos;IN&apos;US &lt;Future at 0x4530888 state=finished returned str&gt; result: &apos;US&apos;5 flags downloaded in 1.57s 从输出中可以看出，前三个 Future 的状态是 running，后两个 Future 的状态是 pending，因为只有三个工作线程可供分配。同时，如果多运行几次，输出结果的顺序也是有变化的。 关于 GILCython 解释器不是线程安全的，它通过 GIL（Global Interpreter Lock，全局解释器锁）强制性地一次只允许一个线程执行 Python 代码。所以通常一个 Python 进程并不能同时使用多个 CPU 核心，即不能够将一个 Python 进程拆分成多个独立执行的线程在多个 CPU 核心上以并行的方式运行。但是 Python 标准库中所有执行阻塞型 I/O 操作的函数，在等待系统返回结果时都会释放 GIL。即在 I/O 密集型的需求场景下，Python 程序可以通过多线程来提升性能。 参考资料Fluent Python]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Program</tag>
        <tag>Advanced</tag>
        <tag>Concurrency</tag>
        <tag>Async</tag>
        <tag>Performance</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Windows 10 通过命令行查看 Wi-Fi 密码]]></title>
    <url>%2F2019%2F10%2F27%2Ffind-wlan-password-in-windows-10%2F</url>
    <content type="text"><![CDATA[本方法仅适用于 Win10 系统，且已连接过该 WiFi 并勾选了记住密码选项。即电脑可自动连接某热点，但之后又不记得该热点密码，无法手动接入其他设备。方法来自于油管上的某视频分享。 前提 Win10 系统 管理员权限 已连接过某热点并保存了密码 步骤一、管理员权限打开命令行可以按下 win+R 组合键调出运行窗口，输出 cmd 并回车。如打开的是普通用户的命令提示符窗口，可右击任务栏中的命令行图标，再右击弹出的 命令提示符 文字，选择 以管理员身份运行。 步骤二、netsh 命令netsh 是 Windows 下的网络配置命令，直接输入该命令并回车，即可进入一个交互式的命令行。运行 wlan show profile 即可获取当前系统中保存的无线热点信息，输出如下：12345678910111213141516C:\Users\Administrator&gt;netshnetsh&gt;wlan show profile接口 WLAN 上的配置文件:组策略配置文件(只读)--------------------------------- &lt;无&gt;用户配置文件------------- 所有用户配置文件 : 3-1-2102 所有用户配置文件 : 103netsh&gt; 上面的 3-1-2102 和 103 即表示此台计算机连接过并记住了密码的热点名称。 步骤三、获取热点密码继续使用 wlan show profile &lt;SSID&gt; key=clear 命令即可获取某热点（由 &lt;SSID&gt; 指定）的具体连接信息：12345678910111213141516171819202122232425262728293031323334353637383940netsh&gt;wlan show profile 103 key=clear接口 WLAN 上的配置文件 103:=======================================================================已应用: 所有用户配置文件配置文件信息------------------- 版本 : 1 类型 : 无线局域网 名称 : 103 控制选项 : 连接模式 : 自动连接 网络广播 : 只在网络广播时连接 AutoSwitch : 请勿切换到其他网络 MAC 随机化: 禁用连接设置--------------------- SSID 数目 : 1 SSID 名称 :“103” 网络类型 : 结构 无线电类型 : [ 任何无线电类型 ] 供应商扩展名 : 不存在安全设置----------------- 身份验证 : WPA2 - 个人 密码 : CCMP 身份验证 : WPA2 - 个人 密码 : GCMP 安全密钥 : 存在 关键内容 : House1886753****费用设置------------- 费用 : 不可用netsh&gt; 其中 关键内容 条目中的 House1886753**** 即代表无线热点 103 的连接密码。 命令列表 netsh wlan show profile wlan show profile &lt;SSID&gt; key=clear]]></content>
      <categories>
        <category>Hack</category>
      </categories>
      <tags>
        <tag>Security</tag>
        <tag>Windows</tag>
        <tag>Wlan</tag>
        <tag>Network</tag>
        <tag>Hack</tag>
        <tag>WiFi</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python Cookbook —— 数据结构技巧]]></title>
    <url>%2F2019%2F10%2F27%2Fpython-cookbook-data-structure-tricks%2F</url>
    <content type="text"><![CDATA[一、序列展开与多重赋值任何数据序列（或可迭代对象）都可以只通过一个赋值操作展开自身并同时赋值给多个变量。只需要确保被赋值的变量的数目和结构与序列相符合即可。如：123456&gt;&gt;&gt; p = (4, 5)&gt;&gt;&gt; x, y = p&gt;&gt;&gt; x4&gt;&gt;&gt; y5 对于嵌套的多层次序列，此种方式的多重赋值仍然适用：123456789101112131415&gt;&gt;&gt; data = [ 'ACME', 50, 91.1, (2012, 12, 21) ]&gt;&gt;&gt; name, shares, price, date = data&gt;&gt;&gt; name'ACME'&gt;&gt;&gt; date(2012, 12, 21)&gt;&gt;&gt; name, shares, price, (year, mon, day) = data&gt;&gt;&gt; name'ACME'&gt;&gt;&gt; year2012&gt;&gt;&gt; mon12&gt;&gt;&gt; day21 序列展开同样适用于任何可迭代对象（即内部实现了 __iter__ 方法的对象，如字符串等），示例如下：12345678&gt;&gt;&gt; s = 'Hello'&gt;&gt;&gt; a, b, c, d, e = s&gt;&gt;&gt; a'H'&gt;&gt;&gt; b'e'&gt;&gt;&gt; e'o' 忽略特定的值在序列展开并赋值时，可以忽略指定项目，方法如下：123456&gt;&gt;&gt; data = [ 'ACME', 50, 91.1, (2012, 12, 21) ]&gt;&gt;&gt; _, shares, price, _ = data&gt;&gt;&gt; shares50&gt;&gt;&gt; price91.1 展开为固定长度在序列展开并赋值时，被赋值的变量数目和结构如果与序列本身不符合，则会报出 ValueError ：12345&gt;&gt;&gt; p = (4, 5, 6)&gt;&gt;&gt; x, y = pTraceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;ValueError: too many values to unpack (expected 2) 如上面的示例，当展开的序列长度大于期望的变量数目时，可以对变量使用 * 操作符将多个项目保存在列表结构中，如：123456789def drop_first_last(grades): first, *middle, last = sorted(grades) return sum(middle) / len(middle)grades = [ 100, 94, 96, 88, 70, 62, 80 ]print(f"&#123; drop_first_last(grades) &#125;")# =&gt; 85.6print(sum([94, 96, 88, 70, 80]) / 5)# =&gt; 85.6 其他应用示例如：12345678&gt;&gt;&gt; record = ('Dave', 'dave@example.com', '773-555-1212', '847-555-1212')&gt;&gt;&gt; name, email, *phone_numbers = record&gt;&gt;&gt; name'Dave'&gt;&gt;&gt; email'dave@example.com'&gt;&gt;&gt; phone_numbers['773-555-1212', '847-555-1212'] * 操作符的特性使其可以很方便地应用在字符串分割中：12345678&gt;&gt;&gt; line = 'nobody:*:-2:-2:Unprivileged User:/var/empty:/usr/bin/false'&gt;&gt;&gt; uname, *fields, homedir, sh = line.split(':')&gt;&gt;&gt; uname'nobody'&gt;&gt;&gt; homedir'/var/empty'&gt;&gt;&gt; sh'/usr/bin/false' 结合 _ 还可以写出如下代码以在赋值时忽略序列中的多个项目：123456&gt;&gt;&gt; record = ('ACME', 50, 123.45, (12, 18, 2012))&gt;&gt;&gt; name, *_, (*_, year) = record&gt;&gt;&gt; name'ACME'&gt;&gt;&gt; year2012 二、检索序列中最大或最小的 N 个项目Python 的 heapq 模块包含 nlargest() 和 nsmallest() 函数，可以用来筛选某个数据序列中最大或最小的 N 个值。12345678import heapqnums = [1, 8, 2, 23, 7, -4, 18, 23, 42, 37, 2]print(heapq.nlargest(3, nums))print(heapq.nsmallest(3, nums))# =&gt; [42, 37, 23]# =&gt; [-4, 1, 2] 上面两个函数也可以接收一个名为 key 的参数，使其可以应用在更复杂的数据结构中：123456789101112131415161718import heapqportfolio = [ &#123;'name': 'IBM', 'shares': 100, 'price': 91.1&#125;, &#123;'name': 'AAPL', 'shares': 50, 'price': 543.22&#125;, &#123;'name': 'FB', 'shares': 200, 'price': 21.09&#125;, &#123;'name': 'HPQ', 'shares': 35, 'price': 31.75&#125;, &#123;'name': 'YHOO', 'shares': 45, 'price': 16.35&#125;, &#123;'name': 'ACME', 'shares': 75, 'price': 115.65&#125;]cheap = heapq.nsmallest(2, portfolio, key=lambda s: s['price'])expensive = heapq.nlargest(2, portfolio, key=lambda s: s['price'])print(cheap)# =&gt; [&#123;'name': 'YHOO', 'shares': 45, 'price': 16.35&#125;, &#123;'name': 'FB', 'shares': 200, 'price': 21.09&#125;]print(expensive)# =&gt; [&#123;'name': 'AAPL', 'shares': 50, 'price': 543.22&#125;, &#123;'name': 'ACME', 'shares': 75, 'price': 115.65&#125;] heapq 中的 nlargest() 和 nsmallest() 两个函数的原理都是将数据序列转换为列表结构并且以堆（heap）的形式进行组织。heap 最重要的属性为， heap[0] 永远是序列中最小的值。使用 heapq.heappop() 方法可以获取 heap 中的第一个值（即最小值），而之前第二小的值则移动到第一的位置。即不断调用 heappop() 可以一直获取当前序列中最小的值。123456789101112131415&gt;&gt;&gt; nums = [1, 8, 2, 23, 7, -4, 18, 23, 42, 37, 2]&gt;&gt;&gt; import heapq&gt;&gt;&gt; heapq.heapify(nums)&gt;&gt;&gt; nums[-4, 2, 1, 23, 7, 2, 18, 23, 42, 37, 8]&gt;&gt;&gt; heapq.heappop(nums)-4&gt;&gt;&gt; heapq.heappop(nums)1&gt;&gt;&gt; heapq.heappop(nums)2&gt;&gt;&gt; heapq.heappop(nums)2&gt;&gt;&gt; heapq.heappop(nums)7 PS：如果只想检索某一个最大值或最小值，min() 或者 max() 更快；如果 nlargest(N, items) 或 nsmallest(N, items) 中的 N 与 items 集合的大小很接近，则先对集合进行排序再分片的方式更快一点，即 sorted(itmes)[:N]（实际 nlargest 和 nsmallest 内部本身也是这样实现的） 三、实现一个加权队列以下的代码实现了一种支持加权的队列，即队列中的项目会按指定的权重排序，并且每次调用 pop 方法都可以返回权重最高的项目。1234567891011121314151617181920212223242526272829import heapqclass PriorityQueue: def __init__(self): self._queue = [] self._index = 0 def push(self, item, priority): heapq.heappush(self._queue, (-priority, self._index, item)) self._index += 1 def pop(self): return heapq.heappop(self._queue)[-1]# test codeq = PriorityQueue()q.push('foo', 1)q.push('bar', 5)q.push('spam', 4)q.push('grok', 1)print(q.pop())# =&gt; barprint(q.pop())# =&gt; spamprint(q.pop())# =&gt; fooprint(q.pop())# =&gt; grok 其中 heapq.heappush() 函数可以向 _queue 序列中插入数据，之后再使用 heapq.heappop() 函数获取序列中的数据时，总能保证取出的数据是当时队列中最小的那个。pop 和 push 操作的复杂度为 O(logN)，比普通列表形式的操作（复杂度为 O(N)）效率更高。同时数据是以元组 (-priority, _index, item) 的形式存入到 _queue 队列中的，元组可以依据自身的每个数据项依次进行比对并确定大小关系。因而权重 -priority 可以作为排序的首要依据（加负号是为了使权重高的值更小，可以优先被 pop() 返回）。当权重一样即 -priority 的值相同时，则根据插入的顺序（_index）返回数据。 四、比较字典中的 Value参考下面的代码示例：12345678910prices = &#123; 'ACME': 45.23, 'AAPL': 612.78, 'IBM': 205.55, 'HPQ': 37.20, 'FB': 10.75&#125;print(min(prices)) # =&gt; 'AAPL'print(max(prices)) # =&gt; 'IBM' 从输出中可以看出，min(prices) 和 max(prices) 函数只是处理字典 prices 中的 keys，对 values 则不做任何操作。可以将其改为如下形式：12min(prices.values()) # =&gt; 10.75max(prices.values()) # =&gt; 612.78 则此时上述两个函数又只对字典中的 values 有效，输出的结果中也只包含 values，不包含与之关联的 key 的值。 如果想根据 values 对字典中的数据进行排序，同时输出的结果中既包含 value，又包含与之关联的 key。则可以使用 zip() 函数。zip() 函数以多个可迭代的对象作为参数，将各对象中位置对应的元素打包成一个个元组。回到前面的需求，则可以将字典的 keys 和 values 拆分到两个列表中，再通过 zip() 函数将其中的数据合并成一个个元组（等同于之前的键值对），而 value 作为元组的第一个元素。12345678910111213141516171819prices = &#123; 'ACME': 45.23, 'AAPL': 612.78, 'IBM': 205.55, 'HPQ': 37.20, 'FB': 10.75&#125;for item in zip(prices.values(), prices.keys()): print(item)# =&gt; (45.23, 'ACME')# =&gt; (612.78, 'AAPL')# =&gt; (205.55, 'IBM')# =&gt; (37.2, 'HPQ')# =&gt; (10.75, 'FB')max_price = max(zip(prices.values(), prices.keys()))print(max_price)# =&gt; (612.78, 'AAPL') 五、去除序列中的重复元素集合（set）是 Python 中的一种数据结构，它包含了一系列无序的不重复元素。因此可以通过将其他类型的数据转为 set 类型，为序列中的数据去重。但此种方法不能保留原序列中数据项原本的排列顺序（因为 set 是无序的）。123&gt;&gt;&gt; a = [1, 5, 2, 1, 9, 1, 5, 10]&gt;&gt;&gt; set(a)&#123;1, 2, 5, 9, 10&#125; 下面的函数 dedupe 则实现了去重并保留原本的排列顺序：123456789101112def dedupe(items): seen = set() for item in items: if item not in seen: yield item seen.add(item)a = [1, 5, 2, 1, 9, 1, 5, 10]print(list(dedupe(a)))# =&gt; [1, 5, 2, 9, 10] 而对于更复杂的数据结构，比如对如下的列表进行去重操作：[{&#39;x&#39;:1, &#39;y&#39;:2}, {&#39;x&#39;:1, &#39;y&#39;:3}, {&#39;x&#39;:1, &#39;y&#39;:2}, {&#39;x&#39;:2, &#39;y&#39;:4}] 则可以将 dedupe 函数改为如下形式：123456789101112131415def dedupe(items, key=None): seen = set() for item in items: val = item if key is None else key(item) if val not in seen: yield item seen.add(val)a = [&#123;'x':1, 'y':2&#125;, &#123;'x':1, 'y':3&#125;, &#123;'x':1, 'y':2&#125;, &#123;'x':2, 'y':4&#125;]print(list(dedupe(a, key=lambda d: (d['x'],d['y']))))# =&gt; [&#123;'x': 1, 'y': 2&#125;, &#123;'x': 1, 'y': 3&#125;, &#123;'x': 2, 'y': 4&#125;]print(list(dedupe(a, key=lambda d: (d['x']))))# =&gt; [&#123;'x': 1, 'y': 2&#125;, &#123;'x': 2, 'y': 4&#125;] 额，自行体会吧。。。 六、找出序列中最常出现的项collections.Counter 类可以用来寻找序列中出现次数最多的几个项目。1234567891011121314151617181920212223from collections import Counterwords = [ 'look', 'into', 'my', 'eyes', 'look', 'into', 'my', 'eyes', 'the', 'eyes', 'the', 'eyes', 'the', 'eyes', 'not', 'around', 'the', 'eyes', "don't", 'look', 'around', 'the', 'eyes', 'look', 'into', 'my', 'eyes', "you're", 'under']word_counts = Counter(words)top_three = word_counts.most_common(3)print(top_three)# =&gt; [('eyes', 8), ('the', 5), ('look', 4)]morewords = ['why', 'are', 'you', 'not', 'looking', 'in', 'my', 'eyes']word_counts.update(morewords)print(word_counts.most_common(3))# =&gt; [('eyes', 9), ('the', 5), ('look', 4)]a = Counter(words)b = Counter(morewords)print(a + b)# =&gt; Counter(&#123;'eyes': 9, 'the': 5, 'look': 4, 'my': 4, 'into': 3, 'not': 2, 'around': 2, "don't": 1, "you're": 1, 'under': 1, 'why': 1, 'are': 1, 'you': 1, 'looking': 1, 'in': 1&#125;) 参考资料Python Cookbook, 3rd Edition]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Tricks</tag>
        <tag>Python</tag>
        <tag>Program</tag>
        <tag>Advanced</tag>
        <tag>Cookbook</tag>
        <tag>Datastructure</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django REST framework 教程（2）—— 序列化]]></title>
    <url>%2F2019%2F10%2F11%2Fdjango-rest-framework-2-serializer%2F</url>
    <content type="text"><![CDATA[参考自 Django REST framework 官方文档 ，权作笔记。 一、环境搭建通过 Python3 内置的 venv 模块创建虚拟开发环境：123$ python3 -m venv env # 创建名为 env 的虚拟环境$ source env/bin/activate # 启用虚拟环境# env\Scripts\activate Windows 系统启用虚拟环境 安装依赖库：123$ pip install django$ pip install djangorestframework$ pip install pygments # We&apos;ll be using this for the code highlighting 初始化项目123$ django-admin startproject tutorial$ cd tutorial$ python manage.py startapp snippets 编辑 tutorial/tutorial/settings.py 配置文件，将 rest_framework 和 snippets APP 添加到 INSTALLED_APPS 选项中。12345INSTALLED_APPS = [ ... &apos;rest_framework&apos;, &apos;snippets.apps.SnippetsConfig&apos;,] 二、创建模型编辑 tutorial/snippets/models.py 文件创建数据模型。内容如下：12345678910111213141516171819from django.db import modelsfrom pygments.lexers import get_all_lexersfrom pygments.styles import get_all_stylesLEXERS = [item for item in get_all_lexers() if item[1]]LANGUAGE_CHOICES = sorted([(item[1][0], item[0]) for item in LEXERS])STYLE_CHOICES = sorted([(item, item) for item in get_all_styles()])class Snippet(models.Model): created = models.DateTimeField(auto_now_add=True) title = models.CharField(max_length=100, blank=True, default=&apos;&apos;) code = models.TextField() linenos = models.BooleanField(default=False) language = models.CharField(choices=LANGUAGE_CHOICES, default=&apos;python&apos;, max_length=100) style = models.CharField(choices=STYLE_CHOICES, default=&apos;friendly&apos;, max_length=100) class Meta: ordering = [&apos;created&apos;] 使用以下命令完成数据库迁移操作：12$ python manage.py makemigrations snippets$ python manage.py migrate 三、创建序列化器序列化是指将 Web API 提供的后台数据已特定的形式（如 json 格式）展示给用户，方便前端程序调用。 以下代码反映了一个序列化器的基本逻辑。编辑 tutorial/snippets/serializers.py 文件，内容如下：1234567891011121314151617181920212223242526272829from rest_framework import serializersfrom snippets.models import Snippet, LANGUAGE_CHOICES, STYLE_CHOICESclass SnippetSerializer(serializers.Serializer): id = serializers.IntegerField(read_only=True) title = serializers.CharField(required=False, allow_blank=True, max_length=100) code = serializers.CharField(style=&#123;&apos;base_template&apos;: &apos;textarea.html&apos;&#125;) linenos = serializers.BooleanField(required=False) language = serializers.ChoiceField(choices=LANGUAGE_CHOICES, default=&apos;python&apos;) style = serializers.ChoiceField(choices=STYLE_CHOICES, default=&apos;friendly&apos;) def create(self, validated_data): &quot;&quot;&quot; Create and return a new `Snippet` instance, given the validated data. &quot;&quot;&quot; return Snippet.objects.create(**validated_data) def update(self, instance, validated_data): &quot;&quot;&quot; Update and return an existing `Snippet` instance, given the validated data. &quot;&quot;&quot; instance.title = validated_data.get(&apos;title&apos;, instance.title) instance.code = validated_data.get(&apos;code&apos;, instance.code) instance.linenos = validated_data.get(&apos;linenos&apos;, instance.linenos) instance.language = validated_data.get(&apos;language&apos;, instance.language) instance.style = validated_data.get(&apos;style&apos;, instance.style) instance.save() return instance ModelSerializerDjango REST framework 框架通过 serializers 中的某些类对序列化器的逻辑功能做了封装与抽象，因此可以通过这些类完成与上面代码同样的需求，同时代码的冗余度大大降低。可以将 tutorial/snippets/serializers.py 文件改为如下版本：1234567from rest_framework import serializersfrom snippets.models import Snippetclass SnippetSerializer(serializers.ModelSerializer): class Meta: model = Snippet fields = [&apos;id&apos;, &apos;title&apos;, &apos;code&apos;, &apos;linenos&apos;, &apos;language&apos;, &apos;style&apos;] 四、Django Shell 操作 Serializer运行以下命令进入 Django Shell：$ python manage.py shell 。 通过 Snippet 模型创建数据纪录：12345678&gt;&gt;&gt; from snippets.models import Snippet&gt;&gt;&gt; from snippets.serializers import SnippetSerializer&gt;&gt;&gt; from rest_framework.renderers import JSONRenderer&gt;&gt;&gt; from rest_framework.parsers import JSONParser&gt;&gt;&gt; snippet = Snippet(code=&apos;foo = &quot;bar&quot;\n&apos;)&gt;&gt;&gt; snippet.save()&gt;&gt;&gt; snippet = Snippet(code=&apos;print(&quot;hello, world&quot;)\n&apos;)&gt;&gt;&gt; snippet.save() 序列化数据对象并以 JSON 的形式展示：123456&gt;&gt;&gt; serializer = SnippetSerializer(snippet)&gt;&gt;&gt; serializer.data&#123;&apos;id&apos;: 2, &apos;title&apos;: &apos;&apos;, &apos;code&apos;: &apos;print(&quot;hello, world&quot;)\n&apos;, &apos;linenos&apos;: False, &apos;language&apos;: &apos;python&apos;, &apos;style&apos;: &apos;friendly&apos;&#125;&gt;&gt;&gt; content = JSONRenderer().render(serializer.data)&gt;&gt;&gt; contentb&apos;&#123;&quot;id&quot;:2,&quot;title&quot;:&quot;&quot;,&quot;code&quot;:&quot;print(\\&quot;hello, world\\&quot;)\\n&quot;,&quot;linenos&quot;:false,&quot;language&quot;:&quot;python&quot;,&quot;style&quot;:&quot;friendly&quot;&#125;&apos; 反序列化：12345678910&gt;&gt;&gt; import io&gt;&gt;&gt; stream = io.BytesIO(content)&gt;&gt;&gt; data = JSONParser().parse(stream)&gt;&gt;&gt; serializer = SnippetSerializer(data=data)&gt;&gt;&gt; serializer.is_valid()True&gt;&gt;&gt; serializer.validated_dataOrderedDict([(&apos;title&apos;, &apos;&apos;), (&apos;code&apos;, &apos;print(&quot;hello, world&quot;)&apos;), (&apos;linenos&apos;, False), (&apos;language&apos;, &apos;python&apos;), (&apos;style&apos;, &apos;friendly&apos;)])&gt;&gt;&gt; serializer.save()&lt;Snippet: Snippet object (3)&gt; 序列化多个模型实例：123&gt;&gt;&gt; serializer = SnippetSerializer(Snippet.objects.all(), many=True)&gt;&gt;&gt; serializer.data[OrderedDict([(&apos;id&apos;, 1), (&apos;title&apos;, &apos;&apos;), (&apos;code&apos;, &apos;foo = &quot;bar&quot;\n&apos;), (&apos;linenos&apos;, False), (&apos;language&apos;, &apos;python&apos;), (&apos;style&apos;, &apos;friendly&apos;)]), OrderedDict([(&apos;id&apos;, 2), (&apos;title&apos;, &apos;&apos;), (&apos;code&apos;, &apos;print(&quot;hello, world&quot;)\n&apos;), (&apos;linenos&apos;, False), (&apos;language&apos;, &apos;python&apos;), (&apos;style&apos;, &apos;friendly&apos;)]), OrderedDict([(&apos;id&apos;, 3), (&apos;title&apos;, &apos;&apos;), (&apos;code&apos;, &apos;print(&quot;hello, world&quot;)&apos;), (&apos;linenos&apos;, False), (&apos;language&apos;, &apos;python&apos;), (&apos;style&apos;, &apos;friendly&apos;)])] 五、视图编辑 tutorial/snippets/views.py 文件，创建 snippet_list 和 snippet_detail 两个视图函数，并添加上对 GET、POST 等请求的响应逻辑。内容如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051from django.http import HttpResponse, JsonResponsefrom django.views.decorators.csrf import csrf_exemptfrom rest_framework.parsers import JSONParserfrom snippets.models import Snippetfrom snippets.serializers import SnippetSerializer@csrf_exemptdef snippet_list(request): &quot;&quot;&quot; List all code snippets, or create a new snippet. &quot;&quot;&quot; if request.method == &apos;GET&apos;: snippets = Snippet.objects.all() serializer = SnippetSerializer(snippets, many=True) return JsonResponse(serializer.data, safe=False) elif request.method == &apos;POST&apos;: data = JSONParser().parse(request) serializer = SnippetSerializer(data=data) if serializer.is_valid(): serializer.save() return JsonResponse(serializer.data, status=201) return JsonResponse(serializer.errors, status=400)@csrf_exemptdef snippet_detail(request, pk): &quot;&quot;&quot; Retrieve, update or delete a code snippet. &quot;&quot;&quot; try: snippet = Snippet.objects.get(pk=pk) except Snippet.DoesNotExist: return HttpResponse(status=404) if request.method == &apos;GET&apos;: serializer = SnippetSerializer(snippet) return JsonResponse(serializer.data) elif request.method == &apos;PUT&apos;: data = JSONParser().parse(request) serializer = SnippetSerializer(snippet, data=data) if serializer.is_valid(): serializer.save() return JsonResponse(serializer.data) return JsonResponse(serializer.errors, status=400) elif request.method == &apos;DELETE&apos;: snippet.delete() return HttpResponse(status=204) 六、路由配置编辑 tutorial/snippets/urls.py 文件，完成视图与 URL 路径的关联：1234567from django.urls import pathfrom snippets import viewsurlpatterns = [ path(&apos;snippets/&apos;, views.snippet_list), path(&apos;snippets/&lt;int:pk&gt;/&apos;, views.snippet_detail),] 在 tutorial/tutorial/urls.py 配置文件中引入 snippets 应用的路由配置：1234567from django.contrib import adminfrom django.urls import pathurlpatterns = [ path(&apos;&apos;, include(&apos;snippets.urls&apos;)), path(&apos;admin/&apos;, admin.site.urls),] 七、测试运行 $ python manage.py runserver 命令开启测试服务，使用 httpie 工具对 API 进行访问测试，结果如下： GET 方法获取数据：12345678910111213141516171819$ http -b 172.20.19.76:8000/snippets/[ &#123; &quot;code&quot;: &quot;foo = \&quot;bar\&quot;\n&quot;, &quot;id&quot;: 1, &quot;language&quot;: &quot;python&quot;, &quot;linenos&quot;: false, &quot;style&quot;: &quot;friendly&quot;, &quot;title&quot;: &quot;&quot; &#125;, &#123; &quot;code&quot;: &quot;print(\&quot;hello, world\&quot;)\n&quot;, &quot;id&quot;: 2, &quot;language&quot;: &quot;python&quot;, &quot;linenos&quot;: false, &quot;style&quot;: &quot;friendly&quot;, &quot;title&quot;: &quot;&quot; &#125;] POST 方法添加新的数据：123456789$ http -b POST 172.20.19.76:8000/snippets/ code=&quot;a = 1\nb = 2\nprint(a + b)&quot;&#123; &quot;code&quot;: &quot;a = 1\\nb = 2\\nprint(a + b)&quot;, &quot;id&quot;: 4, &quot;language&quot;: &quot;python&quot;, &quot;linenos&quot;: false, &quot;style&quot;: &quot;friendly&quot;, &quot;title&quot;: &quot;&quot;&#125;]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Program</tag>
        <tag>Development</tag>
        <tag>Web</tag>
        <tag>Django</tag>
        <tag>REST</tag>
        <tag>API</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 中的同步编程与异步编程（async）实例对比]]></title>
    <url>%2F2019%2F10%2F11%2Fexamples-about-python-sync-and-async-program%2F</url>
    <content type="text"><![CDATA[同步程序（synchronous program）即每次只执行一个步骤，只有在当前操作的步骤彻底完成之后，才会移动到下一个步骤继续执行。异步程序（asynchronous program）同样每次只接受一个操作步骤，但是它并不会等待当前的操作步骤执行完成后才移动到下一个步骤。 一、同步编程下面的代码负责从队列（queue）中依次取出某个特定的任务并执行。队列是 Python 中的一种实现了 FIFO （先入先出）规则的数据结构，可以调用其 put() 和 get() 方法以相同的顺序存入和取出数据。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950import queuedef task(name, work_queue): if work_queue.empty(): print(f"Task &#123;name&#125; nothing to do") else: while not work_queue.empty(): count = work_queue.get() total = 0 print(f"Task &#123;name&#125; running") for x in range(count): total += 1 print(f"Task &#123;name&#125; total: &#123;total&#125;")def main(): """ This is the main entry point for the program. """ # Create the queue of 'work' work_queue = queue.Queue() # Put some 'work' in the queue for work in [15, 10, 5, 2]: work_queue.put(work) # Create some synchronous tasks tasks = [ (task, "One", work_queue), (task, "Two", work_queue) ] # Run the tasks for t, n, q in tasks: t(n, q)if __name__ == "__main__": main()# =&gt; Task One running# =&gt; Task One total: 15# =&gt; Task One running# =&gt; Task One total: 10# =&gt; Task One running# =&gt; Task One total: 5# =&gt; Task One running# =&gt; Task One total: 2# =&gt; Task Two nothing to do 其中的 task() 函数接收字符串和队列作为参数，可以从队列中不断地取出数字并完成计数动作，直到队列为空。而 main() 函数则依次指派两个 task() 函数用于处理同一个队列中的数据。从输出结果中可以看出，排在前面的 Task One 处理了队列中的所有数据，待数据处理完成即 Task One 中的循环退出之后，排在后面的 Task Two 继续执行，但已经没有任何数据需要处理。 二、协作下面版本的程序允许两个工作函数共同处理位于同一队列中的多个任务，达到“协作”的效果。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051import queuedef task(name, queue): while not queue.empty(): count = queue.get() total = 0 print(f"Task &#123;name&#125; running") for x in range(count): total += 1 yield print(f"Task &#123;name&#125; total: &#123;total&#125;")def main(): """ This is the main entry point for the program. """ # Create the queue of 'work' work_queue = queue.Queue() # Put some 'work' in the queue for work in [15, 10, 5, 2]: work_queue.put(work) # Create some tasks tasks = [ task("One", work_queue), task("Two", work_queue) ] # Run the tasks done = False while not done: for t in tasks: try: next(t) except StopIteration: tasks.remove(t) if len(tasks) == 0: done = Trueif __name__ == "__main__": main()# =&gt; Task One running# =&gt; Task Two running# =&gt; Task Two total: 10# =&gt; Task Two running# =&gt; Task One total: 15# =&gt; Task One running# =&gt; Task Two total: 5# =&gt; Task One total: 2 代码中最关键的部分即 yield 关键字，它将 task() 函数转变成了生成器（generator）。 生成器函数和其他 Python 函数在调用时并没有太大区别，但是当生成器中的 yield 语句执行时，对程序的控制权会被返还给调用生成器的函数（caller）。有趣的地方在于，还可以通过将生成器传递给 next() 函数，将程序的控制权再次转移给生成器本身。结合前面生成器中的 yield 关键字，最终达到在程序中来回切换上下文的效果。 上面的 main() 函数通过调用 next(t) 将程序控制权转移给某个 Task() 生成器，而 Task() 代码中的 yield 又会在执行完指派给它的某个任务之后，返回结果并将控制权还给 main() 主程序。而 main() 函数继续通过 next(t) 将队列中的下一个任务分配给另一个 Task() 执行，直到任务队列为空。从输出结果中也可以看出，Task One 和 Task Two 都参与了队列中任务的处理。 虽然 Task Two total: 10 最后先于 Task One total: 15 输出，但并不代表该程序是异步执行的。Task Two 先输出的原因仅在于它执行的计数次数少。从逻辑上看，这种通过生成器函数切换程序上下文的方式虽然使多个工作函数同时参与到了任务中，但仍旧总是在当前任务执行完之后才进行上下文的切换。因此仍属于同步程序。 三、阻塞式请求下面的代码示例和上一个版本基本相同，只不过在 Task() 代码中使用 time.sleep(delay) 函数用于模拟阻塞请求，取代了之前的计数操作。阻塞请求是指在一定时间内阻止 CPU 去处理其他任何事件的代码，比如 time.sleep(delay) 就会阻止 CPU 做其他任何操作直到等待的时间结束。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758import timeimport queuedef task(name, queue): while not queue.empty(): delay = queue.get() start_time = time.time() print(f"Task &#123;name&#125; running") time.sleep(delay) print(f"Task &#123;name&#125; total elapsed time: &#123;time.time() - start_time:.1f&#125;") yielddef main(): """ This is the main entry point for the program. """ # Create the queue of 'work' work_queue = queue.Queue() # Put some 'work' in the queue for work in [15, 10, 5, 2]: work_queue.put(work) tasks = [ task("One", work_queue), task("Two", work_queue) ] # Run the tasks start_time = time.time() done = False while not done: for t in tasks: try: next(t) except StopIteration: tasks.remove(t) if len(tasks) == 0: done = True print(f"\nTotal elapsed time: &#123;time.time() - start_time:.1f&#125;")if __name__ == "__main__": main()# =&gt; Task One running# =&gt; Task One total elapsed time: 15.0# =&gt; Task Two running# =&gt; Task Two total elapsed time: 10.0# =&gt; Task One running# =&gt; Task One total elapsed time: 5.0# =&gt; Task Two running# =&gt; Task Two total elapsed time: 2.0# =&gt; Total elapsed time: 32.0 从输出的结果中可以看出，协作的方式并没有加速程序的运行。整个程序执行完耗费的全部时间刚好是阻塞代码引起的延迟时间之和。 四、非阻塞式请求12345678910111213141516171819202122232425262728293031323334353637383940414243import asyncioimport timeasync def task(name, work_queue): while not work_queue.empty(): delay = await work_queue.get() start_time = time.time() print(f"Task &#123;name&#125; running") await asyncio.sleep(delay) print(f"Task &#123;name&#125; total elapsed time: &#123;time.time() - start_time:.1f&#125;")async def main(): """ This is the main entry point for the program. """ # Create the queue of 'work' work_queue = asyncio.Queue() # Put some 'work' in the queue for work in [15, 10, 5, 2]: await work_queue.put(work) # Run the tasks start_time = time.time() await asyncio.gather( asyncio.create_task(task("One", work_queue)), asyncio.create_task(task("Two", work_queue)), ) print(f"\nTotal elapsed time: &#123;time.time() - start_time:.1f&#125;")if __name__ == "__main__": asyncio.run(main())# =&gt; Task One running# =&gt; Task Two running# =&gt; Task Two total elapsed time: 10.0# =&gt; Task Two running# =&gt; Task Two total elapsed time: 5.0# =&gt; Task Two running# =&gt; Task One total elapsed time: 15.0# =&gt; Task Two total elapsed time: 2.0# =&gt; Total elapsed time: 17.0 同上一个版本的程序不同，此处使用 await asyncio.sleep(delay) 替代了之前的 time.sleep(delay) 和 yield 语句，用来创建一个非阻塞的延迟并将程序控制权返还给调用者（即 main() 函数）。而最后一行代码 asyncio.run(main()) 将创建一个事件循环（event loop），用于执行 main() 函数也就是两个 task() 实例。 事件循环是 Python 中 async 机制的核心内容，它负责调用执行所有的工作代码。当具体的某个工作任务在执行过程中遇到 await 语句时，则程序将控制权返还给主事件循环。同时事件循环持续不断地监视着所有工作任务的状态，并根据触发的某个具体事件将程序控制权分配给对应的工作代码。由此保证 CPU 一直处于“紧张”状态，不会被某个 IO 密集型操作阻塞而处于等待状态。await asyncio.sleep(delay) 对于 CPU 而言是非阻塞的。在处理该代码时，CPU 并不会等待延迟的时间结束，而是在事件循环的任务队列中注册一个 sleep 事件，紧接着将程序的控制权直接交还给了事件循环，由事件循环监视可能会触发的事件（比如延迟结束）并分配任务给工作代码。 五、同步 HTTP 请求代码如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869import queueimport requestsimport timedef task(name, work_queue): with requests.Session() as session: while not work_queue.empty(): url = work_queue.get() print(f"Task &#123;name&#125; getting URL: &#123;url&#125;") start = time.perf_counter() session.get(url) elapsed = time.perf_counter() - start print(f"Task &#123;name&#125; elapsed time: &#123;elapsed&#125;") yielddef main(): """ This is the main entry point for the program """ # Create the queue of work work_queue = queue.Queue() # Put some work in the queue for url in [ "https://www.baidu.com", "https://www.jianshu.com/", "https://www.jd.com/", "https://www.tmall.com/", "https://www.zhihu.com/", "https://www.douban.com/", "https://www.bilibili.com/", ]: work_queue.put(url) tasks = [task("One", work_queue), task("Two", work_queue)] # Run the tasks done = False start = time.perf_counter() while not done: for t in tasks: try: next(t) except StopIteration: tasks.remove(t) if len(tasks) == 0: done = True elapsed = time.perf_counter() - start print(f"Total elapsed time: &#123;elapsed&#125;")if __name__ == "__main__": main()# =&gt; Task One getting URL: https://www.baidu.com# =&gt; Task One elapsed time: 0.1149543260000001# =&gt; Task Two getting URL: https://www.jianshu.com/# =&gt; Task Two elapsed time: 0.09059067599999993# =&gt; Task One getting URL: https://www.jd.com/# =&gt; Task One elapsed time: 0.06162170499999986# =&gt; Task Two getting URL: https://www.tmall.com/# =&gt; Task Two elapsed time: 0.16310405699999975# =&gt; Task One getting URL: https://www.zhihu.com/# =&gt; Task One elapsed time: 5.111851316# =&gt; Task Two getting URL: https://www.douban.com/# =&gt; Task Two elapsed time: 0.31463871600000015# =&gt; Task One getting URL: https://www.bilibili.com/# =&gt; Task One elapsed time: 0.18364096799999974# =&gt; Total elapsed time: 6.082241783 六、异步 HTTP 请求1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162import asyncioimport aiohttpimport timeasync def task(name, work_queue): async with aiohttp.ClientSession() as session: while not work_queue.empty(): url = await work_queue.get() print(f"Task &#123;name&#125; getting URL: &#123;url&#125;") start = time.perf_counter() async with session.get(url) as response: await response.text() elapsed = time.perf_counter() - start print(f"Task &#123;name&#125; elapsed time: &#123;elapsed&#125;")async def main(): """ This is the main entry point for the program """ # Create the queue of work work_queue = asyncio.Queue() # Put some work in the queue for url in [ "https://www.baidu.com", "https://www.jianshu.com/", "https://www.jd.com/", "https://www.tmall.com/", "https://www.zhihu.com/", "https://www.douban.com/", "https://www.bilibili.com/", ]: await work_queue.put(url) # Run the tasks start = time.perf_counter() await asyncio.gather( asyncio.create_task(task("One", work_queue)), asyncio.create_task(task("Two", work_queue)), ) elapsed = time.perf_counter() - start print(f"Total elapsed time: &#123;elapsed&#125;")if __name__ == "__main__": asyncio.run(main())# =&gt; Task One getting URL: https://www.baidu.com# =&gt; Task Two getting URL: https://www.jianshu.com/# =&gt; Task One elapsed time: 0.21159282599999996# =&gt; Task One getting URL: https://www.jd.com/# =&gt; Task One elapsed time: 0.09301454900000006# =&gt; Task One getting URL: https://www.tmall.com/# =&gt; Task One elapsed time: 0.09578595199999995# =&gt; Task One getting URL: https://www.zhihu.com/# =&gt; Task Two elapsed time: 0.4948436540000001# =&gt; Task Two getting URL: https://www.douban.com/# =&gt; Task One elapsed time: 0.218484707# =&gt; Task One getting URL: https://www.bilibili.com/# =&gt; Task Two elapsed time: 0.2670722139999997# =&gt; Task One elapsed time: 0.22822054000000014# =&gt; Total elapsed time: 0.867767489 参考资料Getting Started With Async Features in Python]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Program</tag>
        <tag>Development</tag>
        <tag>async</tag>
        <tag>concurency</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python Web 开发之 Django Models 详解]]></title>
    <url>%2F2019%2F10%2F09%2Fpython-web-development-django-models%2F</url>
    <content type="text"><![CDATA[Django 是由 Python 语言编写的基于 MVC（即 Model View Controller）架构的 Web 开发框架。其架构中的模型（Model）主要负责处理 Web 应用的数据逻辑部分，包括定义数据存储单位（即数据库表）的字段属性和行为、与数据库交互以及其他相关联的操作。通常一个模型映射于一个特定的数据库表。 Django 中的模型有以下几个基本属性： 每个模型都是继承自 django.db.models.Model 类的子类 模型类的属性分别对应于与之相关联的数据表中的字段 Django 会自动生成用于访问数据库的 API 一、基本使用项目初始化在开始编写 Web 应用代码之前，需要先使用如下命令初始化一个 Django 项目并创建应用：123$ django-admin startproject myproject$ cd myproject$ python manage.py startapp myapp 最终生成的项目目录结构如下：123456789101112131415myproject ├─manage.py │ ├─myapp │ ├─admin.py │ ├─apps.py │ ├─models.py │ ├─tests.py │ ├─views.py │ └─migrations │ └─myproject ├─settings.py ├─urls.py └─wsgi.py 定义模型用于定义模型的代码通常保存在 myproject/myapp/models.py 文件中。下面的代码即定义了一个简单的 Person 模型：12345from django.db import modelsclass Person(models.Model): first_name = models.CharField(max_length=30) last_name = models.CharField(max_length=30) 其中的 first_name 和 last_name 两个类属性即对应于数据库表的两个字段。Person 模型会以如下的 SQL 语句创建与之关联的数据库表（id 字段默认会自动添加）：12345CREATE TABLE myapp_person ( &quot;id&quot; serial NOT NULL PRIMARY KEY, &quot;first_name&quot; varchar(30) NOT NULL, &quot;last_name&quot; varchar(30) NOT NULL); 为了使模型生效，还需要将 myapp 包含进 settings.py 配置文件中的 INSTALLED_APPS，编辑 myproject/myproject/settings.py 文件，内容如下：12345INSTALLED_APPS = [ #... &apos;myapp&apos;, #...] 之后即可使用 python manage.py makemigrations myapp 创建数据库迁移文件；再运行 python manage.py migrate 命令将模型中定义的表结构迁移至数据库中。 Django Shell 测试完成数据库迁移后，可使用 python manage.py shell 命令进入 Django Shell 交互式命令行，通过 Django 提供的模型 API 进行测试（插入数据）：1234567&gt;&gt;&gt; from myapp.models import Person&gt;&gt;&gt; john = Person(first_name=&apos;John&apos;, last_name=&apos;Smith&apos;)&gt;&gt;&gt; john.save()&gt;&gt;&gt; Person.objects.all()&lt;QuerySet [&lt;Person: Person object (1)&gt;]&gt;&gt;&gt;&gt; john.first_name&apos;John&apos; 访问 sqlite3 数据库查询最终结果，John Smith 已添加至数据表中：1234567&gt;&gt;&gt; import sqlite3&gt;&gt;&gt; conn = sqlite3.connect(&apos;db.sqlite3&apos;)&gt;&gt;&gt; cursor = conn.cursor()&gt;&gt;&gt; cursor.execute(&apos;select * from myapp_person&apos;)&lt;sqlite3.Cursor object at 0x0000022C36ADBD50&gt;&gt;&gt;&gt; print(cursor.fetchone())(1, &apos;John&apos;, &apos;Smith&apos;) 二、字段（Field）模型中最重要的也是唯一必须存在的项目就是字段，它由模型类的属性定义，用来表述与模型相关联的数据表的结构。 字段的类型与选项模型中的每个字段都是 django.db.models.Field 类的实例，对应于数据库表中的列。Django 内置了大量的字段类型，如 CharField，TextField 和 DateTimeField 等。具体可查看 模型字段参考。 每个字段都可以接收特定的字段相关的参数，比如 CharField 需要传入 max_length 用于定义 VARCHAR 类型的字符长度。 此外还有一些通用的可选的字段选项。如： null：如为 True，则 Django 会将空值在数据库中存为 NULL。该选项默认为 False。 blank：如为 True，则该字段允许为空。与 null 选项不同，blank 是与表单验证相关的，而 null 是数据库相关的。 default：用于设置字段的默认值。 primary_key：用于设置模型的主键。如未指定任何字段为主键，则 Django 会自动添加 IntegerField 字段作为主键。 unique：设置字段的值是否允许重复。 PS：默认情况下，Django 会给每个模型添加如下字段id = models.AutoField(primary_key=True)作为为自增的主键。如果想覆盖此默认行为，直接手动指定其他字段为主键（primary_key=True）即可。 关系额，关系型数据库的强大之处即在于各数据库表之间的相互关联。Django 支持定义三种最常见的数据库关系：多对一、多对多和一对一。 可以通过 django.db.models.ForeignKey 创建多对一关系，只需要像定义其他字段那样将它作为类属性引入即可。如：123456789from django.db import modelsclass Manufacturer(models.Model): name = models.CharField(max_length=20) location = models.CharField(max_length=40)class Car(models.Model): manufacturer = models.ForeignKey(Manufacturer, on_delete=models.CASCADE) price = models.IntegerField() 运行 python manage.py makemigrations 命令创建数据库迁移文件：12345$ python manage.py makemigrations myappMigrations for &apos;myapp&apos;: myapp/migrations/0001_initial.py - Create model Manufacturer - Create model Car 使用 python manage.py sqlmigrate myapp 0001 命令查看具体会执行哪些 SQL 语句（基于 sqlite3）：123456789101112$ python manage.py sqlmigrate myapp 0001BEGIN;---- Create model Manufacturer--CREATE TABLE &quot;myapp_manufacturer&quot; (&quot;id&quot; integer NOT NULL PRIMARY KEY AUTOINCREMENT, &quot;name&quot; varchar(20) NOT NULL, &quot;location&quot; varchar(40) NOT NULL);---- Create model Car--CREATE TABLE &quot;myapp_car&quot; (&quot;id&quot; integer NOT NULL PRIMARY KEY AUTOINCREMENT, &quot;price&quot; integer NOT NULL, &quot;manufacturer_id&quot; integer NOT NULL REFERENCES &quot;myapp_manufacturer&quot; (&quot;id&quot;) DEFERRABLE INITIALLY DEFERRED);CREATE INDEX &quot;myapp_car_manufacturer_id_2be676ab&quot; ON &quot;myapp_car&quot; (&quot;manufacturer_id&quot;);COMMIT; 多对多和一对一的数据库关系则分别可以使用 ManyToManyField 和 OneToOneField 定义。 三、模型的属性与方法Meta 选项模型的 Meta 选项在模型类的定义中是可选的，它基本上包含了除字段以外的所有内容。比如数据纪录的顺序（ordering）、关联的数据库表的名称（db_table）和索引（indexes）等。 Django 模型支持的所有 Meta 选项可以参考 Model Meta options 示例代码：12345678from django.db import modelsclass Ox(models.Model): horn_length = models.IntegerField() class Meta: ordering = [&quot;horn_length&quot;] verbose_name_plural = &quot;oxen&quot; 自定义模型方法在模型中创建自定义方法可以为模型对象添加个性化的“底层”功能。参考如下代码：1234567891011from django.db import modelsclass Person(models.Model): first_name = models.CharField(max_length=50) last_name = models.CharField(max_length=50) birth_date = models.DateField() @property def full_name(self): &quot;Returns the person&apos;s full name.&quot; return &apos;%s %s&apos; % (self.first_name, self.last_name) 此时的 Person 模型除了可以从数据库中读取和写入数据等基本功能外，还可以通过它调用自定义的 full_name 方法完成额外的需求（返回全名）。 覆盖默认的模型方法有些情况下，还可以通过修改模型内置的方法，改变模型与数据库的具体交互方式。尤其是 save() （向数据库中存入数据）和 delete() （从数据库中删除纪录）等方法。如：123456789101112131415from django.db import modelsclass Person(models.Model): first_name = models.CharField(max_length=50) last_name = models.CharField(max_length=50) def save(self, *args, **kwargs): self.first_name = self.first_name.capitalize() self.last_name = self.last_name.capitalize() super().save(*args, **kwargs) @property def full_name(self): &quot;Returns the person&apos;s full name.&quot; return &apos;%s %s&apos; % (self.first_name, self.last_name) 重新迁移数据库，进入 Django Shell 测试，结果如下：1234567&gt;&gt;&gt; from myapp.models import Person&gt;&gt;&gt; john = Person(first_name=&apos;john&apos;, last_name=&apos;smith&apos;)&gt;&gt;&gt; john.save()&gt;&gt;&gt; john&lt;Person: Person object (2)&gt;&gt;&gt;&gt; john.full_name&apos;John Smith&apos; 四、数据库操作一旦创建了数据模型，Django 即会自动生成与数据库交互的 API 供用户创建、获取、更新和删除数据对象。 此处先创建如下的模型文件供测试使用：1234567891011121314151617181920212223242526272829from django.db import modelsclass Blog(models.Model): name = models.CharField(max_length=100) tagline = models.TextField() def __str__(self): return self.nameclass Author(models.Model): name = models.CharField(max_length=200) email = models.EmailField() def __str__(self): return self.nameclass Entry(models.Model): blog = models.ForeignKey(Blog, on_delete=models.CASCADE) headline = models.CharField(max_length=255) body_text = models.TextField() pub_date = models.DateField() mod_date = models.DateField() authors = models.ManyToManyField(Author) n_comments = models.IntegerField() n_pingbacks = models.IntegerField() rating = models.IntegerField() def __str__(self): return self.headline Insert可以通过实例化模型类创建一个数据对象，并调用其 save() 方法将对应的记录插入（执行 INSERT SQL 语句）到数据库表中。123456789&gt;&gt;&gt; from myapp.models import Blog&gt;&gt;&gt; b = Blog(name=&apos;Beatles Blog&apos;, tagline=&apos;All the latest Beatles news&apos;)&gt;&gt;&gt; b.save()&gt;&gt;&gt; b.name = &apos;Beatles Blog All&apos;&gt;&gt;&gt; b.save()&gt;&gt;&gt; b&lt;Blog: Beatles Blog All&gt;&gt;&gt;&gt; b.tagline&apos;All the latest Beatles news&apos; 插入 ForeignKey 与 ManyToManyField更新 ForeignKey 与操作普通字段的方式相同，将正确类型的对象赋值给对应字段并调用 save() 方法即可。如：12345&gt;&gt;&gt; from blog.models import Blog, Entry&gt;&gt;&gt; entry = Entry.objects.get(pk=1)&gt;&gt;&gt; cheese_blog = Blog.objects.get(name=&quot;Cheddar Talk&quot;)&gt;&gt;&gt; entry.blog = cheese_blog&gt;&gt;&gt; entry.save() 更新 ManyToManyField 的方式稍有不同，需要使用 add() 方法：123&gt;&gt;&gt; from blog.models import Author&gt;&gt;&gt; joe = Author.objects.create(name=&quot;Joe&quot;)&gt;&gt;&gt; entry.authors.add(joe) 获取数据从数据库中获取数据会生成一个 QuerySet 对象，它代表从数据库中取出的数据对象的集合。QuerySet 等同于数据库中的 SELECT 语句，它可以有零个或者多个 filter 。filter 对应于数据库中的筛选条件如 WHERE 或 LIMIT 等。 获取单个对象&gt;&gt;&gt; one_entry = Entry.objects.get(pk=1) PS：pk 即 primary key 。 获取所有对象&gt;&gt;&gt; all_entries = Entry.objects.all() 应用筛选器&gt;&gt;&gt; entry = Entry.objects.filter(pub_date__year=2006) Limiting&gt;&gt;&gt; entries = Entry.objects.all()[:5] 排序&gt;&gt;&gt; entry = Entry.objects.order_by(&#39;headline&#39;)[0] 字段查询字段查询对应于 SQL 中的 WHERE 语句，可以通过向 QuerySet 对象的方法 filter()、exclude() 和 get() 中传入特定的参数来实现。基本的查询参数语法如下：field__lookuptype=value 。 如：&gt;&gt;&gt; Entry.objects.filter(pub_date__lte=&#39;2006-01-01&#39;)等同于：SELECT * FROM blog_entry WHERE pub_date &lt;= &#39;2006-01-01&#39;; 其中 lte 即 less or equal（小于等于）。其他类似的 lookuptype 还包括 gt（大于）、gte（大于等于）、lt（小于）、exact、iexact（忽略大小写）、startswith、istartswith、endswith、iendswith、contains、range（指定范围）、regex（正则表达式）、iregex 等。 以下为一些常见的使用示例： exact：&gt;&gt;&gt; Entry.objects.get(headline__exact=&quot;Cat bites dog&quot;)等于 SELECT ... WHERE headline = &#39;Cat bites dog&#39;; iexact：&gt;&gt;&gt; Blog.objects.get(name__iexact=&quot;beatles blog&quot;)等于 SELECT ... WHERE name ILIKE &#39;beatles blog&#39;; startswith：&gt;&gt;&gt; Entry.objects.filter(headline__startswith=&#39;Lennon&#39;)等于 SELECT ... WHERE headline LIKE &#39;Lennon%&#39;; contains：&gt;&gt;&gt; Entry.objects.get(headline__contains=&#39;Lennon&#39;)等于 SELECT ... WHERE headline LIKE &#39;%Lennon%&#39;; in：&gt;&gt;&gt; Entry.objects.filter(id__in=[1, 3, 4])等于 SELECT ... WHERE id IN (1, 3, 4); range： 1234import datetimestart_date = datetime.date(2005, 1, 1)end_date = datetime.date(2005, 3, 31)Entry.objects.filter(pub_date__range=(start_date, end_date)) 等于 SELECT ... WHERE pub_date BETWEEN &#39;2005-01-01&#39; and &#39;2005-03-31&#39;; ManagerManager 是提供给 Django 模型，用于做数据库查询操作的接口。Django 项目中的每一个模型都需要至少包含一个 Manager 对象。 默认情况下，Django 会在每一个模型类中添加一个名为 objects 的 Manager 。通过将 models.Manager() 赋值给除 objects 以外的类属性，可以覆盖此默认行为：12345from django.db import modelsclass Person(models.Model): #... people = models.Manager() 此时 Person.objects.all() 查询语句会报出 AttributeError 错误，而 Person.people.all() 则返回所有的 Person 对象。 自定义 Manager自定义的 Manager 方法可以向模型中添加表级别的查询功能。与之对应的纪录级别的功能则需要使用模型方法。 如：123456789101112# First, define the Manager subclass.class DahlBookManager(models.Manager): def get_queryset(self): return super().get_queryset().filter(author=&apos;Roald Dahl&apos;)# Then hook it into the Book model explicitly.class Book(models.Model): title = models.CharField(max_length=100) author = models.CharField(max_length=50) objects = models.Manager() # The default manager. dahl_objects = DahlBookManager() # The Dahl-specific manager. 以上面的模型为例，Book.objects.all() 会返回数据库中所有的书籍信息，而 Book.dahl_objects.all() 则会返回所有作者为 Roald Dahl 的书籍。Django 允许向模型中添加任意数量的 Manager() 实例，因此可以用来为模型定义一些通用的筛选器。如：123456789101112131415class AuthorManager(models.Manager): def get_queryset(self): return super().get_queryset().filter(role=&apos;A&apos;)class EditorManager(models.Manager): def get_queryset(self): return super().get_queryset().filter(role=&apos;E&apos;)class Person(models.Model): first_name = models.CharField(max_length=50) last_name = models.CharField(max_length=50) role = models.CharField(max_length=1, choices=[(&apos;A&apos;, _(&apos;Author&apos;)), (&apos;E&apos;, _(&apos;Editor&apos;))]) people = models.Manager() authors = AuthorManager() editors = EditorManager() 则 Person.people.all()、Person.authors.all() 和 Person.editors.all() 都可以作为从模型中获取数据的接口，且 authors 与 editors 已预先根据 role 对数据进行了筛选。 参考资料Django 2.2 官方文档]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Program</tag>
        <tag>Development</tag>
        <tag>Web</tag>
        <tag>Django</tag>
        <tag>MVC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django REST framework 教程（1）—— 快速开始]]></title>
    <url>%2F2019%2F09%2F23%2Fdjango-rest-framework-1-quick-start%2F</url>
    <content type="text"><![CDATA[本文参考自 Django REST framework 官方文档 ，创建了一个简单的 API 供 admin 用户查询与修改系统中的用户和用户组信息。 一、创建项目安装依赖库：$ pip install django$ pip install djangorestframework 项目初始化：123$ django-admin startproject tutorial$ cd tutorial$ django-admin startapp quickstart 上述命令执行后，自动生成的 tutorial 项目的目录结构如下：12345678910111213141516tutorial├── manage.py├── quickstart│ ├── __init__.py│ ├── admin.py│ ├── apps.py│ ├── migrations│ │ └── __init__.py│ ├── models.py│ ├── tests.py│ └── views.py└── tutorial ├── __init__.py ├── settings.py ├── urls.py └── wsgi.py 执行以下命令迁移数据库并创建管理员用户：$ python manage.py migrate$ python manage.py createsuperuser 二、序列化创建 tutorial/quickstart/serializers.py 文件并输入以下内容：1234567891011121314from django.contrib.auth.models import User, Groupfrom rest_framework import serializersclass UserSerializer(serializers.HyperlinkedModelSerializer): class Meta: model = User fields = [&apos;url&apos;, &apos;username&apos;, &apos;email&apos;, &apos;groups&apos;]class GroupSerializer(serializers.HyperlinkedModelSerializer): class Meta: model = Group fields = [&apos;url&apos;, &apos;name&apos;] 三、视图修改 tutorial/quickstart/views.py 视图文件：123456789101112131415161718192021from django.contrib.auth.models import User, Groupfrom rest_framework import viewsets, permissionsfrom quickstart.serializers import UserSerializer, GroupSerializerclass UserViewSet(viewsets.ModelViewSet): &quot;&quot;&quot; API endpoint that allows users to be viewed or edited. &quot;&quot;&quot; queryset = User.objects.all().order_by(&apos;-date_joined&apos;) serializer_class = UserSerializer permission_classes = [permissions.IsAuthenticatedOrReadOnly]class GroupViewSet(viewsets.ModelViewSet): &quot;&quot;&quot; API endpoint that allows groups to be viewed or edited. &quot;&quot;&quot; queryset = Group.objects.all() serializer_class = GroupSerializer permission_classes = [permissions.IsAuthenticatedOrReadOnly] 四、URLs 和设置编辑 tutorial/tutorial/urls.py 路由配置文件：12345678910111213141516from django.contrib import adminfrom django.urls import include, pathfrom rest_framework import routersfrom quickstart import viewsrouter = routers.DefaultRouter()router.register(r&apos;users&apos;, views.UserViewSet)router.register(r&apos;groups&apos;, views.GroupViewSet)# Wire up our API using automatic URL routing.# Additionally, we include login URLs for the browsable API.urlpatterns = [ path(&apos;&apos;, include(router.urls)), path(&apos;api-auth/&apos;, include(&apos;rest_framework.urls&apos;, namespace=&apos;rest_framework&apos;)), path(&apos;admin/&apos;, admin.site.urls),] 编辑 tutorial/tutorial/settings.py 文件，在 INSTALLED_APPS 配置中添加上 rest_framework 项目：1234INSTALLED_APPS = [ ... &apos;rest_framework&apos;,] 五、测试运行 $ python manage.py runserver 命令启动测试服务；使用 http 命令对 API 进行访问测试（如未安装 http 工具，运行以下命令安装 $ pip install httpie）。 获取 API 列表：12345$ http -b 127.0.0.1:8000&#123; &quot;groups&quot;: &quot;http://172.20.19.76:8000/groups/&quot;, &quot;users&quot;: &quot;http://172.20.19.76:8000/users/&quot;&#125; GET 方法获取用户组信息：1234567891011$ http -b 127.0.0.1:8000/groups/[ &#123; &quot;name&quot;: &quot;admin&quot;, &quot;url&quot;: &quot;http://172.20.19.76:8000/groups/1/&quot; &#125;, &#123; &quot;name&quot;: &quot;staff&quot;, &quot;url&quot;: &quot;http://172.20.19.76:8000/groups/2/&quot; &#125;] GET 方法获取用户信息：12345678910111213141516171819$ http -b 127.0.0.1:8000/users/[ &#123; &quot;email&quot;: &quot;starky@test.com&quot;, &quot;groups&quot;: [ &quot;http://172.20.19.76:8000/groups/2/&quot; ], &quot;url&quot;: &quot;http://172.20.19.76:8000/users/2/&quot;, &quot;username&quot;: &quot;starky&quot; &#125;, &#123; &quot;email&quot;: &quot;admin@test.com&quot;, &quot;groups&quot;: [ &quot;http://172.20.19.76:8000/groups/1/&quot; ], &quot;url&quot;: &quot;http://172.20.19.76:8000/users/1/&quot;, &quot;username&quot;: &quot;admin&quot; &#125;] POST 方法添加新的用户组（未授权）：1234$ http -b POST 172.20.19.76127.0.0.1:8000/groups/ name=superuser&#123; &quot;detail&quot;: &quot;Authentication credentials were not provided.&quot;&#125; POST 方法添加新的用户组（提供 Admin 账户信息用于验证）12345$ http -b -a admin:123456 POST 127.0.0.1:8000/groups/ name=superuser&#123; &quot;name&quot;: &quot;superuser&quot;, &quot;url&quot;: &quot;http://172.20.19.76:8000/groups/3/&quot;&#125; 六、Web 界面访问 http://127.0.0.1:8000，截图如下： 登录后支持 POST 操作，截图如下：]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Program</tag>
        <tag>WebDev</tag>
        <tag>Django</tag>
        <tag>Restful</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 进阶之类与面向对象编程]]></title>
    <url>%2F2019%2F09%2F23%2Fpro-python-class-and-oop%2F</url>
    <content type="text"><![CDATA[一、类的定义Python 语言中的类实际上是一种数据类型，同时 Python 中所有内置的数据类型也都是由类定义的。123456&gt;&gt;&gt; type(10)&lt;class &apos;int&apos;&gt;&gt;&gt;&gt; type(&apos;10&apos;)&lt;class &apos;str&apos;&gt;&gt;&gt;&gt; type([10,11,12])&lt;class &apos;list&apos;&gt; 可以使用 class 关键字定义一个类，格式如下：12class MyClass: pass 使用赋值的形式实例化某个类，格式如下：instance = MyClass()即创建一个以 MyClass 类为数据类型的新对象 instance 。 实例变量Python 中的类实例可以作为类似于 struct 的数据类型存储数据或记录，且不同于 C 或 Java 语言中的类似结构，关联于类实例的数据不需要提前声明。12345678class Circle: passmy_circle = Circle()my_circle.radius = 4print(2 * 3.14 * my_circle.radius)# =&gt; 25.12 可以通过在类的定义代码中添加 __init__ 方法来初始化实例中的某些数据。每当新的类实例创建时，__init__ 方法都会自动被调用一次，类似于 Java 中的构造器。其第一个参数 self 用来指代新的类实例本身。12345678class Circle: def __init__(self): self.radius = 1my_circle = Circle()print(2 * 3.14 * my_circle.radius)# =&gt; 6.28 上面代码中的 radius 即为 Circle 类的实例变量，每一个由 Circle 类生成的实例对象（如上面代码中的 my_circle）都包含一份独有的 radius 变量。创建实例变量的语法如下：instance.variable = value 类变量类变量是关联于某个类的变量（而不是某个类实例），可以被所有的类实例访问，因此常用来追踪某些类级别的信息。类变量可以通过类结构体中的赋值语句进行创建（而类的实例变量需要在 __init__ 方法中定义）。123456789101112class Circle: pi = 3.14159 def __init__(self, radius): self.radius = radius def area(self): return self.radius * self.radius * Circle.piprint(Circle.pi) # =&gt; 3.14159Circle.pi = 4c = Circle(3)print(c.area()) # =&gt; 36 二、方法方法是指与特定的类对象相关联的函数，比如前面的 __init__ 方法。参考如下代码：123456789101112class Circle: def __init__(self): self.radius = 1 def area(self): return self.radius * self.radius * 3.14159c = Circle()c.radius = 3print(c.area())# =&gt; 28.27431 上面代码中的 area 方法的第一个参数仍为 self，它类似于很多语言中的 this 关键字，用于指代由类生成的实例对象本身。 类的方法定义中可以添加除 self 以外的其他参数，比如可以为 __init__ 方法关联一个 radius 参数，在实例化类的同时直接初始化一个 radius 变量，而不必在类实例创建以后手动进行额外的赋值操作。1234567891011class Circle: def __init__(self, radius=1): self.radius = radius def area(self): return self.radius * self.radius * 3.14159c = Circle(3)print(c.area())# =&gt; 28.27431 三、静态方法与类方法静态方法Python 语言可以通过 @staticmethod 装饰器创建类中的静态方法。静态方法不带 self 参数，即调用时不需要先对类进行实例化，不通过类的实例对象进行调用。它可以没有任何参数，可直接通过类名调用。1234567891011121314151617181920class Circle: all_circles = [] pi = 3.14159 def __init__(self, r=1): self.radius = r self.__class__.all_circles.append(self) def area(self): return self.__class__.pi * self.radius * self.radius @staticmethod def total_area(): total = 0 for c in Circle.all_circles: total = total + c.area() return totalc1 = Circle(1)c2 = Circle(2)print(Circle.total_area())# =&gt; 15.70795 类方法可以使用 @classmethod 装饰器创建 Python 类中的类方法。类方法默认有个 cls 参数（表示类本身），可以被类和实例调用。123456789101112131415161718192021class Circle: all_circles = [] pi = 3.14159 def __init__(self, r=1): self.radius = r self.__class__.all_circles.append(self) def area(self): return self.__class__.pi * self.radius * self.radius @classmethod def total_area(cls): total = 0 for c in cls.all_circles: total = total + c.area() return totalc1 = Circle(1)c2 = Circle(2)print(Circle.total_area())# =&gt; 15.70795 PS：关于实例方法、类方法与静态方法的简要对比 实例方法 定义：第一个参数必须是实例对象，一般约定为 self，通过它来传递实例的属性和方法（也可以传类的属性和方法） 调用：只能由实例对象调用 类方法 定义：使用装饰器 @classmethod。第一个参数必须是当前类对象，一般约定为 cls，通过它来传递类的属性和方法（不能传实例的属性和方法） 调用：实例对象和类对象都可以调用 静态方法 定义：使用装饰器 @staticmethod。参数随意，没有 self 和 cls 参数，方法体中不能使用类或实例的任何属性和方法 调用：实例对象和类对象都可以调用 四、继承12345678910111213141516171819202122class Shape: def __init__(self, x, y): self.x = x self.y = y def move(self, delta_x, delta_y): self.x = self.x + delta_x self.y = self.y + delta_yclass Square(Shape): def __init__(self, side=1, x=0, y=0): super().__init__(x, y) self.side = sideclass Circle(Shape): def __init__(self, r=1, x=0, y=0): super().__init__(x, y) self.radius = rc = Circle(1)c.move(3, 4)print(c.x) # =&gt; 3print(c.y) # =&gt; 4 其中 super 函数用于在子类中调用父类定义的方法。父类 Shape 中定义的 move 方法可以直接被子类实例化后的对象使用。从传统的 OOP 概念来看，即 Python 中所有的方法都是虚拟的，如某个被调用的方法在当前的类中不存在，则 Python 会尝试从它的父类中寻找同名的方法，最终使用第一个找到的结果。 五、私有变量私有变量和私有方法是指在类中定义且在类外不可见的对象。在 Python 语言中，名称前面带双下划线（__）的变量或方法即为私有。 1234567891011class Mine: def __init__(self): self.x = 2 self.__y = 3 def print_y(self): print(self.__y)m = Mine()print(m.x) # =&gt; 2m.print_y() # =&gt; 3print(m.__y) # =&gt; AttributeError: &apos;Mine&apos; object has no attribute &apos;__y&apos; 上面代码中的 __y 变量即为私有变量，因此当直接从类外部访问 __y 时会报出 AttributeError 错误，但是可以通过类内部定义的 print_y 方法（非私有方法）操作 __y 变量。 @propertyPython 允许用户直接访问实例变量，而不需要类似于 Java 或其他面向对象语言中的 getter 或 setter 机制。Python 可以使用 property 机制完成类似的需求。12345678910111213141516171819class Temperature: def __init__(self): self._temp_fahr = 0 @property def temp(self): return (self._temp_fahr - 32) * 5 / 9 @temp.setter def temp(self, new_temp): self._temp_fahr = new_temp * 9 / 5 + 32t = Temperature()print(t._temp_fahr) # =&gt; 0print(t.temp) # =&gt; -17.77777777777778t.temp = 34print(t._temp_fahr) # =&gt; 93.2print(t.temp) # =&gt; 34.0 The Quick Python Book 3rd EditionPython 实例方法、类方法、静态方法的区别与作用]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Program</tag>
        <tag>OOP</tag>
        <tag>Class</tag>
        <tag>Advanced</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 进阶之并发编程中的多线程]]></title>
    <url>%2F2019%2F09%2F23%2Fpro-python-concurrency-with-multi-threading%2F</url>
    <content type="text"><![CDATA[Two events are concurrent if neither can causally affect the other. 从编程的角度讲，某个问题是可并发的，即代表它可以被完全或部分地分解成多个组件，且这几个组件之间是顺序独立的。换句话说，一个事件被分解成多个相互之间无依赖关系的具体步骤，这些步骤可以独立地被完成，且不管各自完成的顺序如何，都不影响最终的结果。 就像华罗庚先生在《统筹方法》中提到的例子，喝茶需要清洗茶具和烧热水，这是两个相互独立的事件。可以先清洗茶具再烧壶热水，则最终的等待时间是两者独立完成所需的时长之和。更有效率的方法为，先执行比较耗费时间的烧热水动作，并且在等待热水烧开的同时清洗好茶具，效果类似于两个准备步骤同时执行而不是按顺序依次执行。 在程序的世界中，类似的需求同样屡见不鲜。比如用户点击链接下载一个大的文件，通常会把文件下载任务放在后台执行，使得用户可以继续浏览网页，不需要等待下载任务完成。 多线程线程是操作系统能够进行运算调度的最小单位，程序设计者可以将其工作划分成多个可同步运行的线程以应对某些场景和需求。不过在单核系统中，多线程并不会加速程序的运行。而多核或多处理器系统可以将不同的线程分配给多个 CPU 核心同时进行处理，因而能够带来性能上的提升（但 Python 由于 GIL 机制使得这种提升仍有一定的限制）。 在单核系统中，可以通过一种名为时间分片（timeslicing）的机制来实现多线程。即 CPU 可以在多个线程间非常迅速地进行切换，达成一种多个线程“同时”在运行的假象。这种虚拟的并行方式虽然不能带来性能上的提升，但是仍然能够非常好地应对某些特定的需求场景。如： 构建响应式接口：比如将长时间运行的任务放在后台执行，使其等待的过程不会阻塞用户其他的交互动作 任务的分发与委派：对于依赖于第三方资源的进程（比如需要对远程 Web 服务进行大量的请求），多线程可以起到很好的加速效果 构建多用户应用：多用户状态下的多线程类似于独立执行的进程，只不过在某些层面上更加易于管理（共享内存） 示例：多线程请求 Web 数据requests 获取天气信息以下是一个简单的 Web 请求示例，通过 requests 模块访问中国天气网，获取部分城市的天气信息：12345678910111213import requestsdef get_weather(cityid): api_url = &apos;http://www.weather.com.cn/data/sk/&apos; + cityid + &apos;.html&apos; results = requests.get(api_url) results.encoding = &apos;utf-8&apos; weather_info = results.json()[&apos;weatherinfo&apos;] return weather_infoprint(get_weather(&apos;101210101&apos;))# =&gt; &#123;&apos;city&apos;: &apos;杭州&apos;, &apos;cityid&apos;: &apos;101210101&apos;, &apos;temp&apos;: &apos;24.8&apos;, &apos;WD&apos;: &apos;东北风&apos;, &apos;WS&apos;: &apos;小于3级&apos;, &apos;SD&apos;:# &apos;81%&apos;, &apos;AP&apos;: &apos;1000.3hPa&apos;, &apos;njd&apos;: &apos;暂无实况&apos;, &apos;WSE&apos;: &apos;&lt;3&apos;, &apos;time&apos;: &apos;17:50&apos;, &apos;sm&apos;: &apos;2.1&apos;, &apos;isRadar&apos;: &apos;1&apos;, &apos;Radar&apos;: &apos;JC_RADAR_AZ9571_JB&apos;&#125; 获取多个城市天气12345678910111213141516171819202122232425262728293031323334353637import requestsimport timedef get_weather(cityid): api_url = &apos;http://www.weather.com.cn/data/sk/&apos; + cityid + &apos;.html&apos; results = requests.get(api_url) results.encoding = &apos;utf-8&apos; weather_info = results.json()[&apos;weatherinfo&apos;] print(&quot;%s (tmp/humi): %s/%s&quot; % ( weather_info[&apos;city&apos;], weather_info[&apos;temp&apos;], weather_info[&apos;SD&apos;] ))cityids = ( &apos;101210101&apos;, &apos;101010100&apos;, &apos;101090201&apos;, &apos;101020100&apos;, &apos;101280101&apos;, &apos;101230201&apos;)def main(): for id in cityids: get_weather(id)if __name__ == &apos;__main__&apos;: started = time.time() main() elapsed = time.time() - started print(&quot;Time elapsed: &#123;:.2f&#125;s&quot;.format(elapsed))# =&gt; 杭州 (tmp/humi): 24.8/81%# =&gt; 北京 (tmp/humi): 27.9/28%# =&gt; 保定 (tmp/humi): 27.5/43%# =&gt; 上海 (tmp/humi): 23.5/80%# =&gt; 广州 (tmp/humi): 26.6/83%# =&gt; 厦门 (tmp/humi): 26.8/87%# =&gt; Time elapsed: 0.16s 多线程获取多个城市的天气完整代码如下，主要修改了 main 函数：1234567891011121314151617181920212223242526272829303132333435363738394041424344from threading import Threadimport requestsimport timedef get_weather(cityid): api_url = &apos;http://www.weather.com.cn/data/sk/&apos; + cityid + &apos;.html&apos; results = requests.get(api_url) results.encoding = &apos;utf-8&apos; weather_info = results.json()[&apos;weatherinfo&apos;] print(&quot;%s (tmp/humi): %s/%s&quot; % ( weather_info[&apos;city&apos;], weather_info[&apos;temp&apos;], weather_info[&apos;SD&apos;] ))cityids = ( &apos;101210101&apos;, &apos;101010100&apos;, &apos;101090201&apos;, &apos;101020100&apos;, &apos;101280101&apos;, &apos;101230201&apos;)def main(): threads = [] for id in cityids: thread = Thread(target=get_weather, args=[id]) thread.start() threads.append(thread) while threads: threads.pop().join()if __name__ == &apos;__main__&apos;: started = time.time() main() elapsed = time.time() - started print(&quot;Time elapsed: &#123;:.2f&#125;s&quot;.format(elapsed))# =&gt; 保定 (tmp/humi): 27.5/43%# =&gt; 广州 (tmp/humi): 26.6/83%# =&gt; 杭州 (tmp/humi): 24.8/81%# =&gt; 上海 (tmp/humi): 23.5/80%# =&gt; 北京 (tmp/humi): 27.9/28%# =&gt; 厦门 (tmp/humi): 26.8/87%# =&gt; Time elapsed: 0.08s 与前一个版本（在单个线程中顺序地依次请求多个城市的天气信息）相比，此版本通过不同的线程同时请求多个城市的天气信息，每个线程负责一个城市。虽然多线程在系统资源上增加了一定程度的消耗，但是相对于网络资源响应以及 IO 传输产生的延迟和等待，仍然在效率上有了一定程度的提升。 同时也可以从对输出结果的比较中看出，多线程方案获取到的天气信息并不是按顺序返回的。即表明在请求某个数据时，程序并没有等待上一个请求完全解决，而是在结果返回之前又发起了新的请求。从而在一定程度上减弱了网络延迟对整个程序的阻塞效果。 线程池一个程序所能运行的线程数量并不是毫无限制的，很多时候需要构建一个固定大小的线程池来处理所有带并行需求的工作任务。这些并行任务可以先存储在一种名为队列（queue）的数据结构中，以先进先出（FIFO）的原则分配给线程池中固定数量的线程去处理。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364from queue import Queue, Emptyfrom threading import Threadimport requestsimport timeTHREAD_POOL_SIZE = 4cityids = ( &apos;101210101&apos;, &apos;101010100&apos;, &apos;101090201&apos;, &apos;101020100&apos;, &apos;101280101&apos;, &apos;101230201&apos;)def worker(work_queue): while not work_queue.empty(): try: item = work_queue.get(block=False) except Empty: break else: get_weather(item) work_queue.task_done()def get_weather(cityid): api_url = &apos;http://www.weather.com.cn/data/sk/&apos; + cityid + &apos;.html&apos; results = requests.get(api_url) results.encoding = &apos;utf-8&apos; weather_info = results.json()[&apos;weatherinfo&apos;] print(&quot;%s (tmp/humi): %s/%s&quot; % ( weather_info[&apos;city&apos;], weather_info[&apos;temp&apos;], weather_info[&apos;SD&apos;] ))def main(): work_queue = Queue() for id in cityids: work_queue.put(id) threads = [ Thread(target=worker, args=(work_queue,)) for _ in range(THREAD_POOL_SIZE) ] for thread in threads: thread.start() work_queue.join() while threads: threads.pop().join()if __name__ == &apos;__main__&apos;: started = time.time() main() elapsed = time.time() - started print(&quot;Time elapsed: &#123;:.2f&#125;s&quot;.format(elapsed))# =&gt; 保定 (tmp/humi): 27.5/43%# =&gt; 杭州 (tmp/humi): 24.8/81%# =&gt; 北京 (tmp/humi): 27.9/28%# =&gt; 上海 (tmp/humi): 23.5/80%# =&gt; 广州 (tmp/humi): 26.6/83%# =&gt; 厦门 (tmp/humi): 26.8/87%# =&gt; Time elapsed: 0.08s 其中 get_weather 函数可以通过 id 值获取对应城市的天气信息，所有需要请求的 id 参数保存在 main 中定义的队列 work_queue 里； worker 函数是与线程相关联的工作代码，它可以逐个获取队列中存储的 id 参数并传递给 get_weather 函数；main 函数中的 threads 列表则初始化了固定数量（THREAD_POOL_SIZE）的线程对象，所有的请求任务最终都由这些线程去处理。队列中的任何一个 id 参数交由任何一个线程处理时都会立即从队列中弹出，因而保证了各线程间的相互独立（同一个任务不会被多个线程获取）。 简单来说，固定数量的线程完成了队列中大量任务的并行处理。可以适当修改线程数量以达到系统资源和执行效率的平衡。 双向队列123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172import timefrom queue import Queue, Emptyfrom threading import Threadimport requestsTHREAD_POOL_SIZE = 4cityids = ( &apos;101210101&apos;, &apos;101010100&apos;, &apos;101090201&apos;, &apos;101020100&apos;, &apos;101280101&apos;, &apos;101230201&apos;)def get_weather(cityid): api_url = &apos;http://www.weather.com.cn/data/sk/&apos; + cityid + &apos;.html&apos; results = requests.get(api_url) results.encoding = &apos;utf-8&apos; weather_info = results.json()[&apos;weatherinfo&apos;] return weather_infodef present_result(weather_info): print(&quot;%s (tmp/humi): %s/%s&quot; % ( weather_info[&apos;city&apos;], weather_info[&apos;temp&apos;], weather_info[&apos;SD&apos;] ))def worker(work_queue, results_queue): while not work_queue.empty(): try: item = work_queue.get(block=False) except Empty: break else: results_queue.put( get_weather(item) ) work_queue.task_done()def main(): work_queue = Queue() results_queue = Queue() for id in cityids: work_queue.put(id) threads = [ Thread(target=worker, args=(work_queue, results_queue)) for _ in range(THREAD_POOL_SIZE) ] for thread in threads: thread.start() work_queue.join() while threads: threads.pop().join() while not results_queue.empty(): present_result(results_queue.get())if __name__ == &apos;__main__&apos;: started = time.time() main() elapsed = time.time() - started print(&quot;Time elapsed: &#123;:.2f&#125;s&quot;.format(elapsed))# =&gt; 杭州 (tmp/humi): 24.8/81%# =&gt; 北京 (tmp/humi): 27.9/28%# =&gt; 保定 (tmp/humi): 27.5/43%# =&gt; 上海 (tmp/humi): 23.5/80%# =&gt; 广州 (tmp/humi): 26.6/83%# =&gt; 厦门 (tmp/humi): 26.8/87%# =&gt; Time elapsed: 0.08s 与上一个版本的方案不同，此处的程序除了创建包含请求信息的 work_queue 队列外，还创建了一个用于保存返回结果的 results_queue 队列。即工作线程只用于对远程 API 发起请求并获取结果，而最终结果的整理及打印输出等则交由主线程来处理。此举可以减弱某些无关的操作可能对工作线程产生的不利影响。 错误处理为了应对请求数据的过程中可能出现的错误，可以在之前代码的基础上添加异常捕获功能。即在 worker 函数中添加 try...except 语句，当执行成功时将返回的结果保存至 results_queue 队列；如有异常发生，则将异常对象保存至 results_queue 队列。然后在 main 函数中对 results_queue 中的内容进行判断，是直接输出结果还是抛出异常对象。最终代码如下：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980import timefrom queue import Queue, Emptyfrom threading import Threadimport requestsTHREAD_POOL_SIZE = 4cityids = ( &apos;101210101&apos;, &apos;101010100&apos;, &apos;101090201&apos;, &apos;101020100&apos;, &apos;101280101&apos;, &apos;101230201&apos;)def get_weather(cityid): api_url = &apos;http://www.weather.com.cn/data/sk/&apos; + cityid + &apos;.html&apos; results = requests.get(api_url) results.encoding = &apos;utf-8&apos; weather_info = results.json()[&apos;weatherinfo&apos;] return weather_infodef present_result(weather_info): print(&quot;%s (tmp/humi): %s/%s&quot; % ( weather_info[&apos;city&apos;], weather_info[&apos;temp&apos;], weather_info[&apos;SD&apos;] ))def worker(work_queue, results_queue): while not work_queue.empty(): try: item = work_queue.get(block=False) except Empty: break else: try: result = get_weather(item) except Exception as err: results_queue.put(err) else: results_queue.put(result) finally: work_queue.task_done()def main(): work_queue = Queue() results_queue = Queue() for id in cityids: work_queue.put(id) threads = [ Thread(target=worker, args=(work_queue, results_queue)) for _ in range(THREAD_POOL_SIZE) ] for thread in threads: thread.start() work_queue.join() while threads: threads.pop().join() while not results_queue.empty(): result = results_queue.get() if isinstance(result, Exception): raise result present_result(result)if __name__ == &apos;__main__&apos;: started = time.time() main() elapsed = time.time() - started print(&quot;Time elapsed: &#123;:.2f&#125;s&quot;.format(elapsed))# =&gt; 杭州 (tmp/humi): 24.8/81%# =&gt; 北京 (tmp/humi): 27.9/28%# =&gt; 保定 (tmp/humi): 27.5/43%# =&gt; 上海 (tmp/humi): 23.5/80%# =&gt; 广州 (tmp/humi): 26.6/83%# =&gt; 厦门 (tmp/humi): 26.8/87%# =&gt; Time elapsed: 0.07s 参考资料Expert Python Programming - Second Edition]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Program</tag>
        <tag>Concurrency</tag>
        <tag>Threading</tag>
        <tag>Performance</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 进阶之 Logging 模块]]></title>
    <url>%2F2019%2F09%2F23%2Fpro-python-logging-module%2F</url>
    <content type="text"><![CDATA[日志可以用于记录软件运行时触发的一系列事件，帮助开发者实时地跟踪和了解程序代码运行的状态。由日志记录的事件消息通常会从属于特定的重要性等级（称为 level 或 severity），该等级由低到高一般包括 DEBUG、INFO、WARNING、ERROR、CRITICAL 等。 一、Level以下为一段简单的 Logging 代码：123456789import logginglogging.warning(&apos;Watch out!&apos;) # will print a message to the consolelogging.error(&apos;Error happend&apos;) # will print a message to the consolelogging.info(&apos;I told you so&apos;) # will not print anythinglogging.debug(&apos;debug information&apos;) # will not print anything# =&gt; WARNING:root:Watch out!# =&gt; ERROR:root:Error happend logging 模块默认的安全等级为 WARNING，只有属于或者高于此等级的日志消息才会被跟踪并进一步处理（比如输出到终端）。因此上面代码中 warning 和 error 记录的信息会输出到终端窗口中，而 info 和 debug 由于所属的安全等级较低，其信息不会输出到终端界面。 logging 模块可以通过 basicConfig 方法中的 level 参数自定义安全等级。示例代码如下：1234567891011import logginglogging.basicConfig(level=logging.INFO)logging.warning(&apos;Watch out!&apos;) # will print a message to the consolelogging.error(&apos;Error happend&apos;) # will print a message to the consolelogging.info(&apos;I told you so&apos;) # will not print anythinglogging.debug(&apos;debug information&apos;) # will not print anything# =&gt; WARNING:root:Watch out!# =&gt; ERROR:root:Error happend# =&gt; INFO:root:I told you so 二、记录日志到文件中参考以下代码：12345import logginglogging.basicConfig(filename=&apos;example.log&apos;,level=logging.DEBUG)logging.debug(&apos;This message should go to the log file&apos;)logging.info(&apos;So should this&apos;)logging.warning(&apos;And this, too&apos;) 上面的代码运行后会在当前目录下生成 example.log 日志文件，其内容如下：123DEBUG:root:This message should go to the log fileINFO:root:So should thisWARNING:root:And this, too 三、更改信息格式记录变量类似于 print 语句，logging 也支持将变量的值嵌入到输出的信息中：1234import logginglogging.warning(&apos; %s before you %s&apos;, &apos;Look&apos;, &apos;Leap!&apos;)# =&gt; WARNING:root: Look before you Leap! 更改输出信息的格式可以通过 basicConfig 方法中的 format 参数定义日志消息的格式，比如具体需要输出哪些内容以及这些信息如何展现给用户等：12345678910import logginglogging.basicConfig(format=&apos;%(levelname)s:%(message)s&apos;, level=logging.DEBUG)logging.debug(&apos;This message should appear on the console&apos;)logging.info(&apos;So should this&apos;)logging.warning(&apos;And this, too&apos;)# =&gt; DEBUG:This message should appear on the console# =&gt; INFO:So should this# =&gt; WARNING:And this, too PS：默认的输出格式为 severity:logger name:message。 显示时间信息123456import logginglogging.basicConfig(format=&apos;%(asctime)s %(message)s&apos;, datefmt=&apos;%m/%d/%Y %I%M%S %p&apos;)logging.warning(&apos;is when this event was logged.&apos;)# =&gt; 09/06/2019 091722 AM is when this event was logged. 其中 datefmt 参数支持的格式定义与 time.strftime() 相同。 四、配置 Logginglogging 库可以通过模块化的方式向外部提供以下几种组件： Loggers：提供可被应用直接调用的接口 Handlers：将 loggers 创建的日志记录发送到指定位置 Filters：提供过滤功能，决定哪些日志记录可以被输出显示 Formatters：决定日志最终显示时的布局格式 对日志的各类操作实际上是通过调用 Logger 实例中包含的方法来实现的。可以使用如下语句创建一个 Logger 类的实例对象并对其命名：logger = logging.getLogger(__name__)同时这种命名方式可以很方便地显示出产生日志信息的模块具体是哪一个。 以下是几种常见的 logging 配置： （1）手动配置：1234567891011121314151617181920212223242526272829303132import loggingimport logging.config# create loggerlogger = logging.getLogger(&apos;simple_example&apos;)logger.setLevel(logging.DEBUG)# create console handler and set level to debugch = logging.StreamHandler()ch.setLevel(logging.DEBUG)# create formatterformatter = logging.Formatter(&apos;%(asctime)s - %(name)s - %(levelname)s - %(message)s&apos;)# add formatter to chch.setFormatter(formatter)# add ch to loggerlogger.addHandler(ch)# &apos;application&apos; codelogger.debug(&apos;debug message&apos;)logger.info(&apos;info message&apos;)logger.warning(&apos;warn message&apos;)logger.error(&apos;error message&apos;)logger.critical(&apos;critical message&apos;)# =&gt; 2019-09-06 09:54:53,574 - simple_example - DEBUG - debug message# =&gt; 2019-09-06 09:54:53,576 - simple_example - INFO - info message# =&gt; 2019-09-06 09:54:53,577 - simple_example - WARNING - warn message# =&gt; 2019-09-06 09:54:53,578 - simple_example - ERROR - error message# =&gt; 2019-09-06 09:54:53,580 - simple_example - CRITICAL - critical message （2）配置多个 handler1234567891011121314151617181920212223242526import logginglogger = logging.getLogger(&apos;simple_example&apos;)logger.setLevel(logging.DEBUG)# create file handler which logs even debug messagesfh = logging.FileHandler(&apos;E:\\spam.log&apos;)fh.setLevel(logging.DEBUG)# create console handler with a higher log levelch = logging.StreamHandler()ch.setLevel(logging.ERROR)# create formatter and add it to the handlersformatter = logging.Formatter(&apos;%(asctime)s - %(name)s - %(levelname)s - %(message)s&apos;)ch.setFormatter(formatter)fh.setFormatter(formatter)# add the handlers to loggerlogger.addHandler(ch)logger.addHandler(fh)# &apos;application&apos; codelogger.debug(&apos;debug message&apos;)logger.info(&apos;info message&apos;)logger.warning(&apos;warn message&apos;)logger.error(&apos;error message&apos;)logger.critical(&apos;critical message&apos;)# =&gt; 2019-09-23 09:20:01,361 - simple_example - ERROR - error message# =&gt; 2019-09-23 09:20:01,377 - simple_example - CRITICAL - critical message 上述代码除了配置 StreamHandler 用于在终端输出日志消息外，还配置了 FileHandler 用于创建 E:\spam.log 日志文件。且日志文件中包含了相对更丰富的信息（因为 Level 级别设置的更低）：123452019-09-23 09:26:58,050 - simple_example - DEBUG - debug message2019-09-23 09:26:58,051 - simple_example - INFO - info message2019-09-23 09:26:58,052 - simple_example - WARNING - warn message2019-09-23 09:26:58,052 - simple_example - ERROR - error message2019-09-23 09:26:58,055 - simple_example - CRITICAL - critical message （3）dictConfig 配置 Logging：1234567891011121314151617181920212223242526272829303132333435import loggingfrom logging.config import dictConfiglogging_config = dict( version=1, formatters=&#123; &apos;f&apos;: &#123;&apos;format&apos;: &apos;%(asctime)s %(name)-12s %(levelname)-8s %(message)s&apos;&#125; &#125;, handlers=&#123; &apos;h&apos;: &#123;&apos;class&apos;: &apos;logging.StreamHandler&apos;, &apos;formatter&apos;: &apos;f&apos;, &apos;level&apos;: logging.DEBUG&#125; &#125;, root=&#123; &apos;handlers&apos;: [&apos;h&apos;], &apos;level&apos;: logging.DEBUG, &#125;,)dictConfig(logging_config)logger = logging.getLogger(&apos;simple_example&apos;)# &apos;application&apos; codelogger.debug(&apos;debug message&apos;)logger.info(&apos;info message&apos;)logger.warning(&apos;warn message&apos;)logger.error(&apos;error message&apos;)logger.critical(&apos;critical message&apos;)# =&gt; 2019-09-09 10:15:47,932 simple_example DEBUG debug message# =&gt; 2019-09-09 10:15:47,934 simple_example INFO info message# =&gt; 2019-09-09 10:15:47,935 simple_example WARNING warn message# =&gt; 2019-09-09 10:15:47,936 simple_example ERROR error message# =&gt; 2019-09-09 10:15:47,938 simple_example CRITICAL critical message （4）YAML 格式的配置文件：12345678910111213141516171819202122# logging.pyimport loggingimport logging.configimport yamlwith open(&apos;E:\\Program\\python\\logging.yml&apos;, &apos;r&apos;) as f: config = yaml.safe_load(f.read()) logging.config.dictConfig(config)logger = logging.getLogger(__file__)# &apos;application&apos; codelogger.debug(&apos;debug message&apos;)logger.info(&apos;info message&apos;)logger.warning(&apos;warn message&apos;)logger.error(&apos;error message&apos;)logger.critical(&apos;critical message&apos;)# =&gt; 2019-09-09 11:18:07,167 - e:\Program\python\logging.py - DEBUG - debug message# =&gt; 2019-09-09 11:18:07,169 - e:\Program\python\logging.py - INFO - info message# =&gt; 2019-09-09 11:18:07,170 - e:\Program\python\logging.py - WARNING - warn message# =&gt; 2019-09-09 11:18:07,171 - e:\Program\python\logging.py - ERROR - error message# =&gt; 2019-09-09 11:18:07,172 - e:\Program\python\logging.py - CRITICAL - critical message 12345678910111213141516171819# logging.ymlversion: 1formatters: simple: format: &apos;%(asctime)s - %(name)s - %(levelname)s - %(message)s&apos;handlers: console: class: logging.StreamHandler level: DEBUG formatter: simple stream: ext://sys.stdoutloggers: sampleLogger: level: DEBUG handlers: [console] propagate: noroot: level: DEBUG handlers: [console] 参考资料Python 3.7.4 documentation]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Program</tag>
        <tag>Advanced</tag>
        <tag>Logging</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 进阶之错误捕获与异常处理]]></title>
    <url>%2F2019%2F09%2F17%2Fpro-python-exception-capturing-and-error-handling%2F</url>
    <content type="text"><![CDATA[一、异常介绍Python 中的异常是一种由 raise 语句自动创建的对象。在异常对象生成之后，Python 程序不会再继续执行 raise 后面紧跟的语句或者操作异常对象本身，而是“搜索”用于处理该异常的某个特定的 handler 。如果该 handler 被 Python 程序找到，则它可以关联并访问异常对象获取更多的信息；如果没有找到与异常对应的 handler，则程序终止并输出错误信息。 PS: LBYL 与 EAFP从理念上讲，Python 倾向于在错误发生之后通过捕获异常来处理程序错误。称为 easier to ask forgiveness than permission (EAFP) 。另外一种错误处理的方式则是尽可能地在错误发生之前检查所有可能发生的情况，这种模式称为 look before you leap (LBYL) 。 Python 提供多种不同类型的异常用以反映错误产生的原因和场景等。每种类型的异常实际上都是一个 Python 类，且其中大多数都继承于 Exception 。对于用户自定义的异常类，也最好作为 Exception 的子类来实现。 Python 异常触发时通常会输出以下内容：12345&gt;&gt;&gt; alist = [1,2,3]&gt;&gt;&gt; alist[7]Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;IndexError: list index out of range 上面代码中的 alist[7] 在请求列表中的项目时超出了列表原本的长度，因此触发了 IndexError 异常。该异常被 Python 交互解释器获取并处理，最终输出错误信息。 如果需要，其实也可以在代码中通过 raise 语句手动触发异常：1234&gt;&gt;&gt; raise IndexError(&quot;Just kidding&quot;)Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;IndexError: Just kidding 二、异常捕获能够终止程序的运行并输出错误信息并不是异常机制的关键所在。异常机制的特殊性在于，通过定义合适的用于处理特定异常的 handler，可以确保一般的异常情况能够被 handler 捕捉，而不会直接导致程序运行失败。handler 可以输出错误信息给用户，或者尝试修复问题，但是重点在于它不会终止程序。 捕获异常的基本语法如下：12345678910111213try: bodyexcept exception_type1 as var1: exception_code1except exception_type2 as var2: exception_code2...except: default_exception_codeelse: else_bodyfinally: finally 其具体的执行流程如下： try 语句首先被执行，如果执行成功（没有抛出异常），则继续执行 else 语句（如果有），try 语句此时结束。最后执行 finally 语句（如果有）。 如果 try 语句执行时抛出异常，则 except 语句根据异常类型进行匹配。如某个 except 语句最终匹配抛出的异常类型，则抛出的异常赋值给其后的变量 var（如果有），对应的 exception_code 被执行。 如果 try 抛出的异常没有任何 except 语句进行匹配，则该异常继续传递看是否有内嵌的 try 语句对其进行处理。 上面格式中最后的 except 语句没有跟任何异常类型，则表示它将匹配关联所有的异常类型。这种方式在调试和创建程序原型时较常用，但并不推荐（因为隐藏了异常包含的细节）。 try 语句中的 else 是可选的且并不常用，它只有在 try 语句执行后未抛出任何异常的情况下才会执行。 finally 语句同样是可选的，它在任何情况下最终都会执行。即便 try 语句抛出异常且没有任何 except 语句进行捕获和处理，该异常也是在 finally 语句执行后抛出给用户。因此 finally 语句多用于一些“清理”任务，比如读写硬盘后的关闭文件等。如：12345try: infile = open(filename) data = infile.read()finally: infile.close() 三、assert 语句assert 语句是 raise 语句的一种特殊形式，其语法格式如下：assert expression, argument 其含义为，如果 expression 表达式的执行结果为 False 且系统变量 __debug__ 的值为 True，则抛出 AssertionError 异常和 argument变量（可选）。 系统变量 __debug__ 的值默认为 True，可以在 Python 运行时添加 -O 或 -OO 选项将该值改为 False。因此可以在开发程序时加入 assert 语句进行调试，而程序发布时将其保留在代码中也不会产生任何影响（只需保证 __debug__ 为 False）。12345&gt;&gt;&gt; x = (1, 2, 3)&gt;&gt;&gt; assert len(x) &gt; 5, &quot;len(x) not &gt; 5&quot;Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;AssertionError: len(x) not &gt; 5 四、代码示例不捕获异常1234while True: x = int(input(&apos;Enter the first number: &apos;)) y = int(input(&apos;Enter the second number: &apos;)) print(x / y) 运行效果（出错后程序退出）：1234567891011$ python exceptions.pyEnter the first number: 5Enter the second number: 41.25Enter the first number: 4Enter the second number: 0Traceback (most recent call last): File &quot;exception.py&quot;, line 4, in &lt;module&gt; print(x / y)ZeroDivisionError: division by zero$ 捕获除数不为零异常1234567while True: try: x = int(input(&apos;Enter the first number: &apos;)) y = int(input(&apos;Enter the second number: &apos;)) print(x / y) except ZeroDivisionError: print(&apos;Division by zero is illegal&apos;) 运行效果（出错后异常被捕获，程序不退出）：12345678$ python exceptions.pyEnter the first number: 4Enter the second number: 0Division by zero is illegalEnter the first number: 5Enter the second number: 41.25Enter the first number: 捕获多个异常123456789while True: try: x = int(input(&apos;Enter the first number: &apos;)) y = int(input(&apos;Enter the second number: &apos;)) print(x / y) except ZeroDivisionError: print(&apos;Division by zero is illegal&apos;) except ValueError: print(&quot;That wasn&apos;t a number, was it?&quot;) 运行效果：1234567891011$ python exceptions.pyEnter the first number: 4Enter the second number: 0Division by zero is illegalEnter the first number: 4Enter the second number: helloThat wasn&apos;t a number, was it?Enter the first number: 5Enter the second number: 41.25Enter the first number: 捕获所有异常（不推荐，隐藏了异常的细节）1234567while True: try: x = int(input(&apos;Enter the first number: &apos;)) y = int(input(&apos;Enter the second number: &apos;)) print(x / y) except: print(&apos;Something wrong happened ...&apos;) 运行效果：12345678$ python exceptions.pyEnter the first number: 4Enter the second number: 0Something wrong happened ...Enter the first number: 4Enter the second number: helloSomething wrong happened ...Enter the first number: 稍好一点的版本（输出异常信息）：1234567while True: try: x = int(input(&apos;Enter the first number: &apos;)) y = int(input(&apos;Enter the second number: &apos;)) print(x / y) except Exception as e: print(&apos;Invalid input:&apos;, e) 运行效果：1234567Enter the first number: 4Enter the second number: 0Invalid input: division by zeroEnter the first number: 4Enter the second number: helloInvalid input: invalid literal for int() with base 10: &apos;hello&apos;Enter the first number: 参考资料Beginning Python 3rdThe Quick Python Book 3rd Edition]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Tools</tag>
        <tag>Tricks</tag>
        <tag>Python</tag>
        <tag>Program</tag>
        <tag>Advanced</tag>
        <tag>Technique</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 进阶之生成器与迭代器]]></title>
    <url>%2F2019%2F09%2F05%2Fpro-python-iterators-and-generators%2F</url>
    <content type="text"><![CDATA[迭代器（Iterators）和生成器（Generators）是 Python 语言中用处很大的工具，在某些情况下通过它们可以写出更加精简、清晰和高效的代码。 迭代器迭代器是一种包含 __next__ 方法、用于处理数据流的对象。当使用 for 循环之类的方式遍历迭代器中的每一项数据时，__next__ 方法即被重复性地调用进而完成特定的动作。 以下代码即为一个不完整的迭代器类：1234567891011121314class MultiplyByTwo: def __init__(self, number): self.number = number self.counter = 0 def __next__(self): self.counter += 1 return self.number * self.countermul = MultiplyByTwo(5)print(next(mul)) # =&gt; 5print(next(mul)) # =&gt; 10print(next(mul)) # =&gt; 15print(next(mul)) # =&gt; 20 通过 next() 函数手动调用 MultiplyByTwo 类的 __next__ 方法，可以依次得到 number 变量与递增的 counter 变量的乘积。但是作为一个可被遍历的迭代器类，理论上也应该可以通过 for 语句直接以循环的方式逐次取出 __next__ 返回的乘积。 此时运行如下代码对 MultiplyByTwo 类进行迭代操作：12for num in MultiplyByTwo(5): print(num) 则会报出 TypeError: &#39;MultiplyByTwo&#39; object is not iterable 错误。即此时的“迭代器”对象还不支持遍历操作。 需要在 MultiplyByTwo 类中添加 __iter__ 方法表明其“可被遍历”，代码如下：12345678910111213141516171819class MultiplyByTwo: def __init__(self, number): self.number = number self.counter = 0 def __iter__(self): return self def __next__(self): self.counter += 1 return self.number * self.counterfor num in MultiplyByTwo(5): print(num)# =&gt; 5# =&gt; 10# =&gt; 15# =&gt; 20# =&gt; ... 此时则可以使用 for 语句循环地调用 __next__ 方法获取 number 和递增的 counter 变量的乘积。比较尴尬的是，以上面的方式遍历 MultiplyByTwo 迭代器，for 循环会一直运行下去。 可以通过以下代码为前面的迭代器添加终止点：12345678910111213141516171819202122232425class MultiplyByTwo: def __init__(self, number, limit): self.number = number self.limit = limit self.counter = 0 def __iter__(self): return self def __next__(self): self.counter += 1 value = self.number * self.counter if value &gt; self.limit: raise StopIteration else: return valuefor num in MultiplyByTwo(5, 20): print(num)# =&gt; 5# =&gt; 10# =&gt; 15# =&gt; 20 所以一个实现了 __next__ 方法（用以不断返回下一个数据）、可进行迭代操作的对象即为迭代器。 对于前面实现的 MultiplyByTwo 迭代器类，在用 for 循环进行遍历操作时，for num in MultiplyByTwo(5, 20): 的最终结果等同于 for 语句对以下列表的遍历：for num in [5 * 1, 5 * 2, 5 * 3, 5 *4]: 。 实际上，迭代器对象表示的是一个数据流，其可以被 next() 函数调用并不断返回下一个数据，直到没有数据时抛出 StopIteration 异常。但这个“数据流”不是原本就完整地存在的，只能通过 next() 函数逐次地计算下一个值。即迭代器的计算是惰性的，只有在需要返回下一个数据时才会被动地进行计算。 生成器生成器可以理解为一种实现了迭代器协议的数据结构。前面提到迭代器是“懒惰”的，一次只计算一个数据。对于项目体量非常庞大的数据集合（比如全体自然数？没有任何一个列表能装下）或文件，生成器相比于其他数据结构，能够更加高效地使用内存资源。 生成器代码示例：123456789101112131415def multiple_generator(num, limit): counter = 1 value = num * counter while value &lt;= limit: yield value counter += 1 value = num * counterfor num in multiple_generator(5, 20): print(num)# =&gt; 5# =&gt; 10# =&gt; 15# =&gt; 20 不同于前面的 MultiplyByTwo 类，此次的 multiple_generator 生成器函数并不需要实现 __next__ 和 __iter__ 方法，也不用检查内部状态确认是否发起异常。而是使用了 yield 关键字。yield 关键字类似于 return，只不过它并不会终止函数的运行。而是暂时中断函数的执行，直到下一个数据被请求（就继续返回下一个值）。因此生成器相对于一般的迭代器更加易读和高效。 以下是一个经典的生成器在现实中的应用，通过生成器以数据块的方式读取文件：1234567891011def read_in_chunks(file_handler, chunk_size=1024): &quot;&quot;&quot; Lazy function (generator) to read a file piece by piece.&quot;&quot;&quot; while True: data = file_handler.read(chunk_size) if not data: break yield dataf = open(&apos;large_number_of_data.dat&apos;)for piece in read_in_chunks(f): print(piece) 通过生成器每次只读取 1k 大小的数据块，循环遍历直到整个文件被处理。而不是将所有文件数据一次性地读入到内存中。当需要读取的文件非常大时，此种方式相对于常规手段更加节省内存的使用。 yield fromyield from 关键字可以用来从其他生成器中获取数据。参考如下代码：12345678910def flat_list(iter_values): &quot;&quot;&quot;flatten a multi list or something&quot;&quot;&quot; for item in iter_values: if hasattr(item, &apos;__iter__&apos;): yield from flat_list(item) else: yield itemprint(list(flat_list([1, 2, [3, 4, [5]], 6])))# =&gt; [1, 2, 3, 4, 5, 6] 上面的短短 6 行代码可以将包含任意层次的可迭代对象（如列表等）转换成单层的列表结构。个人感觉，这代码非常漂亮，有点层层递进、环环相扣的感觉了。值得好好观赏。 PS：可以对比下阶乘的实现代码：12345def factorial(n): if n != 1: return n * factorial(n-1) else: return 1 参考资料Clean Python]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Functional</tag>
        <tag>Programming</tag>
        <tag>Language</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 进阶之 Decorators（装饰器）浅析]]></title>
    <url>%2F2019%2F09%2F05%2Fpro-python-decorators-intro%2F</url>
    <content type="text"><![CDATA[Decorators（装饰器）可以在不更改函数或对象的行为的前提下，动态地向其添加额外的效果。 假设当前的项目中有多个函数需要添加日志功能，即函数执行时向终端或者日志文件中输出特定的内容。 有一种办法就是在每一个函数中添加上若干行记录日志的代码，但这种方式耗费时间的同时，也容易出现意想不到的错误，毕竟会对原本的代码做出相当大的改动。而另一办法就是在每一个函数或类前面添加装饰器，通过装饰器向被装饰函数添加额外的行为（记录日志），这样在提升效率的同时，也不会导致现有的代码中引入了新的 bug 。 比较常见的一种装饰器，比如下面的一段最简单的 flask 应用代码：123456789from flask import Flaskapp = Flask(__name__)@app.route(&quot;/&quot;)def hello(): return &quot;Hello World&quot;if __name__ == &apos;__main__&apos;: app.run() 上面的代码通过 route 装饰器向 hello 函数添加了某些效果，在不变更原 hello 函数内部代码的前提下，将其变成了某个新函数作为 Web 应用中的 API 接受调用 。 一、初级示例函数作为参数下面是一段简单的函数代码，可以用来将某个字符串转换为大写：123456789def to_uppercase(text): if not isinstance(text, str): raise TypeError(&quot;Not a string&quot;) return text.upper()text = &quot;Hello World&quot;upper_text = to_uppercase(text)print(upper_text)# =&gt; HELLO WORLD 这里对 to_uppercase 函数做一点微小的改动：12345678910111213def to_uppercase(func): text = func() if not isinstance(text, str): raise TypeError(&quot;Not a string&quot;) return text.upper()def hello(): return &quot;Hello World&quot;hello = to_uppercase(hello)print(hello)# =&gt; HELLO WORLD 与之前版本的 to_uppercase 不同，此版本的 to_uppercase 函数并不直接使用字符串作为输入，而是接收某个函数作为参数，将该函数执行后返回的字符串转换为大写。 to_uppercase 函数并没有改变 hello 函数原本的行为（输出字符串），而是在其基础上添加了额外的效果（将输出字符串转换为大写），因而起到了装饰器的作用。 上面的代码也可以写成如下的形式（两者效果相同）：12345678910111213def to_uppercase(func): text = func() if not isinstance(text, str): raise TypeError(&quot;Not a string&quot;) return text.upper()@to_uppercasedef hello(): return &quot;Hello World&quot;print(hello)# =&gt; HELLO WORLD @decorator 是 Python 中的语法糖，123@decoratordef func(): ... 等同于123def func(): ...func = decorator(func) 函数作为返回值以下代码为 to_uppercase 装饰器的最终形式：1234567891011121314151617181920def to_uppercase(func): def wrapper(): text = func() if not isinstance(text, str): raise TypeError(&quot;Not a string type&quot;) return text.upper() return wrapper@to_uppercasedef hello(): return &quot;Hello World&quot;&apos;&apos;&apos;等同于def hello(): return &quot;Hello World&quot;hello = to_uppercase(hello)&apos;&apos;&apos;print(hello())# =&gt; HELLO WORLD 之前的 to_uppercase 函数接收另一个函数作为参数，获取其返回值并作出修改，最后返回修改后的结果。而此处的 to_uppercase 函数在代码中内嵌了一个 wrapper 函数并将其作为返回值，wrapper 函数中包含了对被装饰的函数做出的改动。即 to_uppercase 装饰器接收被装饰的函数作为参数，通过内嵌函数对其进行改动，最终返回一个新的函数替代被装饰的原函数。 回到代码中，hello 函数用于返回 Hello World 字符串，而装饰器 to_uppercase 接收 hello 作为参数，通过 wrapper 对其添加新的行为（将返回的字符串转为大写）并替换掉原来的 hello 函数。因此在不改变原 hello 函数内部代码的情况下，通过装饰器生成了新的 hello 函数，最终改变了原函数的行为。 二、使用多个装饰器代码如下：12345678910111213141516171819202122def add_prefix(func): def wrapper(): text = func() result = &quot; &quot;.join([text, &quot;Larry Page!&quot;]) return result return wrapperdef to_uppercase(func): def wrapper(): text = func() if not isinstance(text, str): raise TypeError(&quot;Not a string&quot;) return text.upper() return wrapper()@to_uppercase@add_prefixdef say(): return &quot;welcome&quot;print(say)# =&gt; WELCOME LARRY PAGE! 三、带参数的装饰器示例代码如下：1234567891011121314def to_uppercase(func): def wrapper(*args, **kwargs): text = func(*args, **kwargs) if not isinstance(text, str): raise TypeError(&quot;Not a string&quot;) return text.upper() return wrapper@to_uppercasedef say(greet): return greetprint(say(&quot;hello, how are you&quot;))# =&gt; HELLO, HOW ARE YOU 四、functools.wraps运行如下装饰器代码：1234567891011121314151617def logging(func): def logs(*args, **kwargs): print(func.__name__ + &quot; was called&quot;) return func(*args, **kwargs) return logs@loggingdef foo(x): &quot;&quot;&quot;Calling function for logging&quot;&quot;&quot; return x * xfo = foo(10)# =&gt; foo was calledprint(foo.__name__)# =&gt; logsprint(foo.__doc__)# =&gt; None 从运行结果中可以看出，print(foo.__name__) 并没有输出 foo ，而是打印了装饰器的内嵌函数 logs 的名字。即被装饰的函数 foo 由新函数替代后，其 __name__ 和 __doc__ 等属性也丢失了。 为了避免这种情况，可以使用 functool.wrap ，代码如下：12345678910111213141516171819from functools import wrapsdef logging(func): @wraps(func) def logs(*args, **kwargs): print(func.__name__ + &quot; was called&quot;) return func(*args, **kwargs) return logs@loggingdef foo(x): &quot;&quot;&quot;does some math&quot;&quot;&quot; return x * xfo = foo(10)# =&gt; foo was calledprint(foo.__name__)# =&gt; fooprint(foo.__doc__)# =&gt; does some math 五、场景：基于装饰器的授权很多 Web API 都需要用户携带认证信息才能访问，当然可以在每一段 API 的代码中加入检查授权状态的片段，更便捷的方式则是使用装饰器。如：12345678910from functools import wraps def requires_auth(func): @wraps(func) def wrapper(*args, **kwargs): auth = request.authorization if not auth or not check_auth(auth.username, auth.password): authenticate() return func(*args, **kwargs) return wrapper 则每一个被 require_auth 装饰的函数执行前，都会先获取授权信息并验证。 参考资料Clean PythonPython 函数装饰器]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Function</tag>
        <tag>Programming</tag>
        <tag>Language</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python Tricks —— 使用 asciinema 录制命令行操作]]></title>
    <url>%2F2019%2F08%2F25%2Fusing-asciinema-to-record-Linux-terminal-sessions%2F</url>
    <content type="text"><![CDATA[asciinema 是一个由 Python 语言编写的开源的终端会话录制工具。它可以将命令行的输出内容根据时间保存在 JSON 格式的文件中，以供后续播放时使用。同时该录制文件也可以通过 Web 浏览器进行播放（需要使用由 asciinema-player 提供的 Javascript 和 CSS 文件），或者直接上传到 asciinema.org 网站分享给其他互联网用户。 一、script 命令script 命令是 Linux 系统自带一个的终端录制工具，功能与 asciinema 类似，可以将终端交互内容保存在本地的文本文件中，再使用 scriptreplay 命令进行播放。 录制：script -t 2&gt;time.file -a output.file 其中 time.file 用于保存时间信息，output.file 则用于记录终端输出的内容及光标的移动等。录制完成时使用 exit 命令或者 Ctrl+D 终止录制。 播放：scriptreplay time.file output.file 二、asciinema 本地录制asciinema 安装比较简单，直接使用 pip 命令即可：$ pip install asciinema 可使用如下命令将终端内容录制到本地文件中：$ asciinema rec demo.cast完成后使用 exit 或 Ctrl+D 结束录制。 使用如下命令播放前面录制的内容：$ asciinema play demo.cast播放时可使用 -s &lt;n&gt; 选项控制回放的速度，其中 n 为表示倍率的数字，数值越大播放速度越快。 其他选项的使用方法可通过 asciinema rec -h 或 asciinema play -h 命令查看帮助信息。 三、浏览器播放浏览器播放录制文件需要借助 asciinema-player 项目提供的两个库文件 asciinema-player.css 和 asciinema-player.js 。这两个文件可以从该 Github 项目的 release 中下载，也可以直接通过 CDN 链接引入。 示例 HTML 代码（注意新增的 asciinema-player 标签）如下：123456789&lt;html&gt;&lt;head&gt; &lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;asciinema-player.css&quot; /&gt;&lt;/head&gt;&lt;body&gt; &lt;asciinema-player src=&quot;demo.cast&quot;&gt;&lt;/asciinema-player&gt; &lt;script src=&quot;asciinema-player.js&quot;&gt;&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 播放效果如下： 四、上传至 asciinema.org首先访问 asciinema.org 创建一个新账户。再使用 asciinema auth 命令生成自己电脑独有的 ID ：123456$ asciinema authOpen the following URL in a web browser to link your install ID with your asciinema.org user account:https://asciinema.org/connect/636713f1-db74-41c5-bb45-97fc213f3c94This will associate all recordings uploaded from this machine (past and future ones) to your account, and allow you to manage them (change title/theme, delete) at asciinema.org. 通过浏览器访问上面生成的链接完成绑定，则此台设备上使用 asciinema rec 录制的内容都将自动上传至之前创建的账户中。 录制：asciinema rec停止录制后按下回车键自动上传至云端并返回播放链接。123456asciinema: recording finishedasciinema: press &lt;enter&gt; to upload to asciinema.org, &lt;ctrl-c&gt; to save locallyView the recording at: https://asciinema.org/a/9WlhbT4qnTr5cFWQQZNS0auIY 播放：asciinema play &lt;link&gt; 个人账户界面如下： 如不方便将录制内容放置于外部的公网服务器上，也可以自行搭建类似的私人服务器。参考 asciinema-server 项目，可使用 docker 快速进行部署，不再做详细说明。 注意部署成功后修改 ~/.config/asciinema/config 配置文件，添加如下内容：12[api]url = https://your.asciinema.host 参考资料asciinema Docs]]></content>
      <categories>
        <category>Tools</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Admin</tag>
        <tag>Tools</tag>
        <tag>Tricks</tag>
        <tag>Terminal</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python Tricks —— 使用 Supervisor 控制 Linux 进程]]></title>
    <url>%2F2019%2F08%2F24%2Fusing-supervisor-to-control-Linux-processes%2F</url>
    <content type="text"><![CDATA[Supervisor 是一个 C/S 架构的进程控制系统，完全由 Python 语言编写。它允许用户监视和控制类 Unix 系统上的一个或多个进程。 在 Linux 系统中，一般可以使用 service &lt;service-name&gt; start|stop|restart 或者 systemctl start|stop|restart &lt;service-name&gt; 等命令启动或停止某个服务的进程实例。实际上，service 和 systemctl 命令都是通过调用 /etc/init.d/ 目录下的启动脚本对服务进行控制。 相对而言，这类启动脚本在编写和维护上都需要耗费比较大的精力。而 Supervisor 只通过一个简单的 INI 格式的配置文件即能完成对一个或多个进程的集中管理，且能够控制进程在崩溃后自动重启。同时，Supervisor 还支持对进程进行分组，并为组中的进程分配不同的优先级，使得它们可以按预先定义的顺序进行启动。 一、软件安装Supervisor 支持 Linux、MacOS、Solaris 和 FreeBSD 等操作系统，直接使用对应的包管理命令安装即可。如 Ubuntu 系统：$ sudo apt-get install supervisor Supervisor 完全由 Python 语言编写，因此可以使用 pip 命令安装，也支持安装到由 virtualenv 等工具构建的虚拟环境中。$ pip install supervisor 组件介绍supervisordsupervisord 是 Supervisor 的服务端组件，负责控制子进程的生命周期、响应客户端组件的命令请求、重启崩溃的进程以及日志记录等工作。supervisord 使用 Windows-INI 格式的配置文件。 supervisorctlsupervisorctl 是一个借助命令行进行交互式操作的客户端组件，它可以通过类似 Shell 的接口访问 supervisord 提供的功能。 Web Server如 Supervisor 配置文件中的 [inet_http_server] 项被启用，则可以通过访问 http://localhost:9001 进入一个功能类似 supervisorctl 的 Web 交互界面。 XML-RPC 接口Supervisor 的 HTTP 服务还提供了 XML-RPC 接口，参考 XML-RPC API Documentation 二、配置文件Supervisor 的配置文件一般命名为 supervisord.conf。它可以同时被 supervisord 和 supervisorctl 使用（通过 -c 选项指定）。如执行上述两个命令时并未手动指定配置文件，则依次在以下位置查找名为 supervisord.conf 的配置文件： $CWD/supervisord.conf $CWD/etc/supervisord.conf /etc/supervisord.conf /etc/supervisor/supervisord.conf ../etc/supervisord.conf ../supervisord.conf Section Settings配置文件中的 [inet_http_server] 用于配置可供远程访问的 HTTP 服务，示例如下：1234[inet_http_server]port = 127.0.0.1:9001username = userpassword = 123 [supervisord] 用于配置 supervisord 进程的一些全局选项，示例如下：1234567891011121314151617[supervisord]logfile = /tmp/supervisord.loglogfile_maxbytes = 50MBlogfile_backups=10loglevel = infopidfile = /tmp/supervisord.pidnodaemon = falseminfds = 1024minprocs = 200umask = 022user = chrismidentifier = supervisordirectory = /tmpnocleanup = truechildlogdir = /tmpstrip_ansi = falseenvironment = KEY1=&quot;value1&quot;,KEY2=&quot;value2&quot; [supervisorctl] 用于控制交互式 Shell supervisorctl 的行为，示例如下：12345[supervisorctl]serverurl = unix:///tmp/supervisor.sockusername = chrispassword = 123prompt = mysupervisor [program:x]配置文件中需要至少包含一个 [program:x] 项用于指定某个由 Supervisor 控制的进程。示例如下：123456789101112131415161718192021222324252627282930[program:cat]command=/bin/catprocess_name=%(program_name)snumprocs=1directory=/tmpumask=022priority=999autostart=trueautorestart=unexpectedstartsecs=10startretries=3exitcodes=0stopsignal=TERMstopwaitsecs=10stopasgroup=falsekillasgroup=falseuser=chrismredirect_stderr=falsestdout_logfile=/a/pathstdout_logfile_maxbytes=1MBstdout_logfile_backups=10stdout_capture_maxbytes=1MBstdout_events_enabled=falsestderr_logfile=/a/pathstderr_logfile_maxbytes=1MBstderr_logfile_backups=10stderr_capture_maxbytes=1MBstderr_events_enabled=falseenvironment=A=&quot;1&quot;,B=&quot;2&quot;serverurl=AUTO [include] 可以用来引入其他位置的配置文件。示例如下：12[include]files = /an/absolute/filename.conf /an/absolute/*.conf foo.conf config??.conf 三、实例演示对于一个初始的 Django 项目，可以通过在项目目录下执行 python manage.py runserver 命令启动 Web 服务。这里使用 Supervisor 控制服务进程。 首先在项目目录下创建 supervisord.conf 配置文件。该文件不需要手动创建，可以直接通过 echo_supervisord_conf 命令获取示例配置并定向到 supervisord.conf 文件中，命令如下：$ echo_supervisord_conf &gt; supervisord.conf 该示例配置中并不包含任何 [program:x] 项，需要自行添加。这里在文件末尾添加如下两行内容：12[include]files = ./django_server.ini 即为了清晰起见，这里不把 Supervisor 控制的进程包含在主配置文件中，而是放置在当前目录下的 django_server.ini 文件中，再通过主配置文件中的 [include] 将其导入。django_server.ini 文件内容如下：12345678910111213[program:django_server]command=python manage.py runserver 0000:8000 ;程序启动命令process_name=%(program_name)s ;进程名称directory=/home/starky/project/django/test_project ;启动目录autostart=true ;自动启动autorestart=unexpected ;异常退出后自动重启startsecs=10 ;程序启动 10 秒后未异常退出，则正常启动startretries=3 ;自动重启尝试次数user=starky ;启动该进程的用户redirect_stderr=true ;stderr 重定向至 stdout 。默认 falsestdout_logfile_maxbytes=20MB ;stdout 日志大小，默认 50MBstdout_log_backups=10 ;stdout 日志文件备份数量stdout_logfile=./logs/django_server.log ;stdout 日志路径。目录不存在时需手动创建 配置完成后，运行以下命令启动 supervisord 程序：$ supervisord -c supervisord.conf 运行 supervisorctl 命令进入交互式 Shell ，可以看到 django_server.ini 中配置的 django_server 程序已经启动成功：123$ supervisorctldjango_server RUNNING pid 6210, uptime 0:00:15supervisor&gt; Web Server出于安全上的考虑，Supervisor 提供的 Web Server 默认是关闭的，可以通过修改主配置文件 supervisord.conf 进行启用。删除以下内容前的注释即可：1234[inet_http_server] ; inet (TCP) server disabled by defaultport=0.0.0.0:9001 ; ip_address:port specifier, *:port for all ifaceusername=user ; default is no username (open server)password=123 ; default is no password (open server) 配置完成后，首先使用 pkill -9 supervisord 命令退出 supervisord 程序，再使用 supervisord -c supervisord.conf 命令重新启动。此时通过浏览器访问 http://127.0.0.1:9001/，输入用户名密码后即可进入 Web 控制台界面： supervisorctlsupervisorctl 是一个交互式的 Shell ，可以通过它访问 supervisord 提供的部分功能（如启动和停止某个进程、查看日志等）。它支持的基本命令如下：12345678$ supervisorctldjango_server RUNNING pid 8661, uptime 0:14:11supervisor&gt; helpdefault commands (type help &lt;topic&gt;):=====================================add exit open reload restart start tailavail fg pid remove shutdown status updateclear maintail quit reread signal stop version 输入 help &lt;topic&gt; 可以查看对应命令的帮助信息。 常用命令的使用方法如下： help : Print a list of available actions help &lt;action&gt; : Print help for add &lt;name&gt; [...] : Activates any updates in config for process/group remove &lt;name&gt; [...] : Removes process/group from active config update : Reload config and add/remove as necessary, and will restart affected programs update all : Reload config and add/remove as necessary, and will restart affected programs update &lt;gname&gt; [...] : Update specific groups, and will restart affected programs clear &lt;name&gt; : Clear a process’ log files. clear &lt;name&gt; &lt;name&gt; : Clear multiple process’ log files clear all : Clear all process’ log files fg &lt;process&gt; : Connect to a process in foreground mode Press Ctrl+C to exit foreground pid : Get the PID of supervisord. pid &lt;name&gt; : Get the PID of a single child process by name. pid all : Get the PID of every child process, one per line. reload : Restarts the remote supervisord reread : Reload the daemon’s configuration files, without add/remove (no restarts) start | stop | restart &lt;name&gt; : Start / Stop / Restart a process start | stop | restart &lt;gname&gt;:* : Start / Stop / Restart all processes in a group. start | stop | restart &lt;name&gt; &lt;name&gt; : Start / Stop / Restart multiple processes or groups start | stop | restart all : Start / Stop / Restart all processes. Note: restart does not reread config files. status : Get all process status info. status &lt;name&gt; : Get status on a single process by name. status &lt;name&gt; &lt;name&gt; : Get status on multiple named processes. tail &lt;name&gt; [stdout|stderr] (default stdout) : Output the last part of process logs tail -f &lt;name&gt; : Continuous tail of named process stdout Ctrl-C to exit tail -100 &lt;name&gt; : last 100 bytes of process stdout 参考资料Supervisord Docs]]></content>
      <categories>
        <category>Tools</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Admin</tag>
        <tag>System</tag>
        <tag>Tools</tag>
        <tag>Server</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 磁盘设备和 LVM 管理命令详解]]></title>
    <url>%2F2019%2F08%2F11%2FLinux-disk-and-lvm-management%2F</url>
    <content type="text"><![CDATA[一、设备文件在 Linux 操作系统中，设备文件是一种特殊类型的文件。这些文件绝大多数位于 /dev 目录下，用来表示 Linux 主机检测到的某个具体的硬件设备。比如 /dev/sda 文件通常用来指代系统中的第一块硬盘。Linux 操作系统及其应用与服务则通过这些设备文件与对应的硬件设备进行交互。 对于常见的磁盘（ATA、SATA、SCSI、SAS、SSD 等）和优盘等块存储设备，其设备文件主要以 sd* 的形式命名。如 sda 表示第一块硬盘，sdb2 表示第二块硬盘的第二个分区，以此类推。因此，可直接使用 ls -l /dev/sd* 命令查看系统中的磁盘设备：123$ ls -l /dev/sd*brw-rw---- 1 root disk 8, 0 8月 7 00:47 /dev/sdabrw-rw---- 1 root disk 8, 1 8月 7 00:47 /dev/sda1 即当前系统中只连接了一块硬盘（/dev/sda），且该硬盘只有一个分区（/dev/sda1）。 二、分区分区可以理解为将一整块硬盘划分为一个或多个相互独立的存储区域。 比如可以将系统的第一块硬盘划分为 3 个分区，分别为 sda1、sda2、sda3 。sda1 用于挂载根目录（/），sda2 挂载 /var ，sda3 挂载 /home 目录。则即使 /var 目录下的日志文件等占用了 sda2 全部的存储空间，也不会影响其他两个分区的使用。 可以使用 fdisk -l 命令查看系统中的磁盘和分区信息：1234567891011$ sudo fdisk -lDisk /dev/sda: 10 GiB, 10737418240 bytes, 20971520 sectorsDisk model: VBOX HARDDISKUnits: sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytesDisklabel type: dosDisk identifier: 0x20985120Device Boot Start End Sectors Size Id Type/dev/sda1 2048 20964824 20962777 10G 83 Linux 创建磁盘分区fdisk 命令还可以用来对硬盘进行分区操作，包括创建新分区、删除已有的分区、创建分区表等。我这里通过 VirtualBox 软件为虚拟机中的 Linux 系统添加了一块空白的虚拟硬盘。使用 fdisk -l 命令查看系统检测到的硬盘设备：123456789101112131415161718$ sudo fdisk -lDisk /dev/sda: 10 GiB, 10737418240 bytes, 20971520 sectorsDisk model: VBOX HARDDISKUnits: sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytesDisklabel type: dosDisk identifier: 0x20985120Device Boot Start End Sectors Size Id Type/dev/sda1 2048 20964824 20962777 10G 83 LinuxDisk /dev/sdb: 5 GiB, 5368709120 bytes, 10485760 sectorsDisk model: VBOX HARDDISKUnits: sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytes 此时系统中多了一块不包含任何分区的新硬盘 /dev/sdb 。 使用 fdisk /dev/sdb 命令对新硬盘进行分区操作：12345678910111213141516171819202122232425262728293031323334353637383940414243444546$ sudo fdisk /dev/sdbWelcome to fdisk (util-linux 2.33.1).Changes will remain in memory only, until you decide to write them.Be careful before using the write command.Device does not contain a recognized partition table.Created a new DOS disklabel with disk identifier 0xce119026.Command (m for help): mHelp: DOS (MBR) a toggle a bootable flag b edit nested BSD disklabel c toggle the dos compatibility flag Generic d delete a partition F list free unpartitioned space l list known partition types n add a new partition p print the partition table t change a partition type v verify the partition table i print information about a partition Misc m print this menu u change display/entry units x extra functionality (experts only) Script I load disk layout from sfdisk script file O dump disk layout to sfdisk script file Save &amp; Exit w write table to disk and exit q quit without saving changes Create a new label g create a new empty GPT partition table G create a new empty SGI (IRIX) partition table o create a new empty DOS partition table s create a new empty Sun partition table 进入 fdisk 程序界面之后，按下 m 键并回车，即可打印帮助信息，获取该界面下支持的交互式命令。比如输入 p 可以用来输出当前硬盘的分区信息，输入 n 创建新的分区，输入 d 删除已有的分区。在对分区进行任何操作之后，最后都需要使用 w 将之前的所有更改写入硬盘。 这里先按下 n 开始新分区的创建，根据提示选择分区类型（p 表示主分区，e 表示扩展分区），进一步选择分区编号和第一个扇区的位置（一般默认即可），最后输入新分区中最后一个扇区的位置（也可以直接指定分区大小），格式为 +/-sectors 或 +/-size 。如输入 +3G 则表示创建大小为 3 GB 的新分区。具体步骤如下：123456789101112131415161718192021Command (m for help): nPartition type p primary (0 primary, 0 extended, 4 free) e extended (container for logical partitions)Select (default p): pPartition number (1-4, default 1):First sector (2048-10485759, default 2048):Last sector, +/-sectors or +/-size&#123;K,M,G,T,P&#125; (2048-10485759, default 10485759): +3GCreated a new partition 1 of type &apos;Linux&apos; and of size 3 GiB.Command (m for help): nPartition type p primary (1 primary, 0 extended, 3 free) e extended (container for logical partitions)Select (default p): pPartition number (2-4, default 2):First sector (6293504-10485759, default 6293504):Last sector, +/-sectors or +/-size&#123;K,M,G,T,P&#125; (6293504-10485759, default 10485759):Created a new partition 2 of type &apos;Linux&apos; and of size 2 GiB. 再用同样的步骤将磁盘的剩余空间划分为另一个分区。此时查看分区信息，原本空白的 5GB 新硬盘 sdb 已经被划分为两个分区 sdb1 和 sdb2 ：123456789101112Command (m for help): pDisk /dev/sdb: 5 GiB, 5368709120 bytes, 10485760 sectorsDisk model: VBOX HARDDISKUnits: sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytesDisklabel type: dosDisk identifier: 0xce119026Device Boot Start End Sectors Size Id Type/dev/sdb1 2048 6293503 6291456 3G 83 Linux/dev/sdb2 6293504 10485759 4192256 2G 83 Linux 需要注意的是，如果此时按下 q 按键直接退出 fdisk 程序，则之前所做的全部操作都不会被保存。如确认前面对硬盘的操作没有问题，应使用 w 命令将新的分区信息写入到磁盘中。类似于编辑文件时的保存并退出。 三、文件系统可以将磁盘等存储设备看作一个小型的图书馆，存放在其中的书籍即硬盘中的数据，而分区的作用类似于对书籍分门类存放的书架，形成相对独立的区域。但是书架上的书籍并不是随意放置的，每本书都需要根据一定的规则和顺序有规律地摆放，有时还要记录下摆放的具体位置。这些书籍的摆放规则即对应于分区上的文件系统。 文件系统是对存储设备的空间进行组织和分配，负责文件存取并对存入的文件进行保护和检索的系统。对操作系统而言，文件的读写不会直接作用于硬盘扇区，而是通过文件系统以特定的规则处理和组织文件数据。常见的文件系统如 Windows 中的 NTFS 和 Linux 系统中 Ext4 等。 在 Windows 系统中，通常所说的“分区”操作即包含了创建分区并建立文件系统的过程。而在 Linux 系统中，这两步操作则需要两个独立的命令完成。 可以使用 mkfs.ext4 /dev/sdb1 命令，在之前新加硬盘的第一个分区上创建 Ext4 格式的文件系统。1234567891011$ sudo mkfs.ext4 /dev/sdb1mke2fs 1.44.6 (5-Mar-2019)Creating filesystem with 786432 4k blocks and 196608 inodesFilesystem UUID: d5e21599-12e9-44da-ae51-124d89fe5edaSuperblock backups stored on blocks: 32768, 98304, 163840, 229376, 294912Allocating group tables: doneWriting inode tables: doneCreating journal (16384 blocks): doneWriting superblocks and filesystem accounting information: done 交换分区Linux 系统中的 swap 分区可以看作位于硬盘上的“内存设备”。Linux 会将内存中一部分不需要立即使用的数据临时交换至硬盘上的 swap 分区，以缓解内存不足等情况。 我的 Linux 虚拟机安装时并没有分配 swap 分区，这里通过 mkswap 命令将 2G 大小的 sdb2 分区划分为 swap 空间：123$ sudo mkswap /dev/sdb2Setting up swapspace version 1, size = 2 GiB (2146430976 bytes)no label, UUID=47006330-810c-4321-8d73-d52a5f70bc88 然后使用 swapon 命令立即启用前面创建的 swap 分区：12345$ sudo swapon /dev/sdb2$ free -h total used free shared buff/cache availableMem: 983Mi 223Mi 168Mi 4.0Mi 590Mi 597MiSwap: 2.0Gi 0B 2.0Gi 分区挂载在 Windows 系统中，一般插入一个已经分好区的硬盘或优盘之后，会自动为添加的一个或多个分区分配盘符（如 D:、E:、F: 等），之后就可以直接通过盘符在新分区上读取或写入文件了。 Linux 系统中没有盘符的概念，它的文件层次是一个从根目录（/）开始的树状结构（目录），一直向下延申，每一个分支都是一条具体的路径，指向某个特定的文件。比如 /usr、/root、/var、/var/log 等。 目录可以说是独立于硬件存储设备的抽象的逻辑结构，用于指定文件系统层次中的某个具体位置。而磁盘分区与目录结构的对应关系，则需要通过挂载来指定。 一般在安装系统时，可以将 sda1 分区挂载到根目录下，则该目录下的所有文件之后都将保存在 sda1 上。如果后面又添加了一块新的数据盘 sdb，该硬盘只有一个分区 sdb1。为了将某些文件保存在 sdb1 分区上，可以在目录树中新建一个空白分支（比如 /mnt/data）并将 sdb1 挂载在该分支下。之后 /mnt/data 目录下创建的任何子目录和文件等数据都会保存在 sdb1 上。具体命令如下：12$ sudo mkdir -p /mnt/data$ sudo mount /dev/sdb1 /mnt/data 使用 df -h 命令查看文件系统占用的磁盘空间的具体情况：1234567891011$ df -hFilesystem Size Used Avail Use% Mounted onudev 456M 0 456M 0% /devtmpfs 99M 1.1M 98M 2% /run/dev/sda1 9.8G 5.2G 4.2G 56% /tmpfs 492M 0 492M 0% /dev/shmtmpfs 5.0M 4.0K 5.0M 1% /run/locktmpfs 492M 0 492M 0% /sys/fs/cgrouptmpfs 99M 0 99M 0% /run/user/117tmpfs 99M 0 99M 0% /run/user/1000/dev/sdb1 2.9G 9.0M 2.8G 1% /mnt/data 可以看到新添加的分区 /dev/sdb1 已经挂载到 /mnt/data 目录下了。 或者也可以使用 lsblk 命令查看块存储设备（即磁盘和分区）的容量和挂载点：12345678$ lsblkNAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINTsda 8:0 0 10G 0 disk└─sda1 8:1 0 10G 0 part /sdb 8:16 0 5G 0 disk├─sdb1 8:17 0 3G 0 part /mnt/data└─sdb2 8:18 0 2G 0 part [SWAP]sr0 11:0 1 1024M 0 rom 需要注意的是，手动挂载的分区在系统重启以后会自动卸载。如果想像根目录那样，每次系统启动时自动挂载分区，可以修改 /etc/fstab 配置文件，示例内容如下：1234# &lt;file system&gt; &lt;mount point&gt; &lt;type&gt; &lt;options&gt; &lt;dump&gt; &lt;pass&gt;UUID=f3435713-b2cd-4196-b07b-2ffb116a028d / ext4 defaults 0 1/dev/sdb1 /mnt/data ext4 defaults 0 1/dev/sdb2 none swap sw 0 0 PS：相对于 /dev/sda1 这种形式，使用 UUID 挂载分区往往更保险一点，可以通过 blkid 命令查看磁盘分区的 UUID：1234$ sudo blkid/dev/sda1: UUID=&quot;f3435713-b2cd-4196-b07b-2ffb116a028d&quot; TYPE=&quot;ext4&quot; PARTUUID=&quot;20985120-01&quot;/dev/sdb1: UUID=&quot;d5e21599-12e9-44da-ae51-124d89fe5eda&quot; TYPE=&quot;ext4&quot; PARTUUID=&quot;ce119026-01&quot;/dev/sdb2: UUID=&quot;47006330-810c-4321-8d73-d52a5f70bc88&quot; TYPE=&quot;swap&quot; PARTUUID=&quot;ce119026-02&quot; 四、LVM（逻辑卷管理）对于不包含逻辑卷管理（LVM）的磁盘分区方案，分区的位置、大小和数量一般都是固定的，从而导致扩展当前分区和添加新分区等操作变得困难。此时若添加额外的硬盘和分区，则需要在目录树中创建新的分支作为挂载点，文件数据分散到多个复杂的位置上，不便于合并、备份和管理数据。 LVM 允许将单个或多个分区合并为一个逻辑卷组，且其中包含的逻辑卷可以动态地添加、改变大小或删除。LVM 系统最底层为物理卷（pv），即磁盘、分区和 RAID 阵列等。物理卷可以用来创建逻辑卷组（vg），而逻辑卷组又可以包含任意数量的逻辑卷（lv），逻辑卷从功能上即对应于物理磁盘上的分区。 创建卷组和逻辑卷可以使用 pvcreate 命令将某个存储设备（磁盘或分区等）标记为物理卷。这里我通过 VirtualBox 添加了另一块大小为 5G 的空白的虚拟硬盘，系统检测到该设备为 /dev/sdc ：123456789$ lsblkNAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINTsda 8:0 0 10G 0 disk└─sda1 8:1 0 10G 0 part /sdb 8:16 0 5G 0 disk├─sdb1 8:17 0 3G 0 part /mnt/data└─sdb2 8:18 0 2G 0 part [SWAP]sdc 8:32 0 5G 0 disksr0 11:0 1 1024M 0 rom 创建物理卷：12$ sudo pvcreate /dev/sdc Physical volume &quot;/dev/sdc&quot; successfully created. 通过 pvs 命令列出所有的物理卷：123$ sudo pvs PV VG Fmt Attr PSize PFree /dev/sdc lvm2 --- 5.00g 5.00g 通过 vgcreate 命令在物理卷的基础上创建逻辑卷组：12$ sudo vgcreate data-volume /dev/sdc Volume group &quot;data-volume&quot; successfully created 使用 vgs 命令列出当前所有的逻辑卷组：123$ sudo vgs VG #PV #LV #SN Attr VSize VFree data-volume 1 0 0 wz--n- &lt;5.00g &lt;5.00g 使用 lvcreate 命令在卷组中创建逻辑卷：12$ sudo lvcreate --name data --size 2G data-volume Logical volume &quot;data&quot; created. 访问逻辑卷可以通过 /dev/mapper/&lt;vgname&gt;-&lt;lvname&gt; 或者 /dev/&lt;vgname&gt;/&lt;lvname&gt; 形式的路径，即刚刚创建的 data 逻辑卷可以通过 /dev/data-volume/data 指定。在该逻辑卷上创建 Ext4 文件系统：1234567891011$ sudo mkfs.ext4 /dev/data-volume/datamke2fs 1.44.6 (5-Mar-2019)Creating filesystem with 524288 4k blocks and 131072 inodesFilesystem UUID: 0f24cdd8-62e0-42fd-bc38-aa3bce91e099Superblock backups stored on blocks: 32768, 98304, 163840, 229376, 294912Allocating group tables: doneWriting inode tables: doneCreating journal (16384 blocks): doneWriting superblocks and filesystem accounting information: done 此时该逻辑卷即可挂载到某个目录分支下像普通物理分区一样正常使用了。 操作卷组和逻辑卷可以使用 lvextend 命令动态地扩展逻辑卷的存储空间：12345678$ sudo lvextend --size +2G --resizefs /dev/data-volume/datafsck from util-linux 2.33.1/dev/mapper/data--volume-data: clean, 11/131072 files, 26156/524288 blocks Size of logical volume data-volume/data changed from 2.00 GiB (512 extents) to 4.00 GiB (1024 extents). Logical volume data-volume/data successfully resized.resize2fs 1.44.6 (5-Mar-2019)Resizing the filesystem on /dev/mapper/data--volume-data to 1048576 (4k) blocks.The filesystem on /dev/mapper/data--volume-data is now 1048576 (4k) blocks long. 其中 --size +2G 用于指定增加 2G 空间，--resizefs 指定在扩展逻辑卷大小的同时扩充文件系统的大小（文件系统默认不会随逻辑卷的空间变化而自动扩展）。 或者也可以直接指定扩展后的大小，如：$ sudo lvextend --size 4G --resizefs /dev/data-volume/data 其他常用的命令比如通过 lvresize 命令扩展逻辑卷，使其占用当前卷组中剩余的全部空间：123$ sudo lvresize -l +100%free /dev/data-volume/data Size of logical volume data-volume/data changed from &lt;3.00 GiB (767 extents) to &lt;5.00 GiB (1279 extents). Logical volume data-volume/data successfully resized. 因为上面的命令没有加上 --resizefs 或者 -r 选项，因此文件系统不会随着逻辑卷自动扩展大小，可以通过 resize2fs 命令手动扩展文件系统：$ sudo resize2fs /dev/data-volume/data 假设一段时间以后，逻辑卷 /dev/data-volume/data 的空间即将被数据填满，可以尝试添加另一块硬盘 sdd：1234567891011$ lsblkNAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINTsda 8:0 0 10G 0 disk└─sda1 8:1 0 10G 0 part /sdb 8:16 0 5G 0 disk├─sdb1 8:17 0 3G 0 part /mnt/data└─sdb2 8:18 0 2G 0 part [SWAP]sdc 8:32 0 5G 0 disk└─data--volume-data 253:0 0 5G 0 lvmsdd 8:48 0 5G 0 disksr0 11:0 1 1024M 0 rom 使用 pvcreate 命令创建物理卷：1234$ sudo pvs PV VG Fmt Attr PSize PFree /dev/sdc data-volume lvm2 a-- &lt;5.00g 0 /dev/sdd lvm2 --- 5.00g 5.00g 使用 vgextend 命令将该物理卷添加到之前创建的卷组 data-volume 中：12345$ sudo vgextend data-volume /dev/sdd Volume group &quot;data-volume&quot; successfully extended$ sudo vgs VG #PV #LV #SN Attr VSize VFree data-volume 2 1 0 wz--n- 9.99g &lt;5.00g 此时的 data-volume 卷组包含了两个物理卷（/dev/sdc 和 /dev/sdd）和一个逻辑卷（/dev/data-volume/data），总大小变为 10G，闲置空间为 5G（即刚刚添加的物理卷）。 最后使用 lvresize 命令扩展逻辑卷大小，使其占据两个物理卷的全部存储空间：12345678$ sudo lvresize -l +100%free -r /dev/data-volume/datafsck from util-linux 2.33.1/dev/mapper/data--volume-data: clean, 11/196608 files, 30268/785408 blocks Size of logical volume data-volume/data changed from &lt;5.00 GiB (1279 extents) to 9.99 GiB (2558 extents). Logical volume data-volume/data successfully resized.resize2fs 1.44.6 (5-Mar-2019)Resizing the filesystem on /dev/mapper/data--volume-data to 2619392 (4k) blocks.The filesystem on /dev/mapper/data--volume-data is now 2619392 (4k) blocks long. 此时逻辑卷 /dev/data-volume/data 的大小扩展为 10G，即占用了整个卷组 data-volume（包含两个 5G 的物理卷）的全部空间。 总结：LVM 卷组（vg）的作用类似于物理磁盘，用于承载逻辑卷（lv）。卷组可以由多个物理卷（磁盘或分区等）构成，空间不够时也可以随时添加新的物理卷进行扩展。而卷组上的逻辑卷（lv）类似于磁盘分区，可以挂载到目录作为存储空间。但是物理分区的位置和大小固定，而逻辑卷则可以在卷组的基础上动态的改变大小，甚至跨越多个物理磁盘和分区，使得管理起来更加方便和灵活。 常用 LVM 命令列表： Command Used For pvcreate Labeling devices for use with LVM pvremove Removing the LVM label from a physical volume pvdisplay / pvs Displaying information on the specified device or all physical volumes on the system vgcreate Creating a new volume group vgremove Removing (deleting) a volume group vgextend Adding physical volumes to a volume group vgreduce Removing physical volumes from a volume group vgdisplay / vgs Displaying information about the specified group or all volume groups on the system lvcreate Creating a new logical volume lvremove Removing (deleting) a logical volume lvextend Increasing the size of a logical volume lvreduce Decreasing the size of a logical volume lvdisplay / lvs Displaying all logical volumes on the system or in a specified volume group 参考资料Pro Linux System Administration]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Admin</tag>
        <tag>Tools</tag>
        <tag>Disk</tag>
        <tag>Storage</tag>
        <tag>LVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Nagios4 系统监控工具安装及配置简介（Ubuntu 19.04）]]></title>
    <url>%2F2019%2F08%2F01%2Fmonitor-system-nagios4-installation-and-usage%2F</url>
    <content type="text"><![CDATA[Nagios 是一款开源的系统监控工具。它主要负责对网络环境中的硬件设备及软件服务进行持续的检查，确保这些设备或服务处于正常运行的状态。一旦发现任何错误，Nagios 会在尽可能短的时间内向工作人员发出警报，同时也会在一定程度上尝试自动修复故障（比如重启设备或服务）。 一、简介Nagios 监控的对象主要可分为两类： Hosts 表示网络中的物理（或虚拟化的）设备，如服务器、工作站、路由器和打印机等 Services 表示网络中某些设施提供的特定功能的集合，如 CPU、内存、磁盘空间等。其他如 sshd 服务等也可被定义为 Service 接受 Nagios 的监控 此外，多台主机还可以被划分到不同的主机组中以方便管理和维护。 pluginsNagios 提供的所有的监控操作全部都由插件来完成。插件是一些用来传递监控信息的附加组件，Nagios 通过它们执行具体的监控和检查任务，同时对收集到的结果进行统计与整理。 Nagios 默认提供了一些基础的插件，几乎可以满足所有常见的监控任务。此外，如果有特殊的监控需求，也可以自行编写 Nagios 插件。一般标准插件的默认安装路径为 /usr/lib/nagios/plugins ，插件名称绝大多数以 check_ 开头：1234567$ ls /usr/lib/nagios/pluginscheck_apt check_file_age check_imap check_nagios check_pop check_ssmtpcheck_breeze check_flexlm check_ircd check_nntp check_procs check_swapcheck_by_ssh check_fping check_jabber check_nntps check_radius check_tcpcheck_clamd check_ftp check_ldap check_nt check_real check_timecheck_cluster check_game check_ldaps check_ntp check_rpc check_udp... 二、安装 Nagios我用的是 Ubuntu 19.04 系统，呃，没有尝试过其他的安装方式。就是直接用包管理器（apt-get）安装的，命令（就一条，，，）如下：$ sudo apt-get install nagios4 该命令会同时安装一些标准的 Nagios 插件以及一个简单的 Web 监控平台（需要 Apache2 服务器和 PHP 语言支持）。用 apt-get 命令安装完 Nagios4 之后，Web 监控平台的 Apache2 配置文件会自动添加到 /etc/apache2/conf_enabled 目录下并直接启用。 只不过我这里重启 Apache2 服务时报错，原因是还需要启用额外两个 Apache 的模块，命令如下：12$ sudo a2enmod auth_digest$ sudo a2enmod authz_groupfile 重启 Apache2 后访问 http://127.0.0.1/nagios4 链接，即可进入 Web 监控平台，截图如下： 三、配置文件介绍Nagios 的配置文件一般位于 /etc/nagios 或者 /usr/local/etc/nagios 目录下，最主要的配置文件为 nagios.cfg 。nagios.cfg 像是一个统领性的大纲文件，它除了定义一些全局范围内的基本配置外，还用于指定其他（更细节的）配置文件的位置或者组织方式，类似于书籍中的目录。 如文件中的 cfg_dir 和 cfg_file 两个配置项：12345678910111213...cfg_dir=/etc/nagios-plugins/configcfg_dir=/etc/nagios4/conf.d# You can specify individual object config files as shown below:cfg_file=/etc/nagios4/objects/commands.cfgcfg_file=/etc/nagios4/objects/contacts.cfgcfg_file=/etc/nagios4/objects/timeperiods.cfgcfg_file=/etc/nagios4/objects/templates.cfg# Definitions for monitoring the local (Linux) hostcfg_file=/etc/nagios4/objects/localhost.cfg... 即关于监控对象的详细配置，一般保存在由 cfg_file 指定的配置文件中。或者也可以将包含配置信息的文件放置在由 cfg_dir 指定的目录（及其子目录）下。 比如默认启用的 /etc/nagios4/objects/localhost.cfg 配置文件，其中包含了关联到本地主机的多个监控对象的信息，从中也可以看出 Nagios 在配置监控对象时所遵循的基本语法：123456789101112131415161718192021222324252627282930313233343536# Define a host for the local machinedefine host&#123; use linux-server ; Name of host template to use host_name localhost alias localhost address 127.0.0.1 &#125;# Define a service to check the disk space of the root partitio on the local machine. Warning if &lt; 20% free, critical if &lt; 10% free space on partition.define service&#123; use local-service ; Name of service template to use host_name localhost service_description Root Partition check_command check_local_disk!20%!10%!/ &#125;# Define a service to check the number of currently running procs on the local machine. Warning if &gt; 250 processes, critical if &gt; 400 processes.define service&#123; use local-service ; Name of service template to use host_name localhost service_description Total Processes check_command check_local_procs!250!400!RSZDT &#125;# Define a service to check HTTP on the local machine. Disable notifications for this service by default, as not all users may have HTTP enabled.define service&#123; use local-service ; Name of service template to use host_name localhost service_description HTTP check_command check_http notifications_enabled 0 &#125; 至于 /etc/nagios4/objects 目录下默认启用的几个配置文件，则包含了一些自定义命令（commands.cfg）、联系人信息（contacts.cfg）、时间段配置（timeperiods.cfg）以及主机和服务的模板（templates.cfg）等。 比如 localhost.cfg 中的 use linux-server 配置项，即使用了由 templates.cfg 文件定义的名为 linux-server 的模板：1234567891011121314151617$ cat /etc/nagios4/objects/templates.cfg...define host&#123; name linux-server ; The name of this host template use generic-host ; This template inherits other values from the generic-host template check_period 24x7 ; By default, Linux hosts are checked round the clock check_interval 5 ; Actively check the host every 5 minutes retry_interval 1 ; Schedule host check retries at 1 minute intervals max_check_attempts 10 ; Check each Linux host 10 times (max) check_command check-host-alive ; Default command to check Linux hosts notification_period workhours ; Linux admins hate to be woken up, so we only notify during the day notification_interval 120 ; Resend notifications every 2 hours notification_options d,u,r ; Only send notifications for specific host states contact_groups admins ; Notifications get sent to the admins by default register 0 ; DONT REGISTER THIS DEFINITION - ITS NOT A REAL HOST, JUST A TEMPLATE! &#125;... 其他由 Nagios 默认提供的主机或服务模板、联系人模板等也都保存在该文件中。 总的来说，在创建自定义配置时，objects 目录下的配置文件可以作为很有价值的参考示例，同时其中定义的命令、联系人和模板等也可在需要时直接通过名称调用，减少相关代码的编写。 对于设备数量庞大且种类较复杂的场景，建议将 Nagios 配置文件的组织架构设计成便于管理的形式。比如：1234567891011$ tree /etc/nagios4/conf.d/etc/nagios4/conf.d├── commands├── contactgroups├── contacts├── hostgroups├── hosts│ └── server2.cfg├── servicegroups├── services└── timeperiods 四、定义监控对象宏指令能够通过宏指令来简化配置，是 Nagios 的关键特性之一。宏的运用在很大程度上提高了定义对象和命令的灵活性。如下面的配置示例：1234567891011define host&#123; use linux-server host_name server2 address 192.168.1.102 check_command check-host-ssh&#125;define command&#123; command_name check-host-ssh command_line $USER1$/check_ssh -H $HOSTADDRESS$&#125; 其中的 $USER1$ 和 $HOSTADDRESS$ 即是预先定义的两个宏。$USER1$ 是在资源配置文件（/etc/nagios4/resource.cfg）中指定的 Nagios 插件的安装路径。$HOSTADDRESS$ 则表示 host 定义中的 address 项的内容，即主机的 IP 地址。 定义主机简单的示例代码如下：123456789101112131415define host&#123; host_name server1 hostgroups linux-servers alias Ubuntu 19.04 address 192.168.1.101 check_command check-host-alive check_interval 10 retry_interval 1 max_check_attempts 5 check_period 24x7 contact_groups admins notification_interval 30 notification_period 24x7 notification_options d,u,r&#125; 关于 notification_options： d : the host DOWN state u : the host UNREACHABLE state r : host recovery (UP state) f : the host starts and stops flapping s : notify when scheduled downtime starts or ends 定义主机组示例代码如下：1234567891011121314151617define hostgroup&#123; hostgroup_name linux-servers alias Linux servers members server1,server2&#125;define hostgroup&#123; hostgroup_name aix-servers alias AIX servers members aixbox1,aixbox2&#125;define hostgroup&#123; hostgroup_name unix-servers alias UNIX servers hostgroup_members linux-servers,aix-servers&#125; 其中定义了两个分别包含两台主机的主机组（linux-servers、aix-servers），同时还定义了包含这两个主机组的“大”主机组（unix-servers）。即主机组的定义支持嵌套。 定义服务示例代码如下：12345678910111213define service&#123; host_name server2 service_description www check_command check_http check_interval 10 check_period 24x7 retry_interval 3 max_check_attempts 3 notification_interval 30 notification_period 24x7 notification_options w,c,u,r contact_groups admins&#125; 关于 notification_options： w : the service WARNING state u : the service UNKNOWN state c : the service CRITICAL state r : the service recovery (back to OK) state f : the host starts and stops flapping s : notify when the scheduled downtime starts or ends 定义时间段示例代码如下：12345678910111213141516define timeperiod&#123; timeperiod_name workinghours alias Working Hours, excluding lunch break monday 09:00-13:00,14:00-17:00 tuesday 09:00-13:00,14:00-17:00 wednesday 09:00-13:00,14:00-17:00 thursday 09:00-13:00,14:00-17:00 friday 09:00-13:00,14:00-17:00&#125;define timeperiod&#123; timeperiod_name weekends alias Weekends all day long saturday 00:00-24:00 sunday 00:00-24:00&#125; 定义联系人示例代码如下：123456789101112define contact&#123; contact_name jdoe alias John Doe email john.doe@gmail.com contactgroups admins host_notification_period workinghours service_notification_period workinghours host_notification_options d,u,r service_notification_options w,u,c,r host_notification_commands notify-host-by-email service_notification_commands notify-service-by-email&#125; 参考资料Learning Nagios - Third Edition]]></content>
      <categories>
        <category>Server</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Admin</tag>
        <tag>Tools</tag>
        <tag>Server</tag>
        <tag>DevOps</tag>
        <tag>Monitoring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[消息队列中间件 RabbitMQ 详细介绍——安装与基本应用（Python）]]></title>
    <url>%2F2019%2F07%2F28%2Fmessage-broker-RabbitMQ-intro%2F</url>
    <content type="text"><![CDATA[RabbitMQ 是当前最流行的消息中间件（Message Broker）之一，支持多种消息协议（如 AMQP、MQTT）。同时它也是一个轻量级的非常易于部署的开源软件，可以运行在当前大多数操作系统及云端环境中，也能够部署在分布式的集群环境里以达到高可用、可伸缩的需求。此外，RabbitMQ 还为目前主流的编程语言提供了丰富的开发工具。 一、软件安装可以进入 官方下载界面 阅读针对自己操作系统版本的安装手册，根据需求选择适合的安装方式。Windows 系统可以直接在该页面中获取二进制安装包（还需要安装 Erlang 环境），Linux 系统也可以根据发行版的不同添加特定的软件镜像源。 我这里是 Ubuntu 19.04，没有特别的需求，所以直接从系统默认的软件镜像源里下载安装，命令如下：$ sudo apt-get install rabbitmq-server 安装完成以后，运行 systemctl status rabbitmq-server 命令查看 RabbitMQ 服务的运行状态：123456789101112131415161718$ systemctl status rabbitmq-server● rabbitmq-server.service - RabbitMQ Messaging Server Loaded: loaded (/lib/systemd/system/rabbitmq-server.service; enabled; vendor preset: enabled) Active: active (running) since Fri 2019-07-26 01:03:27 CST; 2min 55s ago Main PID: 770 (beam.smp) Status: &quot;Initialized&quot; Tasks: 85 (limit: 2302) Memory: 85.8M CGroup: /system.slice/rabbitmq-server.service ├─ 741 /bin/sh /usr/sbin/rabbitmq-server ├─ 770 /usr/lib/erlang/erts-10.2.4/bin/beam.smp -W w -A 64 -MBas ageffcbf -MHas ageffcbf -MBlmbcs 512 -MHlmbcs 512 -MMmcs 30 -P 1048576 -t 5000000 -stbt db -zdbbl 128000 -K true -- -root /usr/lib/erlang -progname erl -- -home /var/lib/rabbitmq -- -pa /usr/lib/rabbitmq/lib/rabbitmq_server-3.7.8/ebin -noshell -noinput -s rabbit boot -sname rabbit@server1 -boot start_sasl -kernel inet_default_connect_options [&#123;nodelay,true&#125;] -sasl errlog_type error -sasl sasl_error_logger false -rabbit lager_log_root &quot;/var/log/rabbitmq&quot; -rabbit lager_default_file &quot;/var/log/rabbitmq/rabbit@server1.log&quot; -rabbit lager_upgrade_file &quot;/var/log/rabbitmq/rabbit@server1_upgrade.log&quot; -rabbit enabled_plugins_file &quot;/etc/rabbitmq/enabled_plugins&quot; -rabbit plugins_dir &quot;/usr/lib/rabbitmq/plugins:/usr/lib/rabbitmq/lib/rabbitmq_server-3.7.8/plugins&quot; -rabbit plugins_expand_dir &quot;/var/lib/rabbitmq/mnesia/rabbit@server1-plugins-expand&quot; -os_mon start_cpu_sup false -os_mon start_disksup false -os_mon start_memsup false -mnesia dir &quot;/var/lib/rabbitmq/mnesia/rabbit@server1&quot; -kernel inet_dist_listen_min 25672 -kernel inet_dist_listen_max 25672 ├─1243 erl_child_setup 65536 ├─1286 inet_gethost 4 └─1287 inet_gethost 47月 26 01:02:44 server1 systemd[1]: Starting RabbitMQ Messaging Server...7月 26 01:03:27 server1 systemd[1]: rabbitmq-server.service: Supervising process 770 which is not our child. We&apos;ll most likely not notice when it exits.7月 26 01:03:27 server1 systemd[1]: Started RabbitMQ Messaging Server. Web AdminRabbitMQ 还提供了可以远程访问的 Web 管理与监控工具，默认以插件的形式安装到系统中，需要使用 rabbitmq-plugins 命令开启。具体命令如下：$ sudo rabbitmq-plugins enable rabbitmq_management RabbitMQ 默认创建了一个用户名密码分别为 guest/guest 的用户，只是该用户只允许本地登录。（我这里是远程。。。）如果需要远程访问 Web 控制台，可以通过 rabbitmqctl 命令创建一个新的管理账户：$ sudo rabbitmqctl add_user &lt;username&gt; &lt;password&gt; 此时新创建的账户仍无法登录，还需要为其分配用户角色以及对 vhost 的管理权限，命令如下：12$ sudo rabbitmqctl set_user_tags &lt;username&gt; administrator$ sudo rabbitmqctl set_permissions -p / &lt;username&gt; &quot;.*&quot; &quot;.*&quot; &quot;.*&quot; 权限设置完毕后，即可用之前指定的用户名密码远程登录 Web 管理系统，界面如下图： Web 形式的后台界面为管理工作与监控需求提供了便捷的接口，同时大部分管理操作也可直接通过 rabbitmqctl 命令完成，具体可参考该命令的帮助信息：1234567891011121314$ sudo rabbitmqctlUsage:rabbitmqctl [-n &lt;node&gt;] [-l] [-q] &lt;command&gt; [&lt;command options&gt;]...Commands: add_user &lt;username&gt; &lt;password&gt; add_vhost &lt;vhost&gt; authenticate_user &lt;username&gt; &lt;password&gt; await_online_nodes &lt;count&gt; [-t &lt;timeout&gt;] cancel_sync_queue [-p &lt;vhost&gt;] queue change_cluster_node_type &lt;disc|ram&gt; change_password &lt;username&gt; &lt;password&gt;... 二、架构解析RabbitMQ 是一种高性能、稳定、可伸缩（集群部署）的消息中间件，由 Erlang 语言编写。 Erlang 是一种函数式编程语言，专注于分布式、高容错的软件类实时系统等应用场景。它通过轻量级的进程设计以及进程之间的消息通信，提供了一个高层次的不需要共享状态的并发模型。RabbitMQ 集群通过 Erlang VM 原生的 IPC (inter-process communication) 机制完成跨节点的消息通信。 松耦合架构对于传统的应用架构，比如一个 Web 应用的登录程序，往往需要对后端的数据库表格进行多项实时的写入操作。而当用户的访问量大增时，此时的表格更新操作很容易成为瓶颈并影响到整体的响应速度。 相对于登录程序直接更新表格数据的紧耦合架构，可以将前端的请求数据推送到基于消息的中间件或者某个中心化的消息队列应用，再通过中间件分发消息到多个消费者（Consumer）应用，由消费者独立、异步地完成最终的数据库更新操作。基于消息的中间件对于创建数据导向的灵活的应用架构有非常大的优势。RabbitMQ 支持的松耦合设计可以使应用不再受类似于数据库写操作的性能限制。同时这种架构也非常易于横向扩展，可以在添加作用于相同数据的应用实例时不影响现有的核心功能。 三、消息应用示例代码下文中将使用 Python 语言及其 RabbitMQ 客户端 Pika 创建 5 个基本的消息应用，结构由简单到复杂，源代码均参考自官网 RabbitMQ Tutorials 。安装 pika 库：pip install pika Hello World该应用的结构示意图如下： 由 P (Producer) 发送一条消息到队列（Queue），再由队列转发消息到 C (Consumer) 。 发送端代码 send.py 如下：123456789101112131415#!/usr/bin/env pythonimport pika# 初始化与 RabbitMQ 服务器的连接connection = pika.BlockingConnection( pika.ConnectionParameters(host=&apos;localhost&apos;))channel = connection.channel()# 队列声明channel.queue_declare(queue=&apos;hello&apos;)# 发送消息channel.basic_publish(exchange=&apos;&apos;, routing_key=&apos;hello&apos;, body=&apos;Hello World!&apos;)print(&quot; [x] Sent &apos;Hello World!&apos;&quot;)connection.close() 接收端 reveive.py 代码如下：123456789101112131415161718#!/usr/bin/env pythonimport pikaconnection = pika.BlockingConnection( pika.ConnectionParameters(host=&apos;localhost&apos;))channel = connection.channel()channel.queue_declare(queue=&apos;hello&apos;)# 接收到消息后触发的回调函数def callback(ch, method, properties, body): print(&quot; [x] Received %r&quot; % body)# 消费者声明与消息监听channel.basic_consume( queue=&apos;hello&apos;, on_message_callback=callback, auto_ack=True)print(&apos; [*] Waiting for messages. To exit press CTRL+C&apos;)channel.start_consuming() 测试首先运行 4 次发送程序：12345678$ python send.py [x] Sent &apos;Hello World&apos;$ python send.py [x] Sent &apos;Hello World&apos;$ python send.py [x] Sent &apos;Hello World&apos;$ python send.py [x] Sent &apos;Hello World&apos; 从 Web 管理界面中可以看到，此时队列中缓存了 4 条消息。运行接收端程序：123456$ python receive.py [x] Waiting for messages. To exit press CTRL+C [x] Received b&apos;Hello World&apos; [x] Received b&apos;Hello World&apos; [x] Received b&apos;Hello World&apos; [x] Received b&apos;Hello World&apos; 发送端连续 4 次发送的消息被接收端收取，队列中缓存的消息被清空。同时接收端保持运行状态等待新的消息被转发给自己。消息队列一直处于等待生产者发送消息和将收到或缓存的消息转发给消费者的状态。如未有消费者及时接收和处理被转发的消息，则这部分消息缓存在队列中等待进一步操作。 Work Queue结构示意图：本例中将创建一个 Work Queue 用来将消耗时间长的任务以轮询的方式分发给多个消费者处理。 生产者源代码 new_task.py ：1234567891011121314151617181920#!/usr/bin/env pythonimport pikaimport sysconnection = pika.BlockingConnection( pika.ConnectionParameters(host=&apos;localhost&apos;))channel = connection.channel()channel.queue_declare(queue=&apos;task_queue&apos;, durable=True)message = &apos; &apos;.join(sys.argv[1:]) or &quot;Hello World!&quot;channel.basic_publish( exchange=&apos;&apos;, routing_key=&apos;task_queue&apos;, body=message, properties=pika.BasicProperties( delivery_mode=2, # make message persistent ))print(&quot; [x] Sent %r&quot; % message)connection.close() 消费者源代码 worker.py ：1234567891011121314151617181920212223#!/usr/bin/env pythonimport pikaimport timeconnection = pika.BlockingConnection( pika.ConnectionParameters(host=&apos;localhost&apos;))channel = connection.channel()channel.queue_declare(queue=&apos;task_queue&apos;, durable=True)print(&apos; [*] Waiting for messages. To exit press CTRL+C&apos;)def callback(ch, method, properties, body): print(&quot; [x] Received %r&quot; % body) time.sleep(body.count(b&apos;.&apos;)) print(&quot; [x] Done&quot;) ch.basic_ack(delivery_tag=method.delivery_tag) # Message acknowledgmentchannel.basic_qos(prefetch_count=1)channel.basic_consume(queue=&apos;task_queue&apos;, on_message_callback=callback)channel.start_consuming() Message acknowledgment消费者在处理接收到的任务或消息时有可能会消耗比较多的时间，在此过程中，如消费者端出现软硬件故障，则会出现消息丢失的情况。 RabbitMQ 支持 Message acknowledgment 。即消费者在接收和处理完一个特定的消息后会向 RabbitMQ 返回一个应答（ack），说明该消息可以从队列中移除。如果消费者在返回应答之前丢失与队列的连接，则 RabbitMQ 判定对应的消息未由消费者完全处理，会将该消息保留在队列中并重新分发给其他在线的消费者。 Message durability消息应答的机制可以确保即使消费者宕机的情况下任务仍不会丢失。但是当 RabbitMQ 服务本身出现故障时，队列以及队列中缓存的消息仍旧会被清理掉。 为了保证 RabbitMQ 中队列以及消息的持久化，首先需要在生产者和消费者代码中同时声明队列为 durable ：channel.queue_declare(queue=&#39;task_queue&#39;, durable=True) 此外还需要将生产者代码中的 delivery_mode 属性设置为 2 确保消息的持久化：properties=pika.BasicProperties(delivery_mode=2,) 测试打开两个命令行终端，分别运行 worker.py 程序：123# Shell 1$ python worker.py [x] Waiting for messages. To exit press CTRL+C 123# Shell 2$ python worker.py [x] Waiting for messages. To exit press CTRL+C 打开另一个终端窗口运行 new_task.py 程序发送 4 条消息：123456789# Shell 3$ python new_task.py First Message [x] Sent &apos;First Message&apos;$ python new_task.py Second Message [x] Sent &apos;Second Message&apos;$ python new_task.py Third Message [x] Sent &apos;Third Message&apos;$ python new_task.py Forth Message [x] Sent &apos;Forth Message&apos; 最终两个消费者分别接收到队列分发的两条消息：1234567# Shell 1$ python worker.py [x] Waiting for messages. To exit press CTRL+C [x] Received b&apos;First Message&apos; [x] Done [x] Received b&apos;Third Message&apos; [x] Done 1234567# Shell 2$ python worker.py [x] Waiting for messages. To exit press CTRL+C [x] Received b&apos;Second Message&apos; [x] Done [x] Received b&apos;Forth Message&apos; [x] Done Fair dispatch当 RabbitMQ 以轮询的方式（即平均分配）将队列中的消息转发给多个消费者时，如果这些消费者接收到的任务繁重程度差异很大，则会导致某些消费者端任务的积压。为了避免这种情况发生，可以使用 basic_qos 方法设置 prefetch 的值，如 worker.py 程序中的以下代码：channel.basic_qos(prefetch_count=1) 。 该行代码可以确保同一个消费者在任意时间点最多只接受 1 个任务分配给自己。即如果某个消费者当前有未处理完的消息，则不再接收新的消息直到当前的任务处理完。 Publish/Subscribe结构示意图： Exchange在之前的示例中，用到了消息队列模型中的以下几个组件： producer ：生产者，即发送消息的应用 queue ：队列，即存储消息的缓存 consumer ：消费者，即接收消息的应用 实际上在 RabbitMQ 的消息模型中，生产者从来不会将消息直接发送到队列中，而是将消息发送给一个名为 exchange 的组件。exchange 的一端用来接收生产者发送的消息，一端用来将消息推送到队列中。它通过 exchange type 中的定义判断特定的消息是该推送给某个对应的队列，还是将其广播给多个队列，又或者直接丢弃。 RabbitMQ 主要提供了 4 种 exchange 类型：direct、topic、headers 和 fanout。本例中使用 fanout，即 exchange 会将接收到的消息以广播的形式发送给所有关联的队列，再由队列传递给消费者处理。 源代码（emit_log.py）如下：1234567891011121314#!/usr/bin/env pythonimport pikaimport sysconnection = pika.BlockingConnection( pika.ConnectionParameters(host=&apos;localhost&apos;))channel = connection.channel()channel.exchange_declare(exchange=&apos;logs&apos;, exchange_type=&apos;fanout&apos;)message = &apos; &apos;.join(sys.argv[1:]) or &quot;info: Hello World!&quot;channel.basic_publish(exchange=&apos;logs&apos;, routing_key=&apos;&apos;, body=message)print(&quot; [x] Sent %r&quot; % message)connection.close() receive_logs.py：1234567891011121314151617181920212223#!/usr/bin/env pythonimport pikaconnection = pika.BlockingConnection( pika.ConnectionParameters(host=&apos;localhost&apos;))channel = connection.channel()channel.exchange_declare(exchange=&apos;logs&apos;, exchange_type=&apos;fanout&apos;)result = channel.queue_declare(queue=&apos;&apos;, exclusive=True)queue_name = result.method.queuechannel.queue_bind(exchange=&apos;logs&apos;, queue=queue_name)print(&apos; [*] Waiting for logs. To exit press CTRL+C&apos;)def callback(ch, method, properties, body): print(&quot; [x] %r&quot; % body)channel.basic_consume( queue=queue_name, on_message_callback=callback, auto_ack=True)channel.start_consuming() receive_logs.py 文件中有一行 result = channel.queue_declare(queue=&#39;&#39;, exclusive=True) 代码，用来声明一个临时队列（ queue=&#39;&#39; 没有指定名称，因此会由 RabbitMQ 设置随机的名称），同时 exclusive=True 设置该队列在消费者断开连接后自行删除。 测试同时打开两个命令行窗口分别运行 receive_logs.py 文件：123# Shell 1$ python receive_logs.py [*] Waiting for logs. To exit press CTRL+C 123# Shell 2$ python receive_logs.py [*] Waiting for logs. To exit press CTRL+C 再打开第三个终端执行 emit_log.py 命令 4 次：123456789# Shell 3$ python emit_log.py [x] Sent &apos;info: Hello World!&apos;$ python emit_log.py [x] Sent &apos;info: Hello World!&apos;$ python emit_log.py [x] Sent &apos;info: Hello World!&apos;$ python emit_log.py [x] Sent &apos;info: Hello World!&apos; 此时之前运行的两个 receive 程序同时收到发送的 4 条消息：123456$ python receive_logs.py [*] Waiting for logs. To exit press CTRL+C [x] b&apos;info: Hello World!&apos; [x] b&apos;info: Hello World!&apos; [x] b&apos;info: Hello World!&apos; [x] b&apos;info: Hello World!&apos; 123456$ python receive_logs.py [*] Waiting for logs. To exit press CTRL+C [x] b&apos;info: Hello World!&apos; [x] b&apos;info: Hello World!&apos; [x] b&apos;info: Hello World!&apos; [x] b&apos;info: Hello World!&apos; Routing结构示意图：与上一个例子中以广播的形式转发消息不同，本例中允许消费者通过队列有选择地订阅生产者发送的部分消息。 Binding 和 Direct exchange在 RabbitMQ 中，binding 代表 exchange 与队列的对应关系，即队列会根据 binding 的设置对 exchange 转发的消息有选择性地接收。因此 binding 的最终效果也依赖于 exchange 的类型。比如之前用到的 fanout 类型，由于是广播的形式（转发给所有关联的队列）并不需要选择的动作，则 binding 的值被忽略。 但是对于 direct 类型的 exchange ，则可以通过 binding 对消息进行筛选。在 direct exchange 下，只有当队列的 binding_key 与消息的 routing_key 一致时，队列才会收到 exchange 转发的消息。 emit_log_direct.py：12345678910111213141516#!/usr/bin/env pythonimport pikaimport sysconnection = pika.BlockingConnection( pika.ConnectionParameters(host=&apos;localhost&apos;))channel = connection.channel()channel.exchange_declare(exchange=&apos;direct_logs&apos;, exchange_type=&apos;direct&apos;)severity = sys.argv[1] if len(sys.argv) &gt; 1 else &apos;info&apos;message = &apos; &apos;.join(sys.argv[2:]) or &apos;Hello World!&apos;channel.basic_publish( exchange=&apos;direct_logs&apos;, routing_key=severity, body=message)print(&quot; [x] Sent %r:%r&quot; % (severity, message))connection.close() receive_logs_direct.py：123456789101112131415161718192021222324252627282930313233#!/usr/bin/env pythonimport pikaimport sysconnection = pika.BlockingConnection( pika.ConnectionParameters(host=&apos;localhost&apos;))channel = connection.channel()channel.exchange_declare(exchange=&apos;direct_logs&apos;, exchange_type=&apos;direct&apos;)result = channel.queue_declare(queue=&apos;&apos;, exclusive=True)queue_name = result.method.queueseverities = sys.argv[1:]if not severities: sys.stderr.write(&quot;Usage: %s [info] [warning] [error]\n&quot; % sys.argv[0]) sys.exit(1)for severity in severities: channel.queue_bind( exchange=&apos;direct_logs&apos;, queue=queue_name, routing_key=severity)print(&apos; [*] Waiting for logs. To exit press CTRL+C&apos;)def callback(ch, method, properties, body): print(&quot; [x] %r:%r&quot; % (method.routing_key, body))channel.basic_consume( queue=queue_name, on_message_callback=callback, auto_ack=True)channel.start_consuming() 测试首先运行 receive_logs_direct.py 程序并指定参数为 error（即只接收标记为“error”的消息）：123# Shell 1$ python receive_logs_direct.py error [*] Waiting for logs. To exit press CTRL+C 打开另一终端同样运行 receive_logs_direct.py 程序并指定参数为 info warning（即接收标记为 info 或 warning 的消息）：123# Shell 2$ python receive_logs_direct.py info warning [*] Waiting for logs. To exit press CTRL+C 打开第三个终端并运行 emit_log_direct.py 程序发送 4 条日志消息：123456789# Shell 3$ python emit_log_direct.py error &quot;This is an error&quot; [x] Sent &apos;error&apos;:&apos;This is an error&apos;$ python emit_log_direct.py info &quot;Hi, I am an info&quot; [x] Sent &apos;info&apos;:&apos;Hi, I am an info&apos;$ python emit_log_direct.py warning &quot;Yeah, it&apos;s a warning&quot; [x] Sent &apos;warning&apos;:&quot;Yeah, it&apos;s a warning&quot;$ python emit_log_direct.py error &quot;Hi, it&apos;s an error again&quot; [x] Sent &apos;error&apos;:&quot;Hi, it&apos;s an error again&quot; 此时 Shell 1 中只接收到了标记为 error 的消息：1234$ python receive_logs_direct.py error [*] Waiting for logs. To exit press CTRL+C [x] &apos;error&apos;:b&apos;This is an error&apos; [x] &apos;error&apos;:b&quot;Hi, it&apos;s an error again&quot; 而 Shell 2 中接收到了标记为 info 和 warning 的消息：1234$ python receive_logs_direct.py info warning [*] Waiting for logs. To exit press CTRL+C [x] &apos;info&apos;:b&apos;Hi, I am an info&apos; [x] &apos;warning&apos;:b&quot;Yeah, it&apos;s a warning&quot; Topics结构示意图： direct 类型的 exchange 虽然可以根据消息的 routing_key 以及队列的 binding_key 有选择性的推送消息到队列，但是并不适合更复杂的场景。而 topic 类型的 exchange 与 direct 类型逻辑上大致相同，只是 topic 类型的 exchange 并没有一个明确的 routing_key，而是由几个点号（.）分隔的单词（如 lazy.orange.cat）进行定义。与之对应的 binding_key 也需要遵循同样的形式，只不过 binding_key 额外支持两个特殊含义的字符： 星号（*)可以表示某一个任意的单词 井号（#）可以表示任意 0 个或多个单词 因此对于上图（Topics）中的情形，routing_key 为 quick.orange.rabbit 的消息会被转发给 Q1 和 Q2 队列，quick.orange.fox 则只会转发给 Q1 队列，lazy.orange.male.rabbit 被转发给 Q2 队列。 emit_log_topic：12345678910111213141516#!/usr/bin/env pythonimport pikaimport sysconnection = pika.BlockingConnection( pika.ConnectionParameters(host=&apos;localhost&apos;))channel = connection.channel()channel.exchange_declare(exchange=&apos;topic_logs&apos;, exchange_type=&apos;topic&apos;)routing_key = sys.argv[1] if len(sys.argv) &gt; 2 else &apos;anonymous.info&apos;message = &apos; &apos;.join(sys.argv[2:]) or &apos;Hello World!&apos;channel.basic_publish( exchange=&apos;topic_logs&apos;, routing_key=routing_key, body=message)print(&quot; [x] Sent %r:%r&quot; % (routing_key, message))connection.close() receive_logs_topic.py：123456789101112131415161718192021222324252627282930313233#!/usr/bin/env pythonimport pikaimport sysconnection = pika.BlockingConnection( pika.ConnectionParameters(host=&apos;localhost&apos;))channel = connection.channel()channel.exchange_declare(exchange=&apos;topic_logs&apos;, exchange_type=&apos;topic&apos;)result = channel.queue_declare(&apos;&apos;, exclusive=True)queue_name = result.method.queuebinding_keys = sys.argv[1:]if not binding_keys: sys.stderr.write(&quot;Usage: %s [binding_key]...\n&quot; % sys.argv[0]) sys.exit(1)for binding_key in binding_keys: channel.queue_bind( exchange=&apos;topic_logs&apos;, queue=queue_name, routing_key=binding_key)print(&apos; [*] Waiting for logs. To exit press CTRL+C&apos;)def callback(ch, method, properties, body): print(&quot; [x] %r:%r&quot; % (method.routing_key, body))channel.basic_consume( queue=queue_name, on_message_callback=callback, auto_ack=True)channel.start_consuming() 测试先运行接收端程序（Shell 1 和 Shell 2），再运行发送端（Shell 3），效果如下：1234567# Shell 3$ python emit_log_topic.py &quot;kern.warning&quot; &quot;A kernel warning message&quot; [x] Sent &apos;kern.warning&apos;:&apos;A kernel warning message&apos;$ python emit_log_topic.py &quot;network.critical&quot; &quot;A critical network error&quot; [x] Sent &apos;network.critical&apos;:&apos;A critical network error&apos;$ python emit_log_topic.py &quot;kern.critical&quot; &quot;A critical kernel error&quot; [x] Sent &apos;kern.critical&apos;:&apos;A critical kernel error&apos; 12345# Shell 1$ python receive_logs_topic.py &quot;kern.*&quot; [*] Waiting for logs. To exit press CTRL+C [x] &apos;kern.warning&apos;:b&apos;A kernel warning message&apos; [x] &apos;kern.critical&apos;:b&apos;A critical kernel error&apos; 12345# Shell 2$ python receive_logs_topic.py &quot;*.critical&quot; [*] Waiting for logs. To exit press CTRL+C [x] &apos;network.critical&apos;:b&apos;A critical network error&apos; [x] &apos;kern.critical&apos;:b&apos;A critical kernel error&apos; 参考资料RabbitMQ TutorialsRabbitMQ in Depth]]></content>
      <categories>
        <category>Server</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Admin</tag>
        <tag>Server</tag>
        <tag>DevOps</tag>
        <tag>Development</tag>
        <tag>Deployment</tag>
        <tag>Message</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu 系统 dpkg 命令使用详解]]></title>
    <url>%2F2019%2F07%2F21%2Fdpkg-package-manager-manual%2F</url>
    <content type="text"><![CDATA[dpkg 即 package manager for Debian ，是 Debian 和基于 Debian 的系统中一个主要的包管理工具，可以用来安装、构建、卸载、管理 deb 格式的软件包。 安装软件使用 dpkg 命令安装软件时，可以使用 -i 选项并指定 deb 安装包的路径。和 Ubuntu 下的另一个包管理工具 apt-get（Advanced Package Tool）有所不同。apt-get 命令并不直接操作 deb 安装包文件，而是从 /etc/apt/sources.list 配置文件中定义的软件镜像源里下载软件包并安装，使用时也只需指定软件的名称（或者也可以附加上版本号）。 apt-get 命令安装软件：$ apt-get install &lt;package_name[=version]&gt; dpkg 命令安装软件：$ dpkg -i &lt;package_file_path&gt; 因此，dpkg 主要是用来安装已经下载到本地的 deb 软件包，或者对已经安装好的软件进行管理。而 apt-get 可以直接从远程的软件仓库里下载安装软件。 12345678910111213141516$ sudo apt-get install emacs正在读取软件包列表... 完成正在分析软件包的依赖关系树正在读取状态信息... 完成将会同时安装下列软件： emacs-bin-common emacs-common emacs-el emacs-gtk libm17n-0 libotf0 m17n-db建议安装： mailutils emacs-common-non-dfsg m17n-docs gawk下列【新】软件包将被安装： emacs emacs-bin-common emacs-common emacs-el emacs-gtk libm17n-0 libotf0 m17n-db升级了 0 个软件包，新安装了 8 个软件包，要卸载 0 个软件包，有 115 个软件包未被升级。需要下载 34.4 MB 的归档。解压缩后会消耗 137 MB 的额外空间。您希望继续执行吗？ [Y/n] 123456$ sudo dpkg -i fping_4.2-1_amd64.deb(正在读取数据库 ... 系统当前共安装有 252654 个文件和目录。)准备解压 fping_4.2-1_amd64.deb ...正在解压 fping (4.2-1) 并覆盖 (4.2-1) ...正在设置 fping (4.2-1) ...正在处理用于 man-db (2.8.5-2) 的触发器 ... 列出已安装的软件可以使用 dpkg -l 命令列出当前系统中已经安装的软件以及软件包的状态。如：12345678910111213141516$ dpkg -l期望状态=未知(u)/安装(i)/删除(r)/清除(p)/保持(h)| 状态=未安装(n)/已安装(i)/仅存配置(c)/仅解压缩(U)/配置失败(F)/不完全安装(H)/触发器等待(W)/触发器未决(T)|/ 错误?=(无)/须重装(R) (状态，错误：大写=故障)||/ 名称 版本 体系结构 描述+++-=============================================-===================================-============-===============================================================================ii 2048-qt 0.1.6-1build1 amd64 mathematics based puzzle gameii accountsservice 0.6.50-0ubuntu1 amd64 query and manipulate user account informationii acl 2.2.53-4 amd64 access control list - utilitiesii acpi-support 0.143 amd64 scripts for handling many ACPI eventsii acpid 1:2.0.31-1ubuntu2 amd64 Advanced Configuration and Power Interface event daemonii adduser 3.118ubuntu1 all add and remove users and groupsii adwaita-icon-theme 3.32.0-1ubuntu1 all default icon theme of GNOME (small subset)ii alsa-base 1.0.25+dfsg-0ubuntu5 all ALSA driver configuration filesii alsa-utils 1.1.8-1ubuntu1 amd64 Utilities for configuring and using ALSA... 该命令每行输出中的第一列 ii 表示软件包的安装和配置状态，其格式如下：期望状态|当前状态|错误其中期望状态有以下几种 u：即 unknown，软件包未安装且用户未请求安装 i：即 install，用户请求安装该软件包 r：即 remove，用户请求卸载该软件包 p：即 purge，用户请求卸载该软件包并清理配置文件 h：即 hold，用户请求保持续当前软件包版本 当前状态 有以下几种： n：即 not-installed，软件包未安装 i：即 installed，软件包已安装并完成配置 c：即 config-files，软件包已经被卸载，但是其配置文件未清理 u：即 unpacked，软件包已经被解压缩，但还未配置 f：即 half-configured，配置软件包时出现错误 w：即 triggers-awaited，触发器等待 t：即 triggers-pending，触发器未决 错误状态 有以下几种： h：软件包被强制保持 r：即 reinstall-required，需要卸载并重新安装 x：软件包被破坏 因此 ii 表示该软件需要安装且已经安装，没有出现错误；iu 表示已经安装该软件，但未正确配置；rc 表示该软件已经被删除，但配置文件未清理。 查看处于 rc 状态的软件包：123456$ dpkg -l | grep ^rcrc libmhash2:amd64 0.9.9.9-7 amd64 Library for cryptographic hashing and message authenticationrc linux-image-5.0.0-13-generic 5.0.0-13.14 amd64 Signed kernel image genericrc linux-modules-5.0.0-13-generic 5.0.0-13.14 amd64 Linux kernel extra modules for version 5.0.0 on 64 bit x86 SMPrc linux-modules-extra-5.0.0-13-generic 5.0.0-13.14 amd64 Linux kernel extra modules for version 5.0.0 on 64 bit x86 SMPrc zabbix-proxy-mysql 1:4.0.4+dfsg-1 amd64 network monitoring solution - proxy (using MySQL) 此外，还可以使用 dpkg -l &lt;package_name_pattern&gt; 命令筛选出名称中包含指定模式的软件包。12345678910111213$ dpkg -l &quot;nginx*&quot;期望状态=未知(u)/安装(i)/删除(r)/清除(p)/保持(h)| 状态=未安装(n)/已安装(i)/仅存配置(c)/仅解压缩(U)/配置失败(F)/不完全安装(H)/触发器等待(W)/触发器未决(T)|/ 错误?=(无)/须重装(R) (状态，错误：大写=故障)||/ 名称 版本 体系结构 描述+++-==============-===============-============-=========================================================ii nginx 1.15.9-0ubuntu1 all small, powerful, scalable web/proxy serverii nginx-common 1.15.9-0ubuntu1 all small, powerful, scalable web/proxy server - common filesii nginx-core 1.15.9-0ubuntu1 amd64 nginx web/proxy server (standard version)un nginx-doc &lt;无&gt; &lt;无&gt; (无描述)un nginx-extras &lt;无&gt; &lt;无&gt; (无描述)un nginx-full &lt;无&gt; &lt;无&gt; (无描述)un nginx-light &lt;无&gt; &lt;无&gt; (无描述) 卸载软件dpkg 命令的 -r 选项可以用来卸载已安装的软件包，此时只需要指定软件的名称即可。1234567$ sudo dpkg -r vim(正在读取数据库 ... 系统当前共安装有 252653 个文件和目录。)正在卸载 vim (2:8.1.0320-1ubuntu3.1) ...update-alternatives: 使用 /usr/bin/vim.tiny 来在自动模式中提供 /usr/bin/vi (vi)update-alternatives: 使用 /usr/bin/vim.tiny 来在自动模式中提供 /usr/bin/view (view)update-alternatives: 使用 /usr/bin/vim.tiny 来在自动模式中提供 /usr/bin/ex (ex)update-alternatives: 使用 /usr/bin/vim.tiny 来在自动模式中提供 /usr/bin/rview (rview) 需要注意的是，-r 选项只会移除指定的软件包而不对其配置文件产生影响，可以使用 -P 选项在删除软件包的同时清理配置文件。sudo dpkg -P &lt;package&gt; 其他包管理操作查看软件包的内容dpkg -c &lt;package_file_path&gt;123456789101112131415161718192021$ dpkg -c fping_4.2-1_amd64.debdrwxr-xr-x root/root 0 2019-02-20 06:27 ./drwxr-xr-x root/root 0 2019-02-20 06:27 ./usr/drwxr-xr-x root/root 0 2019-02-20 06:27 ./usr/bin/-rwxr-xr-x root/root 52128 2019-02-20 06:27 ./usr/bin/fpingdrwxr-xr-x root/root 0 2019-02-20 06:27 ./usr/share/drwxr-xr-x root/root 0 2019-02-20 06:27 ./usr/share/bug/-rwxr-xr-x root/root 118 2017-06-19 05:19 ./usr/share/bug/fpingdrwxr-xr-x root/root 0 2019-02-20 06:27 ./usr/share/doc/drwxr-xr-x root/root 0 2019-02-20 06:27 ./usr/share/doc/fping/-rw-r--r-- root/root 495 2017-09-06 08:00 ./usr/share/doc/fping/NEWS.Debian.gz-rw-r--r-- root/root 1615 2019-02-20 06:27 ./usr/share/doc/fping/changelog.Debian.gz-rw-r--r-- root/root 3445 2017-12-07 04:09 ./usr/share/doc/fping/copyrightdrwxr-xr-x root/root 0 2019-02-20 06:27 ./usr/share/lintian/drwxr-xr-x root/root 0 2019-02-20 06:27 ./usr/share/lintian/overrides/-rw-r--r-- root/root 41 2017-06-19 05:19 ./usr/share/lintian/overrides/fpingdrwxr-xr-x root/root 0 2019-02-20 06:27 ./usr/share/man/drwxr-xr-x root/root 0 2019-02-20 06:27 ./usr/share/man/man8/-rw-r--r-- root/root 5733 2019-02-20 06:27 ./usr/share/man/man8/fping.8.gz-rw-r--r-- root/root 1512 2019-02-20 06:27 ./usr/share/man/man8/fping6.8.gzlrwxrwxrwx root/root 0 2019-02-20 06:27 ./usr/bin/fping6 -&gt; fping 查看软件包（已安装）的详细信息dpkg -s &lt;package&gt; 或 dpkg --status &lt;package&gt;1234567891011121314151617181920$ dpkg -s fpingPackage: fpingStatus: deinstall ok installedPriority: optionalSection: netInstalled-Size: 87Maintainer: Ubuntu Developers &lt;ubuntu-devel-discuss@lists.ubuntu.com&gt;Architecture: amd64Version: 4.2-1Depends: libcap2-bin, netbase, libc6 (&gt;= 2.15)Enhances: netdata (&gt;= 1.5)Description: sends ICMP ECHO_REQUEST packets to network hosts fping is a ping like program which uses the Internet Control Message Protocol (ICMP) echo request to determine if a target host is responding. fping differs from ping in that you can specify any number of targets on the command line, or specify a file containing the lists of targets to ping. Instead of sending to one target until it times out or replies, fping will send out a ping packet and move on to the next target in a round-robin fashion.Original-Maintainer: Axel Beckert &lt;abe@debian.org&gt;Homepage: https://www.fping.org/ 查看软件包的安装位置dpkg -L &lt;package&gt; 或 dpkg --list-files &lt;package&gt;123456789101112131415161718192021$ dpkg -L fping/./usr/usr/bin/usr/bin/fping/usr/share/usr/share/bug/usr/share/bug/fping/usr/share/doc/usr/share/doc/fping/usr/share/doc/fping/NEWS.Debian.gz/usr/share/doc/fping/changelog.Debian.gz/usr/share/doc/fping/copyright/usr/share/lintian/usr/share/lintian/overrides/usr/share/lintian/overrides/fping/usr/share/man/usr/share/man/man8/usr/share/man/man8/fping.8.gz/usr/share/man/man8/fping6.8.gz/usr/bin/fping6 筛选出包含指定文件（模式）的软件包dpkg -S &lt;filename_pattern&gt; 或 dpkg --search &lt;filename_pattern&gt;123456$ dpkg -S sites-availableapache2: /etc/apache2/sites-available/default-ssl.confapache2: /etc/apache2/sites-available/000-default.confnginx-common: /etc/nginx/sites-availablenginx-common: /etc/nginx/sites-available/defaultapache2: /etc/apache2/sites-available 参考资料15 Practical Examples of “dpkg commands” for Debian Based DistrosLinux软件安装管理之——dpkg与apt-*详解]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Admin</tag>
        <tag>Tools</tag>
        <tag>Package</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[vimdiff 命令使用介绍]]></title>
    <url>%2F2019%2F07%2F20%2Fvimdiff-manual%2F</url>
    <content type="text"><![CDATA[vimdiff 等同于 vim -d 命令，即 Vim 编辑器的 diff 模式。该命令后面通常会接两个或多个文件名作为参数，这些文件会同时在 Vim 编辑器的分割窗口中打开，并高亮显示文件中内容有差异的部分。同时该模式下还提供部分快捷按键用于完成文件内容的合并等操作。 启动 vimdiffvimdiff 命令常用于编辑同一文件的不同历史版本，对各文件的内容进行比对与调整。如下面两个文件：123456789$ cat file1Line oneLine 2Line threeLine 4Line 5Line 6 123456789$ cat file2Line 1Line 2Line 3Line 4Line 5Line 6Line 7Line 8 可以使用 vim -O2 file1 file2 命令，在左右排列的两个窗口中同时打开 file1 和 file2 两个文件，如下图所示： 而 vimdiff file1 file2 命令会以同样的形式打开这两个文件，并且用不同的背景色高亮显示彼此间有差别的内容，如下图： 从上面的两幅截图中可以得出 vimdiff 标记差别内容时的几个规则： 只在某一个文件中存在的行背景色设置为蓝色，而另一文件中的对应位置则被标记为绿色。（或者说，相对于另一个文件，当前文件中“多余”的行标记为蓝色，“缺少”的行则标记为绿色） 两个文件中同时存在但是内容有差异的行，都标记为粉色，而引起差异的文字标记为红色 除了 vimdiff FILE_LEFT FILE_RIGHT 或者 vim -d FILE_LEFT FILE_RIGHT 的形式外，也可以通过在 Vim 中输入命令进入 diff 模式。 比如先进入 Vim 编辑 FILE_LEFT 文件（vim FILE_LEFT），再输入以下命令进入 diff 模式：:vertical diffsplit FILE_RIGHT 光标移动可以使用下列两种快捷键，在文件的各个差异点之间前后移动： ], c：跳转到下个差异点 [, c：跳转到上个差异点 至于光标在两个窗口之前的切换，可以使用如下按键： Ctrl-w, l：光标切换到右侧的窗口 Ctrl-w, h：光标切换到左侧的窗口 Ctrl-w, w：光标在两个窗口间彼此切换 内容合并可以使用 d, p （即 diff put）命令，将当前差异点中的内容覆盖到另一文件中的对应位置。如当光标位于左侧文件（file1）中的第一行时，依次按下 d、p 键，则 file1 中的 Line one 被推送到右侧，并替换掉 file2 中对应位置上的 Line 1 。截图如下：可与上一幅截图对比查看效果。 而 d, o （即 diff obtain）命令可以将另一窗口中差异点处的内容拉取到当前位置并进行替换操作。截图如下： 即在 file1 的第一行执行 d o 命令后，file2 中的第一行内容 Line 1 被拉取到 file1 中并替换掉原来位置上的 Line one。 同时操作两个文件vimdiff 实际上是 Vim 编辑器的 diff 模式，因此适用于 Vim 编辑器的命令和快捷键也同样可以在该模式下使用。常用的几个命令如下： :qa：退出所有文件 :wa：保存所有文件 :wqa：保存并退出所有文件 qa!：强制退出（不保存）所有文件 z o：查看被折叠的内容 z c：重新折叠 其他常用的命令与快捷键可参考 Vim 速查手册]]></content>
      <categories>
        <category>Tools</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Tools</tag>
        <tag>Tricks</tag>
        <tag>Vim</tag>
        <tag>Efficiency</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Paramiko 代码示例]]></title>
    <url>%2F2019%2F07%2F20%2Fparamiko-examples%2F</url>
    <content type="text"><![CDATA[Paramiko 是由 Python 语言编写的一个扩展模块，提供了基于 SSHv2 协议（包括客户端和服务端）的多种功能实现。通常被用来远程控制类 UNIX 系统。 Paramiko 可以直接使用 pip 命令安装：$ pip install paramiko 此处不作过多介绍，参考后文中的代码示例。 远程执行 Linux 命令代码如下：12345678910111213141516import paramiko# 初始化 SSH 客户端，通过用户名密码连接至远程服务器client = paramiko.SSHClient()client.set_missing_host_key_policy(paramiko.AutoAddPolicy)client.connect(hostname=&apos;remoteserver_ip&apos;, username=&apos;username&apos;, password=&apos;password&apos;)# 通过 RSA 秘钥验证的方式连接至远程 SSH 服务# private_key = paramiko.RSAKey.from_private_key_file(&apos;~/.ssh/id_rsa&apos;)# client.connect(hostname=&quot;remoteserver_ip&quot;, username=&quot;username&quot;, pkey=private_key)# 远程执行 df -h 命令并打印输出stdin, stdout, stderr = client.exec_command(&apos;df -h&apos;)print(stdout.read().decode(&apos;utf-8&apos;))client.close() 运行效果如下： SFTP 文件传输示例代码如下：12345678910111213141516171819import paramikotransport = paramiko.Transport((&apos;hostname_or_ip&apos;, port))# 通过用户名密码完成验证建立连接transport.connect(username=&apos;username&apos;, password=&apos;password&apos;)# 通过 RSA 私钥文件完成验证建立连接# private_key = paramiko.RSAKey.from_private_key_file(&apos;/path/to/private_key_file&apos;)# transport.connect(username=&apos;username&apos;, pkey=private_key)sftp = paramiko.SFTPClient.from_transport(transport)localpath = &quot;localfile&quot;remotepath = &quot;remotefile_fullpath&quot;sftp.put(localpath, remotepath)print(&quot;Successfully uploaded&quot;)transport.close() 综合示例代码如下（文件名 ssh_connection.py）：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374import paramikoimport getpassimport osclass SSHConnection(): def __init__(self, user, host, port=22, password=&apos;&apos;): self.username = user self.host = host self.port = port self.password = password self.keyfile = self.get_keyfile() def get_keyfile(self, path=os.getcwd()): default_keyfile = os.path.join( os.environ[&apos;HOME&apos;], &apos;.ssh&apos;, &apos;id_rsa&apos;) if &apos;id_rsa&apos; in os.listdir(path): keyfile = os.path.join(path, &apos;id_rsa&apos;) elif os.path.isfile(default_keyfile): keyfile = default_keyfile else: keyfile = &apos;&apos; return keyfile def connect(self): transport = paramiko.Transport((self.host, self.port)) if self.password: transport.connect(username=self.username, password=self.password) elif self.keyfile: transport.connect( username=self.username, pkey=paramiko.RSAKey.from_private_key_file(self.keyfile)) else: password = getpass.getpass( &quot;Password for %s@%s: &quot; % (self.username, self.host)) transport.connect(username=self.username, password=password) self._transport = transport print(&quot;Connected to %s as %s&quot; % (self.host, self.username)) def close(self): self._transport.close() def run_cmd(self, command): ssh = paramiko.SSHClient() ssh._transport = self._transport stdin, stdout, stderr = ssh.exec_command(command) res = stdout.read().decode(&apos;utf-8&apos;) error = stderr.read().decode(&apos;utf-8&apos;) if error.strip(): return error else: return res def trans_file(self, localpath, remotepath, method=&apos;&apos;): sftp = paramiko.SFTPClient.from_transport(self._transport) if method == &apos;put&apos;: sftp.put(localpath, remotepath) print(&quot;File %s has uploaded to %s&quot; % (localpath, remotepath)) elif method == &apos;get&apos;: sftp.get(remotepath, localpath) print(&quot;File %s has saved as %s&quot; % (remotepath, localpath)) else: print(&apos;usage: trans_file(localpath, remotepath, method=&quot;get/put&quot;&apos;) def __del__(self): self.close() 测试结果如下：1234567891011121314(python3) D:\Program\python\devops&gt;pythonPython 3.7.2 (default, Jan 2 2019, 17:07:39) [MSC v.1915 64 bit (AMD64)] :: Anaconda, Inc. on win32Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.&gt;&gt;&gt; from ssh_connection import SSHConnection&gt;&gt;&gt; client = SSHConnection(&apos;starky&apos;,&apos;127.0.0.1&apos;)&gt;&gt;&gt; client.connect()Connected to 127.0.0.1 as starky&gt;&gt;&gt; client.run_cmd(&apos;uname -a&apos;)&apos;Linux server1 5.0.0-20-generic #21-Ubuntu SMP Mon Jun 24 09:32:09 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux\n&apos;&gt;&gt;&gt; client.trans_file(&apos;id_rsa.pub&apos;, &apos;/home/starky/id_rsa.pub&apos;, method=&apos;put&apos;)File id_rsa.pub has uploaded to /home/starky/id_rsa.pub&gt;&gt;&gt; client.run_cmd(&apos;ls -l /home/starky/id_rsa.pub&apos;)&apos;-rw-rw-r-- 1 starky starky 410 7月 20 15:01 /home/starky/id_rsa.pub\n&apos;&gt;&gt;&gt; exit() 参考资料Github 项目主页API 手册]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Admin</tag>
        <tag>Python</tag>
        <tag>Program</tag>
        <tag>DevOps</tag>
        <tag>Development</tag>
        <tag>SSH</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[IT 自动化工具 Ansible 入门指南]]></title>
    <url>%2F2019%2F07%2F09%2Fansible-automate-tools-quickstart%2F</url>
    <content type="text"><![CDATA[Ansible 是一个非常简单的 IT 自动化引擎，可以完成诸如云端资源调配、配置管理、应用部署、服务协调等众多 IT 自动化任务。它不需要在受控端安装配置额外的 agent 软件，因此部署流程非常简单。Ansible 的运行机制也并不复杂，它通过 SSH 协议向远程节点推送特定的“小程序”（Ansible modules）并运行，一旦运行完成则将其移除。同时 Ansible 提供了一种非常简单易懂的语言（YAML）用于编写自动化代码。 一、简介Ansible 通常会被描述为一个配置管理工具，类似于 Chef、Puppet 和 Salt 等。通过配置管理工具，我们可以确保服务器主机处于某种期望的状态，比如指定的软件包已安装，配置文件中包含正确的值，运行着特定的服务等。与其他配置管理工具一样，Ansible 也加入了自己的 DSL（领域专用语言）用来描述服务器的状态，即后面会介绍到的用于编写 Playbook 的 YAML 语言。 Ansible 也可以用于部署任务，比如通过源代码生成可执行程序或静态资源并发布到服务器，然后开启远程主机上的对应服务。类似的开源部署工具如 Fabric 等。 还有一种需求叫做流程编排，通常会涉及到多个远程服务器，用于确保各类型的任务以特定顺序执行。比如在开启 Web 服务器之前确保数据库服务处于已经运行的状态。Ansible 从设计之初即关注多个服务器状态下任务的执行，它有一个非常简单的模块用于控制动作的顺序。 此外，Ansible 还提供众多的模块用来与常见的云服务进行交互，包括 Amazon EC2、Azure、Digital Ocean、Linode 以及任何支持 OpenStack API 的云端服务。 一个简单的 Ansible 运行实例如下图： 二、Ansible 安装绝大部分 Linux 发行版的软件镜像中都默认包含了 Ansible 的安装包， 可以直接通过对应的包管理器进行安装。如 Ubuntu 系统：$ sudo apt-get install ansible Ansible 是基于 Python 语言开发的，因此也可以通过 Python 的包管理器进行安装，命令如下：$ pip install ansible 我用 VirtualBox 软件搭建了两台 Ubuntu 19.04 虚拟机，配置了内部网络（Internal）的联网方式。server1 IP 地址为 192.168.1.101，用于安装 Ansible 环境。server2 IP 地址为 192.168.1.102，作为远程主机进行测试。两者可以相互 ping 通。 SSH 的密钥认证为了使得每次运行 Ansible 任务时，不需要重复输入远程主机的认证信息，这里先配置好 SSH 连接的无密码认证（即 RSA 密钥认证）。命令如下：1234$ # 生成 SSH 密钥文件$ ssh-keygen$ # 复制公钥文件到远程主机（需要输入密码）$ ssh-copy-id remoteuser@remoteserver 运行成功后，使用 $ ssh remoteuser@remoteserver 命令进行测试，如可以自动登录，则配置完成。 PS：被控制的远程主机不需要安装任何客户端软件，但是必须确保已安装 Python 且 sshd 服务处于运行状态。如两者未正确安装，可运行以下命令：12$ sudo apt-get install python$ sudo apt-get install openssh-server 主机清单（Inventory）Ansible 通过一个主机清单文件（hosts）存放被管理的服务器的列表。创建 playbooks 目录作为存放 ansible 脚本和配置文件的项目文件夹，进入该目录并编辑如下 hosts 文件：1server2 ansible_ssh_host=192.168.1.102 ansible_ssh_user=skitar ansible_ssh_private_key_file=~/.ssh/id_rsa 通过 hosts 中定义的服务器别名和连接信息访问远程主机并执行 ping 模块：$ ansible server2 -i hosts -m ping 输出内容如下：1234567server2 | SUCCESS =&gt; &#123; &quot;ansible_facts&quot;: &#123; &quot;discovered_interpreter_python&quot;: &quot;/usr/bin/python3&quot; &#125;, &quot;changed&quot;: false, &quot;ping&quot;: &quot;pong&quot;&#125; 如看到类似上面的输出则 Ansible 环境配置完成。 PS：Ansible 主机清单的配置文件默认为 /etc/ansible/hosts。如需使用其他位置的主机清单文件，可以通过 -i 选项手动指定，或者修改 ansible.cfg 配置文件的 inventory 项。 ansible.cfgansible.cfg 文件中包含了 Ansible 工具的一些默认配置，该文件通常位于以下位置： 当前目录（./ansible.cfg） Home 目录（~/.ansible.cfg） /etc/ansible/ansible.cfg 在当前目录下创建 ansible.cfg 文件并输入以下内容：12345[defaults]inventory = hostsremote_user = skitarprivate_key_file = ~/.ssh/id_rsahost_key_checking = False 由于部分默认选项已配置，此时的 hosts 文件则可以省略用户名和密钥文件等信息，简化为如下形式：1server2 ansible_ssh_host=192.168.1.102 使用 ansible 命令时也无需再通过 -i hosts 选项手动指定主机清单文件。即之前的 ping 模块可以这样调用：$ ansible server2 -m ping Ad-Hoc 命令即通过命令行的形式直接调用 Ansible 模块。Ansible 有着功能丰富的内置模块，可以通过 ansible-doc -l 命令显示所有的内置模块，通过 ansible-doc &lt;module&gt; 命令查看指定模块的介绍以及使用案例。 Ansible 内置的 command 模块可以用来在远程主机上执行 Linux 命令。如执行 uptime 命令：123$ ansible server2 -m command -a uptimeserver2 | CHANGED | rc=0 &gt;&gt; 17:45:12 up 5:13, 1 user, load average: 0.12, 0.08, 0.02 其中 -m 用于指定调用的模块，-a 用于指定传递给该模块的选项，此处即某个具体的命令。由于 command 模块很常用，它其实是不指定任何模块情况下的默认模块。所以上面的命令也可以使用如下形式：$ ansible server2 -a uptime 当远程主机上执行的命令中包含空格时，需要用引号括起来，示例如下：123456789101112$ ansible server2 -a &quot;tail /var/log/syslog&quot;server2 | CHANGED | rc=0 &gt;&gt;Jul 8 18:18:18 server2 systemd[6250]: Reached target Paths.Jul 8 18:18:18 server2 systemd[6250]: Listening on GnuPG cryptographic agent and passphrase cache (access for web browsers).Jul 8 18:18:18 server2 systemd[6250]: Listening on D-Bus User Message Bus Socket.Jul 8 18:18:18 server2 systemd[6250]: Reached target Sockets.Jul 8 18:18:18 server2 systemd[6250]: Reached target Basic System.Jul 8 18:18:18 server2 systemd[1]: Started User Manager for UID 1000.Jul 8 18:18:18 server2 systemd[1]: Started Session 47 of user skitar.Jul 8 18:18:18 server2 systemd[6250]: Reached target Default.Jul 8 18:18:18 server2 systemd[6250]: Startup finished in 75ms.Jul 8 18:18:19 server2 python3[6304]: ansible-command Invoked with _raw_params=tail /var/log/syslog warn=True _uses_shell=False stdin_add_newline=True strip_empty_ends=True argv=None chdir=None executable=None creates=None removes=None stdin=None 对于某些需要 root 权限才能执行的操作，则需要加上 --become --ask-become-pass 或者 -b -K 选项通过 sudo 执行。如使用 service 模块重启远程主机上的 nginx 服务：12345678910111213141516$ ansible server2 -b -K -m service -a &quot;name=nginx state=restarted&quot;BECOME password:server2 | CHANGED =&gt; &#123; &quot;ansible_facts&quot;: &#123; &quot;discovered_interpreter_python&quot;: &quot;/usr/bin/python3&quot; &#125;, &quot;changed&quot;: true, &quot;name&quot;: &quot;nginx&quot;, &quot;state&quot;: &quot;started&quot;, &quot;status&quot;: &#123; &quot;ActiveEnterTimestamp&quot;: &quot;Mon 2019-07-08 18:16:58 CST&quot;, &quot;ActiveEnterTimestampMonotonic&quot;: &quot;20685653757&quot;, &quot;ActiveExitTimestamp&quot;: &quot;Mon 2019-07-08 18:16:57 CST&quot;, &quot;ActiveExitTimestampMonotonic&quot;: &quot;20685035019&quot;, &quot;ActiveState&quot;: &quot;active&quot;,... 其他常用的内置模块还有 apt、copy、user 等。 如通过 apt 模块管理远程主机上的软件包：123456789101112$ ansible server2 --b -K -m apt -a &quot;name=nginx state=latest&quot;BECOME password: [WARNING]: Could not find aptitude. Using apt-get insteadserver2 | SUCCESS =&gt; &#123; &quot;ansible_facts&quot;: &#123; &quot;discovered_interpreter_python&quot;: &quot;/usr/bin/python3&quot; &#125;, &quot;cache_update_time&quot;: 1562598780, &quot;cache_updated&quot;: false, &quot;changed&quot;: false&#125; 通过 copy 模块复制本地文件到远程主机：1234567891011121314151617$ ansible server2 -m copy -a &quot;src=ansible.cfg dest=/home/skitar/ansible.cfg owner=skitar mode=644&quot;server2 | CHANGED =&gt; &#123; &quot;ansible_facts&quot;: &#123; &quot;discovered_interpreter_python&quot;: &quot;/usr/bin/python3&quot; &#125;, &quot;changed&quot;: true, &quot;checksum&quot;: &quot;2f4f9975ef1875adcc2a299d3962f70630f49965&quot;, &quot;dest&quot;: &quot;/home/skitar/ansible.cfg&quot;, &quot;gid&quot;: 1001, &quot;group&quot;: &quot;skitar&quot;, &quot;mode&quot;: &quot;0644&quot;, &quot;owner&quot;: &quot;skitar&quot;, &quot;path&quot;: &quot;/home/skitar/ansible.cfg&quot;, &quot;size&quot;: 109, &quot;state&quot;: &quot;file&quot;, &quot;uid&quot;: 1000&#125; 通过 user 模块管理远程主机上的用户（需要先通过 openssl 命令生成密码，因为 user 模块的 password 参数只接受加密后的值）：1234567891011121314151617181920$ echo ansible | openssl passwd -1 -stdin$1$ZyNqkbXH$i.4R0EDQZV.zu8akyJAu10$ ansible server2 -b -K -m user -a &apos;name=starky password=&quot;$1$ZyNqkbXH$i.4R0EDQZV.zu8akyJAu10&quot; shell=/bin/bash&apos;BECOME password:server2 | CHANGED =&gt; &#123; &quot;ansible_facts&quot;: &#123; &quot;discovered_interpreter_python&quot;: &quot;/usr/bin/python3&quot; &#125;, &quot;append&quot;: false, &quot;changed&quot;: true, &quot;comment&quot;: &quot;&quot;, &quot;group&quot;: 1002, &quot;home&quot;: &quot;/home/starky&quot;, &quot;move_home&quot;: false, &quot;name&quot;: &quot;starky&quot;, &quot;password&quot;: &quot;NOT_LOGGING_PASSWORD&quot;, &quot;shell&quot;: &quot;/bin/bash&quot;, &quot;state&quot;: &quot;present&quot;, &quot;uid&quot;: 1001&#125; 三、PlaybooksPlaybooks 即 Ansible 用于执行自动化配置的脚本文件。它使用非常简单的 YAML 语言描述期望达到的状态，YAML 之于 JSON 类似于 Markdown 之于 HTML 。 一个简单的用于配置 nginx 站点的 playbook 示例（web-notls.yml）如下：123456789101112131415161718192021- name: Configure webserver with nginx hosts: webservers become: True tasks: - name: install nginx apt: name=nginx update_cache=yes - name: copy nginx config file copy: src=files/nginx.conf dest=/etc/nginx/sites-available/default - name: enable configuration file: &gt; dest=/etc/nginx/sites-enabled/default src=/etc/nginx/sites-available/default state=link - name: copy index.html template: src=templates/index.html.j2 dest=/var/www/html/index.html mode=0644 - name: restart nginx service: name=nginx state=restarted 编辑 nginx 配置文件，即 playbook 中 copy 模块的 src 选项指定的文件（files/nginx.conf），内容如下：123456789101112server &#123; listen 80 default_server; root /var/www/html; index index.html; server_name 192.168.1.102; location / &#123; try_files $uri $uri/ =404; &#125;&#125; 编辑 nginx 站点的主页文件，即 playbook 中 template 模块的src 选项指定的文件（templates/index.html.j2），内容如下：12345678910&lt;html&gt; &lt;head&gt; &lt;title&gt;Welcome to ansible&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;h1&gt;nginx, configured by Ansible&lt;/h1&gt; &lt;p&gt;If you see this, Ansible successfuly installed nginx.&lt;/p&gt; &lt;p&gt;Current time is &#123;&#123; now() &#125;&#125;&lt;/p&gt; &lt;/body&gt;&lt;/html&gt; 编辑 Inventory （主机清单）即hosts 文件，创建 webservers 主机组：12[webservers]server2 ansible_ssh_host=192.168.1.102 主机组中可以包含一个或多个远程主机，便于同时管理多个远程节点。 上述配置完成后，通过 ansible-playbook web-nolts.yml -K 命令运行 playbook，输出如下：123456789101112131415161718192021222324252627$ ansible-playbook web-notls.yml -KBECOME password:PLAY [Configure webserver with nginx] **********************************************************TASK [Gathering Facts] *************************************************************************ok: [server2]TASK [install nginx] *************************************************************************** [WARNING]: Could not find aptitude. Using apt-get insteadok: [server2]TASK [copy nginx config file] ******************************************************************changed: [server2]TASK [enable configuration] ********************************************************************ok: [server2]TASK [copy index.html] *************************************************************************changed: [server2]TASK [restart nginx] ***************************************************************************changed: [server2]PLAY RECAP *************************************************************************************server2 : ok=6 changed=3 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 注意 ansibe-playbook 命令的 -K 选项（即 --ask-become-pass），由于 playbook 中的部分任务需要 root 权限执行，加上 -K 选项后，执行 playbook 时会出现 BECOME password: 提示用于输入 sudo 密码。否则会报错。 playbook 执行成功后，使用 curl 192.168.1.102 命令访问 server2 上刚刚配置的 nginx 站点，输出如下：1234567891011$ curl 192.168.1.102&lt;html&gt; &lt;head&gt; &lt;title&gt;Welcome to ansible&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;h1&gt;nginx, configured by Ansible&lt;/h1&gt; &lt;p&gt;If you see this, Ansible successfuly installed nginx.&lt;/p&gt; &lt;p&gt;Current time is 2019-07-09 00:28:46.484053&lt;/p&gt; &lt;/body&gt;&lt;/html&gt; 参考资料Ansible: Up and Running, 2nd Edition]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Admin</tag>
        <tag>Server</tag>
        <tag>Ansible</tag>
        <tag>DevOps</tag>
        <tag>Automate</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Arduino（配合舵机、超声波传感器）和 Processing 制作“声呐”探测系统]]></title>
    <url>%2F2019%2F07%2F09%2Farduino-radar-with-servo-and-ultrasound-sensor%2F</url>
    <content type="text"><![CDATA[参考自 Github 项目 ArduinoRadar 。原项目中 Processing 部分代码测试有疏漏，已修改并完成验证。主要是通过超声波传感器测量目标距离，并借助舵机完成扫描巡视动作，最后通过 Processing 收集测量数据，并完成类似声纳显示器的图形绘制。 项目实物图如下： Processing 绘制的类似监视器界面的动态图形如下： 一、准备材料软件： Arduino IDE：编写 C 语言源代码并编译上传至 Ardunio 开发板。 Processing：通过串口与 Arduino 开发板通信，获取实时数据并绘制类似监视器画面的动态图形。 硬件： Arduino Uno 或与之兼容的开发版：控制中心。 超声波传感器：测量目标距离。 舵机：承载超声波传感器，完成扫描动作。 二、连线示意图线路连接示意图如下： 三、源代码arduino C 程序源代码 radar.ino：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354#include &lt;Servo.h&gt;. const int trigPin = 7;const int echoPin = 8;long duration;int distance;Servo myServo; void setup() &#123; pinMode(trigPin, OUTPUT); pinMode(echoPin, INPUT); Serial.begin(9600); myServo.attach(9); &#125;void loop() &#123; for(int i=15;i&lt;=165;i++)&#123; myServo.write(i); delay(30); distance = calculateDistance(); Serial.print(i); Serial.print(&quot;,&quot;); Serial.print(distance); Serial.print(&quot;.&quot;); &#125; for(int i=165;i&gt;15;i--)&#123; myServo.write(i); delay(30); distance = calculateDistance(); Serial.print(i); Serial.print(&quot;,&quot;); Serial.print(distance); Serial.print(&quot;.&quot;); &#125;&#125;int calculateDistance() &#123; digitalWrite(trigPin, LOW); delayMicroseconds(2); digitalWrite(trigPin, HIGH); delayMicroseconds(10); digitalWrite(trigPin, LOW); duration = pulseIn(echoPin, HIGH); distance= duration*0.034/2; return distance;&#125; Processing 源代码 radar.pde：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144import processing.serial.*; // imports library for serial communicationimport java.awt.event.KeyEvent; // imports library for reading the data from the serial portimport java.io.IOException;String Port = &quot;COM3&quot;; //Arduino portSerial myPort; String angle=&quot;&quot;;String distance=&quot;&quot;;String data=&quot;&quot;;String noObject;float pixsDistance;int iAngle, iDistance;int index1=0;int index2=0;PFont orcFont;void setup() &#123; size (1250, 650); //resolution smooth(); myPort = new Serial(this, Port, 9600); myPort.bufferUntil(&apos;.&apos;);&#125;void draw() &#123; fill(98,245,31); noStroke(); fill(0,4); rect(0, 0, width, height-height*0.065); fill(98,245,31); drawRadar(); drawLine(); drawObject(); drawText();&#125;void serialEvent (Serial myPort) &#123; data = myPort.readStringUntil(&apos;.&apos;); data = data.substring(0,data.length()-1); index1 = data.indexOf(&quot;,&quot;); angle= data.substring(0, index1); distance= data.substring(index1+1, data.length()); iAngle = int(angle); iDistance = int(distance);&#125;void drawRadar() &#123; pushMatrix(); translate(width/2,height-height*0.074); noFill(); strokeWeight(2); stroke(98,245,31); arc(0,0,(width-width*0.0625),(width-width*0.0625),PI,TWO_PI); arc(0,0,(width-width*0.27),(width-width*0.27),PI,TWO_PI); arc(0,0,(width-width*0.479),(width-width*0.479),PI,TWO_PI); arc(0,0,(width-width*0.687),(width-width*0.687),PI,TWO_PI); line(-width/2,0,width/2,0); line(0,0,(-width/2)*cos(radians(30)),(-width/2)*sin(radians(30))); line(0,0,(-width/2)*cos(radians(60)),(-width/2)*sin(radians(60))); line(0,0,(-width/2)*cos(radians(90)),(-width/2)*sin(radians(90))); line(0,0,(-width/2)*cos(radians(120)),(-width/2)*sin(radians(120))); line(0,0,(-width/2)*cos(radians(150)),(-width/2)*sin(radians(150))); line((-width/2)*cos(radians(30)),0,width/2,0); popMatrix();&#125;void drawObject() &#123; pushMatrix(); translate(width/2,height-height*0.074); strokeWeight(9); stroke(255,10,10); pixsDistance = iDistance*((height-height*0.1666)*0.025); if(iDistance&lt;40)&#123; //range limit line(pixsDistance*cos(radians(iAngle)),-pixsDistance*sin(radians(iAngle)),(width-width*0.505)*cos(radians(iAngle)),-(width-width*0.505)*sin(radians(iAngle))); &#125; popMatrix();&#125;void drawLine() &#123; pushMatrix(); strokeWeight(9); stroke(30,250,60); translate(width/2,height-height*0.074); line(0,0,(height-height*0.12)*cos(radians(iAngle)),-(height-height*0.12)*sin(radians(iAngle))); popMatrix();&#125;void drawText() &#123; pushMatrix(); fill(0,0,0); noStroke(); rect(0, height-height*0.0648, width, height); fill(98,245,31); textSize(15); text(&quot;10cm&quot;,width-width*0.3854,height-height*0.0833); text(&quot;20cm&quot;,width-width*0.281,height-height*0.0833); text(&quot;30cm&quot;,width-width*0.177,height-height*0.0833); text(&quot;40cm&quot;,width-width*0.0729,height-height*0.0833); textSize(30); text(&quot;Angle: &quot; + iAngle +&quot; °&quot;, width-width*0.48, height-height*0.0277); text(&quot;Distance: &quot;, width-width*0.26, height-height*0.0277); if(iDistance&lt;40) &#123; text(&quot; &quot; + iDistance +&quot; cm&quot;, width-width*0.225, height-height*0.0277); &#125; textSize(25); fill(98,245,60); translate((width-width*0.4994)+width/2*cos(radians(30)),(height-height*0.0907)-width/2*sin(radians(30))); rotate(-radians(-60)); text(&quot;30°&quot;,0,0); resetMatrix(); translate((width-width*0.503)+width/2*cos(radians(60)),(height-height*0.0888)-width/2*sin(radians(60))); rotate(-radians(-30)); text(&quot;60°&quot;,0,0); resetMatrix(); translate((width-width*0.507)+width/2*cos(radians(90)),(height-height*0.0833)-width/2*sin(radians(90))); rotate(radians(0)); text(&quot;90°&quot;,0,0); resetMatrix(); translate(width-width*0.513+width/2*cos(radians(120)),(height-height*0.07129)-width/2*sin(radians(120))); rotate(radians(-30)); text(&quot;120°&quot;,0,0); resetMatrix(); translate((width-width*0.5104)+width/2*cos(radians(150)),(height-height*0.0574)-width/2*sin(radians(150))); rotate(radians(-60)); text(&quot;150°&quot;,0,0); popMatrix();&#125; 注意 radar.pde 文件开头的 String Port = &quot;COM3&quot;; 行代码，将 Port 变量的值 COM3 改为 Ardunio 开发板实际连接的串口号。 四、运行效果演示视频]]></content>
      <categories>
        <category>IOT</category>
      </categories>
      <tags>
        <tag>IOT</tag>
        <tag>DIY</tag>
        <tag>Electronics</tag>
        <tag>Hardware</tag>
        <tag>Ardunio</tag>
        <tag>Maker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu 19.04 配置 Zabbix 监控系统]]></title>
    <url>%2F2019%2F07%2F06%2Fzabbix-moniter-system-for-ubuntu%2F</url>
    <content type="text"><![CDATA[Zabbix 是一个用于监控 IT 设施的企业级解决方案，支持实时监控数千台服务器，虚拟机和网络设备，采集百万级监控指标。Zabbix 完全开源免费。 一、特性与架构特性Zabbix 监控系统的主要特性如下： 一个中心化的简单易用的 Web 管理界面，内置丰富的图形化和可视化组件 Zabbix Server 可以运行在绝大多数 UNIX-like 操作系统上，提供原生的客户端（agent）程序（支持绝大多数 UNIX-like 系统和 Windows 系统） 可以直接监控 SNMP (SNMPv1, SNMPv2, SNMPv3) 和 IPMI 设备 通过 JMX 可以直接监控 Java 应用，通过 VMware API 可以直接监控 vCenter 或 vSphere 实例 灵活的配置，包括丰富的模板系统。方便与其他系统集成的通知系统 系统架构一个简单的 Zabbix 监控系统的示例架构如下图： Zabbix 由以下几个主要的功能组件组成： Server：Zabbix 软件的核心组件，agent 向其报告可用性、系统完整性和其他统计信息 数据库：用于存储所有配置信息以及 Zabbix 采集到的数据 Web 界面：属于 Zabbix Server 的一部分，便于从任何地方、任何设备访问和配置 Zabbix Proxy：可以替代 Zabbix Server 进行数据的采集操作，在 Zabbix 的部署中是可选部分，但是可以很好地分担单个 Server 的负载 agent：Zabbix agents 部署在被监控目标上，用于主动监控本地资源和应用程序，并将数据发送给 Zabbix Server 二、安装获取 Zabbix 主要有以下三种方式： 从发行包安装 下载最新版源码包并编译 通过容器安装部署 这里选择第一种安装方式。Zabbix 官方的软件仓库有提供 RPM 和 DEB 格式的二进制包，本文并没有使用。使用的是 Ubuntu 19.04 默认软件仓库中提供的安装包，具体操作大同小异，详情可参考 Zabbix 4.0 产品手册 使用 apt-get 命令安装 Zabbix 系列软件：12$ sudo apt-get update$ sudo apt-get install zabbix-server-mysql zabbix-frontend-php zabbix-agent 其中 zabbix-server-mysql 是以 MySQL 数据库为后端的 Zabbix Server 软件。zabbix-frontend-php 则是由 PHP 语言编写的 Web 控制台界面。 导入初始数据首先确保已经安装了 MySQL 数据库，使用 root 用户登录进 MySQL 的命令行界面，输入以下命令创建数据库并授权：12345mysql&gt; CREATE DATABASE zabbix CHARACTER SET utf8 COLLATE utf8_bin;Query OK, 1 row affected (0.00 sec)mysql&gt; GRANT ALL ON zabbix.* TO zabbix@localhost IDENTIFIED BY &apos;password&apos;;Query OK, 0 rows affected, 1 warning (0.11 sec) 以上命令创建了一个名为 zabbix 的数据库，并授予用户 zabbix （密码为 password）对它的访问权限。zabbix 数据库用于存储 Zabbix Server 的配置信息和采集到的数据，在正式启用前还需要创建数据表并导入初始数据。 Zabbix 软件包中已经包含了用于创建这些数据表的 SQL 语句，一般保存在 /usr/share/doc/zabbix-server-mysql/ 或者 /usr/share/zabbix-server-mysql/ 目录下的一个或多个数据库备份文件中，直接导入即可。 我这里使用如下命令进行导入：$ zcat /usr/share/zabbix-server-mysql/{schema,images,data}.sql.gz | mysql -u zabbix -p zabbix按照提示输入密码后即可完成导入。 配置数据库连接信息修改 Zabbix Server 的配置文件 /etc/zabbix/zabbix_server.conf，将其中对应的项目改为如下内容：1234DBHost=localhostDBName=zabbixDBUser=zabbixDBPassword=password 重启 zabbix-server 服务：$ sudo systemctl restart zabbix-server Web 前端配置首先确保已经安装了 Apache2 Web 服务器和 PHP 语言支持。并且还需要安装以下几个 PHP 模块： gd mysqli bcmath mbstring gettext 前面安装完 zabbix-frontend-php 软件后，会自动在 /etc/apache2/conf-available/ 目录下创建 zabbix-frontend-php.conf 文件，作为 Zabbix Server 网页后台的配置文件。 该文件的默认配置可直接使用，只需要启用该配置即可，命令如下：$ sudo a2enconf zabbix-frontend-php或者 $sudo ln -s /etc/apache2/conf-available/zabbix-frontend-php.conf /etc/apache2/conf-enabled/ 此外，在正式访问 Web 页面安装之前，需要先修改时区配置。编辑 /etc/apache2/conf-enabled/zabbix-frontend-php.conf 文件，将时区配置改为如下内容：php_value date.timezone Asia/Shanghai Web 安装程序以上步骤完成后，重启 Apache2 服务。进入浏览器访问 http://localhost/zabbix ,根据提示进行操作，部分截图如下： 操作完成后，会自动生成 zabbix.conf.php 文件，内容大致如下：12345678910111213141516171819&lt;?php// Zabbix GUI configuration file.global $DB;$DB[&apos;TYPE&apos;] = &apos;MYSQL&apos;;$DB[&apos;SERVER&apos;] = &apos;localhost&apos;;$DB[&apos;PORT&apos;] = &apos;3306&apos;;$DB[&apos;DATABASE&apos;] = &apos;zabbix&apos;;$DB[&apos;USER&apos;] = &apos;admin&apos;;$DB[&apos;PASSWORD&apos;] = &apos;password&apos;;// Schema name. Used for IBM DB2 and PostgreSQL.$DB[&apos;SCHEMA&apos;] = &apos;&apos;;$ZBX_SERVER = &apos;your_ip_address&apos;;$ZBX_SERVER_PORT = &apos;10051&apos;;$ZBX_SERVER_NAME = &apos;server1&apos;;$IMAGE_FORMAT_DEFAULT = IMAGE_FORMAT_PNG; 如该文件未自动在 /etc/zabbix/ 目录下生成，可将该文件下载后移动到对应位置。 此时访问 http://localhost/zabbix ，即可进入安装配置完成的 Zabbix 监控系统控制台。默认登录用户名为 Admin，密码为 zabbix。截图如下： 三、主机与监控项本文主要介绍 Zabbix 监控系统的安装配置流程，对于主机及其关联的监控项的配置，限于篇幅不作过多介绍。建议参考官方文档。 实际上完成安装之后，Zabbix 已经默认关联了一个名为 Zabbix server 的主机（即安装 Zabbix Server 的 Linux 服务器本身），其中默认配置了众多监控项、触发器、图形和自动发现规则等。可以查看它们的具体配置信息作为参考。部分截图如下： 参考资料Zabbix 产品手册Zabbix Network Monitoring, Second Edition]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Admin</tag>
        <tag>Server</tag>
        <tag>Zabbix</tag>
        <tag>Monitor</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 系统 nginx 服务器安装及负载均衡配置详解]]></title>
    <url>%2F2019%2F07%2F02%2Fconfigure-nginx-for-load-banlance%2F</url>
    <content type="text"><![CDATA[nginx(engine x) 是一个高性能的 HTTP 和反向代理服务器、邮件代理服务器以及通用的 TCP/UDP 代理服务器。其特点为轻量级（占用系统资源少）、稳定性好、可扩展性（模块化结构）、并发能力强、配置简单等。本文主要介绍在测试环境中通过 nginx 实现基本的负载均衡功能。 nginx 可以提供 HTTP 服务，包括处理静态文件，支持 SSL 和 TLS SNI、GZIP 网页压缩、虚拟主机、URL 重写等功能，可以搭配 FastCGI、uwsgi 等程序处理动态请求。 此外，nginx 还可以用于代理、反向代理、负载均衡、缓存等服务器功能，在集群环境中改善网络负载、提高可用性。 一、搭建测试环境这里的测试环境为通过 VirtualBox 安装的两台 Lubuntu 19.04 虚拟机，Linux 系统安装方法不作赘述。为了保证两台 Linux 虚拟机之间的相互访问，虚拟机的网络配置除了默认的 NAT 方式外，还使用了 VirtualBox 软件提供的 内部网络（Internal） 联网方式。此外，还需要将两台虚拟机中与“内部网络”相关联的网卡，绑定上同一网段的静态 IP 地址，则两台主机形成局域网络，相互之间可以直接访问。 网络配置打开 VirtualBox 软件，分别进入两台虚拟机的设置界面，为其添加连接方式为内部网络的网络连接，截图如下（两台虚拟机作同样的配置）： 登录进虚拟机系统，使用 ip addr 命令查看当前的网络连接信息：123456789101112$ ip addr...2: enp0s3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel state UP group default qlen 1000 link/ether 08:00:27:38:65:a8 brd ff:ff:ff:ff:ff:ff inet 10.0.2.15/24 brd 10.0.2.255 scope global dynamic noprefixroute enp0s3 valid_lft 86390sec preferred_lft 86390sec inet6 fe80::9a49:54d3:2ea6:1b50/64 scope link noprefixroute valid_lft forever preferred_lft forever3: enp0s8: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel state UP group default qlen 1000 link/ether 08:00:27:0d:0b:de brd ff:ff:ff:ff:ff:ff inet6 fe80::2329:85bd:937e:c484/64 scope link noprefixroute valid_lft forever preferred_lft forever 可以看到，此时的 enp0s8 网卡还没有绑定 IPv4 地址，需要为其手动指定静态 IP。需要注意的是，从 Ubuntu 17.10 版本开始，一个新的名为 netplan 的工具被引入，原来的网络配置文件 /etc/network/interfaces 不再生效。 所以为网卡设置静态 IP 时需要修改 /etc/netplan/01-network-manager-all.yaml 配置文件，示例如下：1234567891011network: version: 2 renderer: NetworkManager ethernets: enp0s8: dhcp4: no dhcp6: no addresses: [192.168.1.101/24]# gateway4: 192.168.1.101# nameservers:# addresses: [192.168.1.101, 8.8.8.8] 由于两台主机处于同一子网，网关和 DNS 服务器未配置的情况下仍可以互相访问。对应的配置项暂时先注释掉（后续可以尝试自行搭建 DNS 服务器）。 编辑完成后运行 sudo netplan apply 命令，前面配置的静态 IP 即可生效。12345678$ ip addr...3: enp0s8: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel state UP group default qlen 1000 link/ether 08:00:27:0d:0b:de brd ff:ff:ff:ff:ff:ff inet 192.168.1.101/24 brd 192.168.1.255 scope global noprefixroute enp0s8 valid_lft forever preferred_lft forever inet6 fe80::a00:27ff:fe0d:bde/64 scope link valid_lft forever preferred_lft forever 登录进另一台虚拟机中，执行同样的操作（注意配置文件中的 addresses 项改为 [192.168.1.102/24]）。两台虚拟机的网络即配置完成。 此时有 Linux 虚拟机 server1，IP 地址为 192.168.1.101；Linux 虚拟机 server2，IP 地址为 192.168.1.102。两台主机可相互访问。测试如下：12345678starky@server1:~$ ping 192.168.1.102 -c 2PING 192.168.1.102 (192.168.1.102) 56(84) bytes of data.64 bytes from 192.168.1.102: icmp_seq=1 ttl=64 time=0.951 ms64 bytes from 192.168.1.102: icmp_seq=2 ttl=64 time=0.330 ms--- 192.168.1.102 ping statistics ---2 packets transmitted, 2 received, 0% packet loss, time 2msrtt min/avg/max/mdev = 0.330/0.640/0.951/0.311 ms 12345678skitar@server2:~$ ping 192.168.1.101 -c 2PING 192.168.1.101 (192.168.1.101) 56(84) bytes of data.64 bytes from 192.168.1.101: icmp_seq=1 ttl=64 time=0.223 ms64 bytes from 192.168.1.101: icmp_seq=2 ttl=64 time=0.249 ms--- 192.168.1.101 ping statistics ---2 packets transmitted, 2 received, 0% packet loss, time 29msrtt min/avg/max/mdev = 0.223/0.236/0.249/0.013 ms 二、安装 nginx 服务器nginx 的安装方式主要有两种： 预编译的二进制程序。这是最简单和最快速的安装方式，各主流操作系统都可以通过包管理器（如 Ubuntu 的 apt-get）安装。此种方式会安装几乎所有的官方模块或插件。 从源代码编译安装。这种方式相对于前者更加灵活，可以自行选择需要安装的模块或第三方插件。 本示例并没有特殊的需求，所以直接选择第一种安装方式。命令如下：12$ sudo apt-get update$ sudo apt-get install nginx 安装成功后，通过 systemctl status nginx 命令查看 nginx 服务的运行状态：1234567891011$ systemctl status nginx● nginx.service - A high performance web server and a reverse proxy server Loaded: loaded (/lib/systemd/system/nginx.service; enabled; vendor preset: en Active: active (running) since Tue 2019-07-02 01:22:07 CST; 26s ago Docs: man:nginx(8) Main PID: 3748 (nginx) Tasks: 2 (limit: 1092) Memory: 4.9M CGroup: /system.slice/nginx.service ├─3748 nginx: master process /usr/sbin/nginx -g daemon on; master_pro └─3749 nginx: worker process 通过 curl -I 127.0.0.1 命令验证 Web 服务器是否可以正常访问：1234$ curl -I 127.0.0.1HTTP/1.1 200 OKServer: nginx/1.15.9 (Ubuntu)... 三、负载均衡配置负载均衡（load-balancing）即按照一定的规则将负载分摊到多个操作单元上执行，从而提高服务的可用性和响应速度。简单的示例图如下： 如某网站应用部署在多台主机构成的服务器集群上，负载均衡服务器位于终端用户和服务器集群之间，负责接收终端用户的访问流量，并根据一定的规则将用户访问分发给后端的服务器主机，从而提高在高并发状态下的响应速度。 负载均衡服务器nginx 可以通过 upstream 选项配置负载均衡。这里使用虚拟机 server1 作为负载均衡服务器。修改 serve1 上默认站点的配置文件（sudo vim /etc/nginx/sites-available/default），改为如下内容：123456789101112upstream backend &#123; server 192.168.1.102:8000; server 192.168.1.102;&#125;server &#123; listen 80; location / &#123; proxy_pass http://backend; &#125;&#125; 基于测试的目的，当前只有两台虚拟机。server1（192.168.1.101）已经作为负载均衡服务器，所以使用 server2（192.168.1.102）作为应用服务器。这里借助 nginx 的虚拟主机功能，分别将 192.168.1.102 和 192.168.1.102:8000 “模拟”为两台不同的应用服务器。 应用服务器修改 server2 上默认站点的配置文件（sudo vim /etc/nginx/sites-available/default），改为如下内容：12345678910111213server &#123; listen 80; root /var/www/html; index index.html index.htm index.nginx-debian.html; server_name 192.168.1.102; location / &#123; try_files $uri $uri/ =404; &#125;&#125; 在 /var/www/html 目录下创建 index.html 文件，作为 default 站点的 index 页面，内容如下：12345678&lt;html&gt; &lt;head&gt; &lt;title&gt;Index Page From Server1&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;h1&gt;This is Server1, Address 192.168.1.102.&lt;/h1&gt; &lt;/body&gt;&lt;/html&gt; 运行 sudo systemctl restart nginx 命令重启 nginx 服务，此时访问 http://192.168.1.102 即可获取刚刚创建的 index.html 页面：123456789$ curl 192.168.1.102&lt;html&gt; &lt;head&gt; &lt;title&gt;Index Page From Server1&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;h1&gt;This is Server1, Address 192.168.1.102.&lt;/h1&gt; &lt;/body&gt;&lt;/html&gt; 配置“另一台主机”上的站点，在 server2 上创建 /etc/nginx/sites-available/server2 配置文件，内容如下：12345678910111213server &#123; listen 8000; root /var/www/html; index index2.html index.htm index.nginx-debian.html; server_name 192.168.1.102; location / &#123; try_files $uri $uri/ =404; &#125;&#125; 注意监听端口和 index 页面的配置变化。在 /var/www/html 目录下创建 index2.html 文件，作为 server2 站点的 index 页面，内容如下：12345678&lt;html&gt; &lt;head&gt; &lt;title&gt;Index Page From Server2&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;h1&gt;This is Server2, Address 192.168.1.102:8000.&lt;/h1&gt; &lt;/body&gt;&lt;/html&gt; PS：为了测试目的，default 站点和 server2 站点配置在同一个主机 server2 上，且页面稍有不同。实际环境中通常将这两个站点配置在不同的主机上，且内容一致。 运行 sudo ln -s /etc/nginx/sites-available/server2 /etc/nginx/sites-enabled/ 命令启用刚刚创建的 server2 站点。重启 nginx 服务，此时访问 http://192.168.1.102:8000 即可获取刚刚创建的 index2.html 页面：123456789$ curl 192.168.1.102:8000&lt;html&gt; &lt;head&gt; &lt;title&gt;Index Page From Server2&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;h1&gt;This is Server2, Address 192.168.1.102:8000.&lt;/h1&gt; &lt;/body&gt;&lt;/html&gt; 负载均衡测试回到负载均衡服务器即虚拟机 server1 上，其配置文件中设置的反向代理 URL 为 http://backend 。由于未曾配置域名解析服务，无法将 URL http://backend 定位到正确的位置。 可以修改 server1 上的 /etc/hosts 文件，添加如下一条记录：127.0.0.1 backend即可将该域名解析到本地 IP ，完成对负载均衡服务器的访问。 重启 nginx 服务，在 server1 上访问 http://backend ，效果如下：123456789101112131415161718192021222324252627282930313233343536$ curl http://backend&lt;html&gt; &lt;head&gt; &lt;title&gt;Index Page From Server1&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;h1&gt;This is Server1, Address 192.168.1.102.&lt;/h1&gt; &lt;/body&gt;&lt;/html&gt;$ curl http://backend&lt;html&gt; &lt;head&gt; &lt;title&gt;Index Page From Server2&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;h1&gt;This is Server2, Address 192.168.1.102:8000.&lt;/h1&gt; &lt;/body&gt;&lt;/html&gt;$ curl http://backend&lt;html&gt; &lt;head&gt; &lt;title&gt;Index Page From Server1&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;h1&gt;This is Server1, Address 192.168.1.102.&lt;/h1&gt; &lt;/body&gt;&lt;/html&gt;$ curl http://backend&lt;html&gt; &lt;head&gt; &lt;title&gt;Index Page From Server2&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;h1&gt;This is Server2, Address 192.168.1.102:8000.&lt;/h1&gt; &lt;/body&gt;&lt;/html&gt; 从输出中可以看出，server1 对负载均衡服务器 http://backend 的访问，完成了对应用服务器 server2 上两个 Web 站点的轮询，起到负载均衡的作用。 四、负载均衡方法nginx 开源版本提供四种负载均衡的实现方式，简单介绍如下。 1. Round Robin用户请求均匀地分配给后端服务器集群（可以通过 weight 选项设置轮询的权重），这是 nginx 默认使用的负载均衡方式：1234upstream backend &#123; server backend1.example.com weight=5; server backend2.example.com;&#125; 2. Least Connections用户请求会优先转发给集群中当前活跃连接数最少的服务器。同样支持 weight 选项。12345upstream backend &#123; least_conn; server backend1.example.com; server backend2.example.com;&#125; 3. IP Hash用户请求会根据客户端 IP 地址进行转发。即该方式意图保证某个特定的客户端最终会访问同一个服务器主机。12345upstream backend &#123; ip_hash; server backend1.example.com; server backend2.example.com;&#125; 4. Generic Hash用户请求会根据一个自定义键值确定最终转发的目的地，该键值可以是字符串、变量或者组合（如源 IP 和端口号）。12345upstream backend &#123; hash $request_uri consistent; server backend1.example.com; server backend2.example.com;&#125; 权重参考下面的示例配置：12345upstream backend &#123; server backend1.example.com weight=5; server backend2.example.com; server 192.0.0.1 backup;&#125; 默认权重（weight）为 1 。backup 服务器只有在所有其他服务器全部宕机的情况下才会接收请求。如上面的示例，每 6 个请求会有 5 个转发给 backend1.example.com，1 个转发给 backend2.example.com。只有当 backend1 和 backend2 全部宕机时，192.0.0.1 才会接收并处理请求。 参考资料HTTP Load Balancing]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Admin</tag>
        <tag>Server</tag>
        <tag>Nginx</tag>
        <tag>WebDev</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django 学习笔记（四）—— 第一个自定义应用 下篇]]></title>
    <url>%2F2019%2F06%2F29%2Fdjango-manual-4-admin%2F</url>
    <content type="text"><![CDATA[本文接上篇 Django 学习笔记（三）—— 第一个自定义应用 中篇，涉及到代码重构（基于通用视图）和 Django 后台管理系统的定制。 一、通用视图前面编写的 index()、detail() 和 results() 视图都很简单且存在一定的冗余问题。它们都遵循 Web 开发中一种常见的模式，即根据 URL 中的参数从数据库中获取数据、载入模板文件然后返回渲染后的页面。而通用视图可以将这种常见的模式抽象化，使得在编写应用时能够减少大量不必要的代码。 修改视图定义文件（polls/views.py），借助通用视图改为如下版本：1234567891011121314151617181920212223242526272829303132333435363738from django.http import HttpResponseRedirectfrom django.shortcuts import render, get_object_or_404from django.urls import reversefrom django.views import genericfrom .models import Question, Choiceclass IndexView(generic.ListView): template_name = &apos;polls/index.html&apos; context_object_name = &apos;latest_question_list&apos; def get_queryset(self): return Question.objects.order_by(&apos;-pub_date&apos;)[:5]class DetailView(generic.DetailView): model = Question template_name = &apos;polls/detail.html&apos;class ResultsView(generic.DetailView): model = Question template_name = &apos;polls/results.html&apos;def vote(request, question_id): question = get_object_or_404(Question, pk=question_id) try: selected_choice = question.choice_set.get(pk=request.POST[&apos;choice&apos;]) except (KeyError, Choice.DoesNotExist): return render(request, &apos;polls/detail.html&apos;, &#123; &apos;question&apos;: question, &apos;error_message&apos;: &quot;You didn&apos;t select a choice.&quot;, &#125;) else: selected_choice.votes += 1 selected_choice.save() return HttpResponseRedirect(reverse(&apos;polls:results&apos;, args=(question.id,))) ListView 和 DetailView 分别表示显示若干对象的列表和显示一个特定类型对象的详细信息两种视图模板。其中每个通用视图都可以用 model 属性指定其作用于哪个模型。此外，DetailView 期望从 URL 中捕获名为 “pk” 的主键值，因此需要修改 polls/urls.py 中的具体定义。 视图对应的路由定义修改为如下形式（polls/urls.py）：12345678910from django.urls import pathfrom . import viewsapp_name = &apos;polls&apos;urlpatterns = [ path(&apos;&apos;,views.IndexView.as_view(), name=&apos;index&apos;), path(&apos;&lt;int:pk&gt;/&apos;, views.DetailView.as_view(), name=&apos;detail&apos;), path(&apos;&lt;int:pk&gt;/results/&apos;, views.ResultsView.as_view(), name=&apos;results&apos;), path(&apos;&lt;int:question_id&gt;/vote/&apos;, views.vote, name=&apos;vote&apos;),] 二、自定义管理后台自定义后台表单当前的 Django 后台管理界面如下： 对于投票问题（Questions）和选项（Choices）对象的编辑需要进入两个不同的页面分别进行修改，使用的友好性上有待提升。 可以使用如下方法，为 Questions 对象添加关联的 Choices 对象，即在投票问题的后台页面中添加上修改投票选项的接口，使得这两个页面合并为同一个。 编辑后台定义文件（polls/admin.py）：123456789101112131415from django.contrib import adminfrom .models import Question, Choiceclass ChoiceInline(admin.StackedInline): model = Choice extra = 3class QuestionAdmin(admin.ModelAdmin): fieldsets = [ (None, &#123;&apos;fields&apos;: [&apos;question_text&apos;]&#125;), (&apos;Date information&apos;, &#123;&apos;fields&apos;: [&apos;pub_date&apos;], &apos;classes&apos;: [&apos;collapse&apos;]&#125;), ] inlines = [ChoiceInline]admin.site.register(Question, QuestionAdmin) 此时后台界面如下： 不过仍有点小问题，即修改对象时的后台页面占据了大量的屏幕区域用来显示 Choice 对象的字段。Django 提供了一种表格的形式用来显示关联对象，只需作如下修改（polls/admin.py）：123...class ChoiceInline(admin.TabularInline):... 效果如下： 自定义后台对象页面当前后台管理系统中的对象主页（如 Question 对象的主页）布局如下： 从该页面中无法直接看到除 question_text 之外的任何信息。下面将对该页面如下定制： 显示更多的字段信息（如 pub_date） 添加一个用来筛选数据的过滤器（基于日期） 添加搜索功能（基于 question_text） 修改 polls/admin.py 文件，添加如下 3 行代码：123456...class QuestionAdmin(admin.ModelAdmin): list_display = (&apos;question_text&apos;, &apos;pub_date&apos;) list_filter = [&apos;pub_date&apos;] search_fields = [&apos;question_text&apos;]... 效果如下： 参考资料 Django 2.2 官方文档]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Program</tag>
        <tag>WebDev</tag>
        <tag>Django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django 学习笔记（三）—— 第一个自定义应用 中篇]]></title>
    <url>%2F2019%2F06%2F24%2Fdjango-manual-3-views%2F</url>
    <content type="text"><![CDATA[本文接上篇 Django 学习笔记（二）—— 第一个自定义应用 上篇，前提是已经完成了 Django 项目的初始化以及数据模型的创建。本篇主要讲视图（View）的创建，视图即一类具有相同功能和模板的网页的集合。 本应用中需要用到一下几个视图： 问题索引页：展示最近几个问题的列表 问题详情页：展示某个需要投票的问题及其选项 问题结果页：展示某个投票的结果 投票处理器：响应用户为某个问题的特定选项投票的操作 一、模板理论上讲，视图可以从数据库读取记录，可以使用模板引擎（Django 自带的或者第三方模板），可以输出 XML，创建压缩文件，可以使用任何想用的 Python 库。 必须的要求只是返回一个 HttpResponse 对象，或者抛出一个异常。 借助 Django 提供的数据库 API，修改视图（polls/views.py 文件）中的 index() 函数，使得应用首页可以展示数据库中以发布时间排序的最近 5 个问题：123456789from django.http import HttpResponsefrom .models import Questiondef index(request): latest_question_list = Question.objects.order_by(&apos;-pub_date&apos;)[:5] output = &apos;/n&apos;.join([q.question_text for q in latest_question_list]) if output: output = &apos;Em...there is no questions. Please add some.&apos; return HttpResponse(output) 开启测试服务器，进入 Django 后台管理系统（http://127.0.0.1:8000/admin），添加测试问题： 访问 index 视图（http://127.0.0.1:8000/polls）效果如下： 在当前的代码中，页面的布局定义是固定在视图函数中的，页面设计变更时则需要对 Python 代码进行改动。此时可以通过使用 Django 的模板系统，将页面设计从业务代码中分离出来。 在 polls 目录下创建 templates 目录，Django 会自动在该目录下寻找模板文件。在 polls/templates 目录下创建 polls 目录，并在该目录下创建 index.html 文件（即最终的模板文件为 polls/templates/polls/index.html）。 具体代码如下：123456789&#123;% if latest_question_list %&#125; &lt;ul&gt; &#123;% for question in latest_question_list %&#125; &lt;li&gt;&lt;a href=&quot;/polls/&#123;&#123; question.id &#125;&#125;/&quot;&gt;&#123;&#123; question.question_text &#125;&#125;&lt;/a&gt;&lt;/li&gt; &#123;% endfor %&#125; &lt;/ul&gt;&#123;% else %&#125; &lt;p&gt;No polls are available.&lt;/p&gt;&#123;% endif %&#125; 修改视图函数（polls/view.py）已使用刚刚创建的模板：1234567from django.shortcuts import renderfrom .models import Questiondef index(request): latest_question_list = Question.objects.order_by(&apos;-pub_date&apos;)[:5] context = &#123;&apos;latest_question_list&apos;: latest_question_list&#125; return render(request, &apos;polls/index.html&apos;, context) render() 函数用于加载模块并填充上下文，然后返回生成的 HttpResponse 对象。 效果如下： 404 错误此时访问问题详情页面（即点击 index 页面中的 test quesion 链接），会提示 Page not found 错误，原因是模板（index.html）中指定的 polls// 还没有创建对应的路由及其绑定的页面。 这里需要创建一个问题详情页的视图，用于显示指定问题的详细信息。同时当访问的问题不存在时，该视图会抛出 Http404 异常。视图代码（polls/views.py）如下：123456789101112131415from django.http import Http404from django.shortcuts import renderfrom .models import Questiondef index(request): latest_question_list = Question.objects.order_by(&apos;-pub_date&apos;)[:5] context = &#123;&apos;latest_question_list&apos;: latest_question_list&#125; return render(request, &apos;polls/index.html&apos;, context)def detail(request, question_id): try: question = Question.objects.get(pk=question_id) except Question.DoesNotExist: raise Http404(&quot;Question does not exist&quot;) return render(request, &apos;polls/detail.html&apos;, &#123;&apos;question&apos;: question&#125;) 创建对应 detail 视图的模板文件（polls/templates/polls/detail.html）：123456&lt;h1&gt;&#123;&#123; question.question_text &#125;&#125;&lt;/h1&gt;&lt;ul&gt;&#123;% for choice in question.choice_set.all %&#125; &lt;li&gt;&#123;&#123; choice.choice_text &#125;&#125;&lt;/li&gt;&#123;% endfor %&#125;&lt;/ul&gt; 其中 choice 模型还没有关联至后台页面并添加数据，不过暂时不影响页面显示。 修改路由配置文件（polls/urls.py），添加关联 detail 页面的路由，内容如下：12345678from django.urls import pathfrom . import viewsapp_name = &apos;polls&apos;urlpatterns = [ path(&apos;&apos;,views.index, name=&apos;index&apos;), path(&apos;&lt;int:question_id&gt;/&apos;, views.detail, name=&apos;detail&apos;),] 运行效果如下： 去除硬编码 URLpolls/index.html 模板中的问题详情页链接当前是如下形式：&lt;li&gt;&lt;a href=&quot;/polls//&quot;&gt;&lt;/a&gt;&lt;/li&gt; 这种硬编码形式的链接在包含多个应用的项目中，相对而言并不方便修改。因为之前在路由配置文件 polls/urls.py 的 url() 函数中通过 name 参数对 URL 进行了命名（path(&#39;&lt;int:question_id&gt;/&#39;, views.detail, name=&#39;detail&#39;),），所以之前模板文件中的硬编码链接可以修改为如下形式的链接：1&lt;li&gt;&lt;a href=&quot;&#123;% url &apos;detail&apos; question.id %&#125;&quot;&gt;&#123;&#123; question.question_text &#125;&#125;&lt;/a&gt;&lt;/li&gt; 同时，对于包含多个应用的项目，为了避免 URL 重名的情况出现，可以为 URL 名称添加命名空间。即 polls/urls.py 文件中的如下代码行：app_name = &#39;polls&#39; 此时模板文件（polls/templates/polls/index.html）中的链接即改为如下形式：1&lt;li&gt;&lt;a href=&quot;&#123;% url &apos;polls:detail&apos; question.id %&#125;&quot;&gt;&#123;&#123; question.question_text &#125;&#125;&lt;/a&gt;&lt;/li&gt; 完整 index.html 代码如下：123456789&#123;% if latest_question_list %&#125; &lt;ul&gt; &#123;% for question in latest_question_list %&#125; &lt;li&gt;&lt;a href=&quot;&#123;% url &apos;polls:detail&apos; question.id %&#125;&quot;&gt;&#123;&#123; question.question_text &#125;&#125;&lt;/a&gt;&lt;/li&gt; &#123;% endfor %&#125; &lt;/ul&gt;&#123;% else %&#125; &lt;p&gt;No polls are available.&lt;/p&gt;&#123;% endif %&#125; 二、表单修改问题详情页的模板文件（polls/templates/polls/detail.html），添加 &lt;form&gt; 表单：1234567891011&lt;h1&gt;&#123;&#123; question.question_text &#125;&#125;&lt;/h1&gt;&#123;% if error_message %&#125;&lt;p&gt;&lt;strong&gt;&#123;&#123; error_message &#125;&#125;&lt;/strong&gt;&lt;/p&gt;&#123;% endif %&#125;&lt;form action=&quot;&#123;% url &apos;polls:vote&apos; question.id %&#125;&quot; method=&quot;post&quot;&gt;&#123;% csrf_token %&#125;&#123;% for choice in question.choice_set.all %&#125; &lt;input type=&quot;radio&quot; name=&quot;choice&quot; id=&quot;choice&#123;&#123; forloop.counter &#125;&#125;&quot; value=&quot;&#123;&#123; choice.id &#125;&#125;&quot;&gt; &lt;label for=&quot;choice&#123;&#123; forloop.counter &#125;&#125;&quot;&gt;&#123;&#123; choice.choice_text &#125;&#125;&lt;/label&gt;&lt;br&gt;&#123;% endfor %&#125;&lt;input type=&quot;submit&quot; value=&quot;Vote&quot;&gt;&lt;/form&gt; 上面的模板在 Question 中的每个 Choice 前添加一个单选按钮，其 value 为 “choice.id” ，即每次选择并提交表单后，它将发送一个 POST 数据 choice=#choice_id 。 表单的 action 为 url &#39;polls:vote&#39; question_id，且 action 方法为 POST。forloop.counter 用于指示 for 标签当前循环的次数。 csrf_token 用于防御跨站点请求伪造。 接下来需要在视图文件（polls/views.py）中添加定义 vote 视图的函数：1234567891011121314151617181920212223242526272829303132from django.http import HttpResponse, HttpResponseRedirectfrom django.shortcuts import render, get_object_or_404from django.urls import reversefrom .models import Question, Choicedef index(request): latest_question_list = Question.objects.order_by(&apos;-pub_date&apos;)[:5] context = &#123;&apos;latest_question_list&apos;: latest_question_list&#125; return render(request, &apos;polls/index.html&apos;, context)def detail(request, question_id): question = get_object_or_404(Question, pk=question_id) return render(request, &apos;polls/detail.html&apos;, &#123;&apos;question&apos;: question&#125;)def vote(request, question_id): question = get_object_or_404(Question, pk=question_id) try: selected_choice = question.choice_set.get(pk=request.POST[&apos;choice&apos;]) except (KeyError, Choice.DoesNotExist): return render(request, &apos;polls/detail.html&apos;, &#123; &apos;question&apos;: question, &apos;error_message&apos;: &quot;You didn&apos;t select a choice.&quot;, &#125;) else: selected_choice.votes += 1 selected_choice.save() return HttpResponseRedirect(reverse(&apos;polls:results&apos;, args=(question.id,)))def results(request, question_id): question = get_object_or_404(Question, pk=question_id) return render(request, &apos;polls/results.html&apos;, &#123;&apos;question&apos;: question&#125;) 其中 get_object_or_404() 是一个用于获取对象或者抛出 Http404 异常的快捷函数。HttpResponseRedirect 用于将用户重定向至指定页面（投票结果 results 页）。 所以还需要创建一个 polls/templates/polls/results.html 模板文件：123456789&lt;h1&gt;&#123;&#123; question.question_text &#125;&#125;&lt;/h1&gt;&lt;ul&gt;&#123;% for choice in question.choice_set.all %&#125; &lt;li&gt;&#123;&#123; choice.choice_text &#125;&#125; -- &#123;&#123; choice.votes &#125;&#125; vote&#123;&#123; choice.votes|pluralize &#125;&#125;&lt;/li&gt;&#123;% endfor %&#125;&lt;/ul&gt;&lt;a href=&quot;&#123;% url &apos;polls:detail&apos; question.id %&#125;&quot;&gt;Vote again?&lt;/a&gt; 修改 polls/urls.py 文件，添加上 vote 和 results 的路由配置：12345678910from django.urls import pathfrom . import viewsapp_name = &apos;polls&apos;urlpatterns = [ path(&apos;&apos;,views.index, name=&apos;index&apos;), path(&apos;&lt;int:question_id&gt;/&apos;, views.detail, name=&apos;detail&apos;), path(&apos;&lt;int:question_id&gt;/results/&apos;, views.results, name=&apos;results&apos;), path(&apos;&lt;int:question_id&gt;/vote/&apos;, views.vote, name=&apos;vote&apos;),] 项目进行到这里算是基本告一段落了。为了可以在 Django 后台系统中操作 Choice 模型关联的数据库，还需要将 polls/admin.py 文件改为如下形式：12345from django.contrib import adminfrom .models import Question, Choiceadmin.site.register(Question)admin.site.register(Choice) 此时运行测试服务器，最终效果如下： 参考资料 Django 2.2 官方文档]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Program</tag>
        <tag>WebDev</tag>
        <tag>Django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django 学习笔记（二）—— 第一个自定义应用 上篇]]></title>
    <url>%2F2019%2F06%2F24%2Fdjango-manual-2-models%2F</url>
    <content type="text"><![CDATA[本文接上篇 Django 学习笔记（一）—— 快速建站，前提是已经完成了 Django 开发环境的搭建和 MySQL 数据库的关联。本教程将创建一个基本的投票应用程序，包含供人们参与投票的公共站点和一个可操作后台数据的管理平台。主要参考自 Django 2.2 官方文档。 一、创建项目$ django-admin startproject mysite 该命令会在当前目录下创建一个 mysite 目录，并且该框架中已经包含了一个初始 Django 项目必需的所有组件。其目录结构如下：1234567mysite├── manage.py└── mysite ├── __init__.py ├── settings.py ├── urls.py └── wsgi.py 进入 manage.py 所在的目录下，运行 python manage.py runserver 0000:8000 即可运行一个用于开发的简易服务器：12345678910$ python manage.py runserver 0.0.0.0:8000Watching for file changes with StatReloaderPerforming system checks...System check identified no issues (0 silenced).June 22, 2019 - 07:47:04Django version 2.2.2, using settings &apos;mysite.settings&apos;Starting development server at http://0.0.0.0:8000/Quit the server with CONTROL-C. 访问效果如下： 二、创建投票应用 PS：“应用”是一个专门用于做某件事的应用程序，比如博客或者简单的投票程序，“项目”则是一个网站所包含的配置和应用的合集。一个项目可以包含多个应用，一个应用也可以被多个项目使用。 Django 中的每个应用都是一个 Python 包，可以通过 manage.py 创建应用的基础目录结构：$ python manage.py startapp polls 当前目录结构如下：12345678910111213141516mysite├── manage.py├── mysite│ ├── __init__.py│ ├── settings.py│ ├── urls.py│ └── wsgi.py└── polls ├── admin.py ├── apps.py ├── __init__.py ├── migrations │ └── __init__.py ├── models.py ├── tests.py └── views.py 编辑视图（View）编辑 polls/views.py 文件，代码如下：1234from django.http import HttpResponsedef index(request): return HttpResponse(&quot;Hello Wrold. This is the index page.&quot;) 创建 URL 映射在 polls 目录下创建 urls.py 文件，编辑输入如下内容以完成到 index 页面的 URL 映射：123456from django.urls import pathfrom . import viewsurlpatterns = [ path(&apos;&apos;,views.index, name=&apos;index&apos;),] 在根 URLconf 文件中（即 mysite/urls.py）引入刚刚创建的 polls.urls 模块（即 polls/urls.py）：1234567from django.contrib import adminfrom django.urls import path, includeurlpatterns = [ path(&apos;polls/&apos;, include(&apos;polls.urls&apos;)), path(&apos;admin/&apos;, admin.site.urls),] 运行测试服务器，访问 http://127.0.0.1:8000/polls ，最终效果如下： 三、创建模型（Model）在 Django 中，对于一个数据驱动的 Web 应用，定义模型（即数据库的结构设计）是非常必要的一个操作。 本应用中需要创建两个模型： 问题 Question 模型：包含问题描述和发布时间 选项 Choice 模型：包含选项描述和当前票数 编辑 polls/models.py 文件，具体结构定义如下：12345678910from django.db import modelsclass Question(models.Model): question_text = models.CharField(max_length=200) pub_date = models.DateTimeField(&apos;date published&apos;)class Choice(model.Model): question = models.ForeignKey(Question, on_delete=models.CASCADE) choice_text = models.CharField(max_length=200) votes = models.IntegerField(default=0) 上述模型代码可以被 Django 用来创建本应用的数据库 schema（生成 CREATE TABLE 语句）。不过需要先在 mysite/settings.py 配置文件中修改 INSTALLED_APPS 变量，Django 需要依据该变量中对应用的指定进行数据库迁移操作。 123456789INSTALLED_APPS = [ &apos;polls.apps.PollsConfig&apos;, &apos;django.contrib.admin&apos;, &apos;django.contrib.auth&apos;, &apos;django.contrib.contenttypes&apos;, &apos;django.contrib.sessions&apos;, &apos;django.contrib.messages&apos;, &apos;django.contrib.staticfiles&apos;,] 运行 python manage.py makemigrations polls 命令创建数据库迁移文件：12345$ python manage.py makemigrations pollsMigrations for &apos;polls&apos;: polls/migrations/0001_initial.py - Create model Question - Create model Choice 运行 python manage.py migrate 命令完成数据库迁移操作：123456$ python manage.py migrateOperations to perform: Apply all migrations: admin, auth, contenttypes, polls, sessionsRunning migrations: Applying polls.0001_initial... OK... PS：在运行 migrate 命令之前，可以使用 python manage.py sqlmigrate polls 0001 查看该次迁移操作具体会执行哪些 SQL 语句：123456789101112 python manage.py sqlmigrate polls 0001BEGIN;---- Create model Question--CREATE TABLE &quot;polls_question&quot; (&quot;id&quot; integer NOT NULL PRIMARY KEY AUTOINCREMENT, &quot;question_text&quot; varchar(200) NOT NULL, &quot;pub_date&quot; datetime NOT NULL);---- Create model Choice--CREATE TABLE &quot;polls_choice&quot; (&quot;id&quot; integer NOT NULL PRIMARY KEY AUTOINCREMENT, &quot;choice_text&quot; varchar(200) NOT NULL, &quot;votes&quot; integer NOT NULL, &quot;question_id&quot; integer NOT NULL REFERENCES &quot;polls_question&quot; (&quot;id&quot;) DEFERRABLE INITIALLY DEFERRED);CREATE INDEX &quot;polls_choice_question_id_c5b4b260&quot; ON &quot;polls_choice&quot; (&quot;question_id&quot;);COMMIT; 四、数据库 API可以通过运行 python manage.py shell 进入 Python 交互式命令行，尝试 Django 提供的各种数据库 API 。示例如下：12345678910111213141516171819202122$ python manage.py shellPython 3.7.3 (default, Apr 3 2019, 05:39:12)[GCC 8.3.0] on linuxType &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.(InteractiveConsole)&gt;&gt;&gt; from polls.models import Choice, Question&gt;&gt;&gt; Question.objects.all()&lt;QuerySet []&gt;&gt;&gt;&gt; from django.utils import timezone&gt;&gt;&gt; q = Question(question_text=&quot;What&apos;s new?&quot;, pub_date=timezone.now())&gt;&gt;&gt; q.save()&gt;&gt;&gt; q.id1&gt;&gt;&gt; q.question_text&quot;What&apos;s new?&quot;&gt;&gt;&gt; q.pub_datedatetime.datetime(2019, 6, 24, 15, 34, 47, 614267, tzinfo=&lt;UTC&gt;)&gt;&gt;&gt; q.question_text = &quot;What&apos;s up&quot;&gt;&gt;&gt; q.save()&gt;&gt;&gt; Question.objects.all()&lt;QuerySet [&lt;Question: Question object (1)&gt;]&gt;&gt;&gt;&gt; 上面的示例中，&lt;Question: Question object (1)&gt; 类型的输出不便于了解对象的细节，可以通过修改 Question 模型的代码来改变 Question.objects.all() 函数的行为。将 polls/models.py 内容改为如下形式：12345678910111213141516from django.db import modelsclass Question(models.Model): question_text = models.CharField(max_length=200) pub_date = models.DateTimeField(&apos;date published&apos;) def __str__(self): return self.question_textclass Choice(models.Model): question = models.ForeignKey(Question, on_delete=models.CASCADE) choice_text = models.CharField(max_length=200) votes = models.IntegerField(default=0) def __str__(self): return self.choice_text 此时再次尝试使用数据库 API ：123456789$ python manage.py shellPython 3.7.3 (default, Apr 3 2019, 05:39:12)[GCC 8.3.0] on linuxType &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.(InteractiveConsole)&gt;&gt;&gt; from polls.models import Choice, Question&gt;&gt;&gt; Question.objects.all()&lt;QuerySet [&lt;Question: What&apos;s up&gt;]&gt;&gt;&gt;&gt; 给模型增加 __str__() 方法只是其中一种形式，实际上可以添加任意需要的函数以供后期调用。 五、后台管理系统（Django admin）Django admin 是任何一个初始的 Django 项目（完成数据库同步之后）默认开启的应用，用于对项目关联的数据库表进行基本的增删改查操作。 首先需要创建用于登录后台系统的管理员账户：123456$ python manage.py createsuperuserUsername (leave blank to use &apos;starky&apos;):Email address:Password:Password (again):Superuser created successfully. 开启测试服务器（python manage.py runserver 0000:8000），访问 http://127.0.0.1:8000/admin/，使用刚刚创建的管理员账户登录，效果如下： 将 poll 应用添加至后台修改 polls/admin.py 文件，将 Question 对象添加至后台面板：1234from django.contrib import adminfrom .models import Questionadmin.site.register(Question) 启动测试服务器，访问后台页面效果如下： PS：中文界面可以修改配置文件 mysite/settings.py，将 LANGUAGE_CODE 项改为如下形式：LANGUAGE_CODE = &#39;zh-Hans&#39; 之后便可以在后台页面直接对之前 MySQL 中创建的 polls_question 数据表进行编辑操作： 参考资料 Django 2.2 官方文档]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Program</tag>
        <tag>WebDev</tag>
        <tag>Django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django 学习笔记（一）—— 快速建站]]></title>
    <url>%2F2019%2F06%2F20%2Fdjango-manual-1-simple-site%2F</url>
    <content type="text"><![CDATA[Django 是一个基于 MVC（Model-View-Controller）模式的服务器端的 Web 框架，由 Python 语言编写。其创建之初主要是用来与关系型数据库进行交互，随着技术的不断革新，Django 也逐渐融合了众多更现代的 Web 开发趋势。作为一个成熟的 Web 开发框架，Django 有着众多企业级的应用，如 Instagram、Pinterest、National Geographic 等。 Django 设计原则 Don’t Repeat Yourself (DRY) Explicit Is Better Than Implicit Loosely Coupled Architecture 一、环境搭建Ubuntu 系统。为了保证多个 Python 运行环境之间彼此不受影响，这里通过 virtualenv 创建独立的虚拟开发环境。 使用 apt-get 命令安装 virtualenv ：$ sudo apt-get install python3-virtualenv 或者使用 Python 的包管理器（pip 命令）安装：$ python3 -m pip install virtualenv 使用 virtualenv 命令创建独立的虚拟运行环境：$ virtualenv --python=python3 mydjango 激活刚刚创建的虚拟环境：$ source activate mydjango/bin/activate 安装 Django 框架：$ pip install django 二、创建初始项目Django 框架安装成功以后，会提供 django-admin 命令用于常见的管理操作。如 django-admin startproject 命令用于创建一个初始项目的骨架：$ django-admin startproject testproject 该命令会在当前目录下创建 testproject 文件夹，结构如下：1234567testproject├── manage.py└── testproject ├── __init__.py ├── settings.py ├── urls.py └── wsgi.py 其中各文件的作用如下： manage.py：类似于 django-admin 命令，用于执行一些项目相关的任务 settings.py：Django 项目的配置设置 urls.py：Django 项目的 URL 模式 wsgi.py：Django 项目的 WSGI 配置。WSGI 是 Django 应用部署在生产环境中的推荐方式 运行测试服务进入项目目录，运行 python manage.py runserver &lt;IP:Port&gt; 命令开启测试服务，一个可供访问的最简单的 Web 站点即搭建成功：12345678910$ python manage.py runserver 0.0.0.0:8000Watching for file changes with StatReloaderPerforming system checks...System check identified no issues (0 silenced).June 20, 2019 - 09:03:29Django version 2.2.2, using settings &apos;testproject.settings&apos;Starting development server at http://0.0.0.0:8000/Quit the server with CONTROL-C. 效果如下： 三、数据库配置创建数据库 django 并授予用户 admin 访问权限：12345678mysql&gt; CREATE DATABASE django;Query OK, 1 row affected (0.00 sec)mysql&gt; GRANT ALL ON django.* TO admin IDENTIFIED BY &apos;password&apos;;Query OK, 0 rows affected, 1 warning (0.07 sec)mysql&gt; FLUSH PRIVILEGES;Query OK, 0 rows affected (0.00 sec) Django 使用 SQLite3 作为默认的数据库引擎，相关配置位于 settings.py 配置文件中：1234567...DATABASES = &#123; &apos;default&apos;: &#123; &apos;ENGINE&apos;: &apos;django.db.backends.sqlite3&apos;, &apos;NAME&apos;: os.path.join(BASE_DIR, &apos;db.sqlite3&apos;), &#125;... 如需使用 MySQL 数据库，将 settings.py 文件中的对应部分改为如下内容：12345678910DATABASES = &#123; &apos;default&apos;: &#123; &apos;ENGINE&apos;: &apos;django.db.backends.mysql&apos;, &apos;HOST&apos;: &apos;127.0.0.1&apos;, &apos;NAME&apos;: &apos;django&apos;, &apos;PASSWORD&apos;: &apos;password&apos;, &apos;PORT&apos;: &apos;3306&apos;, &apos;USER&apos;: &apos;admin&apos;, &#125;&#125; 安装 Python 操作 MySQL 的依赖库：12$ sudo apt-get install python3-dev libmysqlclient-dev$ pip install mysqlclient 使用 python manage.py migrate 命令完成数据库迁移操作：123456789101112131415161718192021$ python manage.py migrateOperations to perform: Apply all migrations: admin, auth, contenttypes, sessionsRunning migrations: Applying contenttypes.0001_initial... OK Applying auth.0001_initial... OK Applying admin.0001_initial... OK Applying admin.0002_logentry_remove_auto_add... OK Applying admin.0003_logentry_add_action_flag_choices... OK Applying contenttypes.0002_remove_content_type_name... OK Applying auth.0002_alter_permission_name_max_length... OK Applying auth.0003_alter_user_email_max_length... OK Applying auth.0004_alter_user_username_opts... OK Applying auth.0005_alter_user_last_login_null... OK Applying auth.0006_require_contenttypes_0002... OK Applying auth.0007_alter_validators_add_error_messages... OK Applying auth.0008_alter_user_username_max_length... OK Applying auth.0009_alter_user_last_name_max_length... OK Applying auth.0010_alter_group_name_max_length... OK Applying auth.0011_update_proxy_permissions... OK Applying sessions.0001_initial... OK 四、Django 管理后台为了方便管理，Django 将项目主要的业务逻辑和功能以模块化的方式（Django App）进行组织，一般用户自定义的应用位于项目目录下某个单独的文件夹中。这些 App 可以在 settings.py 文件中进行启用或禁用：12345678INSTALLED_APPS = [ &apos;django.contrib.admin&apos;, &apos;django.contrib.auth&apos;, &apos;django.contrib.contenttypes&apos;, &apos;django.contrib.sessions&apos;, &apos;django.contrib.messages&apos;, &apos;django.contrib.staticfiles&apos;,] 从上面的代码中可以看到，任何一个初始的 Django 项目默认已经开启了一个基本的“后台管理系统”（django.contrib.admin），可以对项目关联的数据库进行基本的增删改查操作。 该 admin 应用的路由定义在 urls.py 文件中：123456from django.contrib import adminfrom django.urls import pathurlpatterns = [ path(&apos;admin/&apos;, admin.site.urls),] 数据库迁移操作完成后，启动测试服务，即可访问 http://127.0.0.1:8000/admin 进入后台管理系统： 此时用于登录的后台管理员还未创建，可以使用 python manage.py createsuperuser 命令生成管理员账户：123456$ python manage.py createsuperuser用户名 (leave blank to use &apos;starky&apos;):电子邮件地址:Password:Password (again):Superuser created successfully. 登录成功后效果如下： 即无需编写任何业务代码，只搭建 Django 环境即可生成一个基本的“后台管理系统”。 参考资料Beginning Django]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Program</tag>
        <tag>WebDev</tag>
        <tag>Django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用 laravel-admin 配置后台管理系统]]></title>
    <url>%2F2019%2F06%2F08%2Fbuild-management-console-with-laravel-admin%2F</url>
    <content type="text"><![CDATA[内容有点多，就不说别的了。。需要一个简单的后台管理系统，对接数据库中的用户信息表，完成基本的增删改查操作。最好支持权限管理；有便捷的接口可供调用（不需要深度定制）；前端界面和交互美观简洁，足够“现代化”；架构明晰，配置简单，可快速成型等等。几经查找，感觉 laravel-admin 这个框架还不错。虽说文档算不上完善，小踩几坑，没怎么太费事就构建好了。值得记录一下。 一、安装 Laravel 环境laravel-admin 需要 PHP 7+ 和 Lavavel 5.5+ ，我当前使用的是 VirtualBox 虚拟机里的 Ubuntu 19.04 系统，配置起来还是比较方便的。Laravel 官方的定义是 The PHP Framework For Web Artisans ，优雅和快速成型。依赖于 PHP &gt;= 7.1.3 和一些 PHP 扩展组件。可以使用 Linux 系统自带的包管理器进行安装，命令如下：$ sudo apt-get install php7.2 php7.2-bcmath php7.2-json php7.2-mbstring php7.2-mysql php7.2-xml php7.2-zip php7.2-common 安装 composer 并配置国内镜像Composer 是 PHP 语言的依赖管理工具，类似于 Node.js 下的 npm 。后面需要用到的 Laravel 、laravel-admin 及其相关的依赖项都可以通过 composer 命令安装。$ sudo apt-get install composer 为了提高访问速度，可以把 composer 的镜像源改为国内版本，命令如下（全局配置）：$ composer config -g repo.packagist composer https://packagist.laravel-china.org 安装 Laravel 并初始化项目使用 composer 命令安装 Laravel ：composer global require laravel/installer 将 Laravel 安装路径添加到 PATH 环境变量：$ echo export PATH=&quot;$PATH:~/.config/composer/vendor/bin&quot; &gt;&gt; ~/.zshrc &amp;&amp; source ~/.zshrc 初始化 Laravel 项目：$ laravel new admin 以上步骤完成后，进入 admin 项目目录，运行 $ php artisan serve 命令即可开启一个最基本的 Laravel 站点。 配置数据库安装 mysql 数据库：$ sudo apt-get install mysql-server 创建数据库 admin 并授权给测试账户（用户名 test_user ，密码 test_password）：12345678mysql&gt; CREATE DATABASE admin;Query OK, 1 row affected (0.00 sec)mysql&gt; GRANT ALL ON admin.* to test_user IDENTIFIED BY &apos;test_password&apos;;Query OK, 0 rows affected, 1 warning (0.00 sec)mysql&gt; FLUSH PRIVILEGES;Query OK, 0 rows affected (0.00 sec) 修改数据库配置文件编辑 config/database.php 文件（已经默认使用 mysql 数据库），修改 mysql 配置部分，将 url 改为如下形式：&#39;url&#39; =&gt; env(&#39;DATABASE_URL&#39;, &#39;mysql://test_user:test_password@127.0.0.1/admin&#39;),（数据库 URL 的格式为 mysql://用户名:密码@IP地址/数据库名） 二、安装 laravel-admin数据库配置完成后，即可按照以下步骤安装配置 laravel-admin 。 在 admin 目录下，使用 composer 命令安装 laravel-admin ：$ composer require encore/laravel-admin 发布资源：$ php artisan vendor:publish --provider=&quot;Encore\Admin\AdminServiceProvider&quot; 使用以下命令完成安装：$ php artisan admin:install 运行服务查看效果：$ php artisan serve 访问 http://127.0.0.1:8000/admin ，使用用户名 admin 和密码 admin 登录，效果如下： 安装完成后，laravel-admin 所有的配置都在 config/admin.php 文件中（如站点名称、logo、登录界面的背景等），而项目文件的安装目录为 app/Admin ，目录结构如下：123456app/Admin├── Controllers│ ├── ExampleController.php│ └── HomeController.php├── bootstrap.php└── routes.php 其中 app/Admin/routes.php 文件用来配置后台路由；app/Admin/bootstrap.php 是 laravel-admin 的启动文件；app/Admin/Controllers 目录用来存放后台控制器文件，该目录下的HomeController.php 文件是后台首页的显示控制器。 三、一些必要的配置项目进行到这里，已经具备了一个后台管理系统的所有基本要素，只是在添加或者修改用户时，会报出下图所示的 Config error ，需要添加一个 Disk 配置： 编辑 config/filesystems.php 文件，在 &#39;disks&#39; 配置下添加如下内容：123456&apos;admin&apos; =&gt; [ &apos;driver&apos; =&gt; &apos;local&apos;, &apos;root&apos; =&gt; public_path(&apos;upload&apos;), &apos;visibility&apos; =&gt; &apos;public&apos;, &apos;url&apos; =&gt; env(&apos;APP_URL&apos;).&apos;/public/puload/&apos;, ], 中文配置配置界面语言为中文可以修改 config/app.php 文件，将其中的 locale =&gt; en 改为 locale =&gt; zh-CN 即可。 其他常用的配置如站点名称、页面布局等可以编辑 config/admin.php 文件，文件中的注释信息非常详细，根据需求直接修改即可。 四、绑定自己的数据库前面生成的算是演示用的示例页面，如果需要绑定和管理自己的数据库表格，操作也是比较简单的。 创建数据库表格在之前创建的 admin 数据库中添加名为 employee 的表格：12345678mysql&gt; CREATE TABLE employee ( -&gt; id int(6) unsigned AUTO_INCREMENT PRIMARY KEY, -&gt; number int(6) unsigned NOT NULL UNIQUE, -&gt; name varchar(10) NOT NULL, -&gt; mail varchar(60) UNIQUE, -&gt; password varchar(30), -&gt; department varchar(40) NOT NULL DEFAULT &apos;&apos; -&gt; ) ENGINE=InnoDB DEFAULT CHARSET=utf8 COLLATE=utf8_unicode_ci; 创建模型文件$ php artisan make:model Employee 上述命令会创建 app/Employee.php 模型文件，将该文件改为如下内容：1234567891011&lt;?phpnamespace App;use Illuminate\Database\Eloquent\Model;class Employee extends Model&#123; protected $table = &apos;employee&apos;; public $timestamps = false;&#125; PS：关于模型的使用可以参考 Laravel 官方文档 添加控制器通过以下命令创建一个对接 app/employee 模型的控制器：12$ php artisan admin:make EmployeeController --model=App\\EmployeeApp\Admin\Controllers\EmployeeController created successfully. 添加路由编辑路由配置文件 app/Admin/routers.php ，在根 URL 的路由下添加如下一行内容：$router-&gt;resource(&#39;employee&#39;, EmployeeController::class); 添加网页菜单访问 http://127.0.0.1:8000/admin/auth/menu ，添加页面左侧的菜单项，用于指向前面创建的后台数据。 上述步骤完成后，访问 http://127.0.0.1:8000/admin/employee ，最终效果如下： 五、优化筛选和密码显示上面完成的项目中，筛选按钮只支持通过 ID 搜索，密码也是直接显示。这些和表格相关的行为都可以通过修改 app/Admin/Controllers/EmployeeController.php 文件进行控制。 修改 EmployeeController.php 文件中对表格相关行为的定义：123456789101112131415161718192021222324252627protected function grid()&#123; $grid = new Grid(new Employee); // 不显示 ID 列 //$grid-&gt;id(&apos;ID&apos;);^M $grid-&gt;number(&apos;工号&apos;);^M $grid-&gt;name(&apos;姓名&apos;);^M $grid-&gt;mail(&apos;邮箱&apos;);^M // 隐藏密码列 //$grid-&gt;password(&apos;Password&apos;);^M $grid-&gt;department(&apos;部门&apos;); $grid-&gt;filter(function($filter)&#123; // 去掉默认的 id 过滤器 $filter-&gt;disableIdFilter(); // 添加新的字段过滤器（通过工号过滤） $filter-&gt;like(&apos;number&apos;, &apos;工号&apos;); &#125;); return $grid;&#125; 实际效果如下： 六、nginx 服务器配置首先需要终止默认安装的 Apache2 服务并安装 nginx 和 php-fpm：123$ sudo systemctl stop apache2$ sudo systemctl disable apache2$ sudo apt-get install nginx php7.2-fpm 创建新的站点配置文件（/etc/nginx/sites-available/admin.conf）并写入以下内容：123456789101112131415161718192021222324252627282930313233server &#123; listen 80; server_name 127.0.0.1; root &lt;your-project-path&gt;/public; add_header X-Frame-Options &quot;SAMEORIGIN&quot;; add_header X-XSS-Protection &quot;1; mode=block&quot;; add_header X-Content-Type-Options &quot;nosniff&quot;; index index.html index.htm index.php; charset utf-8; location / &#123; try_files $uri $uri/ /index.php?$query_string; &#125; location = /favicon.ico &#123; access_log off; log_not_found off; &#125; location = /robots.txt &#123; access_log off; log_not_found off; &#125; error_page 404 /index.php; location ~ \.php$ &#123; fastcgi_pass unix:/var/run/php/php7.2-fpm.sock; fastcgi_index index.php; fastcgi_param SCRIPT_FILENAME $realpath_root$fastcgi_script_name; include fastcgi_params; &#125; location ~ /\.(?!well-known).* &#123; deny all; &#125;&#125; 使用以下命令启用新的站点配置并重启 nginx 服务：12$ sudo ln -s /etc/nginx/sites-available/admin.conf /etc/nginx/sites-enabled/$ sudo systemctl restart nginx 需要注意的是，app/storage 目录的属主应该与运行 nginx 服务的用户（默认为 www-data）一致，可通过以下命令修改：$ sudo chown -R www-data storage 参考资料laravel-admin 官方文档Laravel 官网]]></content>
      <categories>
        <category>Program</category>
      </categories>
      <tags>
        <tag>Mysql</tag>
        <tag>Program</tag>
        <tag>Laravel</tag>
        <tag>laravel-admin</tag>
        <tag>Web</tag>
        <tag>PHP</tag>
        <tag>Nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[树莓派使用 snowboy 配置语音唤醒]]></title>
    <url>%2F2019%2F05%2F17%2Fpython-snowboy-voice-arousal%2F</url>
    <content type="text"><![CDATA[语音唤醒算是语音识别领域里最基础的应用，具体的场景如 Android 手机里的 “OK, Google” 或者苹果设备里的 “Hey, Siri”。简单来说就是在后台静默地运行着一个占用较少系统资源的服务（语音识别组件），该组件一直处于监视麦克风输入的状态，如果有检测到特定的语音输入（即唤醒词或“热词”），则激活与之绑定的某个程序“开关”。相当于一个简化版的语音助手吧，只对某一个特定的词汇进行响应，识别后也只完成某一件指定的任务。如果说同语音助手的交互是一段持续的交流，那么语音唤醒即可作为这种连续交流的入口（打招呼）。 snowboy 是一个开源的、轻量级语音唤醒引擎，可以通过它很轻松地创建属于自己的类似“hey, Siri” 的唤醒词。它的主要特性如下： 高度可定制性。可自由创建和训练属于自己的唤醒词 始终倾听。可离线使用，无需联网，保护隐私。精确度高，低延迟 轻量可嵌入。耗费资源非常低（单核 700MHz 树莓派只占用 10% CPU） 开源跨平台。开放源代码，支持多种操作系统和硬件平台，可绑定多种编程语言 详细看了官网提供的安装配置教程（已经很久没更新，有点过于繁琐了），几番尝试之后，感觉下面的介绍算是最新也相对最简单的方法了吧。 PS：只针对 Linux 系统（包含树莓派），其他平台可参考 Github 一、获取源代码并编译安装依赖树莓派原生的音频设备是不支持语音输入的（无法录音），需要在网上购买一支免驱动的USB音频驱动（便携式的和 U 盘差不多），一般插上即可直接使用。建议安装下 pulseaudio 软件，减少音频配置的步骤：$ sudo apt-get install pulseaudio 安装 sox 软件测试录音与播放功能：$ sudo apt-get install sox 安装完成后运行 sox -d -d 命令，对着麦克风说话，确认可以听到自己的声音。 安装其他软件依赖： 安装 PyAudio：$ sudo apt-get install python3-pyaudio 安装 SWIG（&gt;3.0.10)：$ sudo apt-get install swig 安装 ATLS：$ sudo apt-get install libatls-base-dev 编译源代码获取源代码：$ git clone https://github.com/Kitt-AI/snowboy.git编译 Python3 绑定：$ cd snowboy/swig/Python3 &amp;&amp; make 测试：进入官方示例目录 snowboy/examples/Python3 并运行以下命令：$ python3 demo.py resources/models/snowboy.umdl（ 命令中的 snowboy.umdl 文件即语音识别模型） 然后对着麦克风清晰地讲出“snowboy”，如果可以听到“滴”的声音，则安装配置成功。命令行输出如下： PS：官方源代码使用 Python3 测试有报错，经测试需修改 snowboy/examples/Python3 目录下的 snowboydecoder.py 文件。将第 5 行代码 from * import snowboydetect 改为 import snowboydetect 即可直接运行。 二、设置自己的唤醒词可将包含自定义唤醒词的音频文件上传至 snowboy 官网（需要登录），以训练生成自己喜欢的语音模型。需要上传的音频文件数量为 3 个，wav 格式。我试过直接在线录制，貌似有 Bug 。。。（也可能是我浏览器的问题） 训练完成并测试通过后，即可下载 PMDL 后缀的模型文件了。 #####测试将以下文件复制到自己的项目目录下： 上一步中下载好的 model.pmdl 模型文件 之前 snowboy/swig/Python3 目录下编译好的 _snowboydetect.so 库 snowboy/examples/Python3 目录下的 demo.py、snowboydecoder.py、snowboydetect.py 文件以及 resources 目录 在项目目录下执行 $ python3 demo.py model.pmdl 并使用自己的唤醒词进行测试 三、自定义响应官方提供的示例 demo.py 文件的源代码如下：123456789101112131415161718192021222324252627282930313233import snowboydecoderimport sysimport signalinterrupted = Falsedef signal_handler(signal, frame): global interrupted interrupted = Truedef interrupt_callback(): global interrupted return interruptedif len(sys.argv) == 1: print(&quot;Error: need to specify model name&quot;) print(&quot;Usage: python demo.py your.model&quot;) sys.exit(-1)model = sys.argv[1]# capture SIGINT signal, e.g., Ctrl+Csignal.signal(signal.SIGINT, signal_handler)detector = snowboydecoder.HotwordDetector(model, sensitivity=0.5)print(&apos;Listening... Press Ctrl+C to exit&apos;)# main loopdetector.start(detected_callback=snowboydecoder.play_audio_file, interrupt_check=interrupt_callback, sleep_time=0.03)detector.terminate() 通过阅读代码，可以看出唤醒词识别成功以后，程序响应的具体内容由程序末尾 detector.start() 函数的 detected_callback 参数指定。即重新绑定 detected_callback 对应的函数，可改变程序最终的响应。如：123456789101112131415161718192021222324252627282930313233343536import snowboydecoderimport sysimport signalinterrupted = Falsedef signal_handler(signal, frame): global interrupted interrupted = Truedef interrupt_callback(): global interrupted return interrupteddef detected(): print(&quot;Great! I have recognized your words.\n&quot;)if len(sys.argv) == 1: print(&quot;Error: need to specify model name&quot;) print(&quot;Usage: python demo.py your.model&quot;) sys.exit(-1)model = sys.argv[1]# capture SIGINT signal, e.g., Ctrl+Csignal.signal(signal.SIGINT, signal_handler)detector = snowboydecoder.HotwordDetector(model, sensitivity=0.5)print(&apos;Listening... Press Ctrl+C to exit&apos;)# main loopdetector.start(detected_callback=detected, interrupt_check=interrupt_callback, sleep_time=0.03)detector.terminate() 注意添加的 detected 函数。 效果如下： 更复杂的应用形式（如控制 LED 小灯等）也是基本上一样的思路，具体示例代码可参考官方文档。 参考资料snowboy Githubsnowboy 官网snowboy 官方文档]]></content>
      <categories>
        <category>Program</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Program</tag>
        <tag>Voice</tag>
        <tag>Intelligence</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Termux 详解—— Android 平台上完美移植的 Linux 工具集]]></title>
    <url>%2F2019%2F05%2F15%2FTermux-app-details%2F</url>
    <content type="text"><![CDATA[最近换了新手机，系统是基于 Android Pie（即 Android 9）定制的 MIUI 10 。Android 相对而言算得上是比较开放的平台，如此说来，不趁此机会乱搞一下难免有些说不过去。。。额，新手机，刷机Root来一套有点舍不得，好在有 Termux 这个应用。来，耍一耍看上去能够彰显“硬核”范儿的东西。 一、简介Termux 是 Android 平台上的一个终端模拟器，它将众多 Linux 上运行的软件和工具近乎完美的移植到了手机端。无需任何复杂的安装和配置过程，软件装好以后即会自动配置一个基本的运行环境，用以执行一些常见的 Linux 命令。最为关键的是，它还内置了功能健全的包管理工具，可以使用类似于 Ubuntu 系统的 apt （或 pkg）命令安装额外的软件包。 之所以称它为“模拟器”而非“虚拟机”，是因为它并非像 PC 端的 VirtualBox 等虚拟机软件那样，在宿主机中虚拟出一个完全独立且完整的系统环境，而是类似于 Mingw 等软件，只是提供一个接口，以安装和运行面向新环境交叉编译后的程序。 也可以将此时的手机看作是安装了 Linux 系统的树莓派，可以像 PC 端系统那样运行各种类型的软件，只不过这些软件都是针对特定的 CPU 架构和硬件设备编译过的（交叉编译或者在树莓派系统中本地编译）。需要注意的是，由于移动端和 PC 端硬件设备的巨大差异，加上 Android 内核和操作系统的限制，能够直接运行的程序毕竟是少数。不过我刚刚算了一下，实际上可直接安装运行的软件足足有 910 个！ 二、软件包前面提到的可供安装的近一千个（算上为数众多的基础工具、库文件和开发版软件包）程序和软件，数量虽然远不及桌面系统，面对日常使用、学习任务甚至很多高阶应用也已经算得上绰绰有余了。举个例子来说，Termux 是可以直接安装配置 Python 编程环境的。想想 Python 社区里浩如烟海的第三方库，甚至还可以根据特定的需求自行编写程序源代码。这里面包含着无限的可能性。 当然这也只是 Python 罢了，那么 Perl、Ruby、Nodejs、Lua、C/C++、Golang、Rust……我不会拿它去对标电脑上完备的系统环境，单说这种似乎无限的可能性，就已经很值得玩味了。 这里简单的列举下我所熟悉的部分软件： 基础工具：apt、bash、busybox、dpkg、git、htop、make、zsh …… 编程语言：binutils、clang、dart、erlang、golang、lua、nodejs、perl、php、python、ruby、rust …… 服务器软件：apache2、lighttpd、nginx、openssh …… 数据库软件：mariadb、memcached、mosquitto、postgresql、redis、sqlite …… 文本工具和编辑器：emacs、gawk、nano、sed、vim …… 媒体工具：ffmpeg、imagemagick、mpv、sox …… 网络工具：curl、httping、nmap、wget …… 游戏和娱乐：bastet、cmatrix、cowsay、fortune、moon-buggy、nsnake、sl …… 三、有趣的 Terminal 小命令就先不说具体的环境搭建的步骤了，包含的东西很多，实际上跟电脑端的操作并没有太大的区别。就简单列举一些我个人非常喜欢的好玩儿的小命令吧。 fortune：输出一段格言警句、名著节选或者小笑话等 cowsay：将输出的文字内容包含在由 ASCII 字符组成的动物形象的气泡内 lolcat：将苍白的输出文字变成绚丽的彩虹色 安装方法： fortune：$ apt install fortune cowsay：$ apt install cowsay 或者 $ gem install cowsay lolcat：$ gem install lolcat 其中 apt 版本的 cowsay 依赖 Perl ，安装包整个算起来有点大。所以我比较倾向于使用 gem 命令安装，即作为 Ruby 的第三方库。当然为了使用 gem 命令是要先安装好 Ruby 的：$ apt install ruby 效果截图： 虽然名字叫 cowsay ，但它所包含的动物形象事实上不只“牛”这一种。可以使用 cowsay -l 命令列出所有可供选择的动物形象，并通过 cowsay -f animal_name 手动指定另一种动物。 另外，其实 cowsay 的每个动物形象都是由对应的 cow 文件（基本就是 ASCII Art 形式的文本文件）定义的，可以自己扩充，篇幅有限不赘述。类似的小玩意儿还有 sl（突突的小火车）、cmatrix（全屏滚动乱码），试试就知道了。 配置 ssh 服务前面的截图，估计已经暴露了。。。我是在电脑上截的图，但程序确实是在手机上运行的。只是在手机端 Termux 上安装了 SSH 服务，远程登录而已。步骤也非常简单： 软件安装：$ apt install openssh 设置远程登录密码：$ passwd 电脑端登录：$ ssh phone_ip -p 8022 电脑上的 ssh 客户端可自行选择，尤其要注意，这里 SSH 服务监听的端口是 8022 。呃，差点忘记了。远程连接之前，先在手机端运行下 sshd 命令启动 SSH 服务。。。 四、高端的命令行游戏moon-buggymoon-buggy 是一个非常“酷炫”的命令行版本的跑酷游戏。虽说游戏界面完全由 ASCII 字符构成，它仍然具备了一个跑酷游戏必需的所有的基本要素。玩下就懂了，蛮神奇的。 玩儿到后面貌似还有迎面飞来的障碍物，还可以发射武器弹药？记不太清楚了。。。 nsnake嗯，不多说了，看名字就知道，贪吃蛇。。。初始的移动速度有点慢，建议在游戏选项里面改一下。 bastet就是大名鼎鼎的俄罗斯方块啦。看图： 五、oh-my-zsh手机端输入命令总觉得有点别扭，可能还是不习惯吧。这时候有个 oh-my-zsh 就显得很有必要了。反正我个人觉得挺好用的，尤其搭配上 zsh-completions、zsh-autosuggestions、zsh-syntax-highlighting 等插件。可以对命令行中输入的命令进行语法高亮、自动补全等，极大地提高了输入效率。 软件安装 oh-my-zsh 12$ apt install zsh curl$ sh -c &quot;$(curl -fsSL https://raw.githubusercontent.com/robbyrussell/oh-my-zsh/master/tools/install.sh)&quot; zsh-autosuggestions$ git clone https://github.com/zsh-users/zsh-autosuggestions ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-autosuggestions zsh-completions$ git clone https://github.com/zsh-users/zsh-completions ~/.oh-my-zsh/custom/plugins/zsh-completions zsh-syntax-highlighting$ git clone https://github.com/zsh-users/zsh-syntax-highlighting.git ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-syntax-highlighting 软件配置：编辑 ~/.zshrc 文件，将上述插件添加到 plugins 项后面： PS：Termux 貌似是没有默认 SHELL 的配置的，所以安装好 oh-my-zsh 以后，打开软件还是直接进入 Bash 界面。我没有特别去找配置文件的位置，而是用了另外一种方案，，，手动输入 zsh 命令进入，或者，在 ~/.bashrc 文件中加入 zsh 一行内容。。。 六、访问手机文件及 Termux-Api根据默认的 Android 系统的权限设定，Termux 是无法访问手机存储的。当然可以使用命令进行修改。只需要输入 termux-setup-storage 命令，即会弹出授权窗口，允许即可。该命令会在用户主目录下生成 storage 文件夹，里面即包含了到系统主要资源（如手机内存、外置存储卡、Downloads 文件夹、照片等）的链接。 Termux-Api 是 Termux 软件的一个插件，需要安装额外的 APK 包。并且命令行中也需要使用 $ apt install termux-api 命令安装具体的工具。它提供了一种以 API 的形式直接访问 Android 系统硬件和资源（如相机、电池、WiFi、短信、通讯录、指纹、GPS等）的途径。 如获取电池状态和 WiFi 连接信息： 这个感觉可以深挖，用好了可以达到曾经的安卓神器 Tasker 的效果。 今天也不早了，，，就先这样吧。熬，附个软件链接吧。。Termux and Termux-Api 提取码：i37yBy the way，录了段蛮简陋的视频演示，额，第一次剪。。。 参考资料Termux Wiki]]></content>
      <categories>
        <category>Tools</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Admin</tag>
        <tag>Tools</tag>
        <tag>Android</tag>
        <tag>Software</tag>
        <tag>Fun</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python Tricks —— 使用 keyboard 录制键盘事件]]></title>
    <url>%2F2019%2F05%2F11%2Fpython-capture-keyboard-events%2F</url>
    <content type="text"><![CDATA[之前在某本书上看到一个程序，可以通过 Python 记录下全局范围内的键盘事件，使用的是 ctypes 库。后来几经尝试，始终不能成功运行。原来它只支持 Python2 和 32 位的 Windows 系统。。。Excuse me？于是在网上查找可行的替代方案，比如 pyHook 。呃，不合胃口。安装比较繁琐（有依赖库且不能通过 pip 命令安装），只支持 Windows 系统，况且又是十年前就没再更新的项目。。。看了下函数调用也算不上简洁直观。 后来又多番搜索，终于找到了一个名为 keyboard 的 Python 库，可以绑定全局事件、录制及模拟键盘输入、设置热键等。尤其是安装和使用足够简单，比较合我的心意（毕竟 Github 上将近 1.5K 的小星星）。 一、特性 全局范围内的键盘事件绑定。即程序可以后台运行，捕捉其他软件下的按键动作。 跨平台，支持 Windows 、Linux 以及 MacOS 系统。支持 Python2 和 Python3 。 纯 Python 代码，无需编译 C 语言模块。无依赖库，安装简单（只是复制文件就可以）。 通过独立的线程捕捉事件，不阻塞主程序的运行。 很详细的文档，参见项目主页的 README 。 其实我真的有点感觉，这才是我心目中比较“现代”的工具。 二、录制及模拟按键动作就像前面提到的，安装 keyboard 的流程非常简单，无需考虑任何兼容性或者依赖问题，只需要一条简短的命令：$ pip install keyboard 可以使用下面的代码录制 10 秒钟的键盘事件，并将其回放一遍（即重复按下之前操作的按键）：1234567import keyboardimport timekeyboard.start_recording()time.sleep(10)events = keyboard.stop_recording()keyboard.replay(events) 可以使用如下代码制作一个简单的“键盘录制器”（打印输出全局范围内的按键动作，并将按键顺序保存在文本文件中）：1234567891011import keyboarddef print_pressed_keys(e): line = &apos;, &apos;.join(str(code) for code in keyboard._pressed_events) print(line) with open(&apos;keylogger.txt&apos;, &apos;a+&apos;) as f: f.write(line + &apos;\n&apos;)keyboard.hook(print_pressed_keys)keyboard.wait() PS：上述代码中的 code 并不是对应按键的 ASCII 码，而是根据键盘布局为按键指定的数值（如 a 键为 30，b 键为 42 等等），可以通过自行测试确认具体的对应关系。如同时按下两个或多个按键，则上述程序会以 code, code... 的形式输出。 三、热键捕获及绑定可以使用如下代码录制热键并为该热键绑定上特定的触发事件：1234567891011import keyboardprint(&apos;Press and release your desired hotkey: &apos;)hotkey = keyboard.read_hotkey()print(&apos;Hotkey selected: &apos;, hotkey)def on_triggered(): print(&quot;Triggered!&quot;)keyboard.add_hotkey(hotkey, on_triggered)print(&quot;Press ESC to stop.&quot;) 其他的使用方式就不再一一列举了，可以参考 keyboard 项目的 Github 主页。几种常见的使用示例也已经包含在项目代码的 examples 目录下。 参考keyboard]]></content>
      <categories>
        <category>Tools</category>
      </categories>
      <tags>
        <tag>Admin</tag>
        <tag>Tools</tag>
        <tag>Security</tag>
        <tag>Tricks</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ImageMagick 图片处理常用实例简介]]></title>
    <url>%2F2019%2F04%2F11%2Fexamples-about-processing-images-by-imagemagick%2F</url>
    <content type="text"><![CDATA[ImageMagick 是一整套用于图像处理的跨平台的命令行工具，大部分 Linux 发行版都可以直接通过包管理器安装（如 Ubuntu 系统：sudo apt-get install imagemagick）。其他平台也可以从官网下载对应的编译好的程序。通过命令行，ImageMagick 可以很高效的对图片进行编辑、转换和创作，也支持批量处理多张图片，或者内嵌到 Bash 脚本中。这里简单介绍下 ImageMagick 常见的几种用法。 格式转换ImageMagick 提供了 convert 命令用于接收图片文件并对其进行特定的操作后输出。其中最基本的用法即改变图片的格式。 如将 PNG 格式的图片转为 JPEG 格式：$ convert image.png image.jpg 对于 JPEG 图片，在转换时还可以指定压缩等级，如：$ convert image.png -quality 95 image.jpg 其中压缩等级（-quality）的值为 1-100，默认使用输入图片的压缩等级。如该值为空，则压缩等级默认为 92 。 改变图片大小convert 命令还可以用来改变图片的大小。如下面的命令可以将原图片转成大小为 200x100 像素的图片：$ convert image.png -resize 200x100 out.png 需要注意的是，在上面的命令中，ImageMagick 会优先保持原图片的比例（否则图片会发生一定程度的变形）。这样的结果是，改变后的图片可以正好放进一个 200x100 大小的区域，但图片本身并不一定是精确的 200x100 像素。 如果就是需要将原图片转换为特定大小，而不用考虑形变的影响。可以使用如下命令：$ convert image.png -resize 200x100! out.png 当然更多的时候，指定输出图片的大小时并非一定需要“宽x高”这样的形式，其实只需要指定宽或者高中的一项即可。如指定输出图片的宽度：$ convert image.png -resize 200 out.png 或者指定输出图片的高度：$ convert image.png -resize x100 out.png 旋转与翻转将原图片旋转 90° 后输出：$ convert image.jpg -rotate 90 image-rotated.jpg 指定的角度为正时即顺时针旋转图片，为负时逆时针旋转。 左右翻转：$ convert image.png -flop out.png上下翻转：$ convert image.png -flip out.png PS：包括前面的几种情况在内，如果输出图片的文件名和原图片相同，则改变后的图片会直接覆盖掉原图片。 裁剪与缩放convert 命令支持等比例缩放图片，如将图片缩小为原来的一半：$ convert image.png -scale 50% out.png 同时 convert 也可以对图片进行裁剪，包括自动裁剪（剔除图片周围空白的部分或边框等）和自定义范围的裁剪。 自动裁剪：$ convert image.png -trim out.png 自定义裁剪：$ convert image.jpg -crop 600x600+240+240 out.jpg 其中 -crop 的参数为 宽x高+横坐标偏移量+纵坐标偏移量 的形式，即宽和高用来定义裁剪的矩形区域的范围，横纵偏移量用来指定裁剪区域的相对位置（都为 0 时表示从最左上角开始）。 需要注意的是，当用 -crop 选项裁剪 PNG 和 GIF 格式的图片时（这两种格式的图片包含“虚拟画布”），并不是以画面的实际像素为基准，而是需要参考“画布”的大小和位置，所以有时候并不会达到预期的效果。详细介绍可参考官方文档 Cutting and Bordering 。 shave裁剪图片有时候可以采取相反的思路，即剔除图片中不需要的部分：$ convert image.png -shave 100x50 out.png将输入图片的左右两边剔除 100 像素，上下两边剔除 50 像素，获取剩余的部分并输出。 色彩、亮度与饱和度convert 命令可以通过 -modulate 选项调整图片的色彩、亮度和饱和度。如：$ convert image.png -modulate 150,100,100 out.png 上述命令会将原图片的亮度增大为原来的 150% 。其中 150,100,100 三个数值分别表示亮度、饱和度和色相。100 为基准值，即大于 100 表示增强某种属性，小于 100 表示减弱某种属性。 透明度、色彩与位深度 将透明（alpha）通道替换为纯黑色：$ convert image.png -flatten out.png 将原彩色图片转为灰度模式：$ convert image.png -type Grayscale out.png 降低图片的位深度（bits per pixel）：$ convert image.png -depth 8 out.png 减少图片色彩：$ convert image.png -dither -colors 256 out.png PS：dither 会增加像素点，如需要在减少色彩的同时不应用 dither 效果，将命令中的 -dither 替换为 +dither 。 锐化与虚化锐化$ convert image.png -sharpen 2 out.png 模糊$ convert image.png -blur 1 out.png 添加文字和边框添加水印$ convert image.jpg -fill red -draw &quot;text 20 20 &#39;© 2019 example.com&#39;&quot; out.jpg 可以自行定义添加文字的位置（默认为左上角）和字体类型：$ convert image.jpg -fill red -gravity SouthEast -font arial -draw &quot;text 20 20 &#39;© 2019 example.com&#39;&quot; out.jpg 添加边框$ convert image.png -bordercolor blue -border 50 out.png其中 -bordercolor 用于指定边框颜色，-border 用于指定边框宽度，可以为百分比。 组合叠加组合多张图片（垂直方向）$ convert x1.png x2.png x3.png -append out.png 组合多张图片（水平方向）$ convert x1.png x2.png x3.png +append out.png 叠加图片$ composite -gravity center img1.png img2.png out.png将 img1.png 叠加到 img2.png 上并作为 out.png 输出（方位为正中间) 应用效果ImageMagick 可以对图片应用多种样式的特效。如 “charcoal” 效果：$ convert image.png -charcoal 2 out.png其中 -charcoal 后面的数字 2 用于指定该效果的强度。 “implode” 效果：$ convert image.png -implode 1 out.png 结合多种操作前面提到的多种处理方式实际上可以任意组合使用，使得只用一条命令即可以同时完成多种操作。如：$ convert image.png -resize 400x400 -rotate 180 -charcoal 4 -quality 95 out.jpg 批量处理借助 Bash 脚本（Linux 系统）的强大功能，ImageMagick 可以很方便的批量处理多张图片。如下面的命令可以查找当前目录下所有的 PNG 图片，将它们每一张都旋转 90°，再将原文件名添加 “rotated-” 前缀后保存：$ for file in *.png; do convert $file -rotate 90 rotated-$file; done Windows 系统上是没有原生的 Bash Shell 的，但是可以下载安装某些软件以支持 Bash 环境，比如 Git for Windows。其实使用 Windows 系统自带的 PowerShell 也可以完成同样的批量操作：PS &gt; dir *.png | foreach { convert $_ -rotate 90 rotated-$($_.basename).png } 有兴趣的话，可以多玩几下。 GIF 动图制作可以借助 convert 命令将多张图片组合为一张重复播放的 GIF 动图：$ convert -delay 20 -loop 0 *.png out.gif 其中 -delay 选项用于指定图片切换的时间间隔，单位为毫秒。 参考文章ImageMagick TutorialImageMagick basics 拓展阅读Examples of ImageMagick Usage（很详细）]]></content>
      <categories>
        <category>Tools</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Tools</tag>
        <tag>Shell</tag>
        <tag>Tricks</tag>
        <tag>Image</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python Tricks —— 简单的 HTTP 和 FTP 服务]]></title>
    <url>%2F2019%2F04%2F07%2Fpython-simple-http-and-ftp-service%2F</url>
    <content type="text"><![CDATA[之前有位同事，需要在两台电脑间传输一个很大的文件，前提是只开放了 80 端口。我以为他会配个 Apache 之类的 HTTP 服务或者 WebDAV 什么的。他只用了一条命令：python -m http.server 80。后来我想也是，有时候面对问题，一个很简单的且还算完美的方案，即便不是通用的或者完备的，就解决问题而言，未尝不是一个好的选择。 http.serverhttp.server) 是 Python3 的一个内置模块，源代码在 Lib/http/server.py 中，它定义了用来实现 HTTP 服务的类。 其中的 SimpleHTTPRequestHandler 类可以用来在当前目录下创建一个基本的 HTTP 服务：12345678910import http.serverimport socketserverPORT = 8000Handler = http.server.SimpleHTTPRequestHandlerwith socketserver.TCPServer((&quot;&quot;, PORT), Handler) as httpd: print(&quot;serving at port&quot;, PORT) httpd.serve_forever() 运行效果如下：12345$ python test.pyserving at port 8000127.0.0.1 - - [06/Apr/2019 22:55:26] &quot;GET / HTTP/1.1&quot; 200 -127.0.0.1 - - [06/Apr/2019 22:55:41] &quot;GET /pyqt/ HTTP/1.1&quot; 200 -127.0.0.1 - - [06/Apr/2019 22:55:56] &quot;GET /test.py HTTP/1.1&quot; 200 - 因为当前目录下没有 index.html 文件，所以显示的是类似 FTP 站点上的那种文件列表。 当然，http.server 也可以通过 Python 的 -m 选项在命令行中直接调用：$ python -m http.server 默认会在 8000 端口打开 HTTP 服务，也可以手动指定端口：$ python -m http.server 8080 同时，该服务会绑定本机的所有网络接口（0.0.0.0），也可以手动指定：$ python -m http.server 8000 -b 127.0.0.1 其他支持的命令行选项可以参考帮助信息：123456789101112131415$ python -m http.server -husage: server.py [-h] [--cgi] [--bind ADDRESS] [--directory DIRECTORY] [port]positional arguments: port Specify alternate port [default: 8000]optional arguments: -h, --help show this help message and exit --cgi Run as CGI Server --bind ADDRESS, -b ADDRESS Specify alternate bind address [default: all interfaces] --directory DIRECTORY, -d DIRECTORY Specify alternative directory [default:current directory] pyftpdlibpyftpdlib 是一个非常高效的异步的 FTP 库，可以为 Python 编写 FTP 服务提供高级的可移植的编程接口。pyftpdlib 是 Python 的第三方库，需要使用包管理器安装：pip install pyftpdlib 安装成功以后，一个最基本的 FTP 服务器可以通过以下代码实现：12345678910111213141516171819202122232425262728293031323334353637383940import osfrom pyftpdlib.authorizers import DummyAuthorizerfrom pyftpdlib.handlers import FTPHandlerfrom pyftpdlib.servers import FTPServerdef main(): # 初始化验证器 authorizer = DummyAuthorizer() # 创建有读写权限的用户和只读权限的匿名用户 authorizer.add_user(&apos;user&apos;, &apos;12345&apos;, &apos;.&apos;, perm=&apos;elradfmwMT&apos;) authorizer.add_anonymous(os.getcwd()) # 实例化 FTPHandler 类 handler = FTPHandler handler.authorizer = authorizer # 自定义提示信息 handler.banner = &quot;pyftpdlib based ftpd ready.&quot; # Specify a masquerade address and the range of ports to use for # passive connections. Decomment in case you&apos;re behind a NAT. #handler.masquerade_address = &apos;151.25.42.11&apos; #handler.passive_ports = range(60000, 65535) # FTP 服务监听于 0.0.0.0:2121 address = (&apos;&apos;, 2121) server = FTPServer(address, handler) # 连接限制 server.max_cons = 256 server.max_cons_per_ip = 5 # 开启 FTP 服务 server.serve_forever()if __name__ == &apos;__main__&apos;: main() 效果如下：123456789$ python ftp.py[I 2019-04-07 15:08:37] &gt;&gt;&gt; starting FTP server on :::2121, pid=9952 &lt;&lt;&lt;[I 2019-04-07 15:08:37] concurrency model: async[I 2019-04-07 15:08:37] masquerade (NAT) address: None[I 2019-04-07 15:08:37] passive ports: None[I 2019-04-07 15:08:44] ::1:63265-[] FTP session opened (connect)[I 2019-04-07 15:08:44] ::1:63265-[anonymous] USER &apos;anonymous&apos; logged in.[I 2019-04-07 15:08:44] ::1:63265-[anonymous] CWD D:\Program\python 250[I 2019-04-07 15:08:44] ::1:63265-[anonymous] FTP session closed (disconnect). pyftpdlib 从 0.6.0 版本开始是完全支持 FTPS (FTP over TLS/SSL) 的，需要提前安装好 PyOpenSSL 模块：pip install pyopenssl 示例代码如下：12345678910111213141516171819202122232425&quot;&quot;&quot;An RFC-4217 asynchronous FTPS server supporting both SSL and TLS.Requires PyOpenSSL module (http://pypi.python.org/pypi/pyOpenSSL).&quot;&quot;&quot;from pyftpdlib.servers import FTPServerfrom pyftpdlib.authorizers import DummyAuthorizerfrom pyftpdlib.handlers import TLS_FTPHandlerdef main(): authorizer = DummyAuthorizer() authorizer.add_user(&apos;user&apos;, &apos;12345&apos;, &apos;.&apos;, perm=&apos;elradfmwMT&apos;) authorizer.add_anonymous(&apos;.&apos;) handler = TLS_FTPHandler handler.certfile = &apos;keycert.pem&apos; handler.authorizer = authorizer # requires SSL for both control and data channel #handler.tls_control_required = True #handler.tls_data_required = True server = FTPServer((&apos;&apos;, 21), handler) server.serve_forever()if __name__ == &apos;__main__&apos;: main() TLS_FTPHandler 类需要至少一个 certfile 和一个可选的 keyfile。可以参考 Apache FAQs 自行生成证书文件，也可以直接下载 pyftpdlib 提供的示例文件 keycert.pem。 FTP 客户端可以使用 WinSCP，截图如下： 运行效果：12345678$ python ftps.py[I 2019-04-07 17:20:24] &gt;&gt;&gt; starting FTP+SSL server on :::21, pid=5028 &lt;&lt;&lt;[I 2019-04-07 17:20:24] concurrency model: async[I 2019-04-07 17:20:24] masquerade (NAT) address: None[I 2019-04-07 17:20:24] passive ports: None[I 2019-04-07 17:20:36] ::1:64934-[] FTP session opened (connect)[I 2019-04-07 17:20:36] ::1:64934-[user] USER &apos;user&apos; logged in.[I 2019-04-07 17:20:36] ::1:64934-[user] CWD D:\Program\python 250 同 http.server 类似，pyftpdlib 也可以在命令行中通过 python -m 直接调用：12345$ python -m pyftpdlib[I 2019-04-07 17:27:51] &gt;&gt;&gt; starting FTP server on 0.0.0.0:2121, pid=1100 &lt;&lt;&lt;[I 2019-04-07 17:27:51] concurrency model: async[I 2019-04-07 17:27:51] masquerade (NAT) address: None[I 2019-04-07 17:27:51] passive ports: None 匿名用户有写权限：$ python -m pyftpdlib -w 设置特定的监听地址、端口号和 home 目录：$ python -m pyftpdlib -i localhost -p 8021 -d /home/someone 设置登录用户和密码（匿名用户则会被禁用）$ python -m pyftpdlib -u username -P password 获取帮助信息：$ python -m pyftpdlib -h 拓展资料http.server 源码http.server 文档pyftpdlib 文档]]></content>
      <categories>
        <category>Tools</category>
      </categories>
      <tags>
        <tag>Admin</tag>
        <tag>Tools</tag>
        <tag>Tricks</tag>
        <tag>Python</tag>
        <tag>Programming</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[视频处理工具 FFmpeg 常用实例详解]]></title>
    <url>%2F2019%2F03%2F10%2Fffmpeg-user-manual-and-practical-examples%2F</url>
    <content type="text"><![CDATA[FFmpeg 是一个专业的多媒体框架，能够解码、编码、转码、复用、解复用、流式传输、过滤和播放几乎所有格式的媒体文件。其核心就是 FFmpeg 程序本身，是一个基于命令行的视频和音频处理工具，多用于视频转码、基础编辑（修剪和合并）、视频缩放、后期效果制作等场景。这里通过一些示例简单地介绍下 ffmpeg 命令的基本使用。 一、获取详细信息ffmpeg -i &lt;inputfile&gt; -hide_banner其中 -hide_banner 选项用于在输出文件的详细信息时省略 ffmpeg 的版本信息和编译选项等。12345678910111213141516171819202122232425262728$ ffmpeg -i bbb.mp4 -hide_bannerInput #0, mov,mp4,m4a,3gp,3g2,mj2, from &apos;bbb.mp4&apos;: Metadata: major_brand : isom minor_version : 1 compatible_brands: isomavc1 creation_time : 2013-12-17T16:40:26.000000Z title : Big Buck Bunny, Sunflower version artist : Blender Foundation 2008, Janus Bager Kristensen 2013 comment : Creative Commons Attribution 3.0 - http://bbb3d.renderfarming.net genre : Animation composer : Sacha Goedegebure Duration: 00:10:34.53, start: 0.000000, bitrate: 8487 kb/s Stream #0:0(und): Video: h264 (High) (avc1 / 0x31637661), yuv420p, 3840x2160 [SAR 1:1 DAR 16:9], 8002 kb/s, 60 fps, 60 tbr, 60k tbn, 120 tbc (default) Metadata: creation_time : 2013-12-17T16:40:26.000000Z handler_name : GPAC ISO Video Handler Stream #0:1(und): Audio: mp3 (mp4a / 0x6134706D), 48000 Hz, stereo, fltp, 160 kb/s (default) Metadata: creation_time : 2013-12-17T16:40:28.000000Z handler_name : GPAC ISO Audio Handler Stream #0:2(und): Audio: ac3 (ac-3 / 0x332D6361), 48000 Hz, 5.1(side), fltp, 320 kb/s (default) Metadata: creation_time : 2013-12-17T16:40:28.000000Z handler_name : GPAC ISO Audio Handler Side data: audio service type: mainAt least one output file must be specified 二、格式转换ffmpeg -i &lt;inputfile&gt; &lt;outputfile&gt; FFmpeg 是一个强大的音频和视频格式转换器，几乎支持当前所有常用的格式，如：$ ffmpeg -i input.avi output.mp4 或者经常需要用到的，将视频文件转为 GIF 动图：$ ffmpeg -i input.mp4 output.gif 如果在格式转换时需要保留源视频的质量，可以添加上 -qscale 0 选项（-qscale 的值越低，输出视频的质量越高）：$ ffmpeg -i input.webm -qscale 0 output.mp4 可以使用 -formats 选项列出 ffmpeg 命令支持的所有格式（很长很长的一个列表。。。）：123456789101112131415161718192021222324252627282930313233$ ffmpeg -formats -hide_bannerFile formats: D. = Demuxing supported .E = Muxing supported -- D 3dostr 3DO STR E 3g2 3GP2 (3GPP2 file format) E 3gp 3GP (3GPP file format) D 4xm 4X Technologies E a64 a64 - video for Commodore 64 D aa Audible AA format files D aac raw ADTS AAC (Advanced Audio Coding) DE ac3 raw AC-3 D acm Interplay ACM D act ACT Voice file format D adf Artworx Data Format D adp ADP D ads Sony PS2 ADS E adts ADTS AAC (Advanced Audio Coding) DE adx CRI ADX D aea MD STUDIO audio D afc AFC DE aiff Audio IFF D aix CRI AIX DE alaw PCM A-law D alias_pix Alias/Wavefront PIX image DE amr 3GPP AMR D amrnb raw AMR-NB D amrwb raw AMR-WB D anm Deluxe Paint Animation D apc CRYO APC D ape Monkey&apos;s Audio... 三、指定编码可以通过 -c 选项手动指定输出文件的编码，如：$ ffmpeg -i input.mp4 -c:v vp9 -c:a libvorbis output.mkv其中 -c:v 用于指定视频编码，-c:a 指定音频编码 PS：视频文件的后缀如 mp4、mkv、avi 等只是表示用来装载媒体流的“容器”类型，而编码时使用的编码方式则另需指定。当然很多时候 ffmpeg 会根据输出文件的后缀自行选择默认的编码方式，无需手动指定。 只改变视频或者音频流的编码可以在指定编码时，只改变视频或者音频编码中的一项，另一项则保持原来的格式：$ ffmpeg -i input.webm -c:v copy -c:a flac output.mkv-c:v copy 表示复制输入文件中的视频流到输出文件，不重新进行编码 只改变文件后缀即输入文件中的视频流和音频流同时复制到输出文件，只改变文件后缀：$ ffmpeg -i input.webm -c:av copy output.mkv 编码列表查看 FFmpeg 支持的所有音视频编码格式（又一个很长的列表。。。）：123456789101112131415161718192021222324252627$ ffmpeg -codecs -hide_bannerCodecs: D..... = Decoding supported .E.... = Encoding supported ..V... = Video codec ..A... = Audio codec ..S... = Subtitle codec ...I.. = Intra frame-only codec ....L. = Lossy compression .....S = Lossless compression ------- ... DEV.L. flv1 FLV / Sorenson Spark / Sorenson H.263 (Flash Video) (decoders: flv ) (encoders: flv ) D.V..S fmvc FM Screen Capture Codec D.VI.S fraps Fraps D.VI.S frwu Forward Uncompressed D.V.L. g2m Go2Meeting D.V.L. gdv Gremlin Digital Video DEV..S gif GIF (Graphics Interchange Format) DEV.L. h261 H.261 DEV.L. h263 H.263 / H.263-1996, H.263+ / H.263-1998 / H.263 version 2 D.V.L. h263i Intel H.263 DEV.L. h263p H.263+ / H.263-1998 / H.263 version 2 DEV.LS h264 H.264 / AVC / MPEG-4 AVC / MPEG-4 part 10 (decoders: h264 h264_qsv h264_cuvid ) (encoders: libx264 libx264rgb h264_amf h264_nvenc h264_qsv nvenc nvenc_h264 ) DEVIL. hap Vidvox Hap DEV.L. hevc H.265 / HEVC (High Efficiency Video Coding) (decoders: hevc hevc_qsv hevc_cuvid ) (encoders: libx265 nvenc_hevc hevc_amf hevc_nvenc hevc_qsv )... 四、视频压缩编码与比特率有些时候，基于磁盘空间和网络传输的考虑，需要对视频文件进行压缩处理。其中一种方法就是改变视频的比特率。在某些情况下，比特率的适当缩减对视频的观看效果并不会产生太大的影响（人眼察觉的范围内）。当然编码的选择也会对输出文件的大小产生一定的影响，示例如下： $ ffmpeg -i input.webm -c:a copy -c:v vp9 -b:v 1M output.mkv-b:v 用于指定视频的比特率。 帧率另一种方式就是改变视频文件的帧率，也就是人们常常提到的FPS。 $ ffmpeg -i input.webm -c:a copy -c:v vp9 -r 30 output.mkv-r 30 选项用于指定输出视频的帧率为 30 FPS。 分辨率视频的分辨率也会影响文件的大小，可以使用 -s 选项指定输出文件的分辨率。当然，视频的画幅大小也会产生相应的变化： $ ffmpeg -i input.mkv -c:a copy -s hd720 output.mkv或$ ffmpeg -i input.mkv -c:a copy -s 1280x720 output.mkv 五、提取音频通过格式转换，FFmpeg 可以直接将视频文件转为音频文件，只需要指定输出文件的格式为 .mp3 或 .ogg 等。如：$ ffmpeg -i input.mp4 output.mp3 同时，也可以在转换时指定音频的格式选项：$ ffmpeg -i input.mp4 -vn -ar 44100 -ac 2 -ab 320 -f mp3 output.mp3 其中：-vn ：指定输出文件中禁用视频-ar ：指定输出文件中音频的采样率-ac：指定音频的通道数-ab：指定音频的比特率-f：指定输出文件的格式 六、常用实用命令集锦调整分辨率将某视频文件的分辨率改为 1280x720：$ ffmpeg -i input.mp4 -filter:v scale=1280:720 output.mp4或者：$ ffmpeg -i input.mp4 -s 1280x720 output.mp4 压缩视频文件$ ffmpeg -i input.mp4 -vf scale=1280:-1 -c:v libx264 -preset veryslow -crf 24 output.mp4 也可以添加如下选项同时对音频流进行压缩：-c:a aac -strict -2 -b:a 128k 移除音频$ ffmpeg -i input.mp4 -an output.mp4-an 选项表示在输出文件中禁用音频 提取图片$ ffmpeg -i input.mp4 -r 1 -f image2 image-%2d.png 其中各选项的含义：-r ：设置帧率，即每秒有多少帧画面被提取到图片中。默认为 25-f ：指定输出的格式。本例中为图片（image2）-image-%2d.png ：指定提取出的图片的命名方式。本例中最终的命名为 image-01.png、image-02.png 等。如使用 image-%3d.png ，则最终的命名为 image-001.png、imag-002.png 等 裁剪视频即截取指定范围内的视频画面，裁切掉多余的部分：$ ffmpeg -i input.mp4 -vf &quot;crop=w:h:x:y&quot; output.mp4 其中 crop=w:h:x:y 用于指定“裁剪框”的大小和位置。w 表示裁剪部分的宽度（默认为源视频的宽度 iw）；h 表示裁剪部分的高度（默认为源视频的高度 ih；x 表示 x 轴上裁剪的起始位置（最左边为 0，默认为源视频的中间位置）；y 表示 y 轴上裁剪的起始位置（最顶部为 0，默认为源视频的中间位置）。 改变视频比例视频比例即视频画幅的长宽比，也就是通常所说的 4:3 和 16:9 等。$ ffmpeg -i input.mp4 -aspect 16:9 output.mp4 设置音频封面即创建以一张静止的图片为画面的视频。 $ ffmpeg -loop 1 -i inputimage.jpg -i inputaudio.wav -c:v libx264 -tune stillimage -c:a aac -b:a 192k -shortest output.mp4其中的选项和参数可以根据需求自行修改和省略。 截取视频片段$ ffmpeg -i input.mp4 -ss 00:00:50 -codec copy -t 60 output.mp4截取视频中从第 50 秒开始，持续时间为一分钟的视频片段。 其中 -ss 用于指定视频片段的开始时间；-t 指定视频片段的持续时间，单位都为秒。 也可以使用如下方式：$ ffmpeg -i audio.mp3 -ss 00:01:54 -to 00:06:53 -c copy output.mp3 以上命令也适用于音频文件。 视频分割$ ffmpeg -i input.mp4 -t 00:00:30 -c copy part1.mp4 -ss 00:00:30 -codec copy part2.mp4将输入的视频文件分割为两段，第一段为从最开始到第 30 秒；第二段为第 30 秒到视频结束。 其中 -t 00:00:30 前面省略了 -ss 00:00:00；-ss 00:00:30 后面省略了 -t 剩余时间。 有点类似于截取多个连续的视频片段。 视频合并首先创建包含各媒体文件路径列表的文本文件 join.txt ：123file &apos;~/myvideos/part1.mp4&apos;file &apos;~/myvideos/part2.mp4&apos;file &apos;~/myvideos/part3.mp4&apos; 使用 -f concat 选项对多个视频进行合并：$ ffmpeg -f concat -i join.txt -c copy output.mp4 添加字幕文件$ ffmpeg -i input.mp4 -i subtitle.srt -map 0 -map 1 -c copy -c:v libx264 -crf 23 -preset veryfast output.mp4 改变视频播放速度（音频不受影响）$ ffmpeg -i input.mp4 -vf &quot;setpts=0.5*PTS&quot; output.mp4 上述命令会加快视频画面的播放速度，音频播放速度不变。 如果想放慢视频画面的切换速度，可以相应地将 setpts=0.5*PTS 中的 0.5 改为大于 1 的数值。 Padding即宽银幕视频中上下的两道“黑边”，可以使用 FFmpeg 命令添加类似的效果：1$ ffmpeg -i input.mp4 -vf &quot;scale=1920:1080:force_original_aspect_ratio=decrease,pad=1920:1080:(ow-iw)/2:(oh-ih)/2:black&quot; output.mp4 该效果由 -vf 选项的 pad 参数指定，可以根据情况自行修改。 从图片创建视频$ ffmpeg -framerate 1 -i img%02d.jpg -c:v libx264 -r 30 -pix_fmt yuv420p output.mp4把当前目录下的多张图片（名字为 img01.jpg、img02.jpg 的形式）组合为一个视频文件，效果类似于自动播放的 PPT。每秒切换一张图片。 $ ffmpeg -framerate 30 -i img%02d.jpg -c:v libx264 -pix_fmt yuv420p output.mp4也是将当前目录下的多张图片组合成一个完整的视频，该视频帧率为 30 FPS。每帧切换一张图片。 参考资料20 FFmpeg Commands For BeginnersHow to create a video from images with FFmpeg?]]></content>
      <categories>
        <category>Tools</category>
      </categories>
      <tags>
        <tag>Tools</tag>
        <tag>Media</tag>
        <tag>Video</tag>
        <tag>FFmpeg</tag>
        <tag>Audio</tag>
        <tag>Script</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Windows PowerShell 学习笔记其三（字符串与数组）]]></title>
    <url>%2F2019%2F03%2F10%2Fwindows-powershell-cookbook-3%2F</url>
    <content type="text"><![CDATA[字符串介绍两种类型PowerShell 下的字符串主要有两种类型：非扩展（nonexpanding）和扩展（expanding）型。这里的“扩展”和“非扩展”指的是对字符串中包含的变量和转义符是否进行解析。扩展型字符串需要用双引号括起来，而非扩展型字符串使用单引号。 实际效果如下：123456PS &gt; $username = &quot;starky&quot;PS &gt; echo &quot;hi, my name is $username. `nAnother line&quot;hi, my name is starky.Another linePS &gt; echo &apos;hi, my name is $username. `nwithin the same line&apos;hi, my name is $username. `nwithin the same line 在上面的例子中，双引号包裹的字符串对变量 $username 和转义符 `n（等同于 bash 里的 \n）进行了解析，自动替换为变量和转义符代表的内容。而单引号包裹的字符串则只是将美元符、反引号等符号视为普通的字符，不做任何处理直接打印输出。 PS：PowerShell 里的转义符是反引号 ` 而不是 bash 里的反斜杠 多行格式化文本PowerShell 支持创建 here string，即多行格式化文本，类似 Python 里的三引号。只需要将多行文本包裹在成对的 @&quot; 和 &quot;@ 符号中即可。示例如下：123456789101112PS &gt; $mystring = @&quot;&gt;&gt; This is the first line&gt;&gt; of a very long string. A &quot;here string&quot;&gt;&gt; lets you create blocks of text&gt;&gt; that span several lines.&gt;&gt; &quot;@&gt;&gt;PS &gt; $mystringThis is the first lineof a very long string. A &quot;here string&quot;lets you create blocks of textthat span several lines. 字符串中的动态内容前面有提到过，由双引号包裹的字符串为“扩展”型字符串，可以对其中包含的变量等自动进行替换。其实也可以在字符串中，以 $(expression) 的格式插入表达式或一系列 PowerShell 命令，示例如下：12PS &gt; &quot;1 + 2 equals $(1 + 2)&quot;1 + 2 equals 3 还可以使用 PowerShell 的字符串格式化操作符在字符串中插入动态内容，它遵循和 .NET 一样的字符串格式化规则。示例如下：12345PS &gt; $header = &quot;Report for Today&quot;PS &gt; $mystring = &quot;&#123;0&#125;`n&#123;1&#125;&quot; -f $header,(&apos;-&apos; * $header.Length)PS &gt; $mystringReport for Today---------------- 示例 2：123456PS &gt; $number1 = 10PS &gt; $number2 = 32PS &gt; &quot;$number2 divided by $number1 is $($number2 / $number1)&quot;32 divided by 10 is 3.2PS &gt; &quot;&#123;0&#125; divided by &#123;1&#125; is &#123;2&#125;&quot; -f $number2, $number1, ($number2 / $number1)32 divided by 10 is 3.2 字符串操作检索与替换PowerShell 提供了多种用于字符串搜索和匹配的方法。 -like-like 操作符用于判断字符串是否匹配特定的模式，该模式中可以包含通配符： 12PS &gt; &quot;Hello World&quot; -like &quot;*llo W*&quot;True -match-match 操作符用于判断字符串是否匹配特定的正则表达式： 12PS &gt; &quot;Hello World&quot; -match &apos;.*l[l-z]o W.*$&apos;True Contains()Contains() 方法用于判断一个字符串是否包含了另一个较短的字符串： 12PS &gt; &quot;Hello World&quot;.Contains(&quot;World&quot;)True IndexOf()IndexOf() 方法可以用来获取一个字符串在另一个字符串中的位置索引： 12PS &gt; &quot;Hello World&quot;.IndexOf(&quot;World&quot;)6 Replace()Replace() 方法用于将字符串中的一部分替换为另一个字符串： 12PS &gt; &quot;Hello World&quot;.Replace(&quot;World&quot;, &quot;PowerShell&quot;)Hello PowerShell 另外，使用 PowerShell 的 -replace 操作符搭配上正则表达式，可以完成更加高级的替换任务：12PS &gt; &quot;Hello World&quot; -replace &apos;(.*) (.*)&apos;, &apos;$2 $1&apos;World Hello 分割、合并、修剪 -split-split 操作符可以用来将指定字符串分割成一系列的字符片段： 12345678PS &gt; &quot;a-b-c-d-e-f&quot; -split &quot;-c-&quot;a-bd-e-fPS &gt; &quot;a-b-c-d-e-f&quot; -split &quot;b|[d-e]&quot;a--c---f -join-join 操作符用于将多个字符片段合并为一个完整的字符串 123456PS &gt; $list = &quot;Hello&quot;,&quot;World&quot;PS &gt; $listHelloWorldPS &gt; $list -join &quot;, &quot;Hello, World Trim()Trim() 方法用于去除字符串两边的空白字符： 123PS &gt; $text = &quot; `t Test String`t `t&quot;PS &gt; &quot;|&quot; + $text.Trim() + &quot;|&quot;|Test String| 列表、数组与哈希表创建数组和列表创建和初始化一个数组在 PowerShell 里非常简单，用很常见的赋值语句即可：12345PS &gt; $myArray = 1,2,&quot;HelloWorld&quot;PS &gt; $myArray12HelloWorld 用上述方法创建的数组是没有数据类型限制的，即该数组在初始化时可以包含任何类型的数据。 创建指定长度的数组，可以使用 New-Object 命令：1234567891011121314PS &gt; $myArray = New-Object &quot;int32[]&quot; 4PS &gt; $myArray[3] = 3PS &gt; $myArray0003PS &gt; $myArray[4] = 4索引超出了数组界限。所在位置 行:1 字符: 1+ $myArray[4] = 4+ ~~~~~~~~~~~~~~~ + CategoryInfo : OperationStopped: (:) [], IndexOutOfRangeException + FullyQualifiedErrorId : System.IndexOutOfRangeException 创建指定类型的数组，可以使用 .NET 框架提供的强类型的集合：12345678910PS &gt; $list = New-Object Collections.Generic.list[Int]PS &gt; $list.add(10)PS &gt; $list.add(&quot;Hello&quot;)无法将“Add”的参数“item”(其值为“Hello”)转换为类型“System.Int32”:“无法将值“Hello”转换为类型“System.Int32”。错误:“输入字符串的格式不正确。””所在位置 行:1 字符: 1+ $list.add(&quot;Hello&quot;)+ ~~~~~~~~~~~~~~~~~~ + CategoryInfo : NotSpecified: (:) [], MethodException + FullyQualifiedErrorId : MethodArgumentConversionInvalidCastArgument 多维数组PowerShell 可以使用 @() 形式的语法创建多维数组：12345678PS &gt; $jagged = @(&gt;&gt; (1,2,3,4),&gt;&gt; (5,6,7,8)&gt;&gt; )PS &gt; $jagged[0][0]1PS &gt; $jagged[1][3]8 也可以使用下面的方式：123456789101112PS &gt; $multidimensional = New-Object &quot;int32[,]&quot; 2,4PS &gt; $multidimensional[0,0] = 1PS &gt; $multidimensional[1,3] = 8PS &gt; $multidimensional10000008 操作数组中的元素可以通过位置索引（从 0 开始）获取数组中的某个元素：12345PS &gt; $myArray = 1,2,&quot;Hello World&quot;PS &gt; $myArray[0]1PS &gt; $myArray[2]Hello World 当然也可以对数组进行分片操作，即获取数组中的某“一段”元素：1234567PS &gt; $myArray12Hello WorldPS &gt; $myArray[1..2]2Hello World 在对数组进行分片时，PowerShell 提供了如下的一个小技巧，可以对输出后的元素进行灵活的排序：12345678PS &gt; $myArray = 0,1,2,3,4,5PS &gt; $myArray[3..5 + 2 + 0..1]345201 Foreach-Object如果需要挨个访问数组中的每一个元素，可以使用 Foreach-Object 命令：12345PS &gt; $myArray = 1,2,3PS &gt; $sum = 0PS &gt; $myArray | Foreach-Object &#123; $sum += $_ &#125;PS &gt; $sum6 当然也可以稍微复杂点，通过位置索引和 for 循环访问数组的每一个元素：1234567PS &gt; $myArray = 1,2,3PS &gt; $sum = 0PS &gt; for($i = 0; $i -lt $myArray.Count; $i++) &#123;&gt;&gt; $sum += $myArray[$i]&gt;&gt; &#125;PS &gt; $sum6 排序使用 Sort-Object 命令可以对数组元素进行排序后再输出：123456789101112131415161718192021PS &gt; dir 目录: D:\Program\pythonMode LastWriteTime Length Name---- ------------- ------ ----d----- 2019/2/28 22:43 pyqtd----- 2019/3/3 0:17 speech-a---- 2019/3/6 22:13 26434 test.html-a---- 2019/3/6 22:00 174 test.mdPS &gt; Get-ChildItem | Sort-Object -Descending Length | select Name,LengthName Length---- ------test.html 26434test.md 174pyqtspeech 使用 Get-ChildItem 获取当前目录下所有文件的列表，再把该列表传递给 Sort-Object ，根据文件占用空间的大小（Length）逆序输出 Name 和 Length 项。 在使用 Sort-Object 对元素进行排序时，可以自由选择排序依据的条件。如根据首字母对字符串进行排序：12345PS &gt; &quot;Hello&quot;,&quot;World&quot;,&quot;And&quot;,&quot;Shell&quot; | Sort-ObjectAndHelloShellWorld 根据次字母对字符串进行排序：12345PS &gt; &quot;Hello&quot;,&quot;World&quot;,&quot;And&quot;,&quot;Shell&quot; | Sort-Object &#123; $_.Substring(1,1) &#125;HelloShellAndWorld 数组与运算符确定数组与元素的包含关系确定数组与元素的包含关系，可以使用 -contains 或者 -in 操作符：12345678PS &gt; &quot;Hello&quot;,&quot;World&quot; -contains &quot;Hello&quot;TruePS &gt; &quot;Hello&quot;,&quot;World&quot; -contains &quot;Shell&quot;FalsePS &gt; &quot;Hello&quot; -in &quot;Hello&quot;,&quot;World&quot;TruePS &gt; &quot;Shell&quot; -in &quot;Hello&quot;,&quot;World&quot;False 合并数组可以使用算术运算符 + 对数组进行合并操作：12345678910111213141516PS &gt; $firstArray = &quot;Element 1&quot;,&quot;Element 2&quot;,&quot;Element 3&quot;PS &gt; $secondArray = 1,2,3PS &gt; $firstArray + $secondArrayElement 1Element 2Element 3123PS &gt; $array = 1,2PS &gt; $array += 3,4PS &gt; $array1234 匹配数组中的元素可以使用 -eq、-match、-like 操作符对数组中的元素进行匹配：12345678910PS &gt; $array = &quot;Item 1&quot;,&quot;Item 2&quot;,&quot;Item 3&quot;,&quot;Item 1&quot;,&quot;Item 12&quot;PS &gt; $array -eq &quot;Item 1&quot;Item 1Item 1PS &gt; $array -like &quot;*1*&quot;Item 1Item 1Item 12PS &gt; $array -match &quot;Item ..&quot;Item 12 其中 -eq 表示完全匹配，-like 支持使用通配符，-match 支持正则表达式。 更复杂的匹配条件可以使用 Where-Object ：123PS &gt; $array = &quot;Item 1&quot;,&quot;Item 2&quot;,&quot;Item 3&quot;,&quot;Item 1&quot;,&quot;Item 12&quot;PS &gt; $array | Where-Object &#123; $_.Length -gt 6 &#125;Item 12 移除数组中的元素为了移除数组中符合特殊规则的元素，可以使用 -ne、-notlike、-notmatch 等比较操作符：12345678910111213PS &gt;$array = &quot;Item 1&quot;,&quot;Item 2&quot;,&quot;Item 3&quot;,&quot;Item 1&quot;,&quot;Item 12&quot;PS &gt;$array -ne &quot;Item 1&quot;Item 2Item 3Item 12PS &gt; $array -notlike &quot;*1*&quot;Item 2Item 3PS &gt; $array -notmatch &quot;Item ..&quot;Item 1Item 2Item 3Item 1 获取大于或小于特定值的元素-gt、-ge、-lt、-le 等比较操作符可以用来获取数组中大于或小于某个特定值的元素。 PS：其中 -gt 表示大于（great than），-ge 表示大于等于（great and equal），-lt 表示小于（less than），-le 表示小于等于（less and equal）。1234567PS &gt; $array = &quot;Item 1&quot;,&quot;Item 2&quot;,&quot;Item 3&quot;,&quot;Item 1&quot;,&quot;Item 12&quot;PS &gt; $array -ge &quot;Item 3&quot;Item 3PS &gt; $array -lt &quot;Item 2&quot;Item 1Item 1Item 12 ArrayList通过类似 $array = 1,2,3,4 这种赋值的方式创建的数组，其长度是固定的。可以通过位置索引访问其中的某个值，并对它重新赋值。但是不能直接添加或者删除数组中的元素：12345678910111213141516PS &gt; $array = 0,1,2,3PS &gt; $array[1]1PS &gt; $array[0]=1PS &gt; $array1123PS &gt; $array.Add(4)使用“1”个参数调用“Add”时发生异常:“集合的大小是固定的。”所在位置 行:1 字符: 1+ $array.add(4)+ ~~~~~~~~~~~~~ + CategoryInfo : NotSpecified: (:) [], MethodInvocationException + FullyQualifiedErrorId : NotSupportedException 如果需要在数组中添加或者删除元素，可以使用 +、-ne、-match、-gt 等比较操作符，获取数组中匹配某个条件的元素，并将它们赋值给新的变量，原数组中元素的值则不受影响：12345678910111213141516171819202122PS &gt; $original = 1,2,3,4PS &gt; $new1 = $original + 5PS &gt; $new112345PS &gt; $new2 = $original -ne 4PS &gt; $new2123PS &gt; $new3 = $original -gt 2PS &gt; $new334PS &gt; $original1234 在面对长度很大的数组时，上述的添加、移除、搜索等操作就会稍微显得效率较低。所以 PowerShell 提供了 ArrayList 数据类型，可以直接对数组本身进行添加、删除等操作：123456789101112131415PS &gt; $collection = New-Object System.Collections.ArrayListPS &gt; [void] $collection.Add(&quot;Hello&quot;)PS &gt; [void] $collection.AddRange((&quot;World&quot;,&quot;How&quot;,&quot;Are&quot;,&quot;You&quot;))PS &gt; $collectionHelloWorldHowAreYouPS &gt; $collection.RemoveAt(1)PS &gt; $collectionHelloHowAreYou [void] 可以省略操作执行后返回的状态值。 哈希表可以使用 @{} 形式的语法创建哈希表（或关联数组）。123456789PS &gt; $myHashtable = @&#123; Key1 = &quot;Value1&quot;; &quot;Key 2&quot; = 1,2,3 &#125;PS &gt; $myHashtable[&quot;New Item&quot;] = 5PS &gt; $myHashtableName Value---- -----Key 2 &#123;1, 2, 3&#125;Key1 Value1New Item 5 按哈希表的键或值排序1234567891011121314151617181920212223242526272829303132PS &gt; $myHashtable = @&#123;&#125;PS &gt; $myHashtable[&quot;Hello&quot;] = 3PS &gt; $myHashtable[&quot;Ali&quot;] = 2PS &gt; $myHashtable[&quot;Alien&quot;] = 4PS &gt; $myHashtable[&quot;Duck&quot;] = 1PS &gt; $myHashtableName Value---- -----Hello 3Duck 1Alien 4Ali 2PS &gt; $myHashtable.GetEnumerator() | Sort NameName Value---- -----Ali 2Alien 4Duck 1Hello 3PS &gt; $myHashtable.GetEnumerator() | Sort ValueName Value---- -----Duck 1Ali 2Hello 3Alien 4 参考书籍Windows PowerShell Cookbook, 3rd Edition]]></content>
      <categories>
        <category>Windows</category>
      </categories>
      <tags>
        <tag>Admin</tag>
        <tag>Tools</tag>
        <tag>Shell</tag>
        <tag>Windows</tag>
        <tag>Program</tag>
        <tag>Script</tag>
        <tag>PowerShell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[用 Python 实现自己的智能语音助理（百度语音 + 图灵机器人）]]></title>
    <url>%2F2019%2F03%2F03%2Fpython-voice-assistant%2F</url>
    <content type="text"><![CDATA[依稀记得去年生日，对着 Google 说 “Sing me Happy Birthday” 。她真的给我唱了英文版的生日歌，满怀深情地（我感觉……）。最后还加了一串调皮的鼓声。我转头对着公司的前台小姐姐说，看见没有，你的 Siri 不爱我。。。 呃，不瞎扯了。基于以上的渊源，我用 Python 写了一个还算得上智能的语音助理。 截图如下： 演示视频：用 Python 实现的智能语音机器人（一） 源代码 不要慌，用的现成的框架和公共 API，一百来行代码而已，权当游戏。 一、整体结构没有做过多的设计（不懂。。。），整体就是一个简单的线性结构，顺序执行。一次交互完毕后，从头开始重复执行。 SpeechRecognition（录音）--&gt; 百度语音（Speech-to-Text）--&gt; 图灵机器人（语义分析及应答）--&gt; 百度语音（Text-to-Speech）--&gt; PyAudio（音频播放） 二、SpeechRecognitionSpeechRecogintion 是 Python 的一个语音识别框架，已经对接了如谷歌和微软的 STT （语音转文本）服务。 本项目里的语音识别及合成用的是百度的开放服务，所以只是需要 SpeechRecogintion 的录音功能。它可以检测语音中的停顿自动终止录音并保存，比 PyAudio 更人性化（代码写起来也更简单）。 安装依赖库Windows安装 SpeechRecognition 需要提前装好 Python 的 PyAudio 框架。PyAudio 貌似需要编译安装，Windows 系统上估计会有点麻烦。 我用的是 Anaconda 软件，Windows 系统上用它管理 Python 包很方便。嫌这个软件太大的话，也有简化版的 Miniconda 。装好以后直接执行下面的命令即可（当然也可以在 conda 的虚拟环境里安装，不赘述）：conda install pyaudio PyAudio 装好以后，直接使用 Python 的包管理工具 pip 安装 SpeechRecognition 即可：pip install SpeechRecognition LinuxLinux 系统下就显得省事一点了。可以直接使用系统自带的包管理器安装 PyAudio （如 Ubuntu 和 Raspbian 系统的 apt-get）$ sudo apt-get install python3-pyaudio 当然也可以使用 pip 命令安装，不过需要提前装好编译用的依赖库 portaudio19 ：12$ sudo apt-get install portaudio19-dev$ pip install pyaudio 同样的，PyAudio 装好以后，安装 SpeechRecognition ：pip install SpeechRecognition 录音代码123456789101112import speech_recognition as srdef rec(rate=16000): r = sr.Recognizer() with sr.Microphone(sample_rate=rate) as source: print(&quot;please say something&quot;) audio = r.listen(source) with open(&quot;recording.wav&quot;, &quot;wb&quot;) as f: f.write(audio.get_wav_data())rec() 从系统麦克风拾取音频数据，采样率为 16000（貌似百度语音 API 最高就支持到 16k 的采样率）。之后把采集到的音频数据以 wav 格式保存在当前目录下的 recording.wav 文件中，供后面的程序使用。 录音完成后，可以找到录好的音频文件试听一下效果。 三、百度语音（STT）创建应用百度语音是百度云 AI 开放平台提供的支持语音识别和语音合成的服务，注册以后就可以直接访问它的 REST API 了，并且有向普通用户提供免费的调用额度。 注册成功以后，进入语音服务的控制台创建一个新的应用，记下自己的 AppID、API Key 和 Secret Key。 语音识别代码百度 AI 有提供面向 Python 的框架 baidu-aip ，感觉就相当于重新打包以后的 requests 库，用来访问 REST API。这里简单起见，直接使用该框架。安装：pip install baidu-aip 语音识别代码如下（代码中的 Key 替换成自己的）：1234567891011121314151617181920212223from aip import AipSpeechAPP_ID = &apos;Your AppID&apos;API_KEY = &apos;Your API Key&apos;SECRET_KEY = &apos;Your Secret Key&apos;client = AipSpeech(APP_ID, API_KEY, SECRET_KEY)def listen(): with open(&apos;recording.wav&apos;, &apos;rb&apos;) as f: audio_data = f.read() result = client.asr(audio_data, &apos;wav&apos;, 16000, &#123; &apos;dev_pid&apos;: 1536, &#125;) result_text = result[&quot;result&quot;][0] print(&quot;you said: &quot; + result_text) return result_textlisten() 简单来说，将 SpeechRecognition 录制的音频上传至百度语音的服务，返回识别后的文本结果并输出。 四、图灵机器人图灵机器人是一个提供（一定额度内）免费的智能聊天服务的平台，注册以后就可以创建自己的聊天机器人并接入到项目中。 首先进入图灵机器人的控制台并创建一个新的聊天机器人，记下分配到的 apikey。 该平台也提供了开放的 REST API ，但是不像百度那样有打包自己的 SDK 。所以需要使用 Python 的 requests 库访问，代码如下：123456789101112131415161718192021222324252627282930313233343536import requestsimport jsonTURING_KEY = &quot;Your apikey&quot;URL = &quot;http://openapi.tuling123.com/openapi/api/v2&quot;HEADERS = &#123;&apos;Content-Type&apos;: &apos;application/json;charset=UTF-8&apos;&#125;def robot(text=&quot;&quot;): data = &#123; &quot;reqType&quot;: 0, &quot;perception&quot;: &#123; &quot;inputText&quot;: &#123; &quot;text&quot;: &quot;&quot; &#125;, &quot;selfInfo&quot;: &#123; &quot;location&quot;: &#123; &quot;city&quot;: &quot;杭州&quot;, &quot;street&quot;: &quot;网商路&quot; &#125; &#125; &#125;, &quot;userInfo&quot;: &#123; &quot;apiKey&quot;: TURING_KEY, &quot;userId&quot;: &quot;starky&quot; &#125; &#125; data[&quot;perception&quot;][&quot;inputText&quot;][&quot;text&quot;] = text response = requests.request(&quot;post&quot;, URL, json=data, headers=HEADERS) response_dict = json.loads(response.text) result = response_dict[&quot;results&quot;][0][&quot;values&quot;][&quot;text&quot;] print(&quot;the AI said: &quot; + result) return resultrobot(&quot;你好&quot;) 简单来说就是上传一个 json 格式的请求（包含聊天内容和个人信息等），获取到回复。再从收到的对象中提取出回复的文本。 五、百度语音（TTS）其实大部分系统都有内置的 TTS （即文本转语音）引擎，如 MacOS 的 say 命令，只不过其中有很多都显得太“机械”，呃，缺少“人情味儿”。。。 百度的 TTS 引擎语音效果听起来还是很卡哇伊（4 号选手度丫丫）的，比较超出我的预期。 测试代码如下：12345678910111213141516171819from aip import AipSpeechAPP_ID = &apos;Your AppID&apos;API_KEY = &apos;Your API Key&apos;SECRET_KEY = &apos;Your Secret Key&apos;client = AipSpeech(APP_ID, API_KEY, SECRET_KEY)def speak(text=&quot;&quot;): result = client.synthesis(text, &apos;zh&apos;, 1, &#123; &apos;spd&apos;: 4, &apos;vol&apos;: 5, &apos;per&apos;: 4, &#125;) if not isinstance(result, dict): with open(&apos;audio.mp3&apos;, &apos;wb&apos;) as f: f.write(result)speak(&quot;你好啊&quot;) 就是把需要转换成语音的文本内容上传，再将返回的数据保存在本地。貌似只能生成 mp3 格式。 六、PyAudio 播放这个我有点方。。。没找到 Python 播放 MP3 的合适的方法，所以用 os.system 调用系统中的 sox 命令将 MP3 转为 wav 格式，再用 PyAudio 播放。 sox 安装SoX 是一个强大的跨平台的音频处理工具，Linux 系统可以直接使用包管理器安装：$ sudo apt-get install sox libsox-fmt-mp3 Windows 系统安装的默认的 SoX 是不包含 mp3 格式支持的，所以需要自己编译（手动狗头）或者下载已经编译好的 dll 文件（libmad.dll 和 libmp3lame.dll，放置在 SoX 的安装目录下。最后将安装目录添加至系统的 PATH 环境变量即可。 代码如下：1234567891011121314151617181920212223242526272829303132import pyaudioimport waveimport osimport timedef play(): os.system(&apos;sox audio.mp3 audio.wav&apos;) wf = wave.open(&apos;audio.wav&apos;, &apos;rb&apos;) p = pyaudio.PyAudio() def callback(in_data, frame_count, time_info, status): data = wf.readframes(frame_count) return (data, pyaudio.paContinue) stream = p.open(format=p.get_format_from_width(wf.getsampwidth()), channels=wf.getnchannels(), rate=wf.getframerate(), output=True, stream_callback=callback) stream.start_stream() while stream.is_active(): time.sleep(0.1) stream.stop_stream() stream.close() wf.close() p.terminate()play() 七、最终代码及视频演示整合后的最终代码我就不再贴一遍了，100 行左右，已上传至 Github 。第二个视频：用 Python 实现的智能语音机器人（二） 聊天截图： 对了，这个是支持树莓派的。不过需要额外装一个USB音频驱动作为录音设备。参考树莓派3 音频配置及其应用场景（录音、VoIP 电话等） 参考资料SpeechRecognition百度语音图灵机器人PyAudioSoX]]></content>
      <categories>
        <category>Program</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Program</tag>
        <tag>Audio</tag>
        <tag>AI</tag>
        <tag>chatbot</tag>
        <tag>SpeechRecognition</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Windows PowerShell 学习笔记其二（变量与控制语句）]]></title>
    <url>%2F2019%2F02%2F28%2Fwindows-powershell-cookbook-2%2F</url>
    <content type="text"><![CDATA[重定向与管道重定向可以借助管道符和 Out-File 命令将某个命令的输出内容重定向至文本文件中。 如：Get-ChildItem | Out-File files.txt通过 Get-ChildItem（即 dir）获取当前目录下的文件列表，再借助管道和 Out-File 将列表保存在 files.txt 文件中。 在使用 Out-File 命令时可以带上 -Encoding 等选项来指定输出文件的编码等属性：Get-Content filename.cs | Out-File -Encoding ASCII file.txtGet-ChildItem | Out-File -Width 120 files.cs 或者也可以使用类似 bash 里的 &gt; 符号：Get-ChildItem &gt; files.txtGet-ChildItem 2&gt; errors.txtGet-ChildItem n&gt; otherStreams.txt 在文件末尾添加内容在使用 Out-File 命令的同时，可以附加上 -Append 选项用来指明在文件末尾添加内容，如：Get-ChildItem | Out-File -Append files.txt 同样也可以使用类似于 bash 中的 &gt;&gt; 符号：Get-ChildItem &gt;&gt; files.txt 管道简单来说，管道可以用来连接多个命令，使得上一个命令的输出作为下一个命令的输入，从而将多个命令以“首尾相接”的方式执行。 Get-Process | Where-Object WorkingSet -gt 100mb | Sort-Object -Descending WS获取系统当前的进程信息，并筛选出内存占用大于 100MB 的进程，再将筛选结果按照占用的内存由大到小排序后输出。12345678910PS C:\Users\starky&gt; Get-Process | Where-Object WorkingSet -gt 100mb | Sort-Object -Descending WSHandles NPM(K) PM(K) WS(K) CPU(s) Id SI ProcessName------- ------ ----- ----- ------ -- -- ----------- 997 75 342180 249576 1,265.59 8516 0 MsMpEng 1598 103 147168 203348 40.02 5992 3 SearchUI 806 29 167004 173960 186.02 5288 3 chrome 1757 182 75120 153712 195.45 13116 3 chrome 2816 134 89428 124596 126.25 1480 3 explorer 367 38 83872 116840 74.64 416 3 chrome 筛选（Where-Object）Where-Object 命令可以对某个列表内容或命令的输出应用各种类型的筛选条件。它的默认别名为 where 和 ?。 Get-Process | Where-Object { $_.Name -like &quot;Baidu*&quot; }获取当前系统中名字以“Baidu”开头的进程及其信息 上面的命令同时也可以这样表述：Get-Process | Where-Object Name -like &quot;Baidu*&quot; 即先通过 Get-Process 命令获取全部进程信息，再将它们传递给 Where-Object 命令。而 Where-Object 又指定每一个进程的 Name 属性（即进程名称）与模式 Baidu* 进行匹配，最后只输出匹配的结果。123456PS C:\Users\starky&gt; gps | where &#123; $_.Name -like &quot;Baidu*&quot; &#125;Handles NPM(K) PM(K) WS(K) CPU(s) Id SI ProcessName------- ------ ----- ----- ------ -- -- ----------- 1030 251 58288 98076 552.59 204 3 BaiduNetdisk 187 14 11952 8628 0.06 4856 3 BaiduNetdiskHost 其他示例如筛选未响应的进程：Get-Process | where { -not $_.Responding } 筛选已经停止的服务：Get-Process | where { $_.Status -eq &quot;Stopped&quot; } 遍历（Foreach-Object）Foreach-Object 命令（默认别名为 foreach 和 %）用于对某个列表中的每一个对象指定特定的操作。如：123456PS C:\Users\starky&gt; 1..5 | Foreach-Object &#123; $_ * 2 &#125;246810 又比如筛选当前目录下所有的文本文件，并去掉它们的只读属性：Get-ChildItem *.txt | Foreach-Object { attrib -r $_ } 其中 $_ 表示传递给 Foreach-Object 的每一个对象。attrib -r $_ 即表示对 Get-ChildItem *.txt 获取到的所有文本文件去除只读属性。 又如：12345PS C:\Users\starky&gt; $myArray = 1,2,3,4,5PS C:\Users\starky&gt; $sum = 0PS C:\Users\starky&gt; $myArray | Foreach-Object &#123; $sum += $_ &#125;PS C:\Users\starky&gt; $sum15 上面的命令也可以使用如下形式：123PS C:\Users\starky&gt; $myArray = 1,2,3,4,5PS C:\Users\starky&gt; $myArray | Foreach-Object &#123; $sum = 0 &#125; &#123; $sum += $_ &#125; &#123; $sum &#125;15 格式化输出PowerShell 中的许多命令默认情况下是以“表格”的形式来格式化输出的，如：123456PS C:\Users\starky&gt; Get-Process PowerShellHandles NPM(K) PM(K) WS(K) VM(M) CPU(s) Id ProcessName------- ------ ----- ----- ----- ------ -- ----------- 410 41 60444 75004 550 3.21 4212 powershell 428 43 60688 56684 561 4.04 5288 powershell 实际上在多数情况下，命令的“真实”输出包含了更加丰富的信息，可以使用 Format-List 命令比较一下效果：123456789101112131415161718192021222324252627PS C:\Users\starky&gt; Get-Process PowerShell | Format-List *__NounName : ProcessName : powershellHandles : 404VM : 575397888WS : 76754944PM : 61808640NPM : 41736Path : C:\WINDOWS\system32\WindowsPowerShell\v1.0\powershell.exeCompany : Microsoft CorporationCPU : 3.2136206FileVersion : 6.3.9600.16406 (winblue_gdr_oob.130926-1103)ProductVersion : 6.3.9600.16406Description : Windows PowerShellProduct : Microsoft® Windows® Operating SystemId : 4212PriorityClass : NormalHandleCount : 404WorkingSet : 76754944PagedMemorySize : 61808640PrivateMemorySize : 61808640VirtualMemorySize : 575397888TotalProcessorTime : 00:00:03.2136206BasePriority : 8... Format-List 是 4 种格式化输出的命令之一，其他还有 Format-Table、Format-Wide、Format-Custom。Format-List 用来接收输入内容并将其以列表的形式输出。 默认情况下，不带任何参数的格式化命令只会输出对象的一小部分属性，如：1234567891011PS C:\Users\starky&gt; Get-Process PowerShell | Format-ListId : 4212Handles : 428CPU : 3.4632222Name : powershellId : 5288Handles : 392CPU : 4.1028263Name : powershell 而 Format-List * 则会显示输入对象的所有属性。 同时，也可以在格式化命令后面手动指定需要显示的属性或参数，如：123456PS C:\Users\starky&gt; Get-Process PowerShell | Format-Table Id,Name,CPU,WS -Auto Id Name CPU WS -- ---- --- --4212 powershell 3.7128238 787046405288 powershell 4.1028263 58068992 变量与对象变量在 PowerShell 中，可以将命令的输出或其他内容先保存在某个变量（以 $ 符号为前缀）中，以供后续使用（甚至可以把变量直接传递给管道符）。 1234567891011PS C:\Users\starky&gt; $result = 2 + 2PS C:\Users\starky&gt; $result4PS C:\Users\starky&gt; $processes = Get-ProcessPS C:\Users\starky&gt; $processes.Count64PS C:\Users\starky&gt; $processes | Where-Object &#123; $_.ID -eq 0 &#125;Handles NPM(K) PM(K) WS(K) VM(M) CPU(s) Id ProcessName------- ------ ----- ----- ----- ------ -- ----------- 0 0 0 24 0 0 Idle 访问环境变量PowerShell 可以很轻松地访问系统中定义的环境变量，如使用 Get-ChildItem env: 命令获取当前系统已定义的所有环境变量的列表：12345678910111213141516PS C:\Users\starky&gt; Get-ChildItem env:Name Value---- -----ALLUSERSPROFILE C:\ProgramDataAPPDATA C:\Users\starky\AppData\RoamingCommonProgramFiles C:\Program Files\Common FilesCommonProgramFiles(x86) C:\Program Files (x86)\Common FilesCommonProgramW6432 C:\Program Files\Common FilesCOMPUTERNAME Starky-LenovoComSpec C:\windows\system32\cmd.exeFP_NO_HOST_CHECK NOHOMEDRIVE C:HOMEPATH \Users\starkyLOCALAPPDATA C:\Users\starky\AppData\Local... 也可以使用 $env:variablename 形式的变量名直接表示系统环境变量。几种形式的示例如下：12345678910111213141516PS C:\Users\starky&gt; Get-ChildItem env:usernameName Value---- -----USERNAME starkyPS C:\Users\starky&gt; Get-ChildItem Environment::usernameName Value---- -----USERNAME starkyPS C:\Users\H19038&gt; $env:usernamestarky 作用域创建一个只在特定范围内生效的变量，使用如下形式的语法：$SCOPE:variable = value 获取特定的作用域里某个变量的值，使用如下形式的语法：$SCOPE:variable 如创建一个在脚本执行完毕后仍旧有效的变量，使用 GLOBAL 作用域：$GLOBAL:variable = value 在某个函数内部更改脚本范围内的变量，需要显式地指定 SCRIPT 作用域：$SCRIPT:variable = value PowerShell 中变量的作用域，就是控制各变量在不同范围内的可见性。比如当进入一个代码块、函数或别名时，当前的作用域成为新的“本地作用域”（子作用域），原来的作用域则成为“父作用域”。子作用域可以访问父作用域中定义的所有变量，但是没有权限直接修改这些变量的值。换句话说，子作用域可以修改在父作用域中定义的变量，但是这种修改不会将新值自动同步到父作用域。 .NET 对象PowerShell 可以直接访问 .NET 对象的方法（包括静态方法和实例）和属性，比如访问某个静态方法：[ClassName]::MethodName(parameter list) 访问某个对象实例绑定的方法：$objectReference.MethodName(parameter list) 访问某个类的静态属性：[ClassName]::PropertyName 访问某个对象实例的属性：$objectReference.PropertyName 下面是一些具体的例子。 静态方法：12345PS C:\Users\starky&gt; [System.Diagnostics.Process]::GetProcessById(0)Handles NPM(K) PM(K) WS(K) VM(M) CPU(s) Id ProcessName------- ------ ----- ----- ----- ------ -- ----------- 0 0 0 24 0 0 Idle 实例方法：123PS C:\Users\starky&gt; $now = Get-DatePS C:\Users\starky&gt; $now.ToString()2019/2/27 15:34:21 静态属性：123PS C:\Users\starky&gt; [System.DateTime]::Now2019年2月27日 15:36:09 实例属性：123PS C:\Users\starky&gt; $today = Get-DatePS C:\Users\starky&gt; $today.DayOfWeekWednesday 创建对象的实例使用 New-Object 命令可以创建某个 .NET 对象的实例。如：123PS C:\Users\starky&gt; $generator = New-Object System.RandomPS C:\Users\starky&gt; $generator.NextDouble()0.697396862179691 也可以在创建实例的同时直接使用它：12345PS C:\Users\starky&gt; (New-Object Net.WebClient).DownloadString(&quot;http://www.baidu.com&quot;)&lt;!DOCTYPE html&gt;&lt;!--STATUS OK--&gt;&lt;html&gt;&lt;head&gt;... 通常的做法是，创建对象实例的同时，还要为其指定某些属性。如：1234567891011PS C:\Users\starky&gt; $startInfo = New-Object Diagnostics.ProcessStartInfo -Property @&#123;&gt;&gt; &apos;Filename&apos; = &quot;powershell.exe&quot;;&gt;&gt; &apos;WorkingDirectory&apos; = $HOMEPATH;&gt;&gt; &apos;Verb&apos; = &quot;RunAs&quot;&gt;&gt; &#125;&gt;&gt;PS C:\Users\starky&gt; [Diagnostics.Process]::Start($startInfo)Handles NPM(K) PM(K) WS(K) VM(M) CPU(s) Id ProcessName------- ------ ----- ----- ----- ------ -- ----------- 4 2 260 1224 6 0.00 6248 powershell 上述命令中创建 Diagnostics.ProcessStartInfo 对象的语句也可以简写为如下形式：12345$startInfo = [Diagnostics.ProcessStartInfo] @&#123; &apos;Filename&apos; = &quot;powershell.exe&quot;; &apos;WorkingDirectory&apos; = $HOMEPATH; &apos;Verb&apos; = &quot;RunAs&quot;&#125; 有时候为了简写 .NET 类的完整名称，可以借助变量赋值，如：12345PS C:\Users\starky&gt; $math = [System.Math]PS C:\Users\starky&gt; $math::Min(1,10)1PS C:\Users\starky&gt; $math::Sin(3.14)0.00159265291648683 循环与流程控制比较与逻辑运算PowerShell 支持的比较运算符：-eq, -ne, -gt, -in, -notin, -lt, -le, -like, -notlike, -match, -notmatch, -contains, -notcontains, -is, -isnot 逻辑运算符：-and, -or, -xor, -not 逻辑与比较运算符可以用来在数据间进行比较，同时也可以测试当前某些特定的条件是否成立。如判断当前目录下的文件个数是否大于等于 4：12PS C:\Users\starky&gt; (dir).Count -ge 4True 某个字符串是否匹配特定的正则表达式：12PS C:\Users\starky&gt; &quot;Hello World&quot; -match &quot;H.*World&quot;True 默认情况下，PowerShell 里的比较运算是区分大小写的，如果想不区分大小写，可以使用如下版本的比较运算符：-ceq, -cne, -cge, -cgt, -cin, -clt, -cle, -clike, -cnotlike, -cmatch, -cnotmatch, -ccontains, -cnotcontains 逻辑运算符可以组合多个值为 true 或 false 的语句，并根据运算符号的不同返回特定的逻辑运算结果。比如判断某个字符串是否匹配特定模式，且字符串长度大于 10：123PS C:\Users\starky&gt; $data = &quot;Hello World&quot;PS C:\Users\starky&gt; ($data -like &quot;*llo W*&quot;) -and ($data.Length -gt 10)True 条件语句PowerShell 中的 if 语句基本用法如下：123456789101112131415161718$temperature = 35if($temperature -le 0)&#123; &quot;Freezing&quot;&#125;elseif($temperature -le 10)&#123; &quot;Cold&quot;&#125;elseif($temperature -le 20)&#123; &quot;Warm&quot;&#125;else&#123; &quot;Hot&quot;&#125; 除了流程控制，条件语句也经常用来对变量进行赋值，形式如下：123456PS C:\Users\starky&gt; $result = if(Get-Process -n notepad) &#123; &quot;Running&quot; &#125; else &#123; &quot;Not running&quot; &#125;Get-Process : 找不到名为“notepad”的进程。请验证该进程名称，然后再次调用 cmdlet。...PS C:\Users\starky&gt; $resultNot running 通常情况下，使用 switch 语句可以替代包含大量 if ... elseif ... else 的语句。PowerShell 的 switch 在对用户输入进行条件判断时，支持多种选项的使用，如通配符、正则表达式甚至简短的代码块，相比于 C 和 C++ 中的 switch 语句更显得强大。 12345678910$temperature = 35switch($temperature)&#123; &#123; $_ -lt 0 &#125; &#123; &quot;Below Freezing&quot;; break &#125; 32 &#123; &quot;Exactly Freezing&quot;; break &#125; &#123; $_ -le 10 &#125; &#123; &quot;Cold&quot;; break &#125; &#123; $_ -le 20 &#125; &#123; &quot;Warm&quot;; break &#125; default &#123; &quot;Hot&quot; &#125;&#125; 循环for 循环：1234for($counter = 1; $counter -le 10; $counter++)&#123; &quot;Loop number $counter&quot;&#125; foreach 循环：1234foreach($file in dir)&#123; &quot;File name: &quot; + $file.Name&#125; 或者：dir | foreach { &quot;File name: &quot; + $_.Name } while 循环：12345$response = &quot;&quot;while($response -ne &quot;QUIT&quot;)&#123; $response = Read-Host &quot;Type something&quot;&#125; do..while 循环：12345$response = &quot;&quot;do&#123; $response = Read-Host &quot;Type something&quot;&#125; while($response -ne &quot;QUIT&quot;) do..until 循环：12345$response = &quot;&quot;do&#123; $response = Read-Host &quot;Type something&quot;&#125; until($response -eq &quot;QUIT&quot;) 参考书籍Windows PowerShell Cookbook, 3rd Edition]]></content>
      <categories>
        <category>Windows</category>
      </categories>
      <tags>
        <tag>Admin</tag>
        <tag>Tools</tag>
        <tag>Shell</tag>
        <tag>Windows</tag>
        <tag>Program</tag>
        <tag>Script</tag>
        <tag>PowerShell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Windows PowerShell 学习笔记其一（特性介绍）]]></title>
    <url>%2F2019%2F02%2F26%2Fwindows-powershell-cookbook-1%2F</url>
    <content type="text"><![CDATA[PowerShell 是一个跨平台的自动化和配置工具（框架），在处理结构化数据（如 JSON、CSV、XML 等）、REST API 和对象模型方面做了大量针对性的优化。它是一个基于任务的命令行终端，同时也是一个构建在 .NET 上的脚本语言。对于自动化系统管理任务有极大的帮助。 其基本特性如下： PowerShell 可以与基础的 Windows 命令和应用完美整合 引入了一种新的命令形式（cmdlets），使用 Verb-Noun 形式的语法，对比普通命令有更高的灵活度 PowerShell 能够“理解”对象，可以直接处理结构化的数据对象，区别于普通 Shell 的“纯文本” 可以同多种技术直接整合使用。如 .NET、COM、WMI、XML 和活动目录等 简单化 data store 的管理（使用和管理文件同样的技术） 结构化命令（Cmdlets）PowerShell 引入了一种名为 Cmdlets 的新型命令。所有的 cmdlets 命令都以 Verb-Noun 的形式命名，如 Get-Process、Get-Content、Stop-Process 等。123456789PS C:\Users\starky&gt; Get-Process -Name chromeHandles NPM(K) PM(K) WS(K) VM(M) CPU(s) Id ProcessName------- ------ ----- ----- ----- ------ -- ----------- 285 36 80040 93808 620 104.61 1384 chrome 201 16 9000 7296 121 0.05 3572 chrome 1061 73 57324 99924 550 145.21 4052 chrome 221 17 8864 7952 131 0.08 5832 chrome 260 30 209528 154748 534 118.55 6408 chrome 在交互式的命令行环境下，可以使用 &lt;TAB&gt; 键自动补全命令和参数。如上述命令也可以这样输入：Get-Pr&lt;TAB&gt; -N&lt;TAB&gt; c&lt;TAB&gt; 大部分 Verb-Noun 形式的命令手动输入时仍显得有些长，所以 PowerShell 为所有常用的命令提供了简短的别名。如 Get-Process 的别名即为 gps 。123456789101112131415161718192021222324252627282930313233PS C:\Users\starky&gt; gps -n explorerHandles NPM(K) PM(K) WS(K) VM(M) CPU(s) Id ProcessName------- ------ ----- ----- ----- ------ -- ----------- 841 56 36280 35632 279 6.36 2552 explorerPS C:\Users\starky&gt; Get-AliasCommandType Name ModuleName----------- ---- ----------Alias % -&gt; ForEach-ObjectAlias ? -&gt; Where-ObjectAlias ac -&gt; Add-ContentAlias asnp -&gt; Add-PSSnapinAlias cat -&gt; Get-ContentAlias cd -&gt; Set-LocationAlias chdir -&gt; Set-LocationAlias clc -&gt; Clear-ContentAlias clear -&gt; Clear-HostAlias clhy -&gt; Clear-HistoryAlias cli -&gt; Clear-ItemAlias clp -&gt; Clear-ItemPropertyAlias cls -&gt; Clear-HostAlias clv -&gt; Clear-VariableAlias cnsn -&gt; Connect-PSSessionAlias compare -&gt; Compare-ObjectAlias copy -&gt; Copy-ItemAlias cp -&gt; Copy-ItemAlias cpi -&gt; Copy-ItemAlias cpp -&gt; Copy-ItemPropertyAlias curl -&gt; Invoke-WebRequest... 上述命令中，gps 即 Get-Process 命令的别名，-n 选项等同于 -Name。实际上 -n 选项此处也可以省略（默认即根据名称筛选进程）。 对象的深度整合PowerShell 可以直接处理复杂的结构化数据和功能完整的对象。如以下命令用于创建一个简单的字符串：12PS C:\Users\starky&gt; &quot;Hello World&quot;Hello World 事实上刚刚创建的 “Hello World” 字符串是一个具有完整功能的对象（来源于 .NET 框架），可以通过 . 符号访问其属性：12PS C:\Users\starky&gt; &quot;Hello World&quot;.Length11 所有的 PowerShell 命令都会将其输出内容作为对象，同时该对象可以被赋值给任意一个变量。如 Get-Process cmdlet 会生成一个 System.Diagnostics.Process 对象。12345PS C:\Users\starky&gt; notepadPS C:\Users\starky&gt; $process = Get-Process notepadPS C:\Users\starky&gt; $process.kill()PS C:\Users\starky&gt; Get-Process notepadGet-Process : 找不到名为“notepad”的进程。请验证该进程名称，然后再次调用 cmdlet。 管理员作为“第一等级”用户PowerShell 始终将管理员执行的任务作为关注的焦点，比如在终端中可以直接完成此种类型的计算：12PS C:\Users\starky&gt; 40GB / 650MB63.0153846153846 借助于 .NET 框架提供的支持，PowerShell 还可以作为一个强大的“日历”应用：12PS C:\Users\starky&gt; [DateTime]::IsLeapYear(2008)True 12345678PS C:\Users\starky&gt; [DateTime]::Now2019年2月18日 21:32:03PS C:\Users\starky&gt; $result = [DateTime] &quot;06/21/2019&quot; - [DateTime]::NowPS C:\Users\starky&gt; $result.TotalDays122.102253218295 组合命令可以使用管道符（|）将一个命令的输出重定向至另一个命令作为输入，和 Bash 中管道符的使用一样。如：Get-Item Path1\* | Move-Item -Destination Path2上述命令将 Path1 中的所有文件移动到 Path2 目录下。 更复杂的形式如：123456789101112131415PS C:\Users\starky&gt; Get-Process | Where-Object &#123; $_.Handles -ge 1000 &#125; | Sort-Object Handles | Format-Table Handles,Name,Desciption -AutoHandles Name Description------- ---- ----------- 1023 dwm 桌面窗口管理器 1025 chrome Google Chrome 1096 SkypeApp SkypeApp 1107 Microsoft.Photos Microsoft.Photos.exe 1126 svchost Windows 服务主进程 1236 svchost Windows 服务主进程 1425 SearchUI Search and Cortana application 1437 lsass Local Security Authority Process 1712 chrome Google Chrome 2592 explorer Windows 资源管理器 3410 System 通过 Get-Process 命令获取当前所有进程的信息，使用 Where-Object { $_.Handles -ge 1000 } 命令筛选出 Handles 在 1000 以上的对象，Sort-Object Handles 命令用来通过 Handles 属性排序，Format-Table ... 命令用来对输出结果进行格式化。 通过别名的简化，上述命令也可以使用如下的形式：gps | ? { $_.Handles -ge 1000 } | sort Handles | ft Handles,Name,Description -Auto “发现”命令可以通过 Get-Command cmdlet 进行模糊搜索，查询可能用到的命令，如需要使用“进程”相关的命令：123456789101112131415161718PS C:\Users\starky&gt; Get-Command *process*CommandType Name Version Source----------- ---- ------- ------Function Get-AppvVirtualProcess 1.0.0.0 AppvClientFunction Start-AppvVirtualProcess 1.0.0.0 AppvClientCmdlet ConvertTo-ProcessMitigationPolicy 1.0.11 ProcessMitigationsCmdlet Debug-Process 3.1.0.0 Microsoft.PowerShell.ManagementCmdlet Enter-PSHostProcess 3.0.0.0 Microsoft.PowerShell.CoreCmdlet Exit-PSHostProcess 3.0.0.0 Microsoft.PowerShell.CoreCmdlet Get-Process 3.1.0.0 Microsoft.PowerShell.ManagementCmdlet Get-ProcessMitigation 1.0.11 ProcessMitigationsCmdlet Get-PSHostProcessInfo 3.0.0.0 Microsoft.PowerShell.CoreCmdlet Set-ProcessMitigation 1.0.11 ProcessMitigationsCmdlet Start-Process 3.1.0.0 Microsoft.PowerShell.ManagementCmdlet Stop-Process 3.1.0.0 Microsoft.PowerShell.ManagementCmdlet Wait-Process 3.1.0.0 Microsoft.PowerShell.ManagementApplication qprocess.exe 10.0.17... C:\Windows\system32\qprocess.exe 如需要获取某个命令的具体帮助信息，可以使用 Get-Help cmdlet：12345678910111213141516171819202122232425262728293031323334353637383940PS C:\Users\starky&gt; Get-Help Stop-Process名称 Stop-Process摘要 Stops one or more running processes.语法 Stop-Process [-Id] &lt;Int32[]&gt; [-Confirm] [-Force] [-PassThru] [-WhatIf] [&lt;CommonParameters&gt;] Stop-Process [-InputObject] &lt;Process[]&gt; [-Confirm] [-Force] [-PassThru] [-WhatIf] [&lt;CommonParameters&gt;] Stop-Process [-Confirm] [-Force] -Name &lt;String[]&gt; [-PassThru] [-WhatIf] [&lt;CommonParameters&gt;]说明 The Stop-Process cmdlet stops one or more running processes. You can specify a process by process name or process I D (PID), or pass a process object to Stop-Process . Stop-Process works only on processes running on the local compu ter. On Windows Vista and later versions of the Windows operating system, to stop a process that is not owned by the cur rent user, you must start Windows PowerShell by using the Run as administrator option. Also, you are prompted for c onfirmation unless you specify the Force parameter.相关链接 Online Version: http://go.microsoft.com/fwlink/?LinkId=821642 Debug-Process Get-Process Start-Process Stop-Process Wait-Process备注 若要查看示例，请键入: &quot;get-help Stop-Process -examples&quot;. 有关详细信息，请键入: &quot;get-help Stop-Process -detailed&quot;. 若要获取技术信息，请键入: &quot;get-help Stop-Process -full&quot;. 有关在线帮助，请键入: &quot;get-help Stop-Process -online&quot; PowerShell 可以直接操作从 .NET 框架中引入的数据对象，通过将某个对象传递给 Get-Member cmdlet，可以获取绑定于该对象的属性和方法等：1234567891011121314151617181920212223PS C:\Users\starky&gt; &quot;Hello World&quot; | Get-Member TypeName:System.StringName MemberType Definition---- ---------- ----------...IndexOf Method int IndexOf(char value), int IndexOf(char value, int startIndex), int IndexOf...IndexOfAny Method int IndexOfAny(char[] anyOf), int IndexOfAny(char[] anyOf, int startIndex), i...Insert Method string Insert(int startIndex, string value)IsNormalized Method bool IsNormalized(), bool IsNormalized(System.Text.NormalizationForm normaliz...LastIndexOf Method int LastIndexOf(char value), int LastIndexOf(char value, int startIndex), int...LastIndexOfAny Method int LastIndexOfAny(char[] anyOf), int LastIndexOfAny(char[] anyOf, int startI...Normalize Method string Normalize(), string Normalize(System.Text.NormalizationForm normalizat...PadLeft Method string PadLeft(int totalWidth), string PadLeft(int totalWidth, char paddingChar)PadRight Method string PadRight(int totalWidth), string PadRight(int totalWidth, char padding...Remove Method string Remove(int startIndex, int count), string Remove(int startIndex)Replace Method string Replace(char oldChar, char newChar), string Replace(string oldValue, s...Split Method string[] Split(Params char[] separator), string[] Split(char[] separator, int...StartsWith Method bool StartsWith(string value), bool StartsWith(string value, System.StringCom...Substring Method string Substring(int startIndex), string Substring(int startIndex, int length)... 后台运行PowerShell 可以通过 Start-Job 将指定命令以后台任务的形式运行。如：1234567891011121314151617PS C:\Users\starky&gt; Start-Job &#123; while($true) &#123; Get-Random; Start-Sleep 5 &#125; &#125; -Name SleeperId Name PSJobTypeName State HasMoreData Location Command-- ---- ------------- ----- ----------- -------- -------2 Sleeper BackgroundJob Running True localhost while($true) &#123; Get-Ra...PS C:\Users\starky&gt; Receive-Job Sleeper20136626437901827701345543150PS C:\Users\starky&gt; Stop-Job SleeperPS C:\Users\starky&gt; Get-JobId Name PSJobTypeName State HasMoreData Location Command-- ---- ------------- ----- ----------- -------- -------2 Sleeper BackgroundJob Stopped True localhost while($true) &#123; Get-Ra... 其中 Receive-Job 用于获取后台任务的命令输出，Stop-Job 用于停止任务的运行，Get-Job 用于获取当前会话已绑定的所有任务。此外还有 Remove-Job 用于将活动任务从列表里移除。 除了使用 Start-Job cmdlet 生成后台任务，还可以通过在很多 cmdlets 后面加上 -AsJob 选项将其指定为后台运行。 脚本语言PowerShell 实际上不只是一个简单的交互式终端，同时也是一种强大的脚本语言。所以支持众多的编程特性，如：1234PS C:\Users\starky&gt; $handleCount = 0PS C:\Users\starky&gt; foreach($process in Get-Process) &#123; $handleCount += $process.Handles &#125;PS C:\Users\starky&gt; $handleCount69841 甚至如下的形式：123456789101112131415PS C:\Users\starky&gt; $webClient = New-Object System.Net.WebClientPS C:\Users\starky&gt; $content = $webClient.DownloadString(&quot;http://blogs.msdn.com/PowerShell/rss.aspx&quot;)PS C:\Users\starky&gt; $content.Substring(0,500)&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;rss version=&quot;2.0&quot; xmlns:content=&quot;http://purl.org/rss/1.0/modules/content/&quot; xmlns:wfw=&quot;http://wellformedweb.org/CommentAPI/&quot; xmlns:dc=&quot;http://purl.org/dc/elements/1.1/&quot; xmlns:atom=&quot;http://www.w3.org/2005/Atom&quot; xmlns:sy=&quot;http://purl.org/rss/1.0/modules/syndication/&quot; xmlns:slash=&quot;http://purl.org/rss/1.0/modules/slash/&quot; &gt;&lt;channel&gt; &lt;title&gt;PowerShell Team Blog&lt;/title&gt; &lt;atom:link href=&quot;https://blogs.msdn.microsoft.com/powershell/feed/&quot; rel=&quot;s 整合其他技术PowerShell 也支持操作 WMI（Windows Management Instrumentation）和 CIM 对象：12345678PS C:\Users\starky&gt; Get-CimInstance Win32_BiosSMBIOSBIOSVersion : D5CN31WWManufacturer : LENOVOName : D5CN31WWSerialNumber : PF0F77WRVersion : LENOVO - 1 又如操作 COM 对象：1234567891011121314PS C:\Users\starky&gt; $firewall = New-Object -com HNetCfg.FwMgrPS C:\Users\Administrator&gt; $firewall.LocalPolicy.CurrentProfileType : 1FirewallEnabled : TrueExceptionsNotAllowed : FalseNotificationsDisabled : FalseUnicastResponsesToMulticastBroadcastDisabled : FalseRemoteAdminSettings : System.__ComObjectIcmpSettings : System.__ComObjectGloballyOpenPorts : System.__ComObjectServices : System.__ComObjectAuthorizedApplications : System.__ComObject ProvidersPowerShell 可以通过 Providers 像浏览目录那样浏览系统中的 data store。如浏览注册表的某个分支：1234567891011121314151617181920212223242526272829303132333435363738394041PS C:\Users\starky&gt; Set-Location HKCU:\Software\Microsoft\WindowsPS HKCU:\Software\Microsoft\Windows&gt; Get-ChildItem Hive: HKEY_CURRENT_USER\Software\Microsoft\WindowsName Property---- --------AssignedAccessConfigurationCurrentVersionDWM Composition : 1 ColorizationColor : 3288365271 ColorizationColorBalance : 89 ColorizationAfterglow : 3288365271 ColorizationAfterglowBalance : 10 ColorizationBlurBalance : 1 EnableWindowColorization : 0 ColorizationGlassAttribute : 1 AccentColor : 4292311040 ColorPrevalence : 0 EnableAeroPeek : 1 AlwaysHibernateThumbnails : 0 UseDpiScaling : 0ShellTabletPCWindows Error Reporting LastQueuePesterTime : 131873467252566492WinlogonPS HKCU:\Software\Microsoft\Windows&gt; Set-Location CurrentVersion\RunPS HKCU:\Software\Microsoft\Windows\CurrentVersion\Run&gt; Get-ItemProperty .OneDrive : &quot;C:\Users\Administrator\AppData\Local\Microsoft\OneDrive\OneDrive.exe&quot; /backgroundLantern : &quot;C:\Users\Administrator\AppData\Roaming\Lantern\lantern.exe&quot; -startupPSPath : Microsoft.PowerShell.Core\Registry::HKEY_CURRENT_USER\Software\Microsoft\Windows\CurrentVersion\RunPSParentPath : Microsoft.PowerShell.Core\Registry::HKEY_CURRENT_USER\Software\Microsoft\Windows\CurrentVersionPSChildName : RunPSDrive : HKCUPSProvider : Microsoft.PowerShell.Core\Registry 即使用 Set-Location（或 cd）和 Get-ChildItem（或 dir ）像浏览本地目录一样浏览访问注册表。 同样的方式也适用于系统里的证书库：1234567891011121314PS C:\users\starky&gt; Set-Location cert:\CurrentUser\RootPS Cert:\CurrentUser\Root&gt; Get-ChildItem PSParentPath:Microsoft.PowerShell.Security\Certificate::CurrentUser\RootThumbprint Subject---------- -------CDD4EEAE6000AC7F40C3802C171E30148030C072 CN=Microsoft Root Certificate Authority, DC=microsoft, DC=comBE36A4562FB2EE05DBB3D32323ADF445084ED656 CN=Thawte Timestamping CA, OU=Thawte Certification, O=Thawte, L=Durbanvill...A43489159A520F0D93D032CCAF37E7FE20A8B419 CN=Microsoft Root Authority, OU=Microsoft Corporation, OU=Copyright (c) 19...92B46C76E13054E104F230517E6E504D43AB10B5 CN=Symantec Enterprise Mobile Root for Microsoft, O=Symantec Corporation, ...8F43288AD272F3103B6FB1428485EA3014C0BCFE CN=Microsoft Root Certificate Authority 2011, O=Microsoft Corporation, L=R...... 参考书籍Windows PowerShell Cookbook, 3rd Edition]]></content>
      <categories>
        <category>Windows</category>
      </categories>
      <tags>
        <tag>Admin</tag>
        <tag>Tools</tag>
        <tag>Shell</tag>
        <tag>Windows</tag>
        <tag>Program</tag>
        <tag>PowerShell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Arduino IDE 搭建 ESP8266 开发环境及项目演示]]></title>
    <url>%2F2019%2F01%2F11%2Fprogramming-esp8266-with-arduino-ide%2F</url>
    <content type="text"><![CDATA[ESP8266 是一款由乐鑫 Espressif 公司制作的低成本的 Wi-Fi 芯片，具有完整的 TCP / IP 协议栈和微控制器功能。它专为移动设备、可穿戴电子产品和物联网应用设计，功耗很低且价格非常低廉。我这里使用的 NodeMcu 开发板即搭载了这款芯片。 Arduino IDE 是由 Arduino 官方提供的支持 C 语言的集成开发环境，主要是针对 Arduino 系列的开发板进行编程。通过简单的配置，可以在原本的编程环境里添加上对 ESP8266 开发板的支持。对于熟悉 Arduino 函数库和开发流程的用户，基本上没有任何使用上的区别。 一、添加 ESP8266 支持首先从 Arduino 官网 下载最新版本的 Arduino IDE 软件并安装。安装完成以后，进入首选项（Preferences），找到附加开发板管理器地址（Additional Board Manager URLs），并在其后添加如下信息：http://arduino.esp8266.com/stable/package_esp8266com_index.json 之后点击工具 - 开发板 - 开发板管理器，进入开发板管理器界面： 找到 esp8266 并安装： 安装完成后，重启 Arduino IDE 软件。在工具 - 开发板选项中即会看到 ESP8266 开发板的选项： 二、测量温湿度本例中使用 DHT11 温湿度传感器测量室内温度和湿度，再把测量所得的结果输出至 Arduino IDE 的串口监视器中。 源代码在 Arduino IDE 中新建项目并写入如下代码：123456789101112131415161718192021222324252627282930313233#include &quot;DHT.h&quot;#define DHTPIN 5#define DHTTYPE DHT11// Initialize DHT sensorDHT dht(DHTPIN, DHTTYPE, 15);void setup() &#123; // Start Serial Serial.begin(115200); // Init DHT dht.begin();&#125;void loop() &#123; // Reading temperature and humidity float h = dht.readHumidity(); float t = dht.readTemperature(); // Display data Serial.print(&quot;Humidity: &quot;); Serial.print(h); Serial.print(&quot; %\t&quot;); Serial.print(&quot;Temperature: &quot;); Serial.print(t); Serial.println(&quot; *C &quot;); // Wait a few seconds between measurements. delay(2000);&#125; 由于源代码中首行引入的 DHT 库并不是 Arduino IDE 内置的库文件，需要先点击项目 - 加载库 - 管理库进入库管理器，搜索安装如下两个依赖库（Adafruit Unified Sensor 和 DHT sensor library）： 线路连接该测试项目只需要连接好 NodeMcu 开发板与 DHT11 温湿度模块（或者单独的 DHT11 元件配合 5kΩ 的上拉电阻），无需额外的传感器模块和电子组件。 线路连接示意图如下： NodeMcu DHT11 3V3 VCC（Pin1） GND GND（Pin4） D1 DATA(Pin2） 3V3 5k 电阻 - DATA（Pin2） 编译运行Arduino IDE 实际上支持非常多的基于 ESP8266 芯片设计的开发板，如 Adafruit Feather HUZZAH ESP8266、LOLIN (WEMOS) D1 mini 等。可以根据自己购买的开发板的具体型号，在编译前选择对应的开发板选项（工具 - 开发板）。 我这里使用的是开源的 NodeMcu v1.0 开发板，编译代码前确保选择正确： 最终的运行效果如下：呃，，南方的冬天，外面在下雨。室内，没开空调。。。（后面温湿度升高是因为，我对着传感器哈气了。。。） 注意事项可以看到，源代码中的 DHTPIN （即传感器 DATA 引脚需要连接的开发板引脚 ）定义为 5 ，但开发板实际连接的是 D1 引脚（而不是 D5）。 ESP8266 芯片有自己的引脚（GPIO）布局，但是基于该芯片设计的众多开发板，对于芯片上 GPIO 的引出方式却有自己的规则。即源代码中的 5 指的并不是开发板的引脚 D5 ，而是 ESP8266 的引脚 GPIO 5 ，对应到开发板上即是 D1 引脚。 相关的引脚布局如下图所示： 三、Wi-Fi 连接ESP8266 最大的特性就是其超低成本的 Wi-Fi 实现。这里简单贴出其连接 Wi-Fi 的示例代码：123456789101112131415161718192021222324252627// Import required libraries#include &lt;ESP8266WiFi.h&gt;// WiFi parametersconst char* ssid = &quot;your_wifi_name&quot;;const char* password = &quot;your_wifi_password&quot;;void setup(void)&#123; // Start Serial Serial.begin(115200); // Connect to WiFi WiFi.begin(ssid, password); while (WiFi.status() != WL_CONNECTED) &#123; delay(500); Serial.print(&quot;.&quot;); &#125; Serial.println(&quot;&quot;); Serial.println(&quot;WiFi connected&quot;); // Print the IP address Serial.println(WiFi.localIP());&#125; void loop() &#123;&#125; 运行结果如下： 四、aRESTaREST 框架可以为一些常见的嵌入式开发板提供 RESTful 接口，支持通过串口、Wi-Fi、以太网、蓝牙等硬件发送命令至开发板，激发特定的操作，并将数据以 JSON 的格式返回给控制端用户。 使用 aREST 框架既可以在本地网络环境中控制联网设备，也可以借助云端平台进行远程操作或监控。 结合之前的温湿度项目，可以将连接 Wi-Fi 后的 NodeMcu 开发板作为一个实现了 REST API 的本地服务器，通过访问其 IP 地址来获取相应的温湿度数据（JSON 格式）。 代码如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778// Import required libraries#include &quot;ESP8266WiFi.h&quot;#include &lt;aREST.h&gt;#include &quot;DHT.h&quot;// DHT11 sensor pins#define DHTPIN 5#define DHTTYPE DHT11// Create aREST instanceaREST rest = aREST();// Initialize DHT sensorDHT dht(DHTPIN, DHTTYPE, 15);// WiFi parametersconst char* ssid = &quot;wifi-name&quot;;const char* password = &quot;wifi-pass&quot;;// The port to listen for incoming TCP connections #define LISTEN_PORT 80// Create an instance of the serverWiFiServer server(LISTEN_PORT);// Variables to be exposed to the APIfloat temperature;float humidity;void setup(void)&#123; // Start Serial Serial.begin(115200); // Init DHT dht.begin(); // Init variables and expose them to REST API rest.variable(&quot;temperature&quot;,&amp;temperature); rest.variable(&quot;humidity&quot;,&amp;humidity); // Give name and ID to device rest.set_id(&quot;1&quot;); rest.set_name(&quot;esp8266&quot;); // Connect to WiFi WiFi.begin(ssid, password); while (WiFi.status() != WL_CONNECTED) &#123; delay(500); Serial.print(&quot;.&quot;); &#125; Serial.println(&quot;&quot;); Serial.println(&quot;WiFi connected&quot;); // Start the server server.begin(); Serial.println(&quot;Server started&quot;); // Print the IP address Serial.println(WiFi.localIP());&#125;void loop() &#123; // Reading temperature and humidity humidity = dht.readHumidity(); temperature = dht.readTemperature(); // Handle REST calls WiFiClient client = server.available(); if (!client) &#123; return; &#125; while(!client.available())&#123; delay(1); &#125; rest.handle(client);&#125; 运行效果： 以上只是一些基础的使用介绍吧，，，后续比如更加复杂的网络服务、对接公共的物联网云平台及 MQTT 协议等内容，有时间再看看。我去，光着脚，12℃。冻死我了。。。 参考资料Internet of Things with ESP8266 (English Edition)]]></content>
      <categories>
        <category>IoT</category>
      </categories>
      <tags>
        <tag>Programming</tag>
        <tag>Arduino</tag>
        <tag>IoT</tag>
        <tag>ESP8266</tag>
        <tag>NodeMcu</tag>
        <tag>Wi-Fi</tag>
        <tag>Sensor</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[树莓派3 音频配置及其应用场景（录音、VoIP 电话等）]]></title>
    <url>%2F2019%2F01%2F06%2Fraspberry-pi-audio-configuration-and-applications%2F</url>
    <content type="text"><![CDATA[从网上看到一本关于树莓派的电子书 Raspberry Pi for Secret Agents，利用树莓派打造“特工装备”。其中有一章讲到音频设备的配置和几个相关的应用场景（比如录音、监听、搭建电话系统等），感觉比较有意思。 一、音频系统简单配置1. ALSAALSA (Advanced Linux Sound Architecture) 是一个承载树莓派上音频系统的底层框架，为树莓派及其外接的播放或录制设备提供内核驱动程序。同时该框架也包含用于制作音频应用的程序代码，和一些方便的命令行实用程序。 在 ALSA 的定义里，系统中的每一个音频设备都称作“声卡”。可以使用如下命令列出所有已连接的音频设备：123$ cat /proc/asound/cards 0 [ALSA ]: bcm2835_alsa - bcm2835 ALSA bcm2835 ALSA 可以看到此时系统中只有树莓派默认的声卡 0 bcm2835 ALSA。 2. 调节音量大小可以使用 alsamixer 命令打开 AlsaMixer 实用程序，对音量等声音系统参数进行调节： 界面中 View 选项后面的 Playback、Capture、All 分别对应播放、录制和全部。可以按键盘上的 TAB 键切换到对应的界面。按下 fn+F6 或 S 键切换声卡设备（此时只有一个声卡） 使用 ↑ ↓ 方向键调节音量，设置完成后按 ESC 键退出 AlsaMixer。 3. 切换音频输出树莓派提供两种音频输出接口：3.5mm 模拟音频接口和 HDMI。可以通过 $sudo raspi-config 命令（选择 Advanced Options - Audio）配置音频输出方向，使音频输出固定使用 3.5mm 接口或 HDMI： 类似的操作也可以直接通过 amixer 命令完成：$ amixer cset numid=3 1 ：指定音频输出接口为 3.5mm 耳机接口$ amixer cset numid=3 2 ：指定音频输出接口为 HDMI1234$ amixer cset numid=3 1numid=3,iface=MIXER,name=&apos;PCM Playback Route&apos; ; type=INTEGER,access=rw------,values=1,min=0,max=2,step=0 : values=1 4. 播放测试可以使用 $ speaker-test -c2 -t wav 命令测试音频播放是否正常。如一切顺利，则会依次从左耳耳机（或音箱）听到 Front Left，从右耳耳机听到 Front Right 的女声语音，直到按下 Ctrl + C 终止测试。 二、音频录制设备1. USB声卡树莓派提供的音频接口是不支持语音输入的。使用 alsamixer 命令进入 AlsaMixer 程序，按下 TAB 键切换到 Caputre 界面，可以看到此设备无音频采集控制的提示： 为了录制音频，需要使用外接的音频输入设备。可以从网上购买一个便携式的 USB 免驱动声卡。插入USB声卡后，通过 $ cat /proc/asound/cards 命令查看当前系统检测到的音频设备：12345$ cat /proc/asound/cards 0 [ALSA ]: bcm2835_alsa - bcm2835 ALSA bcm2835 ALSA 1 [Device ]: USB-Audio - USB Audio Device GeneralPlus USB Audio Device at usb-3f980000.usb-1.3, full speed 此时输出信息中多了一个序号为 1 的声卡设备 USB-Audio。可以使用 $ speaker-test -D plughw:1 -c2 -t wav 测试 USB 声卡的音频播放是否正常（注意命令中的 -D plughw:1）。 使用 alsamixer -c1 命令调节新声卡的具体参数。其中 -c1 选项用于指定编号为 1 的声卡设备，即新插入的 USB 声卡。 按下 fn+F4 或 TAB 键将视图切换到 Capture 界面，确保 Mic 上方有出现 CAPTURE 字样（说明已开启录制功能，可以使用空格键切换开关状态）： 2. 麦克风测试首先安装 SoX 工具及其 mp3 格式支持：$sudo apt-get install sox libsox-fmt-mp3SoX 是 Linux 系统上一个强大的音频处理工具，详细使用方法可以参考这篇文章：SoX — 音频处理工具里的瑞士军刀。 sox 命令的基本格式为 sox &lt;input&gt; &lt;output&gt; 。其中的 &lt;input&gt; 和 &lt;output&gt; 既可以是某个具体的音频文件，也可以是某个特定的音频设备。所以可以简单的理解为，sox 工具就是对音频进行“传导”： 从文件到设备即为播放 sox music.mp3 &lt;device&gt; 从设备到文件即为录制 sox &lt;device&gt; myrec.wav 从文件到文件即为转码 sox input.wav output.mp3 可以使用 $ sox -t alsa plughw:1 -t alsa plughw:1 命令对麦克风进行测试。其中 -t alsa plughw:1 表示 ALSA 声卡设备 1（即USB声卡）。上述命令表示既使用USB声卡（麦克风） 作为音频输入，又用它（耳机）作为音频输出。 如一切正常，此时可以通过耳机听到自己对着麦克风讲话的声音。 3. 切换默认的音频设备可以通过修改配置文件，将树莓派默认用于播放和录制的音频设备（即树莓派内置声卡），改为当前插入的USB声卡。编辑 ~/.asoundrc 文件，改为如下内容：123456789pcm.!default &#123; type hw card 1&#125;ctl.!default &#123; type hw card 1&#125; 其实就是将配置文件中的 card 0 改为 card 1。 此时可直接使用 $ sox -d -d 命令测试USB声卡上连接的麦克风和耳机，无需再通过 -t alsa plughw:1 选项手动指定USB声卡（-d 选项表示默认音频设备，即已配置成默认的USB声卡）。 4. 录制与播放默认音频设备切换为USB声卡后，可以使用以下命令录制一段音频并将其保存在 myrec.mp3 文件中：$ sox -d myrec.mp3 或 $ rec myrec.mp3 播放前面录制的音频文件可使用如下命令：$ sox myrec.mp3 -d 或 $ play myrec.mp3 如未能配置USB声卡为默认音频设备或配置不成功，也可以使用如下命令进行录制与播放：录制：$ sox -t alsa plughw:1 myrec.mp3播放：$ sox myrec.mp3 -t alsa plughw:1 录制固定长度的音频片段（如 30 分钟）并保存在指定文件中：$ sox -t alsa plughw:1 myrec.wav trim 0 00:30:00 持续录制很长时间的音频，保存在几个不同的文件中，每隔一小时保存一次：$ sox -t alsa plughw:1 myrec.wav trim 0 01:00:00 : newfile : restart 三、远程监听可以通过树莓派的 SSH 服务，在另一台电脑上远程收听树莓派通过USB声卡收集到的音频数据。也就是说，用树莓派的麦克风录制周围环境的声音，同时在远程的另一台电脑上实时地（有短暂延迟）收听录制的内容，达到监听的效果。 在电脑端（也需要安装 sox 程序）执行如下命令即可：$ ssh pi@[IP address] sox -t alsa plughw:1 -t sox - | sox -q -t sox - -d 对于 Windows 系统，除安装 SoX 工具外，还需要先下载完整版 PuTTY 工具，并确保这两个工具的安装路径都已添加至 PATH 环境变量。 则可以使用如下命令通过树莓派进行远程监听：plink pi@[IP address] -pw [password] sox -t alsa plughw:1 -t sox - | sox -q -t sox - -t waveaudio default 可以使用如下命令，将远程树莓派收集到的音频数据直接保存在本地文件中：$ ssh pi@[IP address] sox -t alsa plughw:1 -t mp3 - &gt; ~/Desktop/myrec.mp3或 plink pi@[IP address] sox -t alsa plughw:1 -t mp3 - &gt; D:\myrec.mp3 （Windows 系统） 使用如下命令，让本地电脑作为音频输入源，将其麦克风收集到的音频数据，通过 SSH 发送到远程树莓派上进行播放。也就是将自己对着本地电脑讲的话通过树莓派进行远程广播：$ sox -d -t sox - | ssh pi@[IP address] sox -q -t sox - -d或 sox -t waveaudio default -t sox - | plink pi@[IP address] -pw [password] sox -q -t sox - -d （Windows 系统） 同样的原理，也可以将本地磁盘上的音频文件直接发送到远程的树莓派上进行播放：$ cat ~/Desktop/media/audios/Faded.wav | ssh pi@[IP address] sox -t wav - -d或 type D:\myrec.mp3 | plink pi@[IP address] -pw [password] sox -t mp3 - -d （Windows 系统） 四、蓝牙耳机树莓派 3 代 B+ 自带了 WIFI 和蓝牙功能，可以直接通过蓝牙连接音频设备。 1. 使用 hciconfig 命令获取蓝牙模块的相关信息：1234567891011121314151617$ hciconfig -ahci0: Type: Primary Bus: UART BD Address: B8:27:EB:C3:38:31 ACL MTU: 1021:8 SCO MTU: 64:1 UP RUNNING RX bytes:4181 acl:136 sco:0 events:135 errors:0 TX bytes:5065 acl:128 sco:0 commands:60 errors:0 Features: 0xbf 0xfe 0xcf 0xfe 0xdb 0xff 0x7b 0x87 Packet type: DM1 DM3 DM5 DH1 DH3 DH5 HV1 HV2 HV3 Link policy: RSWITCH SNIFF Link mode: SLAVE ACCEPT Name: &apos;raspberrypi&apos; Class: 0x480000 Service Classes: Capturing, Telephony Device Class: Miscellaneous, HCI Version: 4.2 (0x8) Revision: 0xfc LMP Version: 4.2 (0x8) Subversion: 0x6119 Manufacturer: Broadcom Corporation (15) 2. 使用 hcitool scan 命令扫描附近可供连接的蓝牙设备，并记下蓝牙耳机对应的 MAC 地址1234$ hcitool scanScanning ... FC:58:FA:F4:67:33 A2 50:8F:4C:0D:31:3A Starky 3. 通过 MAC 地址与蓝牙耳机进行配对使用 bluetoothctl 命令进入蓝牙控制台，依次通过 power on 、agent on 、scan on 、pair 、trust 、connect 等命令连接蓝牙设备。 注意此处连接蓝牙使用的是蓝牙设备的 MAC 地址。123456789101112131415161718192021$ bluetoothctl[NEW] Controller B8:27:EB:C3:38:31 raspberrypi [default][NEW] Device 50:8F:4C:0D:31:3A Starky...[bluetooth]# power onChanging power on succeeded[bluetooth]# agent onAgent registered[bluetooth]# scan onDiscovery started[bluetooth]# pair FC:58:FA:F4:67:33Attempting to pair with FC:58:FA:F4:67:33Pairing successful[bluetooth]# trust FC:58:FA:F4:67:33[CHG] Device FC:58:FA:F4:67:33 Trusted: yesChanging FC:58:FA:F4:67:33 trust succeeded[bluetooth]# connect FC:58:FA:F4:67:33Attempting to connect to FC:58:FA:F4:67:33[CHG] Device FC:58:FA:F4:67:33 Connected: yesConnection successful[A2]# 退出蓝牙控制台，使用 hcitool con 命令查看当前已连接的蓝牙设备，确认连接成功：123$ hcitool conConnections: &gt; ACL FC:58:FA:F4:67:33 handle 11 state 1 lm SLAVE AUTH ENCRYPT 4. 将音频输出设备切换为蓝牙耳机首先安装 PulseAudio 软件包：$ sudo apt-get install pulseaudio pulseaudio-module-bluetooth 启动 PulseAudio 守护进程：$ pulseaudio --start 列出 PulseAudio 检测到的音频设备名称：1234$ pacmd list-sinks short | grep -e &apos;name:&apos; name: &lt;alsa_output.platform-soc_audio.analog-mono&gt; name: &lt;bluez_sink.FC_58_FA_F4_67_33.headset_head_unit&gt; 使用 pactl 命令将蓝牙耳机作为默认的音频设备：$ pactl set-default-sink bluez_sink.FC_58_FA_F4_67_33.headset_head_unit 此时，即可直接使用 play 命令通过蓝牙耳机播放音频文件： 如果需要将音频输出切换回系统默认，运行如下命令即可：$ pactl set-default-sink alsa_output.platform-soc_audio.analog-mono 或者自己对当前的音频配置有点混乱了。。。可以使用如下命令重置：$ sudo /etc/init.d/alsa-utils reset 5. 注意事项经过测试，发现蓝牙耳机连接后音质相差太大，播放速度明显放缓，未确定是哪里的问题。 同时我的蓝牙耳机开启后会自动连接至树莓派，如无法自动连接或者连接以后并没有被 PulseAudio 检测到，可以先使用 bluetoothctl 命令进入蓝牙控制台：依次输入 power off、power on 命令重启蓝牙，使用 connect &lt;MAC address&gt; 命令重新连接蓝牙耳机（无需再配对和信任，直接连接）。 蓝牙耳机为默认音频设备后，之前配置 USB 声卡为默认的操作不再生效（除非断开蓝牙或关闭蓝牙耳机）。可以在播放或录制时通过 -t alsa plughw:1 选项手动指定 USB 声卡。如 $ sox -t alsa plughw:1 -d ：使用 USB 声卡作为音频输入源，再将采集到的音频输出到默认设备（蓝牙耳机） 这一章的蓝牙介绍只供参考，推荐使用 USB 声卡进行音频的录制和播放。 五、配置 VoIP 服务器VoIP 即 Voice over Internet Protocol ，就是将声音信号经过压缩与封包之后，以数据封包的形式在IP网络间进行传输。通俗地说也就是互联网电话或IP电话。 这里使用 Linux 平台上的 SIP Witch 软件作为 VoIP 系统的服务端。 1. 安装 SIP Witch$ sudo apt-get install sipwitch 2. 编辑配置文件/etc/default/sipwtich ：找到 PLUGINS 选项，删除该行前面的注释 /etc/sipwitch.conf ：编辑文件内容，在 &lt;provision&gt; 标签下添加用户注册信息：12345678910&lt;user id=&quot;phone1&quot;&gt; &lt;extension&gt;201&lt;/extension&gt; &lt;secret&gt;SecretSauce201&lt;/secret&gt; &lt;display&gt;Agent 201&lt;/display&gt;&lt;/user&gt;&lt;user id=&quot;phone2&quot;&gt; &lt;extension&gt;202&lt;/extension&gt; &lt;secret&gt;SecretSauce202&lt;/secret&gt; &lt;display&gt;Agent 202&lt;/display&gt;&lt;/user&gt; 其中的 id 表示注册用户登录时需要输入的用户名，&lt;secret&gt; 表示登录时使用的密码。 找到 &lt;stack&gt; 标签，并在其后添加如下一行配置，用于指定 VoIP 服务器的IP地址：&lt;localnames&gt;[Your Pi&#39;s IP address]&lt;/localnames&gt; 编辑完成后，重启 sipwitch 服务：$ sudo systemctl restart sipwitch 3. 客户端配置Windows 系统“软电话”可使用 MicroSIP 软件，Android 客户端可使用 CSipSimple，Mac 客户端有 Telephone。 使用 /etc/sipwitch.conf 文件中定义的用户ID、密码以及服务器地址完成用户注册，各客户端之间即可直接拨打语音电话。 参考资料Raspberry Pi for Secret Agents - Third Edition]]></content>
      <categories>
        <category>IoT</category>
      </categories>
      <tags>
        <tag>Media</tag>
        <tag>Audio</tag>
        <tag>Raspberrypi</tag>
        <tag>SecretAgent</tag>
        <tag>sox</tag>
        <tag>VoIP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NodeMCU 开发板刷入 MicroPython 固件并通过 Atom 进行编程]]></title>
    <url>%2F2018%2F12%2F28%2Fprogramming-esp8266-with-micropython%2F</url>
    <content type="text"><![CDATA[MicroPython 是对 Python 3 语言的一种精简的实现，主要是为了在微控制器这种硬件资源受限的环境中可以高效地运行，因此做了很多针对性的优化。它类似于一个小型的 Python 操作系统，可以通过串口与内置的 Python 解释器直接交互，也可以上传程序文件在开机上电后自动运行。MicroPython 支持多种开发板，包括价格很实惠的基于 ESP8266 芯片的 NodeMCU。 一、刷入固件我这里使用的开发板是从网上购买的 NodeMCU ，出厂时预先刷好了基于 Lua 语言编程的固件。为了使用 MicroPython ，需要先对该开发板重新刷写固件。 1. 准备工作首先进入 MicroPython 官方的下载页面，下载最新版的针对 ESP8266 硬件的固件包。我这里使用的是 esp8266-20180511-v1.9.4.bin 。 刷写软件可以使用支持多种操作系统的命令行工具 esptool。对于已经装好 Python 环境的电脑，直接使用 pip 命令安装即可：pip install esptool 2. 刷入固件首先将 NodeMCU 开发板连接至电脑，Mac 系统可以使用 ls /dev/tty.usbserial* 命令查看当前系统已经检测到的 USB 串口：12$ ls /dev/tty.usbserial*/dev/tty.usbserial-14110 使用 esptool.py -p &lt;SerialPort&gt; flash_id 命令确认该串口是否能成功检测到 ESP8266 设备：123456789101112131415$ esptool.py -p /dev/tty.usbserial-14110 flash_idesptool.py v2.5.1Serial port /dev/tty.usbserial-14110Connecting....Detecting chip type... ESP8266Chip is ESP8266EXFeatures: WiFiMAC: 2c:3a:e8:06:aa:7eUploading stub...Running stub...Stub running...Manufacturer: c8Device: 4016Detected flash size: 4MBHard resetting via RTS pin... 使用 esptool.py -p &lt;SerialPort&gt; erase_flash 命令对开发板进行擦除操作（以防万一）：1234567891011121314$ esptool.py -p /dev/tty.usbserial-14110 erase_flashesptool.py v2.5.1Serial port /dev/tty.usbserial-14110Connecting....Detecting chip type... ESP8266Chip is ESP8266EXFeatures: WiFiMAC: 2c:3a:e8:06:aa:7eUploading stub...Running stub...Stub running...Erasing flash (this may take a while)...Chip erase completed successfully in 2.5sHard resetting via RTS pin... 擦除完成后即可使用 esptool 工具的 write_flash 命令将 MicroPython 固件刷入开发板。（完整命令为：esptool.py -p &lt;SerialPort&gt; --baud 460800 write_flash --flash_size=detect -fm dio 0 &lt;FirmwareFile&gt;）12345678910111213141516171819202122$ esptool.py -p /dev/tty.usbserial-14110 --baud 460800 write_flash --flash_size=detect -fm dio 0 ~/Downloads/esp8266-20180511-v1.9.4.binesptool.py v2.5.1Serial port /dev/tty.usbserial-14110Connecting....Detecting chip type... ESP8266Chip is ESP8266EXFeatures: WiFiMAC: 2c:3a:e8:06:aa:7eUploading stub...Running stub...Stub running...Changing baud rate to 460800Changed.Configuring flash size...Auto-detected Flash size: 4MBFlash params set to 0x0240Compressed 604872 bytes to 394893...Wrote 604872 bytes (394893 compressed) at 0x00000000 in 9.9 seconds (effective 490.0 kbit/s)...Hash of data verified.Leaving...Hard resetting via RTS pin... 二、固件测试刷写固件的操作完成后，可以使用 Mac 系统自带的串口调试工具 cu 连接 USB 串口，对刚刷好的开发版进行测试：sudo cu -l &lt;SerialPort&gt; -s &lt;BaudRateSpeed&gt;效果如下：12$ sudo cu -l /dev/tty.usbserial-14110 -s 115200Connected. 终端窗口提示 Connected 之后，按下 NodeMCU 板子上的 RST (Reset) 按钮。如固件刷写成功，则终端在输出部分乱码后会立即进入 Python 解释器界面：123MicroPython v1.9.4-8-ga9a3caad0 on 2018-05-11; ESP module with ESP8266Type &quot;help()&quot; for more information.&gt;&gt;&gt; 在解释器界面下，输入 help() 命令获取基本的帮助信息：12345678910111213141516171819202122232425262728MicroPython v1.9.4-8-ga9a3caad0 on 2018-05-11; ESP module with ESP8266Type &quot;help()&quot; for more information.&gt;&gt;&gt; help()Welcome to MicroPython!For online docs please visit http://docs.micropython.org/en/latest/esp8266/ .For diagnostic information to include in bug reports execute &apos;import port_diag&apos;.Basic WiFi configuration:import networksta_if = network.WLAN(network.STA_IF); sta_if.active(True)sta_if.scan() # Scan for available access pointssta_if.connect(&quot;&lt;AP_name&gt;&quot;, &quot;&lt;password&gt;&quot;) # Connect to an APsta_if.isconnected() # Check for successful connection# Change name/password of ESP8266&apos;s AP:ap_if = network.WLAN(network.AP_IF)ap_if.config(essid=&quot;&lt;AP_NAME&gt;&quot;, authmode=network.AUTH_WPA_WPA2_PSK, password=&quot;&lt;password&gt;&quot;)Control commands: CTRL-A -- on a blank line, enter raw REPL mode CTRL-B -- on a blank line, enter normal REPL mode CTRL-C -- interrupt a running program CTRL-D -- on a blank line, do a soft reset of the board CTRL-E -- on a blank line, enter paste modeFor further help on a specific object, type help(obj)&gt;&gt;&gt; 更多的交互命令和详细的指导手册可以参考官方文档 使用如下的命令，能够让 GPIO2 上连接的 LED（即 ESP8266 模块内置的蓝色 LED）以 1s 的频率闪烁：1234567891011&gt;&gt;&gt; from machine import Pin&gt;&gt;&gt; import time&gt;&gt;&gt; p=Pin(2,Pin.OUT)&gt;&gt;&gt; while True:... p.on()... time.sleep(1)... p.off()... time.sleep(1)......... 三、配置 Atom 编程环境当前可以使用多种软件作为简单的 IDE 对 MicroPython 开发板进行编程。主要是提供连接串口的终端，可以交互地执行 Python 命令，以及程序文件的上传和下载。 我个人比较喜欢 Atom 编辑器搭配上 pymakr-atom 插件。 Atom 的配置较简单，主要步骤如下： 从 Atom 官网下载对应系统版本的安装程序并安装 通过 Atom 内置的插件管理器搜索安装 pymakr 插件，安装完成后重启编辑器 编辑器重启后底部自动出现 pymakr 的终端窗口，点击终端右上角设置 - 全局设置，找到设备地址并改为开发板连接的端口号 直接点击终端顶部的连接按钮即可完成串口连接 此时的 Atom 编辑器，可以通过底部的终端窗口直接输入 Python 命令。也可以在编辑器中新建项目目录，并创建 main.py 源文件。编辑好 Python 代码后，点击执行按钮即可直接运行，或者点击上传按钮将代码文件上传至开发板。 参考资料MicroPython documentation]]></content>
      <categories>
        <category>IoT</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Programming</tag>
        <tag>IoT</tag>
        <tag>ESP8266</tag>
        <tag>Microcontroller</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SoX — 音频处理工具里的瑞士军刀]]></title>
    <url>%2F2018%2F12%2F18%2Fprocessing-audio-with-sox%2F</url>
    <content type="text"><![CDATA[SoX（即 Sound eXchange）是一个跨平台（Windows，Linux，MacOS 等）的命令行实用程序，可以将各种格式的音频文件转换为需要的其他格式。SoX 还可以对输入的音频文件应用各种效果，也支持在大多数平台上播放和录制音频文件。 一、简介SoX 可以读取和写入常见格式的音频文件，并在此过程中选择性的加入一些声音效果。它可以组合多个输入源及合成音效，在许多系统上也可以作为音频播放器或多轨录音机使用。SoX 工具在大部分 Linux 系统上都可以直接通过软件包管理器安装（如 sudo apt-get install sox），Mac 系统上则可以使用 brew install sox 命令。 SoX 处理音频的基本流程如下：Input(s) -&gt; Combiner -&gt; Effects -&gt; Output(s) SoX 工具的所有功能都可以通过一个简单的 sox 命令及相应的选项实现。但它同时提供了 play 命令用于播放音频文件，rec 命令用于录制音频，以及 soxi 命令用于获取音频的文件头中包含的信息。 上述几个命令的基本格式如下：12345678910111213SYNOPSIS sox [global-options] [format-options] infile1 [[format-options] infile2] ... [format-options] outfile [effect [effect-options]] ... play [global-options] [format-options] infile1 [[format-options] infile2] ... [format-options] [effect [effect-options]] ... rec [global-options] [format-options] outfile [effect [effect-options]] ... soxi [-V[level]] [-T] [-t|-r|-c|-s|-d|-D|-b|-B|-p|-e|-a] infile1 ... 二、基本使用1. 获取音频文件的元数据soxi 或 sox --i 命令可以通过分析音频文件的文件头，获取其元数据（如通道数、采样率、编码等）。12345678910$ soxi Faded.wavInput File : &apos;Faded.wav&apos;Channels : 2Sample Rate : 44100Precision : 16-bitDuration : 00:03:32.63 = 9376836 samples = 15947 CDDA sectorsFile Size : 37.5MBit Rate : 1.41MSample Encoding: 16-bit Signed Integer PCM soxi 命令跟上某个特定的选项可以只获取该选项对应的信息，如只显示某音频文件 Faded.wav 的比特率（Bit Rate）：12$ soxi -B Faded.wav1.41M soxi 命令支持的所有选项及其含义如下：1234567891011121314151617$ soxiUsage: soxi [-V[level]] [-T] [-t|-r|-c|-s|-d|-D|-b|-B|-p|-e|-a] infile1 ...-t Show detected file-type-r Show sample-rate-c Show number of channels-s Show number of samples (0 if unavailable)-d Show duration in hours, minutes and seconds (0 if unavailable)-D Show duration in seconds (0 if unavailable)-b Show number of bits per sample (0 if not applicable)-B Show the bitrate averaged over the whole file (0 if unavailable)-p Show estimated sample precision in bits-e Show the name of the audio encoding-a Show file comments (annotations) if availableWith no options, as much information as is available is shown foreach given file. 2. 获取音频的统计信息可以使用 sox &lt;inputfile&gt; -n stat 命令获取某音频文件的统计信息。示例如下：12345678910111213141516$ sox Faded.wav -n statSamples read: 18753672Length (seconds): 212.626667Scaled by: 2147483647.0Maximum amplitude: 0.977417Minimum amplitude: -0.977478Midline amplitude: -0.000031Mean norm: 0.229415Mean amplitude: -0.000006RMS amplitude: 0.302594Maximum delta: 1.765564Minimum delta: 0.000000Mean delta: 0.202369RMS delta: 0.273320Rough frequency: 6339Volume adjustment: 1.023 3. 播放与录制play 和 rec 命令提供了最基本的播放和录制功能。播放：$ play existing-file.wav录制：$ rec new-file.wav 上述命令等同于 sox 命令的如下形式：$ sox existing-file.wav −d（播放）和 sox −d new-file.wav（录制）其中 -d 选项用于指定播放或录制时使用的音频设备，不指定时则表示使用默认设备。 可以这样理解： sox existing-file.wav -d 就是从 existing-file.wav 文件中读取其包含的音频数据，再输出到 -d （默认音频设备，扬声器）进行播放； sox -d new-file.wav 就是从 -d （默认音频设备，麦克风）中读取音频数据，再输出（录制）到 new-file.wav 文件中。 其实都遵循了一个基本的格式，即 sox &lt;input&gt; &lt;output&gt; 。而其中的 &lt;input&gt; 和 &lt;output&gt; 根据需要既可以为某个具体的音频文件，也可以是某个具体的音频设备。 播放或录制的同时，也可以对音频文件应用指定的编辑操作或效果选项，因此在对音频数据应用某效果前，可以先使用 play 命令进行“预览”。 如 trim 效果可以从音频文件中裁剪提取指定的片段到输出文件。play 命令通过该效果可以直接播放指定片段：$ play foo.wav trim 10.0 5.0 或 $ play foo.wav trim 10.0 =15.0播放 foo.wav 文件中 10-15s 之间的音频片段 使用 echo 效果播放 Faded.wav 文件：123456789101112$ play Faded.wav echo 0.8 0.88 200.0 0.4Faded.wav: File Size: 37.5M Bit Rate: 1.41M Encoding: Signed PCM Channels: 2 @ 16-bitSamplerate: 44100HzReplaygain: off Duration: 00:03:32.63In:12.1% 00:00:25.82 [00:03:06.81] Out:1.14M [-=====|=====-] Hd:2.7 Clip:0 4. 音频格式转换文件格式类型对于音频数据格式的描述，主要通过以下 4 种属性： 采样率（sample rate）：指声音由模拟信号转换成数字信号的过程中，每秒从连续信号中提取的用于组成离散信号的样本个数。音频CD所用的采样率为 44100 Hz，数字音频磁带和许多计算机系统使用 48000 Hz，专业级音频系统通常使用 96000 Hz。 采样大小（sample size 或 Precision）：音频采样时用于存储每个样本的数据位数（bits）。如今 16 bit 的采样大小已被广泛使用，24 bit 主要用于专业音频领域。 编码格式（data encoding）：即每个音频样本的表示（即“编码”）方式。常用的编码类型包括 floating-point、μ-law、ADPCM、singed-integer PCM、MP3 和 FLAC 等。 通道（channel）：即文件中包含的音频通道的数量。其中单声道（mono）和双声道（stereo）是最常见的两种，“环绕声”音频（Surround sound）通常包含六个或更多声道。 此外，音频文件还使用比特率（Bit Rate）表示一个单位时间内编码音频信号占用的存储空间大小， 它的数值一般取决于所有的上述四个参数。MP3 编码的立体声音乐通常具有 128-196kbps 的比特率， FLAC 编码的立体声音乐通常具有 550-760kbps 的比特率。 我个人是这样想的，，，可以将一段音频数据看成很长很长的一排苹果树，从头走到尾，每隔一段距离停下，摘下满满一筐苹果。。。筐的大小就是采样大小，停下来采摘的次数就是采样频率，比特率就是把一定数量的苹果“榨成汁”（以特定的格式对音频编码）以后的重量，当然有些榨汁方法会造成一定的损失。 格式转换形式最简单的 sox 命令即使用两个文件名作为参数，如：$ sox Faded.wav Faded.mp3 ：将 Faded.wav 文件的格式由 wav 转为 mp3 上述命令执行时，SoX 会先从 Faded.wav 文件中读取音频数据，再将其输出到 Faded.mp3 文件中。而 SoX 程序会根据参数中文件名的后缀推断出相应的格式，并在复制音频数据的过程中自动进行转码。 SoX 可以处理 self-describing 和 raw 格式的音频文件。self-describing 格式（如 WAV、FLAC、MP3）的文件包含一个用于描述信号和编码属性的文件头，而 raw 或 headless 格式的音频则不包含这些信息。 所以当 raw 格式的音频作为输入文件时，需要在 sox 命令的格式选项里指定其信号和编码属性。 常用的音频格式选项： 选项 描述 b 每个编码样本占用的数据位数 c 音频文件包含的通道数 e 音频文件的编码类型 r 音频文件的采样率 t 音频文件的文件类型 上述选项适用于输入或输出文件，主要用于说明 raw（或 headless）文件作为输入时的格式信息，或格式转换时指定输出文件的具体参数。 $ sox −r 48k −e float −b 32 −c 2 input.raw output.wav将某个特定的 raw 格式的音频文件转换为 wav 格式 $ sox Faded.wav Faded.raw将音频文件 Faded.wav 转为 raw 格式 $ play -r 44800 -b 16 -e signed-integer -c 2 Faded.raw播放 raw 格式的音频文件 $ sox Faded.wav -c 1 Faded-mono.wav将 Faded.wav 文件转换成单声道（-c 1）后输出 三、音频效果SoX 工具可以在音频处理的过程中，对输入的音频数据应用众多的效果。可以使用如下命令查看所有效果的帮助信息：1234567891011121314151617181920212223$ sox --help-effect all | lesssox: SoX vEffect usage:allpass frequency width[h|k|q|o]band [-n] center [width[h|k|q|o]]bandpass [-c] frequency width[h|k|q|o]bandreject frequency width[h|k|q|o]bass gain [frequency(100) [width[s|h|k|q|o]](0.5s)]bend [-f frame-rate(25)] [-o over-sample(16)] &#123;start,cents,end&#125;: 也可以直接查看具体某个音频效果的使用方法：123456$ sox --help-effect echosox: SoX vEffect usage:echo gain-in gain-out delay decay [ delay decay ... ] 以下是一些简单的应用场景。 1. 更改声道数sox 命令可以更改音频文件中声道的数目，如将单声道音频转换成双声道：$ sox foo.wav foostereo.wav channels 2 或 $ sox foo.wav -c 2 foostereo.wav 但是上述命令并没有创建一个“真实”的双声道音频，而是将单声道音频复制成完全一致的两个声道再合并到输出文件中。 可以通过 sox 命令的 -M 选项将左右两个声道的单声道音频合并成一个双声道文件：$ sox -M left.wav right.wav stereo.wav 当然，也可以通过对双声道文件中两个声道的均一化处理，将其输出为单声道音频：$ sox original.wav mono.wav channels 1 或 $ sox original.wav -c 1 mono.wav remix通过 sox 命令的 remix 效果也可以完成对声道数据的提取或融合。 提取双声道音频文件中单个声道的数据并作为单声道音频输出：$ sox stereo.wav left.wav remix 1 （提取左声道音频）$ sox stereo.wav right.wav remix 2 （提取右声道音频） 融合双声道文件中两个声道的音频数据并作为单声道音频输出：$ sox stereo.wav mono.wav remix 1,2 或$ sox stereo.wav mono.wav remix 1-2 此外，remix 还可以将输入文件中的多个声道数据分别进行融合。如使用 -M 选项将两个双声道音频合并，再通过 remix 将合并得到的四个声道两两融合，生成一个只包含两个声道的输出文件。$ sox -M stereo1.wav stereo2.wav output.wav remix 1,3 2,4 2. 改变音量sox 命令的 -v 选项可以用来（成倍地）改变音量的大小：$ sox -v 0.5 foo.wav bar.wav上述命令将 foo.wav 音频放大 0.5 倍音量后输出至 bar.wav 文件 可以将音量放大功能与 stat 效果结合。以 sox foo.wav -n stat -v 命令返回的数字作为放大倍数，将最大化 foo.wav 的音量而不至于出现削波：12$ sox foo.wav -n stat -v 2&gt; vc$ sox -v `cat vc` foo.wav foo-maxed.wav 此外，还有一个选项 --norm 用来归一化音频响度。为了最大化音频的声音强度，可以在处理输入音频时将该选项设置为 -1：sox --norm=-1 &lt;inputfile&gt; &lt;outputfile&gt; 3. 提取文件的某个部分sox 命令的 trim 效果可以将输入音频的某一段裁剪出来并提取到输出文件中。 trim 接收两个参数，一个作为裁剪片段的起始位置，另一个作为该片段持续的时间。可以使用整数+s格式的参数以样本个数作为计量单位，也可以直接使用 ((hh:)mm:)ss(.fs) 形式的时间参数。当参数为纯整数时，单位为秒。 $ sox Input.wav Half1.wav trim 0 30:00 截取输入文件中前 30 分钟的音频$ sox Input.wav Half2.wav trim 30:00 30:00 截取输入文件中从第 30 分钟开始到第 60 分钟的音频 4. 拼接文件与前面裁剪提取的操作相反，sox 命令还可以实现对两个或多个音频文件的拼接。 $ sox Half1.wav Half2.wav Full.wav将 Half1.wav 和 Half2.wav 合并至 Full.wav 文件。注意合并前的音频文件需保持一致的类型和采样率等。 5. 合成音频sox 命令可以通过 synth 效果合成许多标准波形和噪声类型。 $ sox -n sine.wav synth 1.0 sine 1000.0合成频率为 1000 Hz 长度为 1 秒的正弦波，保存至 sine.wav 文件中。 synth 支持合成的声音类型包括 sine、square、triangle、sawtooth、trapetz (trapezoidal)、exp (exponential)、whitenoise、pinknoise 和 brownnoise。 6. 静音效果sox 命令可以创建静音状态的音频片段，使用 -n 选项表示没有输入，通过 trim 效果指定需要静音的片段。 $ sox -n -r 48000 silence.wav trim 0.0 0.250在 slience.wav 文件中创建一段长度为 250ms 采样率为 48000Hz 的静音片段。 7. 混合音频sox 命令的 -m 选项可以将两个音频文件混合以后生成输出文件。 $ sox -m sine100.wav sine250.wav sine100-250.wav将 sine100.wav 和 sine250.wav 两个音频文件融合以后作为 sine100-250.wav 文件的音频数据。 $ sox -m -v0.5 music.mp3 -v2 speech.wav presentation.wav将背景音乐（music.mp3）音量降低一半后与放大 2 倍音量的人声数据（speech.wav）融合。 如果不确定融合效果，可以先通过 play 命令使用相同的参数对结果进行“预览”：$ play -m -v0.5 music.mp3 -v2 speech.wav PS：与前面的 -M 选项不同，-m 选项倾向于对声道数据的混合，即两个单声道文件通过 -m 混合以后输出仍是单声道数据。输出文件中的单个声道包含了输入的两个声道的特征。 而 -M 选项更倾向于对音频文件的合并，默认不对声道数据进行混合。所以两个单声道文件通过 -M 合并以后默认输出双声道音频。输出文件中的两个声道分别对应于输入的两个声道（数据没有混合）。除非通过 -c 选项手动指定输出文件的声道数量。 8. 改变播放速度可以通过 stretch 效果改变音频文件的播放速度，同时不会导致音高的变化。 如以 2x 倍速播放 Faded.wav 文件：$ play Faded.wav stretch 0.5 也可以通过 speed 效果调节播放速度（相应地音高也会发生变化）：$ play Faded.wav speed 2 此外，可以使用 pitch 效果调节音频片段的音高，以音分（cents）为单位。 $ play Faded.wav pitch 200将 Faded.wav 文件中的音频提高 200 音分，即提高 2 个半音的音程（每一个半音的音程等于 100 音分）。 参考资料SoXUsing SoxHow to process audio files from the command line with SoX]]></content>
      <categories>
        <category>Tools</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Tools</tag>
        <tag>Software</tag>
        <tag>Media</tag>
        <tag>Audio</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 使用 crontab 创建定时任务]]></title>
    <url>%2F2018%2F12%2F14%2Fmake-schedule-using-crontab%2F</url>
    <content type="text"><![CDATA[Linux 系统里的 cron 守护进程，可以跟随系统启动，是一个用来创建定时任务的基本工具。所谓定时或计划任务，指的是在某些特定的时间点自动地执行特定的命令。这些相对应的时间点和命令可以在其配置文件 Crontab 中设置。 Crontabcrontab 即 cron table 的缩写，是 cron 程序的配置文件。其中包含了对每一条计划任务的具体配置。可以在命令行中使用 crontab -e 命令对该配置文件进行编辑。 使用限制 只有当用户的账号出现在 /usr/lib/cron/cron.allow 文件中时，该用户才可以使用 crontab 命令 如 /usr/lib/cron/cron.allow 文件不存在，而用户的账号没有出现在 /usr/lib/cron/cron.deny 文件中，该用户可以使用 crontab 命令 当只存在 cron.deny 文件且该文件内容不为空，则系统中所有用户都可以使用 crontab 两个文件都不存在时，则只有 root 用户可以访问 crontab 命令选项 crontab filename 从 filename 文件中导入配置，并替代之前的内容 crontab -e 编辑配置文件 crontab 的内容，定义计划任务 crontab -l 列出当前用户已经定义好的计划任务 crontab -r 移除当前用户的 crontab 配置文件（删除该用户所有的计划任务）配置文件格式crontab 配置文件中的每一行内容都代表了对某个计划的定义，一共分成 6 个字段。前 5 个字段定义时间，最后一个字段表明执行的命令。具体如下：minute hour day month weekday command 各时间字段的含义和取值范围如下： 字段 描述 取值范围 minute 具体某分钟 0 到 59 hour 具体某小时 0 到 23 day 一个月内的某天 1 到 31 month 具体某个月份 1 到 12 weekday 一周内的某天 0 到 6（0=周日） 时间字段的合法格式： 单独的数字（n）：表示当前时间单位下某个特定的时间点 被 - 连接的两个数字（n1-n2）：表示当前时间单位下的某个时间范围 由逗号分隔的多个数字或范围（n1,n2,n3...）：表示当前时间单位下多个时间点（段） 星号（*）：表示当前时间范围下的所有时间点（段） 时间范围和步长（n1-n2/n）：表示在某个时间范围内，每隔一段固定的时长，执行一次特定的命令 如： minute hour day/month month day/week 任务执行时间 30 9 14 3 * 每年3月14号的上午9点30分 0 2 * * 6 每个周六的凌晨2点整 45 8 * * 1-5 每周一到周五的上午8点45分 0 0 1,15 * * 每个月1号或15号半夜0点 0 * 13 * 5 每个月13号的整点，或者每个周五的整点 */10 * * * * 每隔10分钟 二、配置示例 30 1 * * * find /tmp -atime +3 -type f -exec rm -f {} &#39;;&#39;每天凌晨1点半，使用 find 命令查找 /tmp 目录下所有3天以上没有被访问过的文件，并将它们全部删除 30 2 * * 1 cd /home/joe/project; make每天凌晨2点半，进入 /home/joe/project 目录并执行 make 命令]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Admin</tag>
        <tag>System</tag>
        <tag>Tools</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[我喜欢的开源软件合集（设计类）]]></title>
    <url>%2F2018%2F12%2F11%2Fmy-favorite-opensource-design-software%2F</url>
    <content type="text"><![CDATA[本人非专业设计人员，甚至连业余都算不上。。。只是平时无聊，偶尔做点小东西聊以自娱。下面列出的软件多为自己平时用过或者想要学习的一些开源软件，主要面向设计类工作（如图像编辑、音视频剪辑、CAD和电子电路等）。稍作总结，并无足够的专业性与参考价值。 一、图像处理1. GIMPGIMP (GNU Image Manipulation Program) 是一个开源的图像处理工具，支持多种操作系统。其基本理念和操作都和 Adobe Photoshop 类似，对于使用过同类软件的人基本没有入手难度。 GIMP 既可以作为简单的画图程序，也能作为专家级的照片处理（包括照片润饰、图像合成和创建图像等）程序，或在线批处理系统、批量图像渲染器，以及图像格式转换工具等。GIMP 具有可延伸性和可扩展性，能通过扩展插件完成各种事情。其高级脚本接口允许用户通过编写简单的脚本完成从最简单到最复杂的各种图像处理过程。 特性与功能 完整的图像工具套件，包括画笔、铅笔、喷枪、克隆等。 基于平铺(Tile-based)的内存管理使图像大小限制在可用的磁盘空间内。 对所有涂画工具都使用次像素(Sub-pixel)取样，能产生高品质的反锯齿效果。 完全的 Alpha 通道支持。 支持图层和通道。 允许从外部程序(如 Script-Fu) 调用 GIMP 内部命令。 先进的脚本化处理能力。 多级撤消/重做(只受磁盘空间大小限制)。 变换工具包括旋转，缩放，切变和翻转。 支持包括 GIF、JPEG、PNG、XPM、TIFF、TGA、MPEG、PS、PDF、PCX、BMP 在内的多种文件格式。 选择工具包括矩形、椭圆、自由、模糊、贝赛尔曲线和智能剪刀。 通过插件可以轻松地添加对新文件格式的支持和效果滤镜。 相关资源 GIMP 官网 用户手册 官方教程 2. InkscapeInkscape 是一个开源的矢量图形编辑软件，功能类似于 Adobe Illustrator。只不过它使用 SVG (Scalable Vector Graphics) 作为原生的格式。 Inkscape 拥有复杂的绘图工具，常被设计从业人员及爱好者用于创建各种图形，如插图，图标，徽标，图表，地图和 Web 图形。同时也支持导入和导出各种文件格式，包括 SVG，AI，EPS，PDF，PS 和 PNG。 相关资源 Inkscape 官网 软件功能与特性 官方教程 Logos By Nick 的视频教程（YouTube） 3. KritaKrita 是一款开源的专业绘画软件，其目标是打造一款人人都用得起的数字绘画工具。适用于： 概念美术设计 材质与电影布景 插画和漫画等 相关资源 Krita 官网 功能介绍 用户手册 二、音视频剪辑1. AudacityAudacity 是一款开源的多轨道音频编辑软件。虽然界面看起来有点复古。。。但是功能很实用且非常容易上手。 软件特性 Audacity 可以通过麦克风或混音器录制现场音频，或者将其他媒体上的录音数字化 导入、编辑和组合声音文件，并以多种不同的文件格式导出录制内容 通过重采样和抖动完成高质量的采样率和格式转换，支持 16位、24位和32位音频 支持 LADSPA、LV2、Nyquist、VST 和 Audio Unit 效果插件。可以在文本编辑器中轻松修改效果，甚至可以编写自己的插件。 通过剪切、复制、粘贴和删除轻松编辑音频数据，无限制地顺序撤销和重做 LADSPA、LV2、VST 和音频单元（macOS）效果的实时预览 可以使用键盘快捷键完全操纵音频轨道和选区 频谱图视图模式用于可视化和选择频率； Plot Spectrum 窗口用于详细的频谱分析 相关资源Audacity 官网官方文档（在线版） 2. ShotcutShotcut 是一个开源且跨平台的视频剪辑软件，适用于 Windows、Mac 和 Linux 系统。主要功能包括多种音视频格式的支持、原生时间线编辑、分辨率支持 4k 等。 相关资源 Shotcut 官网 功能与特性 官方教程 3. HandBrakeHandBrake 是一种视频格式转换器，可以将视频从几乎任何格式转换为当前受广泛支持的编码格式。 相关资源 Handbrake 官网 特性 官方文档 三、3D建模与CAD1. BlenderBlender 是一个开源的3D动画套件，支持整个 3D建模、装配、动画、模拟、渲染、合成和运动跟踪等创作流程，甚至包含视频编辑和游戏创建功能。 Blender 是非常惊艳的 Big Buck Bunny 等开源动画电影的主要制作工具。嗯，不明觉厉。。。 相关资源Blender 官网功能与特性官方支持（文档、示例等）Blender 2.79 中文手册 2. OpenSCADOpenSCAD 是用于创建实体3D CAD模型的开源软件。与主要用于创作3D动画的开源软件 Blender 不同，它更关注于3D建模的CAD方面。OpenSCAD 不是一个交互式建模软件。相反，它类似于3D编译器，读入描述对象的脚本文件并通过脚本中定义的建模过程呈现3D模型。 OpenSCAD 提供了两种主要的建模技术：建设性的实体几何（onstructive solid geometry）以及对2D轮廓的挤压。Autocad DXF文件可用作此类2D轮廓的数据交换格式。 相关资源OpenSCAD 官网官方文档Github 主页 2. QCADQCAD 是一个用于二维计算机辅助绘图（CAD）的开源软件。可以用其创建建筑物、室内设计、机械部件或电路图等技术图表。支持 Windows, macOS 和 Linux 系统，其源代码在 GPLv3 协议下发布。 相关资源QCAD 官网功能与特性官方文档 3. SketchUpSketchUp 是一个闭源的3D建模软件，适用于建筑学、室内设计、景观建设、土木和机械工程、电影和视频游戏设计等多种绘图场景。SketchUp 是一种以易用性著称的 CAD 解决方案，使用该应用程序就像使用笔和纸绘图一样简单。它采用直接推拉的编辑技术，确保可以快速生成日常用品的模型。 SketchUp 包括一个免费的在线版应用程序 SketchUp Free，一个免费的桌面版应用程序 SketchUp Make（2017年12月以后停止更新，迁移到在线版。但仍提供下载），和一个带有附加功能的付费版本 SketchUp Pro。SketchUp 还提供一个在线的模型组件库（门窗、桌椅、花草等等）3D Warehouse，用户可以向其上传自己创作的模型，同时也支持模型的免费下载。 相关资源SketchUp 官网帮助中心官方 YouTube 电子电路1. EAGLEAutodesk EAGLE 是一款闭源的电子设计自动化（EDA）软件。它使得印刷电路板（PCB）设计人员能够轻松地完成原理图设计、元件布局、PCB布线等工作。EAGLE 有提供免费版供爱好者和创客使用。 原理图编辑器 使用一整套SPICE仿真方法快速测试方案并验证电路性能 在项目之间自由拖放可重复使用的区块，自动同步原理图和PCB电路 通过一整套电子规则检查验证原理图设计 PCB布局编辑器 使用一整套对齐工具精确地组合与排列PCB设计对象 根据预先定义的设计原则自适应地、交互式地完成路径选择 通过完全可定制的PCB设计规则和约束控制设计流程，避免无法预料的意外情况。 再一次不明觉厉。。。有点尴尬 相关资源EAGLE 官网特性与功能官方学习中心 2. FritzingFritzing 是一款电子设计自动化软件，学习难度非常低，很适合创客和业余爱好者们的需求，在 Arduino 和 Raspberry Pi 用户中非常受欢迎。它提供了真实的“面包板”视图，其丰富的零件库中也包含了许多常用的高级组件（如各种型号的 Arduino 板）。Fritzing 还可以将实物化的电子设计转换成可用于生产的PCB布局，不过貌似功能很基础。。。 相关资源Fritzing 官网Github 主页官方文档 3. TinkercadTinkercad是一款免费的用于3D设计、电子电路和编程的在线软件，主要面向老师、孩子、业余爱好者和设计师。该软件主要由 3D Designs 和 Circuits 两个模块构成，之所以放在电子电路这一小节，主要是我个人对它的 3D设计模块不太感冒（有点儿童化？）。。。不过创建用于 3D打印的模型应该是挺方便的。 倒是电路模块比较吸引我，界面类似于 Fritzing ，可以利用 Blocks 或者纯代码编写程序。竟然还支持仿真，能够真实地模拟出实际电路的运行状态（比如 LED 闪烁、舵机转动等）。 相关资源Tkinercad 官网素材库官方教程 其他（个人爱好）StellariumStellarium 是一款免费开源的天文馆软件，可以用来显示逼真的 3D星空景观，就像用肉眼、双筒望远镜或天文望远镜看到的一样。。。有手机版，不过好像从 Google 应用商店下载是付费的。移动版可以开启“跟随”模式，根据手机方向的变动自动切换显示的星空区域，野外看星星的利器。 相关资源Stellarium 官网 附注呃，有点划水了。。基本上相关资源都是官网的链接，，有时间好好找找其他地方有价值的资料。。]]></content>
      <categories>
        <category>Tools</category>
      </categories>
      <tags>
        <tag>Tools</tag>
        <tag>Software</tag>
        <tag>Design</tag>
        <tag>Opensource</tag>
        <tag>media</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux find 命令详解]]></title>
    <url>%2F2018%2F12%2F05%2FLinux-find-reference-manual%2F</url>
    <content type="text"><![CDATA[Linux 上的 find 命令是 findutil 软件包的一部分，一般已经默认集成在了几乎所有的发行版中。find 命令有非常大的灵活性，可以向其指定丰富的搜索条件（如文件权限、属主、属组、文件类型、日期和大小等）来定位系统中的文件和目录。此外，find 还支持对搜索到的结果进行多种类型的命令操作。 一、简介find 命令的基本结构如下：find [paths] [expression] [actions] find 命令接受一个或多个路径（paths）作为搜索范围，并在该路径下递归地搜索。即检索完指定的目录后，还会对该目录下的子目录进行检索，以及子目录下的子目录。。。直到到达目录树底部。 默认情况下（不带任何搜索条件），find 命令会返回指定目录下的所有文件，所以常常需要通过特定的 expression 对结果进行筛选。 find 命令默认的 action 是将所有检索结果打印至标准输出。可以通过自定义 action ，让 find 命令对搜索到的结果执行特定的操作。 这里先不做详细解释，简单地测试下 find 命令： 有如下结构的示例目录 directory 1234567891011121314$ tree directorydirectory├── file1├── file2├── sub-dir1│ ├── file1│ ├── file2│ └── file3└── sub-dir2 ├── file2 └── sub-subdir1 └── file13 directories, 7 files 默认的 find 命令会列出该目录下的所有文件 123456789101112$ find directorydirectorydirectory/sub-dir1directory/sub-dir1/file3directory/sub-dir1/file2directory/sub-dir1/file1directory/file2directory/sub-dir2directory/sub-dir2/file2directory/sub-dir2/sub-subdir1directory/sub-dir2/sub-subdir1/file1directory/file1 为 find 命令指定 expression 以筛选出特定的文件 1234$ find directory -name file2directory/sub-dir1/file2directory/file2directory/sub-dir2/file2 为 find 命令指定特殊的 action（此处 -delete 表示删除搜索结果） 12345678910$ find directory -name file2 -delete$ find directorydirectorydirectory/sub-dir1directory/sub-dir1/file3directory/sub-dir1/file1directory/sub-dir2directory/sub-dir2/sub-subdir1directory/sub-dir2/sub-subdir1/file1directory/file1 二、搜索条件（expression）1. 根据文件名检索find 命令中的 -name 选项可以根据文件名称进行检索（区分大小写）。如需要忽略文件名中的大小写，可以使用 -iname 选项。 -name 和 -iname 两个选项都支持 wildcards 。如： ? 可以表示任意一个单一的符号 * 可以表示任意数量（包括 0）的未知符号 find /usr -name &#39;*.txt&#39; 查找 /usr 目录下所有文件名以 .txt 结尾的文件find /usr -name &#39;????&#39; 查找 /usr 目录下所有文件名刚好为 4 个字符的文件 有些时候，你需要在搜索时匹配某个文件或目录的完整路径，而不仅仅是匹配文件名。可以使用 -path 或 -ipath 选项。 如查找 /usr 下所有文件名以 .txt 结尾的文件或目录，且该文件的父目录必须是 src。可以使用以下命令：find /usr -path &#39;*/src/*.txt&#39; 2. 根据文件类型检索如果只想搜索得到文件或目录，即不想它们同时出现在结果中。可以使用 -type 选项指定文件类型。 -type 选项最常用的参数如下： f: 文件 d: 目录 l: 符号链接 find /usr -type d -name &#39;python*&#39; 检索 /usr 下所有文件名以 python 开头的目录 3. 检索空文件find 命令支持 -empty 选项用来检索为空的文件或目录。空文件即文件里没有任何内容，空目录即目录中没有任何文件或子目录。 find ~ -type d -empty 检索用户主目录下所有的空目录 4. 反义匹配find 命令也允许用户对当前的匹配条件进行“反义”（类似于逻辑非操作）。 如需要检索 /usr 下所有文件名不以 .txt 为后缀的文件。可以使用以下命令：find /usr -type f ! -name &#39;*.txt&#39; 也可以“翻转”任何其他的筛选条件，如：find /usr -type f ! -empty 检索 /usr 下所有内容不为空的文件 5. 根据文件的所属权检索为了检索归属于特定用户的文件或目录，可以使用 -user 选项。 find / -type f -user starky 检索根目录下所有属主为 starky 的文件 类似于 -user选项，-group 选项则可以根据文件或目录的属组进行检索。 6. 根据时间日期进行检索有些时候，需要根据文件创建或修改的时间进行检索。 Linux 系统中，与文件相关联的时间参数有以下三种： 修改时间（Modification time）：最后一次文件内容有过更改的时间点 访问时间（Access time）：最后一次文件有被读取过的时间点 变更时间（Change time）：最后一次文件有被变更过的时间点（如内容被修改，或权限等 metadata 被修改） 与此对应的是 find 命令中的 -mtime，-atime 和 -ctime 三个选项。 这三个选项的使用遵循以下示例中的规则： -mtime 2：该文件 2 天前被修改过 -mtime -2：该文件 2 天以内被修改过 -mtime +2：该文件距离上次修改已经超过 2 天时间 find /usr -type f -mtime 2 检索 /usr 下两天前被修改过的文件 如果觉得 -mtime 等选项以天为单位时间有点长，还可以使用 -mmin，-amin，-cmin 三个选项：find /usr -type f -mtime +50 -mtime -100 检索 /usr 下 50 到 100 天之前修改过的文件find /usr -type f -mtime 2 -amin 5 检索 /usr 下两天前被修改过且 5 分钟前又读取过的文件 7. 根据文件大小检索-size 选项允许用户通过文件大小进行搜索（只适用于文件，目录没有大小……）。 表示文件大小的单位由以下字符组成： c：字节 k：Kb M：Mb G：Gb 另外，还可以使用 + 或 - 符号表示大于或小于当前条件。 find / -size +1G 检索文件大小高于 1 GB 的文件 8. 根据文件权限检索find 命令可以使用 -perm 选项以文件权限为依据进行搜索。 使用符号形式如需要检索 /usr 目录下权限为 rwxr-xr-x 的文件，可以使用以下命令：find /usr -perm u=rwx,g=rx,o=rx 搜索 /usr 目录下所有权限为 r-xr-xr-x（即系统中的所有用户都只有读写权限）的文件和目录，可以使用以下命令：find /usr -perm a=rx 很多时候，我们只想匹配文件权限的一个子集。比如，检索可以直接被任何用户执行的文件，即只关心文件的执行权限，而不用管其读写权限是什么。 上述的需求可以通过以下命令实现：find / -type f -perm /a=x其中 a=x 前面的 / 符号即用来表示只匹配权限的某个子集（执行权限），而不用关心其他权限的具体设置。 使用数字形式-perm 选项也支持数字形式的文件权限标记。 find /usr -perm 644 搜索 /usr 目录下权限为 644（即 rwxr-xr-x）的文件 9. 限制遍历的层数find 命令默认是以递归的方式检索项目的，这有时候会导致得到的结果数量非常巨大。可以使用 -maxdepth 限制 find 命令递归的层数。 find / -maxdepth 3 搜索时向下递归的层数最大为 3 10. 逻辑组合在之前的例子中有出现多个搜索条件的组合以及对某个搜索条件的反转。实际上 find 命令支持 “and” 和 “or” 两种逻辑运算，对应的命令选项分别是 -a 和 -o。通过这两个选项可以对搜索条件进行更复杂的组合。 此外还可以使用小括号对搜索条件进行分组。注意 find 命令中的小括号常需要用单引号包裹起来。因小括号在 Shell 中有特殊的含义。 如检索 /usr 下文件名以 python 开头且类型为目录的文件find /usr -type d -name &#39;python*&#39; 该命令等同于：find /usr -type d -a -name &#39;python*&#39; 更复杂的组合形式如：find / &#39;(&#39; -mmin -5 -o -mtime +50 &#39;)&#39; -a -type f 三、对搜索结果执行命令1. 删除文件-delete 选项可以用来删除搜索到的文件和目录。 如删除 home 目录下所有的空目录：find ~ -type d -empty -delete 2. 执行自定义命令-exec 选项可以对搜索到的结果执行特定的命令。 如需要将 home 目录下所有的 MP3 音频文件复制到移动存储设备（假设路径是 /media/MyDrive），可使用下面的命令：find ~ -type f -name &#39;*.mp3&#39; -exec cp {} /media/MyDrive &#39;;&#39; 其中的大括号（{}）作为检索到的文件的 占位符 ，而分号（ ;）作为命令结束的标志。因为分号是 Shell 中有特殊含义的符号，所以需要使用单引号括起来。每当 find 命令检索到一个符合条件的文件，会使用其完整路径取代命令中的 {}，然后执行 -exec 后面的命令一次。 另一个很重要的用法是，在多个文件中检索某个指定的字符串。如在用户主目录下的所有文件中检索字符串 hello ，可以使用如下命令：find ~ -type f -exec grep -l hello {} &#39;;&#39; -exec 选项中的 + 符号创建 Gzip 格式的压缩文件的命令为：tar -czvf filename.tar.gz &lt;list of files&gt; 现在假设需要将用户主目录下所有的 MP3 文件添加到压缩包 music.tar.gz 中，直观的感觉是，其命令应为如下形式：find ~ -type f -name &#39;*.mp3&#39; -exec tar -czvf music.tar.gz {} &#39;;&#39; 实际情况是，这样得到的 music.tar.gz 其实只包含一个 MP3 文件。原因是 find 命令每次发现一个音频文件，都会再执行一次 -exec 选项后面的压缩命令。导致先前生成的压缩包被覆盖。 可以先让 find 命令检索出所有符合条件的音频文件，再将得到的文件列表传递给后面的压缩命令。完整的命令如下：find ~ -type f -name &#39;*.mp3&#39; -exec tar -czvf music.tar.gz {} + 显示文件信息如果想浏览搜索到的文件（目录）的详细信息（如权限和大小等），可以直接使用 -ls 选项。 find / -type file -size +1G -ls 浏览所有 1G 以上大小的文件的详细信息 四、常用参数汇总 参数 解析 -atime n[smhdw] 距离文件上次被访问时的时间间隔 -ctime n[smhdw] 距离文件创建时的时间间隔 -delete 删除检索到的文件 -depth n 检索深度为 n 的文件，即位于指定目录以下 n 层的文件 -empty 检索空文件或空目录 -fstype type 指定文件所在的文件系统的类型 -group gname 指定文件的属组 -iname pattern 同 -name，忽略大小写 -ipath pattern 同 -path，忽略大小写 -ls 打印搜索到的文件的详细信息 -maxdepth n 指定递归的最大层数为 n -mtime n[smhdw] 距离文件上次发生变更时的时间间隔 -name pattern 搜索时使用 pattern 对文件名进行匹配 -path pattern 搜索时使用 pattern 对文件路径进行匹配 -perm mode 根据文件权限搜索 -size n[ckMGTP] 根据文件大小搜索 -type t 根据文件类型搜索 -user uname 指定文件的属主 参考资料A Guide to the Linux “Find” Commandfind 命令手册：man find]]></content>
      <categories>
        <category>Tools</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Admin</tag>
        <tag>Tools</tag>
        <tag>Commands</tag>
        <tag>Shell</tag>
        <tag>Manual</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Shell 脚本编程（高级篇）]]></title>
    <url>%2F2018%2F12%2F03%2Fshell-script-programming-professional%2F</url>
    <content type="text"><![CDATA[高级篇一、处理用户输入1. 读取脚本参数Bash Shell 将命令行中传递给脚本的参数赋值给一组特殊的变量，叫做位置变量（positional parameters）。位置变量用 $number 的形式表示。如 $0 表示脚本文件的名称，$1 表示脚本收到的第一个参数，$2 表示第二个参数，以此类推，直到 $9 表示第九个参数。从第十个参数起，使用 ${number} 的形式。即第十个参数表示为 ${10}。 示例程序：1234567891011$ cat add.sh#!/bin/bashtotal=$[ $1 + $2 ]echo &quot;The first parameter is $1&quot;echo &quot;The second parameter is $2&quot;echo &quot;The total value is $total&quot;$ ./add.sh 2 5The first parameter is 2The second parameter is 5The total value is 7 2. 参数检查在 Shell 脚本中使用命令行参数时，一般需要先对传入的参数进行检查。如脚本执行时没有接收到预想的参数，往往会在执行过程中报出错误。如：12345$ ./add.sh 2./add.sh: line 3: 2 + : syntax error: operand expected (error token is &quot; &quot;)The first parameter is 2The second parameter isThe total value is 所以参数检查是写脚本时很有必要的步骤：12345678910111213$ cat check_parameter.sh#!/bin/bashif [ -n &quot;$1&quot; ]then echo Hello $1, glad to meet you.else echo &quot;Sorry, you did not identify youself&quot;fi$ ./check_parameter.sh starkyHello starky, glad to meet you.$ ./check_parameter.shSorry, you did not identify youself 3. 特殊参数变量参数计数上面有提到，应该在使用命令行参数前先检查其是否符合要求。对于接收多个参数的脚本，有时候需要获取命令行中输入的参数数量。Shell 脚本中的 $# 变量保存了该脚本执行时接收到的参数的数量。如：12345678910$ cat count_parameters.sh#!/bin/bashecho There were $# parameters supplied.$ ./count_parameters.shThere were 0 parameters supplied.$ ./count_parameters.sh testThere were 1 parameters supplied.$ ./count_parameters.sh test testThere were 2 parameters supplied. 通过 $# 变量的使用，可以将之前的 add.sh 脚本优化为以下形式：1234567891011121314151617$ cat add2.sh#!/bin/bashif [ $# -ne 2 ]then echo Usage: add2.sh a belse total=$[ $1 + $2 ] echo The total is $totalfi$ ./add2.shUsage: add2.sh a b$ ./add2.sh 2Usage: add2.sh a b$ ./add2.sh 2 4The total is 6 获取所有参数某些情况下需要获取命令行提供的所有参数。除了通过 $# 变量使用循环，还可以直接使用另外两个特殊的变量。 $* 和 $@ 变量都可以包含命令行输入的所有参数。其中 $* 变量接收所有参数并将它们保存在一个单一的字符串中。$@ 变量接收所有参数并将它们保存在分开的字符串中。即 $* 变量将收到的所有参数作为整体的一个参数对待，而 $@ 变量将收到的所有参数作为不同的多个对象，可以使用 for 命令进行遍历。 这两个变量的区别可以通过以下脚本来区分：123456789101112131415161718192021222324$ cat iterate_parameters.sh#!/bin/bashcount=1for param in &quot;$*&quot;do echo &quot;\$* Parameter $count = $param&quot; count=$[ $count + 1 ]doneechocount=1for param in &quot;$@&quot;do echo &quot;\$@ Parameter $count = $param&quot; count=$[ count + 1 ]done$ ./iterate_parameters.sh rich barbara katie jessica$* Parameter 1 = rich barbara katie jessica$@ Parameter 1 = rich$@ Parameter 2 = barbara$@ Parameter 3 = katie$@ Parameter 4 = jessica 4. shift 命令shift 命令可以用来操作命令行参数，对它们进行整体的移位。默认情况下，shift 命令会将所有的命令行参数整体的向左移动一个位置。即 $3 变量的值移动到 $2，$2 变量的值移动到 $1。如：123456789$ cat shift.sh#!/bin/bashecho &quot;The original parameters: $*&quot;shift 2echo &quot;Here&apos;s the new parameters: $*&quot;$ ./shift.sh 1 2 3 4 5The original parameters: 1 2 3 4 5Here&apos;s the new parameters: 3 4 5 二、函数1. 函数定义Bash Shell 脚本中的函数可以用以下两种格式定义：123function name &#123; commands&#125; 或123name() &#123; commands&#125; 具体示例如下：1234567891011121314151617181920212223242526$ cat func1.sh#!/bin/bashfunction func1 &#123; echo &quot;This is an example of a function&quot;&#125;count=1while [ $count -le 5 ]do func1 count=$[ $count + 1 ]doneecho &quot;This is the end of the loop&quot;func1echo &quot;Now this is the end of the script&quot;$ ./func1.shThis is an example of a functionThis is an example of a functionThis is an example of a functionThis is an example of a functionThis is an example of a functionThis is the end of the loopThis is an example of a functionNow this is the end of the script 2. 函数返回值退出状态默认情况下，某个函数的完成状态（即退出码）即是该函数中最后一条命令的退出码。在该函数执行后，可以使用 $? 变量获取其退出状态码。12345678910111213141516$ cat exit_status.sh#!/bin/bashfunc1() &#123; echo &quot;trying to display a non-existent file&quot; ls -l badfile&#125;echo &quot;testing the function: &quot;func1echo &quot;The exit status is: $?&quot;$ ./exit_status.shtesting the function:trying to display a non-existent filels: badfile: No such file or directoryThe exit status is: 1 因为函数 func1 中的最后一条命令 ls -l badfile 没有执行成功，所以函数执行完后，变量 $? 的值为 1 而不是 0 return 命令Bash Shell 可以使用 return 命令指定函数退出时的状态值（整数）。123456789101112131415$ cat return.sh#!/bin/bashfunction db1 &#123; read -p &quot;Enter a value: &quot; value echo &quot;doubling the value&quot; return $[ $value *2 ]&#125;db1echo &quot;The new value is $?&quot;$ ./return.shEnter a value: 32doubling the valueThe new value is 64 PS: 注意函数执行结束后需要立即使用 $? 获取其返回值，前面不能隔有其他命令；使用 return 指定的退出码必须介于 0 到 255 之间 3. 函数输出就像可以把命令的输出内容赋值给 Shell 变量一样，函数的输出同样也可以通过 variable=$(function_name) 的形式赋值给某个变量。12345678910111213$ cat func2.sh#!/bin/bashfunction db1 &#123; read -p &quot;Enter a value: &quot; value echo $[ $value * 2 ]&#125;result=$(db1)echo &quot;The new value is $result&quot;$ ./func2.shEnter a value: 32The new value is 64 4. 函数中的变量向函数传递参数Bash Shell 对待函数就像对待普通的脚本文件一样。我们可以向脚本程序传递参数，也可以以类似的方式向函数传递参数。 函数可以使用标准参数（如 $#）环境变量代表它从命令语句里接收到的参数。如函数本身的名称由 $0 定义，$1 代表第一个参数，$# 代表接收到的参数的数目，$* 表示函数接收到的所有参数 ($1 $2 ...) ，&quot;$@&quot; 表示函数接收到的所有参数，且每一个参数都被双引号包裹 (&quot;$1&quot; &quot;$2&quot; ...) 123456789101112131415161718192021222324252627282930313233343536$ cat parameter.sh#!/bin/bashfunction addem &#123; if [ $# -eq 0 ] || [ $# -gt 2 ] then echo -1 elif [ $# -eq 1 ] then echo $[ $1 + $1 ] else echo $[ $1 + $2 ] fi&#125;echo &quot;Adding 10 and 15:&quot;value=$(addem 10 15)echo $valueecho &quot;Try adding just one number:&quot;value=$(addem 10)echo $valueecho &quot;Now trying adding no numbers:&quot;value=$(addem)echo $valueecho &quot;Finally, try adding three numbers:&quot;value=$(addem 10 15 20)echo $value$ ./parameter.shAdding 10 and 15:25Try adding just one number:20Now trying adding no numbers:-1Finally, try adding three numbers:-1 上述脚本中的 addem 函数通过 $# 变量检查传递给它的参数的数量： 如果参数数量等于 0 或大于 2（[ $# -eq 0 ] || [ $# -gt 2 ]），则返回 -1 表示程序非正常退出。 如果参数数量等于 1（[ $# -eq 1 ]），则返回两倍于该参数的值（$[ $1 + $1 ]）。 如果参数数量等于 2（[ $# -eq 2 ]），则返回这两个参数的加和（$[ $1 + $2 ]） 全局变量与局部变量变量的作用域是一个很容易引起问题的点。函数中定义的变量可以拥有区别于普通变量的作用域，即它们可以对脚本中的其他部分“不可见”。 函数使用两种类型的变量： 全局变量 局部变量 全局变量是在整个脚本中都保持有效的变量。默认情况下，Shell 脚本中的任何变量都是全局变量。12345678910111213$ cat global.sh#!/bin/bashfunction db1 &#123; value=$[ $value * 2 ]&#125;read -p &quot;Enter a value: &quot; valuedb1echo &quot;The new value is: $value&quot;$ ./global.shEnter a value: 24The new value is: 48 区别于函数中的全局变量，任何只在函数内部生效的变量可以声明为局部变量，只需要在变量的声明前面加上 local 关键字即可。 1234567891011121314151617$ cat local.sh#!/bin/bashfunction func1 &#123; local temp=$[ $value + 5 ] result=$[ $temp * 2 ]&#125;temp=4value=6func1echo &quot;The result is $result&quot;echo &quot;The temp value is $temp&quot;$ ./local.shThe result is 22The temp value is 4 由于使用了 local 关健字指定函数内部的 $temp 变量为局部变量，所以函数 func1 中 $temp 变量值的变化（变为 11）并不影响函数外部 $temp 变量的值（仍为 4）。 5. 函数递归这里用一个计算阶乘的示例简单说明下 Shell 脚本中的函数递归。1234567891011121314151617181920$ cat factorial.sh#!/bin/bashfunction factorial &#123; if [ $1 -eq 1 ] then echo 1 else local temp=$[ $1 - 1 ] local result=$(factorial $temp) echo $[ $result * $1 ] fi&#125;read -p &quot;Enter value: &quot; valueresult=$(factorial $value)echo &quot;The factorial of $value is: $result&quot;$ ./factorial.shEnter value: 4The factorial of 4 is: 24 6. 库函数的使用可以减少脚本中的重复代码。即可以在脚本的其他部分直接使用函数名调用函数，完成该函数定义的功能，而无需再重新输入一遍定义该函数的大段语句。 这种形式的代码复用可以扩展到多个脚本文件。Bash Shell 允许用户创建库文件，其他 Shell 脚本可以通过引用该库文件使用其中定义的函数。 示例如下：12345678910111213141516171819202122232425262728293031323334$ cat myfuncsfunction addem &#123; echo $[ $1 + $2 ]&#125;function multem &#123; echo $[ $1 * $2 ]&#125;function divem &#123; if [ $2 -ne 0 ] then echo $[ $1 / $2 ] else echo -1 fi&#125;$ cat use_myfuncs.sh. ./myfuncsvalue1=10value2=5result1=$(addem $value1 $value2)result2=$(multem $value1 $value2)result3=$(divem $value1 $value2)echo &quot;The result of adding them is: $result1&quot;echo &quot;The result of multiplying them is: $result2&quot;echo &quot;The result of dividing them is: $result3&quot;$ ./use_myfuncs.shThe result of adding them is: 15The result of multiplying them is: 50The result of dividing them is: 2 其中 myfuncs 库文件中分别定义了 addem multem divem 三个函数，use_myfuncs.sh 脚本用来引用 myfuncs 库并使用其中定义的函数（这两个文件都需要有执行权限）。 重点在于 use_myfuncs.sh 脚本中的第一行命令 . ./myfuncs。其中第一个 . 是 source 命令的缩写。source 命令的作用是在当前语境下调用另一个脚本，而不是创建一个新的 Shell 会话。这样 myfuncs 中的函数就可以直接被 use_myfuncs.sh 脚本使用。 参考资料Linux Command Line and Shell Scripting Bible 3rd Edition]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Admin</tag>
        <tag>Shell</tag>
        <tag>Programming</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用 JavaScript 对 Arduino 编程]]></title>
    <url>%2F2018%2F12%2F03%2Fjavascript-arduino-programming%2F</url>
    <content type="text"><![CDATA[一、向 Arduino 刷入 Firmata 固件 Firmata 是一种开放的类似于 MIDI 的协议。它可以将微控制器（microcontroller）变成“客户端”，通过串口通信直接被“主机”电脑访问和控制。通过 Firmata 协议，可以在主机上使用 Ruby、Python、JavaScript 等多种语言与刷入了 Firmata 固件的 arduino 板“交流”。 首先安装 firmata-party 工具：$ npm install -g firmata-party安装成功后通过 USB 线将 Arduino 板连接至电脑，运行以下命令：123456$ firmata-party uno --debugfound uno on port /dev/tty.usbserial-14110connectedreset complete.flashing, please wait...flash complete. 刷写完毕后，可以通过 Firmata Test Program 进行简单的测试。 二、使用 JavaScript 编程Blink LED可以使用如下命令开始一个新的项目：123$ mkdir test &amp;&amp; cd test$ npm init -y$ npm install --save firmata 编辑源代码文件（blink_led.js）用来控制 LED 闪烁：123456789101112131415161718192021222324252627282930313233// 加载依赖库var Board = require(&apos;firmata&apos;);// 连接初始化Board.requestPort(function(error, port) &#123; if (error) &#123; console.log(error); return; &#125; var board = new Board(port.comName); // 等待连接 board.on(&quot;ready&quot;,function() &#123; console.log(&quot;Ready!&quot;); var ledOn = true; // 设置 pin 13 为输出引脚 board.pinMode(13, board.MODES.OUTPUT); // 使 LED 闪烁 setInterval(function() &#123; if (ledOn) &#123; console.log(&apos;ON&apos;); board.digitalWrite(13, board.HIGH); &#125; else &#123; console.log(&apos;OFF&apos;); board.digitalWrite(13, board.LOW); &#125; ledOn = !ledOn; &#125;, 1000); &#125;);&#125;); 运行效果如下：123456$ node blink_led2.jsReady!ONOFFON... 代码执行后 arduino 板子上 13 号引脚上接的 LED 开始以 1 秒的间隔闪烁，同时电脑的终端界面不停地输出 ON 和 OFF 表示当前 LED 的开关状态。 Analog InputArduino 板子上的 A0 - A5 引脚可以读取模拟输入信号。示例代码如下：1234567891011121314151617181920212223242526272829303132333435363738394041424344// 加载依赖库var Board = require(&apos;firmata&apos;);// 引脚定义const LED = 5;const POT = 0;// 初始化变量var ledOn = 0;var delay = 0;// map 函数。将初始的数值范围转变成需要的数值范围function map(x, in_min, in_max, out_min, out_max)&#123; return (x - in_min) * (out_max - out_min) / (in_max - in_min) + out_min;&#125;// 连接初始化Board.requestPort(function(error, port) &#123; if (error) &#123; console.log(error); return; &#125; var board = new Board(port.comName); // 等待连接 board.on(&quot;ready&quot;, function() &#123; // 控制 LED 闪烁的 blink() 函数 function blink() &#123; board.digitalWrite(LED, ledOn); ledOn = !ledOn; setTimeout(blink, delay); &#125; // 读取并更新可变电阻提供的模拟输入 board.analogRead(0, function(d) &#123; delay = map(d, 0, 1023, 400, 1600); &#125;); blink(); &#125;);&#125;); 线路连接图如下： 即通过 Arduino 板 A0 引脚可以读取模拟输入的功能（连接可变电阻的中间引脚），获取可变电阻的取值（0-1023）。再通过程序中的 map 函数将 0-1023 的取值转变成 400-1600，作为接在 5 号引脚上 LED 的闪烁频率。 程序运行后，调节可变电阻的旋钮，LED 即以不同的时间间隔（0.4s - 1.6s）闪烁。 PWM (Pulse-Width Modulation)PWM 即脉宽调制，其机制为：通过 MCU 内部的计时器设置引脚的状态，使该引脚在一个周期内的某段时间保持 HIGH 状态，在该周期内的其余时间保持 LOW 状态。平均下来看，该引脚的输出电压即介于最低和最高输出电压之间（即 0~5 V 之间）。所以一般可以近似地作为模拟电压输出。1234567891011121314151617181920212223242526272829303132333435// 加载依赖库var Board = require(&apos;firmata&apos;);// 变量定义const LED = 5;var brightness = 0;var fadeAmount = 5;// 连接初始化Board.requestPort(function(error, port) &#123; if (error) &#123; console.log(error); return; &#125; var board = new Board(port.comName); // 等待连接 board.on(&quot;ready&quot;, function() &#123; // 设置 LED 引脚为 PWM 模式 board.pinMode(LED, board.MODES.PWM); // 每隔 30 ms 提高或降低 LED 亮度，循环执行 function fadeLed() &#123; brightness += fadeAmount; if (brightness ==0 || brightness == 255) &#123; fadeAmount = -fadeAmount; &#125; board.analogWrite(LED, brightness); setTimeout(fadeLed, 30); &#125; fadeLed(); &#125;);&#125;); 该程序执行后，5 号引脚上的 LED 灯先是缓慢变亮，达到最亮以后再缓慢变暗。依照此规则循环。 附录：JavaScript 的串口通信即使 Arduino 板子未刷入 Firmata 固件，运行的是普通的程序。也可以利用 Node.js 的 serialport 库和串口通信完成 Arduino 与主机的对话。 首先初始化项目并安装依赖库：123$ mkdir test2 &amp;&amp; cd test2$ npm init -y$ npm install --save-dev serialport 扫描串口设备12345678910$ cat list_ports.jsvar serialPort = require(&quot;serialport&quot;);serialPort.list(function (error, ports) &#123; ports.forEach(function(port) &#123; console.log(port.comName); &#125;);&#125;);$ node list_ports.js/dev/tty.Bluetooth-Incoming-Port/dev/tty.usbserial-14110 从 Arduino 收取数据首先通过 Arduino IDE 刷入以下 counter.ino 文件，作为发送端程序：123456789void setup() &#123; Serial.begin(9600);&#125;int i=0;void loop() &#123; Serial.println(i++); delay(1000);&#125; 上述代码以 1000 ms 的间隔生成从 0 开始不断递增的数字并发送至串口。 电脑端的接收程序 read_port.js 代码如下：123456const SerialPort = require(&apos;serialport&apos;)const Readline = require(&apos;@serialport/parser-readline&apos;)const port = new SerialPort(&apos;/dev/cu.usbserial-14110&apos;)const parser = port.pipe(new Readline(&#123; delimiter: &apos;\n&apos; &#125;))parser.on(&apos;data&apos;, console.log) 运行效果如下：发送端不断生成新的数字并发送至串口，接收端通过串口接收该数字后将其打印到终端输出。 向 Arduino 发送数据首先通过 Arduino IDE 刷入以下 receiver.ino 文件，作为接收端程序：1234567891011121314void setup() &#123; Serial.begin(9600);&#125;void loop() &#123; int incoming = 0; if (Serial.available() &gt; 0) &#123; incoming = Serial.parseInt(); if (Serial.read() == &apos;\n&apos;) &#123; Serial.println(incoming); &#125; &#125;&#125; 发送端的 write_port.js 程序代码如下：1234567891011121314151617var SerialPort = require(&apos;serialport&apos;);var Stream = require(&apos;stream&apos;);var modem = &apos;cu.usbserial-14110&apos;;var ws = new Stream();ws.writable = true;ws.write = function(data) &#123; serialPort.write(data);&#125;;ws.end = function(buf) &#123; console.log(&apos;bye&apos;);&#125;var serialPort = new SerialPort(&apos;/dev/&apos; + modem);process.stdin.pipe(ws); 运行效果如下： 电脑端发送程序运行后（$ node write_port.js），打开 Arduino IDE 的 Serial Monitor。在命令行输入任何一个数字并按回车进行确认，Arduino 板子接收到该数字后，又将其打印到串口输出（即 Serial Monitor 中显示的内容） 参考资料Node.js for Embedded Systems: Using Web Technologies to Build Connected Devices 1st EditionNode SerialPort]]></content>
      <categories>
        <category>IoT</category>
      </categories>
      <tags>
        <tag>IOT</tag>
        <tag>Maker</tag>
        <tag>JavaScript</tag>
        <tag>Programming</tag>
        <tag>Arduino</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Shell 脚本编程（中级篇）]]></title>
    <url>%2F2018%2F11%2F15%2Fshell-script-programming-medium%2F</url>
    <content type="text"><![CDATA[中级篇一、结构化命令在基础篇的示例中，Shell 大多数情况下都以顺序的方式依次执行脚本中的每一条指令。但是在实际情况中，很多程序都需要对脚本中命令的逻辑流进行控制。即根据某些条件和状态完成判断，再决定其后的程序该以怎样的规则执行。Shell 脚本支持一部分结构化指令，如 if-then、case 等，用于修改程序的执行流程。 1. if-then最基本的结构化语句即 if-then，其格式如下：1234if commandthen commandsfi Bash Shell 中的 if-then 语句和其他编程语言中的类似语句稍有些不同，其作用如下： 首先执行 if 后面跟随的命令 command 如果该命令 command 的结束状态（exit status）为 0（即该命令执行成功），则 then 后面的命令依次被执行 如果该命令 command 的结束状态为任何非 0 的值（该命令非正常退出），则不执行 then 后面的命令，Bash Shell 移动到脚本中的下一条命令（即 fi 后面的内容） 如下面的两个示例：1234567891011121314151617181920212223$ cat if1.sh#!/bin/bashif pwdthen echo &quot;It worked&quot;fiecho &quot;We are outside the if statement&quot;$ ./if1.sh/Users/starky/program/scripts/shellIt workedWe are outside the if statement$ cat if2.sh#!/bin/bashif IamNotaCommandthen echo &quot;It worked&quot;fiecho &quot;We are outside the if statement&quot;$ ./if2.sh./if2.sh: line 3: IamNotaCommand: command not foundWe are outside the if statement 在上面的示例中，if1.sh 脚本里 if 后面的命令（pwd）成功执行，则 then 后面的 echo 命令也跟着执行。而 if2.sh 脚本里 if 后面的命令（IamNotaCommand）执行出错，则 then 后面的 echo 命令不会被执行。无论如何，判断结束后都会继续执行 fi 后面的语句。 2. if-else-thenif-else-then 语句的格式为：123456if commandthen commandselse commandsfi 当 if 后面的命令执行后返回值为 0 时，则 then 后面的命令被执行。如果 if 后面的命令执行后返回值不为 0，则执行 else 后面的命令。 示例（if3.sh）如下：123456789101112#!/bin/bashtestuser=starkyif ls -d /Users/$testuserthen echo &quot;The bash files for user $testuser are:&quot; ls -a /Users/$testuser/.b* echoelse echo &quot;The user $testuser does not exist on this system.&quot; echofi 运行结果：1234567$ chmod +x if3.sh$ ./if3.sh/Users/starkyThe bash files for user starky are:/Users/starky/.bash_history /Users/starky/.bash_profile/Users/starky/.bash_sessions:... 但是将上述脚本中 testuser 变量的值改为系统中并不存在的用户时（如 NoSuchUser），则运行效果如下：12345678910111213141516$ cat if3.sh#!/bin/bashtestuser=NoSuchUserif ls -d /Users/$testuserthen echo &quot;The bash files for user $testuser are:&quot; ls -a /Users/$testuser/.b* echoelse echo &quot;The user $testuser does not exist on this system.&quot; echofi$ ./if3.shls: /Users/NoSuchUser: No such file or directoryThe user NoSuchUser does not exist on this system. 其他结构形式的 if-else-then 语句还包括：12345678if command1then commandselif command2then more commandsfi 当然理论上讲可以使用足够的 if-then-elif-then 语句，只不过类似情况下多使用更为恰当的 case 语句。 3. test 命令上面提到的 if-then 语句，大多是这样的形式：1234if commandthen commands... 即 if 后面跟的是一条普通的 Shell 命令，根据该命令是否成功执行，来确定 then 后面的命令是否执行。 可以借助 test 命令，使用如下形式的 if-then 语句：1234if test conditionthen commandsfi 如果 test 命令后面的 condition 计算后为 TRUE，则 test 命令退出并返回一个值为 0 的状态值。then 后面的命令被执行。如果 test 命令后面的 condition 计算后为 FALSE，则 test 命令退出并返回一个非零的状态值。跳出当前的 if-then 并继续执行后面的内容。 如下面的两个例子：1234567891011121314151617181920212223242526$ cat test_condition.sh#!/bin/bashmy_variable=&quot;Full&quot;if test $my_variablethen echo &quot;The $my_variable expression returns a True&quot;else echo &quot;The $my_variable expression returns a False&quot;fi$ ./test_condition.shThe Full expression returns a True$ cat test_condition2.sh#!/bin/bashmy_variable=&quot;&quot;if test $my_variablethen echo &quot;The $my_variable expression returns a True&quot;else echo &quot;The $my_variable expression returns a False&quot;fi$ ./test_condition2.shThe expression returns a False Bash Shell 还提供了另外一种不需要使用 test 命令来完成条件检查的 if-then 语句：1234if [ condition ]then commandsfi 方括号中的内容用来定义判断条件。注意方括号和 condition 之间的空格。支持三种形式的判断条件： 数字比较 字符串比较 文件比较 数字比较 操作 描述 n1 -eq n2 检查 n1 是否等于 n2 n1 -ge n2 检查 n1 是否大于或等于 n2 n1 -gt n2 检查 n1 是否大于 n2 n1 -le n2 检查 n1 是否小于或等于 n2 n1 -lt n2 检查 n1 是否 小于 n2 n1 -ne n2 检查 n1 是否不等于 n2 示例如下：1234567891011121314151617181920$ cat numeric_test.sh#!/bin/bashvalue1=10value2=11if [ $value1 -gt 5 ]then echo &quot;The test value $value1 is greater than 5&quot;fiif [ $value1 -eq $value2 ]then echo &quot;The values are equal&quot;else echo &quot;The values are different&quot;fi$ ./numeric_test.shThe test value 10 is greater than 5The values are different 字符串比较 操作 描述 str1 = str2 检查 str1 和 str2 是否相同 str1 != str2 检查 str1 和 str2 是否不相同 str1 &lt; str2 检查 str1 是否小于 str2 str1 &gt; str2 检查 str1 是否大于 str2 -n str1 检查 str1 是否长度大于 0（不为空） -z str1 检查 str1 是否长度为 0（为空） 示例1：12345678910111213$ cat welcome.sh#!/bin/bashtestuser=starkyif [ $USER = $testuser ]then echo &quot;Welcome $testuser&quot;else echo &quot;This is not $testuser&quot;fi$ ./welcome.shWelcome starky PS：在使用 str1 &gt; str2 或 str1 &lt; str2 这种类型的条件时，需加上转义符号。否则大于号或小于号会被当作重定向处理。 示例2：1234567891011121314$ cat compare.sh#!/bin/bashval1=Testingval2=testingif [ $val1 \&gt; $val2 ]then echo &quot;$val1 is greater than $val2&quot;else echo &quot;$val1 is less than $val2&quot;fi$ ./compare.shTesting is less than testing PS：if 条件中的比较依据的是基本 ASCII 顺序，通过每个字符（从首字母开始）的 ASCII 值的比较来判断大小顺序。 -n 和 -z 常常用来确定指定字符串是否为空。示例3：123456789101112131415161718192021222324252627282930$ cat empty.sh#!/bin/bashval1=testingval2=&apos;&apos;if [ -n $val1 ]then echo &quot;The string &apos;$val1&apos; is not empty&quot;else echo &quot;The string &apos;$val1&apos; is empty&quot;fiif [ -z $val2 ]then echo &quot;The string &apos;$val2&apos; is empty&quot;else echo &quot;The string &apos;$val2&apos; is not empty&quot;fiif [ -z $val3 ]then echo &quot;The string &apos;$val3&apos; is empty&quot;else echo &quot;The string &apos;$val3&apos; is not empty&quot;fi$ ./empty.shThe string &apos;testing&apos; is not emptyThe string &apos;&apos; is emptyThe string &apos;&apos; is empty 上述脚本中 val3 变量自始至终没有被定义，被自动判断为空字符串。 文件比较 操作 描述 -d file 检查 file 是否存在且是一个目录 -e file 检查 file 是否存在 -f file 检查 file 是否存在且是一个文件 -r file 检查 file 是否存在且可读 -s file 检查 file 是否存在且不为空 -w file 检查 file 是否存在且可写 -x file 检查 file 是否存在且可执行 -O file 检查 file 是否存在且其属主为当前用户 -G file 检查 file 是否存在且其默认数组和当前用户相同 file1 -nt file2 检查 file1 是否比 file2 更新（newer than） file1 -ot file2 检查 file1 是否比 file2 更老（older than） 示例1，目录检查：1234567891011121314151617$ cat check_dir.sh#!/bin/bashdirectory=/Users/starkyif [ -d $directory ]then echo &quot;The $directory directory exists&quot; cd $directory lselse echo &quot;The $directory directory does not exist&quot;fi$ ./check_dir.shThe /Users/starky directory existsApplications Downloads Music extract.sh programDesktop Library Pictures id_rsa.pub softwareDocuments Movies Public miniconda3 vim.tar.gz 示例2，删除空文件：12345678910111213141516171819202122232425262728293031323334$ cat check_empty.sh#!/bin/bashfile_name=empty# 检查该文件是否存在if [ -f $file_name ]then # 该文件存在，则继续检查其内容是否非空 if [ -s $file_name ] then # 内容不为空 echo &quot;The $file_name file exists and has data in it.&quot; echo &quot;Will not remove this file&quot; else # 内容为空 echo &quot;The $file_name file exists, but is empty.&quot; echo &quot;Deleting empty file...&quot; rm $file_name fielse # 该文件不存在 echo &quot;File, $file_name, does not exist.&quot;fi$ ./check_empty.shFile, empty, does not exist.$ touch empty$ ls emptyempty$ ./check_empty.shThe empty file exists, but is empty.Deleting empty file...$ ls emptyls: empty: No such file or directory 4. 组合判断if-then 语句允许使用布尔逻辑来完成多个条件的组合判断。 [ condition1 ] &amp;&amp; [ condition2 ] 逻辑与 [ condition1 ] || [ condition2 ] 逻辑或 示例：1234567891011121314cat compound_test.sh#!/bin/bashif [ -d $HOME ] &amp;&amp; [ -w $HOME/testing ]then echo &quot;The file exists and you can write to it&quot;else echo &quot;I cannot write to the file&quot;fi$ ./compound_test.shI cannot write to the file$ touch ~/testing$ ./compound_test.shThe file exists and you can write to it if-then 中的双括号Bash Shell 在 if-then 语句中还提供了一些更高级的特性： 双小括号（数学表达式） 双中括号（字符串处理函数） 使用双小括号双小括号允许在条件测试时使用（相对于 test 命令）更高级的数学表达式。格式：(( expression ))其中 expression 除了支持前面提到过的基本的数学操作符外，还支持以下操作符： 操作符 描述 val++ Post-increment val– Post-decrement ++val Pre-increment –val Pre-decrement ! 逻辑非 ~ 按位否定 ** 幂 &lt;&lt; 向左按位移位 &gt;&gt; 向右按位移位 &amp; 按位逻辑与 管道符 按位逻辑或 &amp;&amp; 逻辑与 双管道符 逻辑或 示例：123456789101112$ cat parenthesis.sh#!/bin/bashval1=10if (( $val1 ** 2 &gt; 90 ))then (( val2 = $val1 ** 2 )) echo &quot;The square of $val1 is $val2&quot;fi$ ./parenthesis.shThe square of 10 is 100 使用双中括号双中括号为 test 命令中的字符串比较提供了更高级的特性，尤其是支持 模式匹配。示例：1234567891011cat pattern.sh#!/bin/bashif [[ $USER == s* ]]then echo &quot;Hello $USER&quot;else echo &quot;Sorry, I do not know you&quot;fi$ ./pattern.shHello starky 4. case 命令很多时候，某个作为判断条件的变量有多个可能的取值，而你需要根据不同的取值指导程序去做对应的操作。如果使用 if-then-else 语句，代码免不了会拉得有些长。。。 不同于 if 语句（通过 if、elif 依次检查同一个变量的所有取值），case 命令以类似列表的形式检查同一个变量的所有取值：12345case variable in pattern1 | pattern2) commands1;; pattern3) commands2;; *) default commands;;esac case 命令将指定的变量与多个不同的模式进行比对，如果匹配，则执行该模式后面的命令。可以在同一行中定义多个模式（使用 | 符号分隔）。星号表示当之前的所有模式都不能被匹配时，默认执行的命令。 示例：12345678910111213141516$ cat case_user.sh#!/bin/bashcase $USER in rich | barbara) echo &quot;Welcome, $USER&quot; echo &quot;Please enjoy your visit&quot;;; testing) echo &quot;Special testing account&quot;;; jessica) echo &quot;Do not forget to logout when you&apos;re done&quot;;; *) echo &quot;Sorry, you are not allowed here&quot;;;esac$ ./case_user.shSorry, you are not allowed here 上面的脚本等同于使用如下的 if 语句：123456789101112131415161718$ cat if_user.sh#!/bin/bashif [[ $USER = &quot;rich&quot; || $USER = &quot;barbara&quot; ]]then echo &quot;Welcome $USER&quot; echo &quot;Please enjoy your visit&quot;elif [ $USER = &quot;testing&quot; ]then echo &quot;Special testing account&quot;elif [ $USER = &quot;jessica&quot; ]then echo &quot;Do not forget to logout when you&apos;re done&quot;else echo &quot;Sorry, you are not allowed here&quot;fi$ ./if_user.shSorry, you are not allowed here 二、更多结构化命令1. for 命令很多时候，需要重复地执行一系列命令直到某个特定的情况出现。for 命令允许创建一个循环来遍历某些值，并在循环过程中使用遍历到的值执行一系列特定的指令。语法格式：1234for var in listdo commandsdone 遍历列表12345678910111213$ cat for_list.sh#!/bin/bashfor state in Alabama Alaska Arizonado echo The next state is $statedoneecho &quot;The last state we visited was $state&quot;$ ./for_list.shThe next state is AlabamaThe next state is AlaskaThe next state is ArizonaThe last state we visited was Arizona 每次 for 命令遍历由后面列表（Alabama Alaska Arizona）提供的值时，会把列表中当前项目的值赋值给 $state 变量（即第一次遍历把 Alabama 赋值给 $state，第二次是 Alaska），该 $state 变量可以在后面的 do ... done 结构中使用。 for 循环结束后 $state 变量依旧有效，即保留最后一次遍历时得到的值（列表的最后一项 Arizona）而不会被销毁。 遍历字符串中的列表Bash Shell 中的for 循环可以直接接收单个字符串作为参数，并通过其中的空格将该字符串分割成多个遍历的项目。123456789101112131415$ cat for_variable.sh#!/bin/bashstates=&quot;Alabama Alaska Arizona&quot;states=$states&quot; Arkansas&quot;for state in $statesdo echo &quot;Have you ever visited $state?&quot;done$ ./for_variable.shHave you ever visited Alabama?Have you ever visited Alaska?Have you ever visited Arizona?Have you ever visited Arkansas? 脚本中 state=$state&quot; Arkansas&quot; 可以用来在 $state 变量中的字符串末尾添加新的字符串，也等同于在列表末尾添加了新的项目。 从命令中获取遍历的列表可以通过命令替换将命令的输出作为 for 命令循环遍历的列表：12345678910111213$ cat states.txtAlabama Alaska Arizona$ cat for_command.sh#!/bin/bashfor state in $(cat states.txt)do echo &quot;Visit beautiful $state&quot;done$ ./for_command.shVisit beautiful AlabamaVisit beautiful AlaskaVisit beautiful Arizona 遍历文件目录可以使用 for 命令直接循环目录中的文件（结合 * 符号）：1234567891011121314151617181920$ cat for_dir.sh#!/bin/bashfor file in /Users/starky/.vim* /Users/starky/badtestdo if [ -d &quot;$file&quot; ] then echo &quot;$file is a directory&quot; elif [ -f &quot;$file&quot; ] then echo &quot;$file is a file&quot; else echo &quot;$file doesn&apos;t exit&quot; fidone$ ./for_dir.sh/Users/starky/.vim is a directory/Users/starky/.viminfo is a file/Users/starky/.vimrc is a file/Users/starky/badtest doesn&apos;t exit 注意脚本中的 if [ -d &quot;$file&quot; ] 段代码， &quot;$file&quot; 被双引号包裹了起来。原因是文件名中包含空格是合法的，但是在 Shell 脚本中，如果 $file 变量的值包含空格，则不能直接在 if [ -d ... ] 中使用，所以这里使用了双引号。 2. C 语法的 for 命令Bash Shell 也支持语法类似于 C 语言形式的 for 循环：for (( variable assignment ; condition ; iteration process )) 123456789101112131415161718$ cat for_c.sh#!/bin/bashfor (( i=1; i &lt;= 10; i++ ))do echo &quot;The next number is $i&quot;done$ ./for_c.shThe next number is 1The next number is 2The next number is 3The next number is 4The next number is 5The next number is 6The next number is 7The next number is 8The next number is 9The next number is 10 3. while 命令while 命令允许用户定义一个判断指令，然后循环执行一系列命令，并在每次执行前检查判断指令的返回值（exit status）。直到判断指令的返回值不为 0，终止循环。 while 命令的格式为：1234while test commanddo other commandsdone 示例程序如下：123456789101112131415$ cat while.sh#!/bin/bashvar=5while [ $var -gt 0 ]do echo $var var=$[ $var -1 ]done$ ./while.sh54321 4. break 命令break 命令用于跳出循环 123456789101112131415161718$ cat break.sh#!/bin/bashfor var in 1 2 3 4 5 6 7 8 9 10do if [ $var -eq 5 ] then break fi echo &quot;Iteration number: $var&quot;doneecho &quot;The for loop is completed&quot;$ ./break.shIteration number: 1Iteration number: 2Iteration number: 3Iteration number: 4The for loop is completed 5. 处理循环的输出可以使用管道或重定向处理循环的输出信息，直接将管道或重定向命令加在循环语句块的末尾即可。 12345678910111213141516$ cat output.sh#!/bin/bashfor (( a = 1; a &lt; 5; a++ ))do echo &quot;The number is $a&quot;done &gt; numbers.txtecho &quot;The command is finished&quot;$ ./output.shThe command is finished$ cat numbers.txtThe number is 1The number is 2The number is 3The number is 4 参考资料Linux Command Line and Shell Scripting Bible 3rd Edition]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Admin</tag>
        <tag>Shell</tag>
        <tag>Programming</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Shell 脚本编程（基础篇）]]></title>
    <url>%2F2018%2F11%2F15%2Fshell-script-programming-basic%2F</url>
    <content type="text"><![CDATA[基础篇一、输出信息大部分 Shell 命令都会生成自己的输出信息，在脚本运行时打印到终端屏幕上。但是很多时候，仍需要在输出的信息中添加上自己的内容，以提示用户脚本运行时究竟发生着什么，达到更好的交互效果。 echo 命令可以用来打印字符串内容。123$ echo This is a testThis is a test$ PS：默认是不需要将 echo 命令后面的字符串包含在一对引号中的 echo 可以使用引号作为文本字符串的分隔符。如：1234$ echo &apos;Hello&gt; World&apos;HelloWorld 当输出的文本内容中本来就有引号出现时，如（Let&#39;s see if this&#39;ll work），可以结合单、双引号的使用，或者使用转义符（\）前缀123456789# 引号作为字符串分隔符而不是文本内容$ echo Let&apos;s see if this&apos;ll workLets see if thisll work# 使用 \ 前缀进行转义$ echo Let\&apos;s see if this\&apos;ll workLet&apos;s see if this&apos;ll work# 使用双引号作为分隔符，单引号作为中间的文本正常输出$ echo &quot;Let&apos;s see if this&apos;ll work&quot;Let&apos;s see if this&apos;ll work 脚本的执行权限默认创建的脚本文件没有执行权限，不能直接在命令行中运行。如创建包含以下内容的脚本文件 info.sh ：123456#!/bin/bashecho The time and date are:dateecho &quot;Let&apos;s see who&apos;s logged into the system:&quot;who 直接执行上述脚本时会提示 permission denied 错误：12$ ./info.shzsh: permission denied: ./info.sh 需要使用 chomd +x filename 命令为该脚本文件添加执行权限后再运行，效果如下：12345678$ chmod +x info.sh$ ./info.shThe time and date are:Wed Oct 31 11:48:07 CST 2018Let&apos;s see who&apos;s logged into the system:starky console Oct 28 12:42starky ttys000 Oct 29 16:52starky ttys001 Oct 31 11:23 二、变量1. 系统环境变量在 Shell 脚本中可以直接访问系统中的环境变量，以获取相关的系统信息（如计算机名称，当前登录用户的账户名、 UID 和主目录等）。当前定义的所有环境变量可以通过 set 命令获取。12345678910$ set...HISTCMD=2217HISTFILE=/Users/starky/.zsh_historyHISTSIZE=50000HOME=/Users/starkyHOST=skitars-MacBook-Pro.localIFS=$&apos; \t\n\C-@&apos;ITERM_PROFILE=starky... 当需要在 Shell 脚本中使用具体某个环境变量的值时，可以用环境变量名称加上 $ 前缀表示（如 $HOST）。编辑如下 sys_info.sh 文件：12345#!/bin/bashecho User info for userid: $USERecho UID: $UIDecho HOME: $HOME 运行效果如下：12345$ chmod +x sys_info.sh$ ./sys_info.shUser info for userid: starkyUID: 501HOME: /Users/starky PS：由于在 Shell 脚本中，$ 作为变量的前缀符，所以当需要在文本输出中显示 $ 时，应使用转义。123456# $15 被当成了代入到字符串中的“变量”$ echo &quot;The cost of the item is $15&quot;The cost of the item is# 使用 \ 转义后正常打印 $ 字符$ echo &quot;The cost of the item is \$15&quot;The cost of the item is $15 2. 用户自定义变量Shell 脚本允许用户自行定义和使用变量，这样就可以将脚本中用到的数据临时存储在指定的变量中，使用时再通过 $变量名 的形式获取。 变量赋值：var=value （注意 = 号两边不能有空格，即 var = value 是错误的） 变量使用：$var PS：Shell 脚本中的变量名区分大小写Shell 脚本会自动判断变量值的数据类型变量的有效性贯穿脚本的整个生命周期，即脚本执行完毕后变量会自行删除 编辑如下 variables.sh 文件：123456789#!/bin/bashdays=10guest=&quot;Katie&quot;echo &quot;$guest checked in $days days ago&quot;days=5guest=&quot;Jessica&quot;echo &quot;$guest checked in $days days ago&quot; 运行效果如下：12345$ chmod +x variables.sh$ ./variables.shKatie checked in 10 days agoJessica checked in 5 days ago$ echo $days 可以看到，脚本退出后，脚本中定义的 $days 变量又恢复为未定义的状态。 三、命令替换Shell 脚本最有用处的特性之一，就是它可以提取某个命令的输出信息，并将其赋值给一个变量。可以通过以下两种方式将命令输出赋值给变量： 反单引号（`） $() 格式 如：1234567891011# 使用 date 命令获取当前的日期和时间$ dateThu Nov 1 01:03:02 CST 2018# 将 date 命令的输出（即当前日期和时间）赋值给 var1 变量$ var1=`date`$ echo Today is: $var1Today is: Thu Nov 1 01:03:15 CST 2018# 将 date 命令的输出赋值给 var2 变量（使用 $() 格式）$ var2=$(date)$ echo Today is: $var2Today is: Thu Nov 1 01:03:36 CST 2018 示例程序：使用命令替换完成一个脚本（log.sh），该脚本可以创建以当前时间水印为后缀的文本文件，内容为 /usr/bin 目录下的所有文件列表。12345#!/bin/bashtoday=$(date +%y%m%d%H%M%S)ls -al /usr/bin &gt; log.$todayecho The file log.$today has been created, you can check it later. 其中 date +%y%m%d%H%M%S 命令可以输出纯数字格式的日期和时间运行效果如下：12345678910111213141516$ chmod +x log.sh$ ./log.shThe file log.181101012159 has been created, you can check it later.$ ls log*log.181101012159 log.sh$ head log.181101012159total 103992drwxr-xr-x 971 root wheel 31072 Oct 13 17:44 .drwxr-xr-x@ 9 root wheel 288 Sep 21 12:01 ..-rwxr-xr-x 4 root wheel 925 Aug 18 08:45 2to3-lrwxr-xr-x 1 root wheel 74 Oct 13 17:44 2to3-2.7 -&gt; ../../System/Library/Frameworks/Python.framework/Versions/2.7/bin/2to3-2.7-rwxr-xr-x 1 root wheel 55072 Sep 21 12:16 AssetCacheLocatorUtil-rwxr-xr-x 1 root wheel 53472 Sep 21 12:16 AssetCacheManagerUtil-rwxr-xr-x 1 root wheel 48256 Sep 21 12:17 AssetCacheTetheratorUtil-rwxr-xr-x 1 root wheel 18320 Sep 21 12:17 BuildStrings-rwxr-xr-x 1 root wheel 18288 Sep 21 12:17 CpMac 四、重定向输入和输出1. 输出重定向最基本的重定向，就是通过大于号（&gt;），将某个命令的输出内容保存至一个文件中。 格式：command &gt; outputfile 12345$ date &gt; current_date.txt$ ls -l current_date.txt-rw-r--r-- 1 starky staff 29 Nov 1 01:29 current_date.txt$ cat current_date.txtThu Nov 1 01:29:45 CST 2018 PS：如使用重定向时，指定的文件已存在，则该文件的原始内容会被新内容覆盖。如果只是想在文件末尾追加内容，则可以使用双大于号（&gt;&gt;）1234$ date &gt;&gt; current_date.txt$ cat current_date.txtThu Nov 1 01:29:45 CST 2018Thu Nov 1 01:34:03 CST 2018 2. 输入重定向输入重定向和输出重定向相反。即从文件中读取内容，并将该内容传递给某个命令。 格式：command &lt; inputfile 12345678910111213141516171819202122$ ls -l /Users/starkytotal 49864drwx------@ 3 starky staff 96 Oct 13 18:00 Applicationsdrwx------+ 23 starky staff 736 Oct 31 19:14 Desktopdrwx------+ 21 starky staff 672 Oct 31 19:15 Documentsdrwx------+ 37 starky staff 1184 Oct 30 20:18 Downloadsdrwx------+ 72 starky staff 2304 Oct 27 01:38 Librarydrwx------+ 6 starky staff 192 Oct 29 10:34 Moviesdrwx------+ 3 starky staff 96 Sep 27 13:13 Music...$ cat directory.txt/Users/starky$ ls -l &lt; directory.txttotal 49864drwx------@ 3 starky staff 96 Oct 13 18:00 Applicationsdrwx------+ 23 starky staff 736 Oct 31 19:14 Desktopdrwx------+ 21 starky staff 672 Oct 31 19:15 Documentsdrwx------+ 37 starky staff 1184 Oct 30 20:18 Downloadsdrwx------+ 72 starky staff 2304 Oct 27 01:38 Librarydrwx------+ 6 starky staff 192 Oct 29 10:34 Moviesdrwx------+ 3 starky staff 96 Sep 27 13:13 Music... 3. 管道有些时候，需要将某个命令的输出内容作为另一个命令的输入。如：123456$ ls -al &gt; tmp_file$ grep vim &lt; tmp_filedrwxr-xr-x 3 starky staff 96 Oct 20 15:45 .vim-rw------- 1 starky staff 23799 Nov 1 01:40 .viminfo-rw-r--r-- 1 starky staff 3935 Oct 21 01:31 .vimrc-rw-r--r-- 1 starky staff 24849808 Oct 25 21:02 vim.tar.gz 上面的命令先将当前目录下的文件列表（ls -al）保存在 tmp_file 中，再使用 grep 命令读取 tmp_file 的内容，筛选文件名中包含 vim 的文件。 其实可以通过管道（|）的使用，将前面命令的输出，定向给后面的命令作为输入。 格式：command1 | command2 12345ls -al | grep vimdrwxr-xr-x 3 starky staff 96 Oct 20 15:45 .vim-rw------- 1 starky staff 23799 Nov 1 01:40 .viminfo-rw-r--r-- 1 starky staff 3935 Oct 21 01:31 .vimrc-rw-r--r-- 1 starky staff 24849808 Oct 25 21:02 vim.tar.gz 五、数学运算操作数学运算对于任何编程语言来说，都是一个很重要的特性。但是 Shell 脚本并不能直接完成算术运算的操作，只能通过以下两种方式来实现。 1. expr 命令Shell 提供了一个特殊的命令（expr）用来处理数学算式，如：12$ expr 1 + 23 PS：注意算式中 + 号两边的空格expr 命令支持的算术操作符如下： 操作符 含义 ARG1 双管道符 ARG2 如果两个参数值都不为 null 或 0，返回 ARG1，否则返回 ARG2 ARG1 &amp;&amp; ARG2 如果两个参数值都不为 null 或 0，返回 ARG1，否则返回 0 ARG1 &lt; ARG2 如果 ARG1 小于 ARG2，返回 1，否则返回 0 ARG1 &gt; ARG2 如果 ARG1 大于 ARG2，返回 1，否则返回 0 ARG1 = ARG2 如果 ARG1 等于 ARG2，返回 1，否则返回 0 ARG1 &gt;= ARG2 如果 ARG1 大于或等于 ARG2，返回 1，否则返回 0 ARG1 &lt;= ARG2 如果 ARG1 小于或等于 ARG2，返回 1，否则返回 0 ARG1 != ARG2 如果 ARG1 不等于 ARG2，返回 1，否则返回 0 ARG1 + ARG2 返回 ARG1 与 ARG2 的数字加和 ARG1 - ARG2 求 ARG1 减去 ARG2 的数字差 ARG1 * ARG2 返回 ARG1 与 ARG2 的数字乘积 ARG1 / ARG2 求 ARG1 除以 ARG2 的数字商（结果为整数） ARG1 % ARG2 对 ARG1 和 ARG2 进行求余操作 数学运算示例（divide.sh）： 1234567#!/bin/bashvar1=10var2=20var3=`expr $var2 / $var1`echo $var2 divided by $var1 equals $var3 运行效果：123$ chmod +x divide.sh$ ./divide.sh20 divided by 10 equals 2 2. 使用中括号Bash Shell 中的 expr 命令主要是为了保持和 Bourne Shell 的兼容性，它其实还提供了一种更简单的方式用来处理数学运算。即使用这样的形式：$[ operation ] 如下面的脚本（compute.sh）12345678#!/bin/bashvar1=100var2=50var3=45result=$[$var1 * ($var2 - $var3)]echo The final result is $result 运行效果：123$ chmod +x compute.sh$ ./compute.shThe final result is 500 但是在进行除法运算时，上面的方式只支持整数。如两个数相除结果为小数，则该结果会舍去小数部分只保留整数。12345$ var1=10$ var2=3$ result=$[$var1 / $var2]$ echo The result is $resultThe result is 3 3. 浮点数运算有很多种方案可以克服 bash 的整数限制，最常用的一种就是使用系统内置的 bash calculator，即 bc 程序。bash calculator 其实是一种支持浮点数运算的编程语言，可以识别以下几种类型的数据： 数字（整数和浮点数） 变量（简单变量和数组） 注释（单行注释 # 和多行注释 /* */） 表达式 编程语句（如 if-then 语句） 函数 12345678910$ bcbc 1.06Copyright 1991-1994, 1997, 1998, 2000 Free Software Foundation, Inc.This is free software with ABSOLUTELY NO WARRANTY.For details type `warranty&apos;.12 * 5.464.83.156 * (3 + 5)25.248quit 计算结果精确到的小数位数是通过一个内建的 scale 变量定义的，默认的 scale 数值为 0（即默认舍去商的小数位数）：1234567$ bc -q3.44 / 50scale=43.44 / 5.6880quit bc 程序也支持自定义变量：12345678$ bc -qvar1=10var1 * 440var2=var1 / 5print var22quit 4. 在脚本中使用 bc可以通过管道将数学表达式传递给 bc 程序，再将计算得出的结果通过赋值语句赋值给某个变量：variable=$(echo &quot;options; expression&quot; | bc) 其中 variable=$(...) 用于提取命令的输出并赋值给一个变量（参考第三章命令替换） 如下面的脚本（bc1.sh）:1234567#!/bin/bashvar1=100var2=45var3=$(echo &quot;scale=4; $var1 / $var2&quot; | bc)echo The answer for this is $var3 运行效果：123$ chmod +x bc1.sh$ ./bc1.shThe answer for this is 2.2222 更复杂的形式在脚本中使用 bc 还可以通过如下的形式：123456variable=$(bc &lt;&lt; EOFoptionsstatementsexpressionsEOF) 示例程序如下（bc2.sh）： 12345678910111213141516#!/bin/bashvar1=10.46var2=43.67var3=33.2var4=71var5=$(bc &lt;&lt; EOFscale = 4a1=$var1 * $var2b1=$var3 * $var4a1 + b1EOF)echo The final answer for this mess is $var5 运行结果：123$ chmod +x bc2.sh$ ./bc2.shThe final answer for this mess is 2813.9882 参考资料Linux Command Line and Shell Scripting Bible 3rd Edition]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Admin</tag>
        <tag>Shell</tag>
        <tag>Programming</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[树莓派作为 FM 发射台和 Webcam 服务器（视频监控）]]></title>
    <url>%2F2018%2F11%2F11%2Fraspberry-pi-fm-emitter-and-webcam-server%2F</url>
    <content type="text"><![CDATA[在网上发现两个很有意思的树莓派小项目。一个是作为 FM 发射器，向短距离内的 FM 收音机发送声波信号。一个是作为视频监控（配合摄像头），可以远程访问网页客户端获取摄像头监控的画面。实际尝试了下效果都还不错。 一、FM 发射器项目地址：PiFmRds该发射器使用树莓派的 PWM 生成器 来产生 VHF（甚高频） 信号。 编译安装PiFmRds 需要从源代码编译，而编译时依赖 sndfile 库。可以使用如下命令安装：$ sudo apt-get install libsndfile1-dev之后使用如下命令获取源代码并完成编译：1234$ git clone https://github.com/ChristopheJacquet/PiFmRds.git$ cd PiFmRds/src$ make clean$ make 编译完成后 src 目录下的文件如下图： 其中的 pi_fm_rds 即编译好的二进制程序，可使用如下命令进行测试（注意命令开头的 sudo）：$ sudo ./pi_fm_rds -audio sound.wav该命令会以 107.9M HZ 的频率向空中发射包含 sound.wav 文件音频的电磁波。 详细使用为了使用起来更加方便，可以将编译好的二进制程序移动到 /usr/local/bin 目录并改名为 fm 命令:$ sudo cp pi_fm_rds /usr/local/bin/fm fm 命令主要有两个选项： -audio 用来指定生成电磁声波的媒体文件 -freq 用来指定生成的电磁声波的频率 1234567891011$ sudo fm -audio ~/Music/Faded_Alan\ Walker.wav -freq 108Using mbox device /dev/vcio.Allocating physical memory: size = 3403776 mem_ref = 5 bus_addr = fe7b0000 virt_addr = 0x768dc000ppm corr is 0.0000, divider is 1096.4912 (1096 + 2012*2^-12) [nominal 1096.4912].Using audio file: /home/pi/Music/Faded_Alan Walker.wavInput: 44100 Hz, upsampling factor: 5.172 channels, generating stereo multiplex.Created low-pass FIR filter for audio channels, with cutoff at 12000.0 HzPI: 1234, PS: &lt;Varying&gt;.RT: &quot;PiFmRds: live FM-RDS transmission from the RaspberryPi&quot;Starting to transmit on 108.0 MHz. 下面是用小米手机自带的 FM Radio 录制的一段，呃，截图。。。 对于传输 mp3 格式文件，可以借助 sox 工具：$ sudo apt-get install sox 再通过 sox 命令将 mp3 格式文件转换为 wav 格式的音频流，重定向给 fm 命令：sox -t mp3 ~/Music/baby-alian.mp3 -t wav - | sudo fm -freq 108 -audio - 实际的使用效果还是非常棒的，近距离内收音很清晰。如果需要增加收音距离，可以在树莓派的 GPIO 4（即 7 号引脚）插上一根 杜邦线 作为天线。 二、视频监控上面的图片即树莓派 3 代 B+ 装上专用的摄像头模块后的样子。该模块某宝上可以轻松买到，不附链接。树莓派也支持大多数常用的 USB 摄像头，没尝试过，不细说。 拍照和摄像树莓派需要先在软件配置里开启摄像头模块。可以使用 sudo raspi-config 命令，选择 5 Interfacing Options -&gt; P1 Camera。 之后就可以使用 raspistill 命令拍照和 raspivid 命令录制视频了。 1234# 拍摄照片并保存至 image1.jpg 文件$ raspistill -o image1.jpg# 拍摄长度为 10000 毫秒的视频并保存至 video.h264 文件$ raspivid -o video.h264 -t 10000 mjpg-streamermjpg-streamer 是一个命令行工具，可以通过多种输入和输出插件将 JPEG 帧作为视频流传输。支持树莓派摄像头模块。项目地址：mjpg-streamer 编译安装使用如下命令获取 mjpg-streamer 源代码并编译安装：1234567# 安装依赖库和编译工具 cmake$ sudo apt-get install cmake libjpeg8-dev libv4l-dev# 获取源代码并编译$ git clone https://github.com/jacksonliam/mjpg-streamer.git$ cd mjpg-streamer/mjpg-streamer-experimental$ make$ sudo make install 安装成功后便可以通过如下命令开启视频流：12345678910111213141516171819$ mjpg_streamer -i &quot;input_raspicam.so -x 640 -y 480 -fps 30&quot; -o &quot;output_http.so -w ./www&quot;MJPG Streamer Version: git rev: ddb69b7b4f114f3c2ca01adf55712792ca8aed43 i: fps.............: 30 i: resolution........: 640 x 480 i: camera parameters..............:Sharpness 0, Contrast 0, Brightness 50Saturation 0, ISO 0, Video Stabilisation No, Exposure compensation 0Exposure Mode &apos;auto&apos;, AWB Mode &apos;auto&apos;, Image Effect &apos;none&apos;Metering Mode &apos;average&apos;, Colour Effect Enabled No with U = 128, V = 128Rotation 0, hflip No, vflip NoROI x 0.000000, y 0.000000, w 1.000000 h 1.000000 o: www-folder-path......: ./www/ o: HTTP TCP port........: 8080 o: HTTP Listen Address..: (null) o: username:password....: disabled o: commands.............: enabled i: Starting CameraEncoder Buffer Size 81920 其中 -i 选项后面的 input_rapsicam.so 用于指定输入插件，-x 和 -y 选项指定视频流的分辨率，-fps 选项指定帧率。-o 选项后面的 output_http.so 用于指定输出插件，-w 指定 Webcam 服务器使用的网页源文件。 使用测试Webcam 服务器地址为：IP_Address:8080使用浏览器访问时效果如下： 可以借助 VLC 或 iina 视频播放器的 打开 URL 功能，远程播放视频流。URL 为 IP_Address:8080/?action=stream 。效果如下： 呃，实话说，有点小卡。。。哈哈哈 参考资料Raspberry Pi Cookbook, 2nd EditionRaspberry Pi for Secret Agents (English Edition)]]></content>
      <categories>
        <category>IoT</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Admin</tag>
        <tag>Hardware</tag>
        <tag>RaspberryPi</tag>
        <tag>Raspbian</tag>
        <tag>Camera</tag>
        <tag>Webcam</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[通过 JavaScript (Johnny-Five) 控制 Ardunio]]></title>
    <url>%2F2018%2F10%2F25%2Fprogramming-arduino-with-javascript%2F</url>
    <content type="text"><![CDATA[Johnnt-Five 是一个支持 JavaScript 语言编程的机器人和 IOT 开发平台，基于 Firmata 协议。该项目在 Github 上有着很好的活跃度，官网上也有非常详尽的 API 文档和丰富的实例(http://johnny-five.io/examples/)。 一、环境搭建安装 Arduino IDE安装 Arduino IDE 主要是为了向开发板中刷入 Firmata 程序。Firmata 是计算机软件和微控制器之间的一种通信协议。当开发板中刷入了 Firmata 固件后，电脑端就可以使用同样基于 Firmata 协议的框架对开发板进行编程。 Arduino IDE 可以直接从官网下载对应系统版本的安装包进行安装。安装完成后进入 文件 - 示例 - Firmata - StandardFirmataPlus ，将该程序刷入 Arduino 开发板即可。 Johnny-Five 框架Johnny-Five 框架需要电脑系统里安装 Node.js 程序。可以直接从官网下载安装，也可以使用 nvm 等版本管理工具安装需要的版本nvm 方式可参考版本管理工具（nvm、virtualenv(wrapper) 和 rbenv 的安装与使用） 安装完成后直接在项目目录下运行 npm 命令即可：$ npm install johnny-five 二、Hello World在项目目录下创建 blink.js 文件：12345678910111213var five = require(&quot;johnny-five&quot;);var board = new five.Board();// The board&apos;s pins will not be accessible until the board has reported that it is readyboard.on(&quot;ready&quot;, function() &#123; console.log(&quot;Ready!&quot;); // Create a standard `led` component instance var led = new five.Led(13); // &quot;blink&quot; the led in 500ms led.blink(500);&#125;); 使用 node blink.js 命令运行项目，效果如下：此时 Arduino 板子上接 13 引脚的红色 LED 开始以 500 毫秒的间隔闪烁，使用 .exit 命令退出 nodejs 的 Repl 后停止闪烁。 REPL在项目目录下创建 repl.js 文件：12345678910111213141516171819var five = require(&quot;johnny-five&quot;);var board = new five.Board();board.on(&quot;ready&quot;, function() &#123; console.log(&quot;Ready event. Repl instance auto-initialized!&quot;); var led = new five.Led(13); this.repl.inject(&#123; // Allow limited on/off control access to the // Led instance from the REPL. on: function() &#123; led.on(); &#125;, off: function() &#123; led.off(); &#125; &#125;);&#125;); 运行效果如下：可以在 nodejs 的 Repl 中使用 on() 和 off() 对 LED13 进行打开和关闭操作。 三、Button创建 button.js 文件：12345678910111213141516171819202122232425262728293031var five = require(&quot;johnny-five&quot;), board, button;board = new five.Board();board.on(&quot;ready&quot;, function() &#123; // Create a new `button` hardware instance. button = new five.Button(2); // Inject the `button` hardware into the Repl instance&apos;s context; board.repl.inject(&#123; button: button &#125;); // &quot;down&quot; the button is pressed button.on(&quot;down&quot;, function() &#123; console.log(&quot;down&quot;); &#125;); // &quot;hold&quot; the button is pressed for specified time. // defaults to 500ms (1/2 second) button.on(&quot;hold&quot;, function() &#123; console.log(&quot;hold&quot;); &#125;); // &quot;up&quot; the button is released button.on(&quot;up&quot;, function() &#123; console.log(&quot;up&quot;); &#125;);&#125;); 线路接法如下： 运行效果： 按下并释放按键时 Repl 中输出 down 和 up。按住按键时输出 hold。 尬尴，JavaScript 不太熟悉，以后有时间多做尝试。官方文档真的很详细。。。]]></content>
      <categories>
        <category>IoT</category>
      </categories>
      <tags>
        <tag>IOT</tag>
        <tag>JavaScript</tag>
        <tag>Programming</tag>
        <tag>Arduino</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用 Docker 创建简单的 Web 应用]]></title>
    <url>%2F2018%2F10%2F19%2Fsimple-web-application-with-docker%2F</url>
    <content type="text"><![CDATA[一、Flask 小程序首先创建一个简单的 Flask 小程序，用来返回一个比较原始的 HTML 页面。该项目的文件结构如下：1234avatar├── Dockerfile├── app └── avatar.py 其中的 Dockerfile 用于创建运行该项目的容器，app 目录下的 avatar.py 为程序的源文件。 avatar.py 的源代码如下：12345678910111213141516171819202122from flask import Flaskapp = Flask(__name__)default_name = &apos;skitarniu&apos;# 创建关联于网站根 URL（&apos;/&apos;）的路由。当该 URL 被请求时，将返回 get_avatar() 函数的结果@app.route(&apos;/&apos;)def get_avatar(): name = default_name header = &apos;&lt;html&gt;&lt;head&gt;&lt;title&gt;Avatar&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&apos; body = &apos;&apos;&apos;&lt;form method=&quot;POST&quot;&gt; Hello &lt;input type=&quot;text&quot; name=&quot;name&quot; value=&quot;&#123;&#125;&quot;&gt; &lt;input type=&quot;submit&quot; value=&quot;submit&quot;&gt; &lt;/form&gt; &lt;p&gt;You look like a: &lt;img src=&quot;&quot;/&gt; &apos;&apos;&apos;.format(name) footer = &apos;&lt;/body&gt;&lt;/html&gt;&apos; return header + body + footer# 初始化 web 服务器if __name__ == &apos;__main__&apos;: app.run(debug=True, host=&apos;0.0.0.0&apos;) 编辑 Dockerfile用于构建容器的 Dockerfile 内容如下：123456789101112# 初始镜像为 Docker Hub 上的 Python:3.6 镜像FROM python:3.6# 执行 pip 命令安装 flask 框架RUN pip install Flask==1.0.2# 工作目录设置为容器中的 /appWORKDIR /app# 将本地主机上的项目源码文件夹复制到容器中COPY app /app# 容器运行时其内部执行的命令CMD [&quot;python&quot;, &quot;avatar.py&quot;] 构建容器并运行项目可以使用 docker build 命令根据 Dockerfile 中的步骤创建容器的镜像文件，之后使用 docker run 命令利用刚刚创建的镜像加载容器并运行项目。12345678$ docker build -t avatar ....$ docker run -d -p 5000:5000 --name simple-flask avatare90b14c39b23fb97956af8128ae01c73b9bd5e8917578d755e477efd6337e740$ curl localhost:5000&lt;html&gt;&lt;head&gt;&lt;title&gt;Avatar&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;form method=&quot;POST&quot;&gt; &lt;h3&gt;Hello&lt;/h3&gt;... 其中 docker run 命令的 -d 选项指定容器在后台运行；-p 5000:5000 用来指定本地主机到容器的端口映射（即访问本地主机的 5000 端口等同于访问容器中 5000 端口上运行的服务）；--name simple_flask 用于指定容器的名字为 simple_flask ；最后的 avatar 指定使用的镜像文件。 可以使用 docker logs &lt;container_name&gt; 命令查看后台运行的容器的输出：12345678910111213$ docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESe90b14c39b23 avatar &quot;python avatar.py&quot; 11 minutes ago Up 11 minutes 0.0.0.0:5000-&gt;5000/tcp simple_flask$ docker logs simple_flask * Serving Flask app &quot;avatar&quot; (lazy loading) ... * Debug mode: on * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit) * Restarting with stat * Debugger is active! * Debugger PIN: 495-921-379172.17.0.1 - - [10/Oct/2018 11:28:31] &quot;GET / HTTP/1.1&quot; 200 -10.2.67.88 - - [10/Oct/2018 11:32:02] &quot;GET / HTTP/1.1&quot; 200 - Bind Mounts可以在执行 docker run 命令时使用 -v HOST_DIR:CONTAINER_DIR 选项，将本地主机上的项目目录映射到容器中，并覆盖容器中原目录下的内容。这样当本地主机上的项目源码被修改后，更新的内容会直接同步至容器中的对应文件，就不需要重新构建容器了。123456789101112131415$ docker stop $(docker ps -lq)e90b14c39b23$ docker rm $(docker ps -lq)e90b14c39b23$ docker run -d -p 5000:5000 -v &quot;$(pwd)&quot;/app:/app avatar899831690d5eb7e2e1f7882c00bfef7284f629518e66b581bd5979f3712bf241$ curl localhost:5000&lt;html&gt;&lt;head&gt;&lt;title&gt;Avatar&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;form method=&quot;POST&quot;&gt; &lt;h3&gt;Hello&lt;/h3&gt;...$ sed -i &apos;s/Avatar/Avatar_Modified/&apos; app/avatar.py$ curl localhost:5000&lt;html&gt;&lt;head&gt;&lt;title&gt;Avatar_Modified&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;form method=&quot;POST&quot;&gt; &lt;h3&gt;Hello&lt;/h3&gt;... 以上的命令中，docker stop 和 docker rm 用于停止并删除当前的容器。docker run 命令中的 -v &quot;$(pwd)&quot;/app:/app 选项用来将本地主机上的项目目录（&quot;$(pwd)&quot;/app）关联给容器中的 /app 目录。所以当使用 sed -i 命令将源码中的 Avatar 替换为 Avatar_Modified 之后，不需要重新构建，容器返回的 HTML 文档中的 标签已经变成新值。 二、uWSGI 服务器WSGI（即 Web Server Gateway Interface）是 Web 服务器（如 nginx）和 Web 应用程序或框架（如 Flask）之间的一种通用接口。它就像是一个桥梁，一边连着 Web 服务器，一边连着 Web 应用程序。 很多框架都自带了 WSGI server（如 Flask 的 webserver），但更多是测试用途，发布时则使用生产环境的 WSGI server 或是联合 nginx 做 uwsgi 。 而 uWSGI 是一个 Web 服务器，实现了 WSGI、uwsgi、http 等协议。 这里使用 uWSGI 替代 Flask 自带的 webserver，可对之前的 Dockerfile 做如下修改：1234567FROM python:3.6RUN pip install Flask==1.0.2 uWSGI==2.0.17.1WORKDIR /appCOPY app /appCMD [&quot;uwsgi&quot;, &quot;--http&quot;, &quot;0.0.0.0:9090&quot;, &quot;--wsgi-file&quot;, &quot;/app/avatar.py&quot;, &quot;--callable&quot;, &quot;app&quot;, &quot;--stats&quot;, &quot;0.0.0.0:9191&quot;] 重新构建 docker 镜像并运行容器：12345678$ docker build -t avatar ....$ docker run -d -p 9090:9090 -p 9191:9191 avatar2ca0aed60d53803a7eaaf6ca9146c1786593f3d1d1c86b498ea4577cade854e8$ curl localhost:9090&lt;html&gt;&lt;head&gt;&lt;title&gt;Avatar_Modified&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;form method=&quot;POST&quot;&gt; &lt;h3&gt;Hello&lt;/h3&gt;... 上面的 uWSGI 是在 root 用户下运行的，存在安全隐患。需要将 Dockerfile 改为如下版本：1234567891011121314FROM python:3.6# 创建用户和用户组，名为 uwsgiRUN groupadd -r uwsgi &amp;&amp; useradd -r -g uwsgi uwsgiRUN pip install Flask==1.0.2 uWSGI==2.0.17.1WORKDIR /appCOPY app /app# 使用 EXPOSE 声明可供外部访问的端口号EXPOSE 9090 9191# USER 用于指定某个用户，其后的所有命令（包括 CMD 和 ENTRYPOINT）都将由该用户执行USER uwsgiCMD [&quot;uwsgi&quot;, &quot;--http&quot;, &quot;0.0.0.0:9090&quot;, &quot;--wsgi-file&quot;, &quot;/app/avatar.py&quot;, &quot;--callable&quot;, &quot;app&quot;, &quot;--stats&quot;, &quot;0.0.0.0:9191&quot;] 区分测试和生产环境可以将 Dockerfile 中 CMD 调用的命令单独存放在一个 Shell 脚本中。如在 avatar 目录下新建 cmd.sh 文件并添加执行权限（chmod +x cmd.sh），再添加文件内容如下：1234567891011#!/bin/bashset -eif [ &quot;$ENV&quot; = &apos;DEV&apos; ]; then echo &quot;Running Development Server&quot; exec python &quot;avatar.py&quot;else echo &quot;Running Production Server&quot; exec uwsgi --http 0.0.0.0:9090 --wsgi-file /app/avatar.py \ --callable app --stats 0.0.0.0:9191fi 此时的 Dockerfile 内容如下：12345678910111213FROM python:3.6RUN groupadd -r uwsgi &amp;&amp; useradd -r -g uwsgi uwsgiRUN pip install Flask==1.0.2 uWSGI==2.0.17.1WORKDIR /appCOPY app /appCOPY cmd.sh /EXPOSE 9090 9191USER uwsgi# CMD 选项改为包含一系列命令且拥有执行权限的脚本文件CMD [&quot;/cmd.sh&quot;] 重新构建 Docker 镜像，在测试环境下运行时使用$ docker run -e &quot;ENV=DEV&quot; -p 5000:5000 avatar其中 -e 选项用于指定环境变量 在生产环境下运行时则使用$ docker run -d -p 9090:9090 -p 9191 avatar 三、Docker ComposeCompose 工具用于快速地搭建和运行 Docker 开发环境。它使用 YAML 文件保存容器集群的配置信息。 安装 ComposeUbuntu 系统下安装 Compose 可参考以下命令：1234# 从 Github 上获取 Docker Compose 的二进制程序$ sudo curl -L &quot;https://github.com/docker/compose/releases/download/1.22.0/docker-compose-$(uname -s)-$(uname -m)&quot; -o /usr/local/bin/docker-compose# 为获取到的 compose 程序添加执行权限$ sudo chmod +x /usr/local/bin/docker-compose 编辑 docker-compose 文件在 avatar 目录下新建一个名为 docker-compose.yml 的文件，该文件是 docker-compose 命令运行时参考的配置文件：12345678avatar: build: . ports: - &quot;5000:5000&quot; environment: ENV:DEV volumes: - ./app:/app 其中第一行的 avatar 用于声明需要构建的容器的名称，同一个 YAML 文件中可以同时存在多个容器的定义；第二行的 build: . 表示用于构建容器镜像文件的 Dockerfile 位于当前目录下；ports 项等同于 docker run 命令中的 -p 选项，用于定义端口转发；environment 项等同于 docker run 命令中的 -e 选项，用于定义容器中的环境变量；volumes 项等同于 docker run 命令中的 -v 选项，用于定义存储卷。 运行项目可直接使用 docker-compose up 命令构建容器并执行项目：12345678910111213$ docker-compose upBuilding avatar...Successfully built 1f883bd34e9fSuccessfully tagged avatar_avatar:latestWARNING: Image for service avatar was built because it did not already exist. To rebuild this image you must use `docker-compose build` or `docker-compose up --build`.Creating avatar_avatar_1 ... doneAttaching to avatar_avatar_1avatar_1 | Running Development Serveravatar_1 | * Serving Flask app &quot;avatar&quot; (lazy loading)...avatar_1 | * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)... 四、关联其他镜像（dnmonster）dnmonster 镜像是一个整合在 Docker 容器中的 Node.js 应用，可以直接从 Docker Hub 上拉取到本地。它提供了一个 RESTful API，当访问 http://0.0.0.0:8080/monster/MY_ID 时，返回一个独一无二的“怪兽”头像。1234$ docker pull amouat/dnmonster...$ docker run -d -p 8080:8080 amouat/dnmonster... 此时打开浏览器访问 http://ip_address:8080/monster/some_string?size=200 ，结果如图所示： 整合 dnmonster 镜像前面的 flask 应用只包含一个最基本的功能，即访问它的主页时返回一个简单的 HTML 页面，页面中包含一个获取用户输入的表单，和一个 src 属性值为空字符串的 &lt;img&gt; 标签（“空白”图片）。 结合 dnmonster 镜像的使用，可以将表单中获取到的输入整合到图片标签 的 src 属性中（/monster/&lt;user_input&gt;） 将该图片的 URL 路径 （/monster/&lt;user_input&gt;）绑定给另一个函数（get_avatar），该函数访问 dnmonster 容器中的 RESTful API，所以网页中最终显示的是从 dnmonster 容器获取到的头像图片，并随用户输入而更新。 app/avatar.py 文件的具体代码如下：12345678910111213141516171819202122232425262728293031323334353637383940from flask import Flask, Response, requestimport requestsimport hashlibapp = Flask(__name__)default_name = &apos;skitarniu&apos;# 声明网站主页将会处理 GET 和 POST 请求（因为表单的提交属于 POST 请求），主页绑定 mainpage 函数@app.route(&apos;/&apos;, methods=[&apos;GET&apos;,&apos;POST&apos;])def mainpage(): name = default_name# 表单提交时，获取用户输入的内容，调用 hashlib 库将其变成 hash 形式，保存在 name_hash 变量中 if request.method == &apos;POST&apos;: name = request.form[&apos;name&apos;] name_hash = hashlib.sha256(name.encode()).hexdigest() header = &apos;&lt;html&gt;&lt;head&gt;&lt;title&gt;Avatar_Modified&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&apos; body = &apos;&apos;&apos;&lt;form method=&quot;POST&quot;&gt; &lt;h3&gt;Hello&lt;/h3&gt; &lt;input type=&quot;text&quot; name=&quot;name&quot; value=&quot;&#123;0&#125;&quot;&gt; &lt;input type=&quot;submit&quot; value=&quot;submit&quot;&gt; &lt;/form&gt; &lt;p&gt;You look like a: &lt;img src=&quot;/monster/&#123;1&#125;&quot;/&gt; &apos;&apos;&apos;.format(name, name_hash)# img 标签的 src 属性由 name_hash 的值确定，即网页中图片的源路径根据用户输入自行更新 footer = &apos;&lt;/body&gt;&lt;/html&gt;&apos; return header + body + footer# 网页中图片的 URL 绑定给 get_avatar 函数，该函数通过 requests 库访问 dnmonster 容器中的 API 以获取“怪兽”图像@app.route(&apos;/monster/&lt;name&gt;&apos;)def get_avatar(name): r = requests.get(&apos;http://dnmonster:8080/monster/&apos; + name + &apos;?size=200&apos;) image = r.content return Response(image, mimetype=&apos;image/png&apos;)if __name__ == &apos;__main__&apos;: app.run(debug=True, host=&apos;0.0.0.0&apos;) 然后在 Dockerfile 里添加上前面用到的 requests 模块123456789101112FROM python:3.6RUN groupadd -r uwsgi &amp;&amp; useradd -r -g uwsgi uwsgiRUN pip install Flask==1.0.2 uWSGI==2.0.17.1 requests==2.5.1WORKDIR /appCOPY app /appCOPY cmd.sh /EXPOSE 9090 9191USER uwsgiCMD [&quot;/cmd.sh&quot;] 输入以下命令运行项目：123456$ docker build -t avatar ....$ docker run -d --name dnmonster amouat/dnmonster48f77f0e0f7ac503b27786f73ea8d25fa4237eea042dfb5cc1331bb07001dae3$ docker run -d -p 5000:5000 -e &quot;ENV=DEV&quot; --link dnmonster:dnmonster avatar8cf94a54744f92c206917c474dc8619c417d7b7937fd6d38df3cd05d5813d5fd 其中 docker build 命令用于重新构建容器。docker run -d --name dnmonster amouat/dnmonster 命令用于加载 dnmonster 容器并指定其名称（--name）为 dnmonster 。docker run -d -p 5000:5000 -e &quot;ENV=DEV&quot; --link dnmonster:dnmonster avatar 命令中的 --link dnmonster:dnmonster 选项用于将 flask 应用容器和 dnmonster 容器关联起来。其中第一个 dnmonster 用于指定关联容器的名称，第二个 dnmonster 用于指定该容器的别名。关联后 flask 应用容器就可以通过别名直接访问 dnmonster 容器，而无需获知其 IP 地址（所以源文件 app/avatar.py 中 get_avatar 函数才可以通过 http://dnmonster:8080/ 类似的 URL 访问 dnmonster 的 API）。 效果如下：输入不同的字符串并提交，将得到不一样的头像图片。 使用 Docker Compose上面的例子虽然可以正常运行，但运行项目时手动输入 docker run 命令过于繁琐。可以通过修改 docker-compose.yml 配置文件，借助 Compose 的“自动化”简化操作步骤。 12345678910111213avatar: build: . ports: - &quot;5000:5000&quot; environment: ENV: DEV volumes: - ./app:/app links: - dnmonsterdnmonster: image: amouat/dnmonster 其中 links 项定义了 avatar 容器到 dnmonster 容器的关联dnmonster 及后面的内容则定义了 dnmonster 容器的配置信息，通过 image 项指定用于生成该容器的镜像文件。 停止并删除之前的容器，重新构建运行项目：123456789$ docker-compose build...$ docker-compose up -dCreating avatar_dnmonster_1 ... doneCreating avatar_avatar_1 ... done$ docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES1f85363c7ed4 avatar_avatar &quot;/cmd.sh&quot; 10 seconds ago Up 9 seconds 9090/tcp, 0.0.0.0:5000-&gt;5000/tcp, 9191/tcp avatar_avatar_15cca0f851ac5 amouat/dnmonster &quot;npm start&quot; 11 seconds ago Up 9 seconds 8080/tcp avatar_dnmonster_1 五、添加缓存支持（Redis）当前的 flask 应用，每获取一次“怪兽”头像，dnmonster 服务就会收到一次比较消耗资源的请求。由于通过特定的输入生成的图片是保持不变的，所以可以利用缓存对应用进行优化。 Redis 是一种基于内存的 Key-Value 类型的数据库，适合此处的应用场景。 最终的项目文件结构如下：123456avatar├── app│ └── avatar.py├── cmd.sh├── docker-compose.yml└── Dockerfile app/avatar.py:1234567891011121314151617181920212223242526272829303132333435363738394041from flask import Flask, Response, requestimport requestsimport hashlibimport redisapp = Flask(__name__)cache = redis.StrictRedis(host=&apos;redis&apos;, port=6379, db=0)default_name = &apos;skitarniu&apos;@app.route(&apos;/&apos;, methods=[&apos;GET&apos;,&apos;POST&apos;])def mainpage(): name = default_name if request.method == &apos;POST&apos;: name = request.form[&apos;name&apos;] name_hash = hashlib.sha256(name.encode()).hexdigest() header = &apos;&lt;html&gt;&lt;head&gt;&lt;title&gt;Avatar_Modified&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&apos; body = &apos;&apos;&apos;&lt;form method=&quot;POST&quot;&gt; &lt;h3&gt;Hello&lt;/h3&gt; &lt;input type=&quot;text&quot; name=&quot;name&quot; value=&quot;&#123;0&#125;&quot;&gt; &lt;input type=&quot;submit&quot; value=&quot;submit&quot;&gt; &lt;/form&gt; &lt;p&gt;You look like a: &lt;img src=&quot;/monster/&#123;1&#125;&quot;/&gt; &apos;&apos;&apos;.format(name, name_hash) footer = &apos;&lt;/body&gt;&lt;/html&gt;&apos; return header + body + footer@app.route(&apos;/monster/&lt;name&gt;&apos;)def get_avatar(name): image = cache.get(name) if image is None: print(&quot;Cache miss&quot;, flush=True) r = requests.get(&apos;http://dnmonster:8080/monster/&apos; + name + &apos;?size=200&apos;) image = r.content cache.set(name, image) return Response(image, mimetype=&apos;image/png&apos;)if __name__ == &apos;__main__&apos;: app.run(debug=True, host=&apos;0.0.0.0&apos;) Dockerfile:123456789101112FROM python:3.6RUN groupadd -r uwsgi &amp;&amp; useradd -r -g uwsgi uwsgiRUN pip install Flask==1.0.2 uWSGI==2.0.17.1 requests==2.5.1 redis==2.10.6WORKDIR /appCOPY app /appCOPY cmd.sh /EXPOSE 9090 9191USER uwsgiCMD [&quot;/cmd.sh&quot;] docker-compose.yml:1234567891011121314151617avatar: build: . ports: - &quot;5000:5000&quot; environment: ENV: DEV volumes: - ./app:/app links: - dnmonster - redisdnmonster: image: amouat/dnmonsterredis: image: redis cmd.sh 文件保持之前的版本即可。然后就可以先使用 docker-compose stop 命令停止之前版本的容器，再使用 docker-compose build 和 docker-compose up -d 命令重新构建并运行新版本的容器。 1234567891011$ docker-compose builddnmonster uses an image, skippingredis uses an image, skippingBuilding avatar...Successfully built d1aa92c39f97Successfully tagged avatar_avatar:latest$ docker-compose up -dCreating avatar_redis_1 ... doneCreating avatar_dnmonster_1 ... doneCreating avatar_avatar_1 ... done 参考资料Using Docker: Developing and Deploying Software with Containers]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Development</tag>
        <tag>Web</tag>
        <tag>Docker</tag>
        <tag>Service</tag>
        <tag>Operation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[树莓派 3B+ 连接 WPA2 企业级加密的 WIFI]]></title>
    <url>%2F2018%2F10%2F16%2Fraspberry-pi-wifi-wpa2-eap%2F</url>
    <content type="text"><![CDATA[陪伴我三年多的树莓派二代目前阵子挂掉了，电源指示灯异常。想修，无从下手。含泪送别，然后买了个三代目。。。已经算驾轻就熟了。直接刷好系统，走起。结果连 WIFI 的时候出了点小状况 树莓派 3 代 B+ 已自带了蓝牙和 WIFI 模块，且支持 2.4/5G HZ 双频段无线网络。本以为连下 WIFI 就是动动手指的事情。偏偏公司是 WPA2 企业级加密的无线网，图形界面下显示的 WIFI 名称是灰色的，无法直接连接。无奈只好通过命令行配置。 树莓派用的是当前最新版本的 Raspbian 系统（2018-10-09），无线网络配置文件为 /etc/wpa_supplicant/wpa_supplicant.conf。所以直接将 WIFI 的连接信息补充到该配置文件中即可。 一、连接普通 WIFI连接“最简单”的 WIFI （如手机热点）时配置如下：12345678ctrl_interface=DIR=/var/run/wpa_supplicant GROUP=netdevupdate_config=1country=CNnetwork=&#123; ssid=&quot;your_AP_name&quot; psk=&quot;password&quot;&#125; 其中 WIFI 的连接信息主要是 network 项中的内容。 多个 WIFI 设置优先级如果需要同时配置多个 WIFI 的连接并为其设置优先级，可参考以下配置：12345678910111213network=&#123; ssid=&quot;your_AP_name1&quot; psk=&quot;password1&quot; priority=1 id_str=&quot;Home1&quot;&#125;network=&#123; ssid=&quot;your_AP_name2&quot; psk=&quot;password2&quot; priority=2 id_str=&quot;Home2&quot;&#125; 其中 priority 项用于设置优先级，该值越大则优先级越高。 二、连接隐藏 WIFI隐藏 WIFI 不能被自动搜索到，需要手动添加连接。12345network=&#123; ssid=&quot;your_AP_name&quot; scan_ssid=1 psk=&quot;password&quot;&#125; 主要是添加 scan_ssid=1 项。 三、连接 WPA2 企业级加密的 WIFI示例配置文件如下（尴尬，不是很懂。但我是可以连的。。）：1234567891011network=&#123; ssid=&quot;your_AP_name&quot; key_mgmt=WPA-EAP pairwise=CCMP TKIP group=CCMP TKIP eap=PEAP identity=&quot;your_username&quot; password=&quot;password&quot; phase1=&quot;peaplabel=auto pepver=auto&quot; phase2=&quot;MSCHAPV2&quot;&#125; 更多 wpa_supplicant.conf 文件的配置实例可参考 man wap_supplicant.conf 附录wpa_supplicant.conf 文件的配置说明可参考这篇博客（内容太详细了，，没细看）]]></content>
      <categories>
        <category>IoT</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Admin</tag>
        <tag>Network</tag>
        <tag>IoT</tag>
        <tag>Wi-Fi</tag>
        <tag>RaspberryPi</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker 初探]]></title>
    <url>%2F2018%2F10%2F09%2Fdocker-beginning-guide%2F</url>
    <content type="text"><![CDATA[一、容器（Container）简单来说，容器是对应用程序及其依赖库的一种封装。 乍看上去，容器就像一个轻量级的虚拟机系统（VM），也封装了一个操作系统实例用来运行某些应用程序。相比于传统的虚拟机，容器的优势主要在以下几个方面： 容器与宿主机共享资源，使得其效率有了很大的提升。与主机中运行的应用程序相比，在容器中运行的应用程序几乎没有任何额外的开销。 容器可移植的特性可以解决由于运行环境的微小变化引发的一系列问题。 容器的轻量级特性意味着开发人员可以同时运行数十个容器，从而可以模拟出生产级别的分布式系统。 对于不使用云端应用的用户和开发者，用户可以节省下大量的安装和配置时间，也不用担心系统的依赖冲突等问题。而开发者同时也可以避免由于用户系统环境的差异导致的可用性问题。 总的来说，虚拟机的基本目标，是完整地虚拟出一个外部（独立）的系统环境，而容器是为了达到应用程序的可移植和自成一体。 而 Docker Engine 为运行容器提供了快捷方便的交互接口。 二、Docker 安装（Linux）Linux 系统上的 Docker 安装，可以直接使用官方提供的安装脚本（https://get.docker.com），命令如下：$ sudo wget -qO- https://get.docker.com/ | sh或$ sudo curl -sSL https://get.docker.com/ | sh Mac OS 和 Windows 系统上的 Docker 安装，可参考官方文档 Docker for Mac 和 Docker for Windows Docker 官方镜像访问缓慢，可以使用阿里云提供的加速服务（参考 镜像加速器） Docker 需要特定的权限才能运行，即普通用户执行 docker 命令时需要加上 sudo 。12345678910$ docker versionClient: Version: 18.06.1-ce API version: 1.38 Go version: go1.10.3 Git commit: e68fc7a Built: Tue Aug 21 17:24:51 2018 OS/Arch: linux/amd64 Experimental: falseGot permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Get http://%2Fvar%2Frun%2Fdocker.sock/v1.38/version: dial unix /var/run/docker.sock: connect: permission denied 可以将本地用户添加到 docker 用户组中，后续使用 docker 命令时即无需加上 sudo 前缀。$ sudo usermod -aG docker &lt;username&gt;12345678910111213141516171819$ docker versionClient: Version: 18.06.1-ce API version: 1.38 Go version: go1.10.3 Git commit: e68fc7a Built: Tue Aug 21 17:24:51 2018 OS/Arch: linux/amd64 Experimental: falseServer: Engine: Version: 18.06.1-ce API version: 1.38 (minimum version 1.12) Go version: go1.10.3 Git commit: e68fc7a Built: Tue Aug 21 17:23:15 2018 OS/Arch: linux/amd64 Experimental: false 三、操作入门Hello World1234567$ docker run debian echo &quot;Hello World&quot;Unable to find image &apos;debian:latest&apos; locallylatest: Pulling from library/debian05d1a5232b46: Pull completeDigest: sha256:07fe888a6090482fc6e930c1282d1edf67998a39a09a0b339242fbfa2b602fffStatus: Downloaded newer image for debian:latestHello World docker run 命令用于加载容器并执行某个命令。上述命令中的 debian 用于指定所使用的镜像的名字。如本地磁盘上没有名为 debian 的镜像文件，则 Docker 会检查在线的 Docker Hub 并将最新版本的 Debian 镜像下载到本地。之后将下载好的镜像转化成运行的容器，并在容器中执行指定的命令。命令执行完毕后，输出的结果传送到标准输出，容器停止运行。 交互式 Shell12345$ docker run -i -t debian /bin/bashroot@4af9c13b78d7:/# echo &quot;Hello from Container&quot;Hello from Containerroot@4af9c13b78d7:/# exitexit 上面的命令会在容器中打开一个交互式 Shell （就像是 ssh 到了一个远程主机上）。-i 和 -t 选项表示打开一个已绑定了 tty 的交互式会话。当使用 exit 命令退出 bash 后，运行中的容器也会停止。 可以使用 docker start -i &lt;Container_name&gt; 命令回到交互式 Shell 中 列出 / 删除容器docker ps 命令可以列出当前正在运行的容器及其相关信息，如容器ID、使用的镜像、执行的命令、创建时间、当前状态和名称等，加上 -a 选项则列出所有容器（包含已停止运行的容器）1234567$ docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES$$ docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESf74ed03352d3 debian &quot;/bin/bash&quot; 9 minutes ago Exited (0) 8 minutes ago mystifying_beaver45e9e3f3847b debian &quot;echo &apos;Hello World&apos;&quot; 9 minutes ago Exited (0) 9 minutes ago quirky_mirzakhani 可以使用 docker rm &lt;Container_name&gt; 命令删除已停止运行的容器。12345$ docker rm mystifying_beavermystifying_beaver$ docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES45e9e3f3847b debian &quot;echo &apos;Hello World&apos;&quot; 13 minutes ago Exited (0) 13 minutes ago quirky_mirzakhani docker rm -v $(docker ps -aq -f status=exited) 命令可以删除所有已停止运行的容器 手动创建镜像文件从 Docker Hub 上拉取的镜像很多为初始的精简系统，运行容器后，可以给容器内的系统安装软件并提交更改，做成新的镜像供后期使用。 使用 docker run 命令运行容器并安装软件1234567891011121314$ docker run -it --name cowsay --hostname cowsay debian /bin/bashroot@cowsay:/# apt-get update...root@cowsay:/# apt-get install -y cowsay fortune...root@cowsay:/# /usr/games/fortune | /usr/games/cowsay ________________________&lt; Don&apos;t get to bragging. &gt; ------------------------ \ ^__^ \ (oo)\_______ (__)\ )\/\ ||----w | || || 其中 --name 选项用于指定容器的名称，--hostname 选项用于指定容器系统的主机名。 使用 docker commit 命令将容器转换成镜像文件。123456$ docker commit cowsay test/cowsayimagesha256:2dcb2f4d09f824120db19f79d3bfbdb85c24d4888732bcc7eaae79f707d80e32$ docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEtest/cowsayimage latest 2dcb2f4d09f8 13 minutes ago 159MBdebian latest f2aae6ff5d89 3 weeks ago 101MB docker images 命令用来查看已在本地的镜像文件及其信息。 使用生成的镜像文件12345678910$ docker run test/cowsayimage /usr/games/fortune | /usr/games/cowsay _______________________________________/ Today is the first day of the rest of \\ the mess. / --------------------------------------- \ ^__^ \ (oo)\_______ (__)\ )\/\ ||----w | || || 使用 Dockerfile 创建镜像文件Dockerfile 其实就是一个简单的文本文件，里面包含了创建 Docker 镜像的一系列步骤。相比于手动创建 Docker 镜像，使用 Dockerfile 自动地创建镜像文件，省去了大量重复的操作，同时也便于分享给其他人。 编辑 Dockerfile12$ mkdir cowsay &amp;&amp; cd cowsay$ vim Dockerfile 然后在新建的 Dockerfile 中填入以下内容：12FROM debian:wheezyRUN apt-get update &amp;&amp; apt-get install -y cowsay fortune 其中 FROM 用于指定构建时使用的初始镜像文件，RUN 用于指定在镜像中执行的 Shell 命令 构建镜像并测试使用 docker build 命令创建 Docker 镜像： 12345678910111213141516$ docker build -t test/cowsay-dockerfile .Sending build context to Docker daemon 2.048kBStep 1/2 : FROM debian:wheezywheezy: Pulling from library/debian703d6f3fb41c: Pull completeDigest: sha256:d00f167f8f2e70ecc2e0f5410a3cb74cd4ad720e33b9810da6a2dcfa81dccfc0Status: Downloaded newer image for debian:wheezy ---&gt; 94825a89630cStep 2/2 : RUN apt-get update &amp;&amp; apt-get install -y cowsay fortune ---&gt; Running in efcf246bde0f...Setting up cowsay (3.03+dfsg1-4) ...Removing intermediate container efcf246bde0f ---&gt; 88e9a0f834cdSuccessfully built 88e9a0f834cdSuccessfully tagged test/cowsay-dockerfile:latest 测试刚构建好的 Docker 镜像：123456789$ docker run test/cowsay-dockerfile /usr/games/cowsay &quot;Moo&quot; _____&lt; Moo &gt; ----- \ ^__^ \ (oo)\_______ (__)\ )\/\ ||----w | || || ENTRYPOINTDockerfile 中的 ENTRYPOINT 选项可以用来指定容器运行时自动执行的命令。如将 Dockerfile 改为如下内容并重新构建：123FROM debian:wheezyRUN apt-get update &amp;&amp; apt-get install -y cowsay fortuneENTRYPOINT [&quot;/usr/games/cowsay&quot;] 1234567891011$ docker build -t test/cowsay-dockerfile ....$ docker run test/cowsay-dockerfile &quot;Moo&quot; _____&lt; Moo &gt; ----- \ ^__^ \ (oo)\_______ (__)\ )\/\ ||----w | || || 运行上述容器时则不需要再指定命令（/usr/games/cowsay）而只输入命令的参数即可。 可以在当前目录下新建一个 entrypoint.sh 脚本：123456#!/bin/bashif [ $# -eq 0 ]; then /usr/games/fortune | /usr/games/cowsayelse /usr/games/cowsay &quot;$@&quot;fi 上述内容为 Shell 脚本文件，不作详细解释。作用是当 docker run 没有为容器中执行的命令提供参数时，执行 fortune | cowsay ，如提供了参数，则执行 cowsay &lt;参数&gt; 命令。 将 entrypoint.sh 文件添加执行权限：chmod +x entrypoint.sh将 Dockerfile 改为如下版本：1234FROM debian:wheezyRUN apt-get update &amp;&amp; apt-get install -y cowsay fortuneCOPY entrypoint.sh /ENTRYPOINT [&quot;/entrypoint.sh&quot;] 其中 COPY 选项用来将本地主机上的文件复制到镜像的文件系统里（类似于 cp 命令） 最终效果如下：123456789101112131415161718192021$ docker build -t test/cowsay-dockerfile ....$ docker run test/cowsay-dockerfile _________________________________________/ You don&apos;t become a failure until you&apos;re \\ satisfied with being one. / ----------------------------------------- \ ^__^ \ (oo)\_______ (__)\ )\/\ ||----w | || ||$ docker run test/cowsay-dockerfile Hello Moo ___________&lt; Hello Moo &gt; ----------- \ ^__^ \ (oo)\_______ (__)\ )\/\ ||----w | || || 四、基本使用端口转发假如容器中运行着一个 Web 服务器，需要外部世界可以访问。此时可以使用 docker 的 -p 或 -P 选项，将本地主机的端口转发至容器内的端口。如：12345678910111213141516$ docker run -d -p 8000:80 nginxUnable to find image &apos;nginx:latest&apos; locallylatest: Pulling from library/nginx...Status: Downloaded newer image for nginx:latestbe1364f343ae7aff9d5a6f6040b5d6ca69363a4480f04548988dd798ab04ab7b$ docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESbe1364f343ae nginx &quot;nginx -g &apos;daemon of…&quot; 8 seconds ago Up 7 seconds 0.0.0.0:8000-&gt;80/tcp quirky_mclean$ curl localhost:8000&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;title&gt;Welcome to nginx!&lt;/title&gt;...$ 其中 docker run -d -p 8000:80 nginx 命令用于在后台（-d 选项）启动一个由官方 nginx 镜像创建的容器，并将本地主机的 8000 端口映射到容器的 80 端口（-p 8000:80），即容器中 nginx 服务运行的端口。可以看到，当使用 curl 命令访问本地主机的 8000 端口时，等同于访问了容器的 80 端口，即容器中的 nginx 服务。 而 -P 选项可以自动选择空闲的端口进行转发，无需指定本地主机或容器的端口。可参考以下实例：123456789$ ID=$(docker run -d -P nginx)$ docker port $ID 800.0.0.0:32768$ curl localhost:32768&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;title&gt;Welcome to nginx!&lt;/title&gt;... 关联容器容器关联可以允许同一个主机上的多个容器相互交换数据。当使用默认的网络模型时，这些关联的容器通过其“内部”网络传输数据，即关联容器间的相互交流不会暴露给本地主机。 可参考以下实例：1234567$ docker run --name myredis -d redisbe53967c42fd3292dfd59fd5d15e7025fa436816e46f6e7cbc3c47b06ddb0047$ docker run --rm -it --link myredis:redis redis /bin/bashroot@b27aca7d8c54:/data# redis-cli -h redis -p 6379redis:6379&gt; pingPONGredis:6379&gt; 其中 docker run --name myredis -d redis 命令用于在后台启动一个 redis 容器，并将其命名为 myredis 。同时返回该容器的 ID 到标准输出。docker run --rm -it --link myredis:redis redis /bin/bash 命令用于启动另一个 redis 容器，并以交互的方式访问其 Shell （-it /bin/bash）。--rm 选项表示该容器退出后将自动删除。 容器关联的操作则由 --link myredis:redis 选项实现。表示将新容器关联至已存在的 “myredis” 容器，并为 “myredis” 容器设置别名为 “redis” ，即可以在当前的新容器中通过别名（redis）访问。 该选项会在新容器的 /etc/hosts 文件中添加一条主机名为 redis 的记录，并将其指向 “myredis” 容器的 IP 地址。所以在当前容器的 Shell 中使用 redis-cli 命令访问 “myredis” 中的服务时，可以无需指定其 IP 地址，直接使用主机名 “redis” 即可。（redis-cli -h redis -P 6379） 存储卷可以在使用 docker run 时通过 -v 选项指定存储卷：123456$ docker run -it --name container-test -h CONTAINER -v /data debian /bin/bashroot@CONTAINER:/# cd /data ; touch test-fileroot@CONTAINER:/data# lstest-fileroot@CONTAINER:/data# exitexit 上面的命令会将容器中的 /data 目录变成一个存储卷，该目录下的任何文件都会被复制到卷中。 可以使用 docker inspect 命令查看该存储卷在本地主机中的位置：12$ docker inspect -f &#123;&#123;.Mounts&#125;&#125; container-test[&#123;volume 7ae1... /var/lib/docker/volumes/7ae1.../_data /data local true &#125;] 可以在存储卷对应于本地主机的目录（/var/lib/docker/volumes/7ae1.../_data）中创建文件，容器中对应目录（/data）下则会立即出现同样的文件。1234567$ sudo ls /var/lib/docker/volumes/7ae1.../_datatest-file$ sudo touch /var/lib/docker/volumes/7ae1.../_data/test-file2$ docker start -i container-testroot@CONTAINER:/# ls /datatest-file test-file2root@CONTAINER:/# 存储卷还可以通过 Dockerfile 中的 VOLUME 选项指定。如：VOLUME /data 共享数据可以使用 -v HOST_DIR:CONTAINER_DIR 选项在本地主机和一（或多）个容器间共享数据。如：$ docker run -v /home/starky/data:/data debian ls /data 该命令会将本地主机上的 /home/starky/data 目录挂载到容器中的 /data 目录下，所有已经存在于 /home/starky/data 目录下的文件在容器中也同样能够被访问。但是原本存在于容器的 /data 目录下的文件则被隐藏。如某些配置文件可以一直存放在本地主机上，并在需要时挂载到通用镜像构建的容器中。 可以使用 docker run 命令的 --volumes-from CONTAINER 选项在容器间共享数据。如：1234$ docker run -it -h NEWCONTAINER --volumes-from container-test debian /bin/bashroot@NEWCONTAINER:/# ls /datatest-file test-file2root@NEWCONTAINER:/# 上面的命令创建了一个新的容器，且通过 --volumes-from 选项，该容器可以访问之前的 container-test 容器中的存储卷（/data）。 数据容器数据容器是一种特殊的容器，其唯一目的是为了方便其他容器之间共享数据。如创建一个 PostgreSQL 的数据容器：$ docker run --name dbdata postgres echo &quot;Data-only container for postgres&quot;该命令用于从 postgres 镜像创建一个容器，并初始化镜像里定义的存储卷。 之后可以通过 --volumes-from 选项使用数据容器里的存储卷：$ docker run -d --volumes-from dbdata --name db1 postgres 参考资料Using Docker: Developing and Deploying Software with Containers]]></content>
      <categories>
        <category>Tools</category>
      </categories>
      <tags>
        <tag>Admin</tag>
        <tag>Tools</tag>
        <tag>Development</tag>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 用户管理简介]]></title>
    <url>%2F2018%2F09%2F05%2Flinux-user-management%2F</url>
    <content type="text"><![CDATA[一、创建用户Linux 系统可以使用 useradd 命令创建新用户：$ sudo useradd -m -c &#39;description&#39; &lt;username&gt; -m 选项表示创建用户的同时自动在 /home 目录下创建该用户的主目录（/home/&lt;username&gt;） -c 选项用于给新用户添加描述，该描述将保存在 /etc/passwd 文件中 默认情况下，新创建的用户并未分配密码，需要使用 passwd &lt;username&gt; 命令为新用户设置密码。 PS：在为新用户自动创建主目录时，可以同时复制一部分初始文件（如 .bashrc、.vimrc 等）到该目录下，复制的源文件默认位于 /etc/skel 目录下。即创建新用户时，/etc/skel/ 目录下的所有文件都将自动复制到新用户的主目录中。 在 Ubuntu 系统中，useradd 命令默认不自动创建用户主目录，所以需要带上 -m 选项。而 Centos 系统中，useradd 命令默认会自动创建用户主目录，可以加上 -M 选项表示不创建用户主目录。 useradd 的部分命令选项： 选项 描述 -c, –comment 添加用户描述 -d, –home-dir 指定用户的主目录位置 -m, –create-home 自动创建用户的主目录 -M, –no-create-home 不自动创建用户主目录（CentOS） -N, –no-user-group 不创建同名的组 -s, –shell 指定用户使用的 shell -g, –gid 指定新账户主组的名称或 ID -G, –groups 指定新账户的附加组列表 -e, –expiredate 指定新账户的过期日期 -f, –inactive 用户密码过期后的重置密码期限 -r, –system 创建一个系统账户 -k, –skel 指定骨架目录（默认为 /etc/skel） PS：其中的 INACTIVE 和 EXPIRE 选项用于设置用户账号的过期时间。INACTIVE 用于指定一个期限（单位为天），在该期限内，密码失效后的用户可以重置自己的密码。该选项值为 -1 时则禁用此特性。EXPIRE 用于指定一个截止日期（格式为 YYYY-MM-DD），超过该期限后用户账号即被禁用。 $ sudo useradd -e 2018-10-01 temp_account # 创建一个临时账户 用户默认设置新用户在创建时会使用一些预先定义的默认设置，useradd 命令从 /etc/default/useradd 文件中获取这些默认设置。当前的默认配置可以通过 useradd -D 命令显示：12345678$ useradd -DGROUP=100HOME=/homeINACTIVE=-1EXPIRE=SHELL=/bin/bashSKEL=/etc/skelCREATE_MAIL_SPOOL=no 可以直接通过 useradd -D 命令修改 useradd 的默认设置，如：$ sudo useradd -D -s /bin/bash # 将新建用户的默认 shell 设置为 /bin/bash 二、用户组在 Linux 系统中，任何一个用户都必须属于至少一个用户组。对于大多数 Linux 发行版，创建新用户时会自动创建一个同名的用户组。初始创建的同名用户组叫做基础属组（primary group），用户还可以同时属于其他用户组，这些用户组叫做附加属组（supplementary group）。如：12$ id starkyuid=1000(starky) gid=1000(starky) 组=1000(starky),4(adm),24(cdrom),27(sudo),30(dip),46(plugdev),116(lpadmin),122(sambashare) 每一个用户和用户组在创建时都会关联于一个唯一的 UID 和 GID。UID 和 GID 的取值范围都是 0-65535 。root 用户的 UID 和 GID 永远都是 0 。12$ id rootuid=0(root) gid=0(root) 组=0(root Ubuntu 系统中 1-99 的 ID 只用于分配给系统用户（如运行服务的 www-data），100-999 则动态地分配给系统守护进程的用户（在创建用户时使用 --system 选项）。而第一个普通用户则分配值为 1000 的 UID 和 GID。 使用 useradd 命令创建新用户时，可以同时为该用户关联附加属组。用户创建完成后也可以使用 usermod 命令修改该用户的附加属组。如：1234567$ sudo useradd test -G lpadmin,sambashare # 创建用户 test 并将其添加至 lpadmin 和 sambashare 组$ id testuid=1004(test) gid=1004(test) 组=1004(test),116(lpadmin),122(sambashare)$$ sudo usermod test -a -G sudo # 将用户 test 添加（-a）至 sudo 用户组$ id testuid=1004(test) gid=1004(test) 组=1004(test),27(sudo),116(lpadmin),122(sambashare) usermod 命令用于修改已经存在的用户。常用命令选项如下：1234567891011121314151617181920212223$ usermod用法：usermod [选项] 登录选项： -c, --comment 注释 GECOS 字段的新值 -d, --home HOME_DIR 用户的新主目录 -e, --expiredate EXPIRE_DATE 设定帐户过期的日期为 EXPIRE_DATE -f, --inactive INACTIVE 过期 INACTIVE 天数后，设定密码为失效状态 -g, --gid GROUP 强制使用 GROUP 为新主组 -G, --groups GROUPS 新的附加组列表 GROUPS -a, --append GROUP 将用户追加至上边 -G 中提到的附加组中， 并不从其它组中删除此用户 -h, --help 显示此帮助信息并退出 -l, --login LOGIN 新的登录名称 -L, --lock 锁定用户帐号 -m, --move-home 将家目录内容移至新位置 (仅与 -d 一起使用) -o, --non-unique 允许使用重复的(非唯一的) UID -p, --password PASSWORD 将加密过的密码 (PASSWORD) 设为新密码 -R, --root CHROOT_DIR chroot 到的目录 -s, --shell SHELL 该用户帐号的新登录 shell -u, --uid UID 用户帐号的新 UID -U, --unlock 解锁用户帐号 -Z, --selinux-user SEUSER 用户账户的新 SELinux 用户映射 具体可参考命令：man usermod 三、删除用户（组）userdel 命令可用于删除用户，groupdel 命令可用于删除用户组。默认不带选项的 userdel 命令只会删除用户，并不会同时删除该用户的主目录（可以通过加上 -r 选项强制删除用户主目录）。 删除某个用户后，所有原本属于该用户的文件都将失去属主，而被对应的 UID 所代替。如果之后又新建一个用户，而该用户使用了之前已删除账户的 UID，则新账户会替代已删除的账户并获取其文件的权限。 可以使用 find / -user UID -o -group GID 命令定位已删除账户拥有的所有文件 四、密码使用 useradd 命令创建新用户时，是不自动提示创建密码的（Ubuntu系统中的 adduser 命令可以交互地创建新用户）。需要使用 passwd 命令为用户设置或修改密码。 密码时效可以使用 chage 命令设置用户密码的有效期限，如：$ sudo chage -M 30 test30 天后，用户 test 的密码将会过期，并收到提示需要输入新的密码。 chage 命令的常用选项：|选项|描述||-|-||-m days|设置用户修改密码的最小间隔时间，值为 0 时表示可以在任何时间修改密码||-M days|设置密码保持有效的最长期限，即修改密码的最大间隔时间||-E date|设置用户账户过期并自动被禁用的日期||-W days|设置密码过期前多少天用户被警告需要修改密码||-I days|设置密码失效多长时间后账户被锁定| 不带任何选项的 chage 命令可以交互地修改密码时效：1234567891011$ sudo chage test正在为 test 修改年龄信息请输入新值，或直接敲回车键以使用默认值 最小密码年龄 [0]: 最大密码年龄 [99999]: 60 最近一次密码修改时间 (YYYY-MM-DD) [2018-09-03]: 密码过期警告 [7]: 密码失效 [-1]: 帐户过期时间 (YYYY-MM-DD) [-1]:$ 用户可以使用 chage -l 命令查看自己的密码时效设置：12345678chage -l test最近一次密码修改时间 ： 9月 03, 2018密码过期时间 ： 11月 02, 2018密码失效时间 ： 从不帐户过期时间 ： 从不两次改变密码之间相距的最小天数 ：0两次改变密码之间相距的最大天数 ：60在密码过期之前警告的天数 ：7 账户禁用可以使用 passwd -l &lt;user&gt; 命令禁用用户账号，使用 passwd -u &lt;user&gt; 命令解除账户禁用。但是这个命令并不能完全禁止用户的访问（如该用户还可以通过 SSH 远程登录主机）。 如需完全禁用某用户，可使用如下命令：sudo usermod --expiredate 1 &lt;user&gt;该命令会将用户账号的失效日期设置为 1970 年 1月 1 日（即立即禁用）。 或者将用户登录时的 shell 修改为 /bin/false 或 /usr/sbin/nologin。$ sudo usermod -s /bin/false &lt;user&gt;该命令不会锁定账户，但该账户对 shell 的访问已被限制。 五、用户信息Linux 系统将用户、群组等信息保存在以下三个文件中：/etc/passwd、/etc/shadow、/etc/group。 /etc/passwd 文件中包含了所有用户的列表及详细信息：1234567root:x:0:0:root:/root:/bin/bash...starky:x:1000:1000:starky,,,:/home/starky:/bin/bashsshd:x:112:65534::/run/sshd:/usr/sbin/nologinpostgres:x:113:123:PostgreSQL administrator,,,:/var/lib/postgresql:/bin/bashskitar:x:1001:1001::/home/skitar:/bin/bashmysql:x:114:124:MySQL Server,,,:/nonexistent:/bin/false 该文件的格式为：username:password:UID:GID:GECOS:Home:Shell其中的 password 项都为 x ，而实际的密码保存在 /etc/shadow 文件中。 /etc/group 文件的格式为：groupname:password:GID:member,member 六、sudosudo 命令可以使普通用户以 root 用户身份执行命令。使用 useradd 命令新建的用户默认没有使用 sudo 命令的权限，需要先将其加入 sudo 用户组。$ sudo usermod &lt;user&gt; -a -G sudo sudo 的本意是以其他用户的身份执行命令。可以使用 -u 选项指定“其他”身份。默认即为 root 用户。 配置 sudosudo 命令通过检查 /etc/sudoers 中的配置来确定授权规则。可以通过编辑 sudoers 文件将 sudo 的授权限制为指定的用户，指定的主机，或只能执行特定的命令。sudoers 文件的格式为：username host = command如：test ALL=/bin/userdel,/bin/useradd 可以将授权使用的命令限定为某个目录中的所有命令（但不包含其子目录），如：test ALL=/bin/*,/sbin/* 有些时候需要将使用某个命令的权限授予一个用户，但该命令又需要使用另外一个用户的身份执行（如 MySQL 的系统守护进程）。可以使用下面的格式：test ALL=(mysql) /usr/bin/mysqld 还可以通过用户组信息完成对 sudo 的授权：%groupname ALL=(ALL) ALL上面的配置表示该组的所有用户可以在任何主机使用 sudo 执行任何命令。 别名设置/etc/sudoers 文件中允许定义和使用别名，包括用户别名（User_Alias）、主机别名（Host_Alias）和命令别名（Cmnd_Alias）。如：123User_Alias ADMIN = skitar,testCmnd_Alias USER_COMMANDS = /bin/userdel,/bin/useraddADMIN ALL=/bin/groupadd,USER_COMMANDS 上面的配置表示用户 skitar 和 test 可以在所有主机上通过 sudo 执行 userdel、useradd 和 groupadd 命令。 还可以在别名前加上感叹号（!）表示拒绝：12Cmnd_Alias DENIED_COMMANDS = /bin/su,/bin/mount/,/bin/umounttest ALL=/bin/*,!DENIED_COMMANDS 上面的配置表示 test 用户可以在所有主机上通过 sudo 执行除了 su、mount 和 umount 以外的所有 /bin 目录下的命令。 参考资料Pro Linux System Administration: Learn to Build Systems for Your Business Using Free and Open Source Software 2nd Edition]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Admin</tag>
        <tag>System</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Glances - Linux 上的实时系统监控工具]]></title>
    <url>%2F2018%2F08%2F24%2Fuse-glances-monitor-system%2F</url>
    <content type="text"><![CDATA[Glances 是一个跨平台的、基于命令行的系统监控工具，由 Python 语言编写，使用 Python 的 psutil 库来抓取系统数据。可以监控 CPU、负载均衡、内存、网络设备、磁盘 I/O、进程和文件系统使用等。 一、输出信息概览 CPU 信息（用户应用、系统核心程序、闲置） 内存信息，包括 RAM、交换空间、闲置内存等 CPU 的平均负载（过去 1 分钟、5 分钟、15 分钟） 网络连接的下载 / 上传速率 进程总数（running、sleeping 等） 磁盘 I/O 读写速度 当前已挂载设备的磁盘空间使用 高资源占用的进程，及其 CPU/内存占用、PID、状态等 底部显示当前时间 将资源消耗过高的进程红色高亮显示 二、软件安装Glances 一般已集成到大多数 Linux 发行版的官方软件源中。可以直接使用系统的包管理器（如 apt-get、yum）安装：sudo apt-get install glances当然也可以使用 Python 的包管理器（pip 命令）进行安装：pip install glances 默认情况下，监控信息的刷新时间为 1 秒钟。可以使用 -t 选项自定义间隔时间：glances -t 2 Glances 有 4 种颜色标记，分别表示不同的紧急程度： 绿色：OK 蓝色：CAREFUL 紫色：WARNING 红色：CRITICAL 可以在配置文件（默认为 /etc/glances/glances.conf ）中自行更改阈值。默认为 careful = 50、warning = 70、critical = 90 。 三、命令选项（热键） a：自动排序进程 c：按 CPU 使用率排序进程 m：按内存占用排序进程 p：按名称排序进程 i：按 I/O 速率排序进程 d：显示或隐藏磁盘 I/O 统计 f：显示或隐藏文件系统使用统计 n：显示或隐藏网络流量统计 s：显示或隐藏传感器数据统计 l：显示或隐藏日志 h：显示帮助信息 q：退出 四、监控远程系统可以在远程系统中以服务模式运行 glances 程序，再通过客户端上的 glances 连接到远程系统，以监控其状态。可以使用 -s 选项启用服务器/客户端模式：123456$ glances -s --usernameDefine the Glances server username: starkyDefine the Glances server password (starky username):Password (confirm):Do you want to save the password? [Yes/No]: YesGlances XML-RPC server is running on 0.0.0.0:61209 客户端使用 -c 选项连接：glances -c ip_address --username WebServer 模式在 glances 的 WebServer 模式下，客户端只通过浏览器访问就可以获取远程服务器的运行状态。该模式需要额外安装 Python 的 Bottle 模块：pip install bottle安装成功后，使用 glances -w 命令即可开启 WebServer 模式。客户端使用浏览器访问 http://SERVER_IP:61208/ 进入监控界面。 将 WebServer 模式配置为系统服务 创建 Unit 文件sudo vim /etc/systemd/system/glancesweb.service文件内容如下： 1234567[Unit]Description = Glances in Web Server ModeAfter = network.target[Service]ExecStart = /usr/bin/glances -w -t 5 # glances路径因安装方法不同根据实际情况确定，可使用 which glances 命令获取[Install]WantedBy = multi-user.target 启用 systemd 服务并运行 123456789101112$ sudo systemctl enable glancesweb$ sudo systemctl start glancesweb$ systemctl status glancesweb● glancesweb.service - Glances in Web Server Mode Loaded: loaded (/etc/systemd/system/glancesweb.service; enabled; vendor preset: enabled) Active: active (running) since Tue 2018-08-21 00:02:17 CST; 8min ago Main PID: 27912 (glances) Tasks: 1 (limit: 4915) CGroup: /system.slice/glancesweb.service └─27912 /home/starky/miniconda3/envs/python2/bin/python /home/starky/miniconda3/envs/python2/bin/glances -w -t 58月 21 00:02:17 starky-ThinkPad-T460 systemd[1]: Started Glances in Web Server Mode. 参考文章Glances – An Advanced Real Time System Monitoring Tool for LinuxUse Glances to Monitor Remote Linux in Web Server Mode]]></content>
      <categories>
        <category>Tools</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Admin</tag>
        <tag>System</tag>
        <tag>Tools</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 进程管理详解]]></title>
    <url>%2F2018%2F08%2F17%2FLinux-Process-Management%2F</url>
    <content type="text"><![CDATA[进程 是 Unix 和 Linux 系统中对正在运行中的应用程序的抽象，通过它可以管理和监视程序对内存、处理器时间和 I / O 资源的使用。 一、进程的组成一个进程包含内核中的一部分地址空间和一系列数据结构。其中地址空间是内核标记的一部分内存以供进程使用，而数据结构则用来纪录每个进程的具体信息。 最主要的进程信息包括： 进程的地址空间图 进程当前的状态（ sleeping、stopped、runnable 等） 进程的执行优先级 进程调用的资源信息 进程打开的文件和网络端口信息 进程的信号掩码（指明哪种信号被屏蔽） 进程的属主 PID ：进程 ID每个进程都会从内核获取一个唯一的 ID 值。绝大多数用来操作进程的命令和系统调用，都需要用 PID 指定操作的进程对象。 PPID ：父进程 ID在 Unix 和 Linux 系统中，一个已经存在的进程必须“克隆”它自身来创建一个新的进程。当新的进程克隆后，最初的进程便作为父进程存在。 UID &amp; EUID：真实用户 ID 和有效用户 ID一个进程的 UID 是其创建者的身份标志（也是对其父进程 UID 的复制）。通常只有进程的创建者和超级用户才有操作该进程的权限。EUID 是一个额外的 UID，用来决定在任意一个特定时间点，一个进程有权限访问的文件和资源。对绝大多数进程而言，UID 和 EUID 是相同的（特殊情况即 setuid） Niceness一个进程的计划优先级决定了它能获取到的 CPU 时间。内核有一个动态的算法来计算优先级，同时也会关注一个 Niceness 值，来决定程序运行的优先顺序。 二、信号信号属于进程级别的中断请求。它们可以作为进程间通信的手段，或者由终端发送以杀死、中断、挂起某个进程。 常见信号列表： # name Description Default Can catch? Can block? Dump core? 1 HUP Hangup Terminate Yes Yes No 2 INT Interrupt（Ctrl + C） Terminate Yes Yes No 3 Quit Quit（Ctrl + \） Terminate Yes Yes Yes 9 KILL Kill Terminate No No No BUS Bus error Terminate Yes Yes Yes 11 SEGV Segmentation fault Terminate Yes Yes Yes 15 TERM Software terminatation Terminate Yes Yes No STOP Stop（Ctrl + Z） Stop No No No TSTP Keyboard stop Stop Yes Yes No CONT Continue after stop Ignore Yes No No 三、Kill 命令kill 命令常用来终止某个进程，它可以向进程传递任意信号（默认为 TERM）。kill [-signal] pid不带任何数字（信号）选项的 kill 命令并不能保证指定进程被杀死，因为 kill 命令默认发送 TERM 信号，而 TERM 是可以被捕获、屏蔽或忽略的。可以使用 kill -9 pid 命令强制杀死进程（9 代表 KILL 信号，不可被捕获、屏蔽或忽略）。 kill 命令需要指定进程的 PID 号。pgrep 命令可以通过程序名称（或其他属性如 UID）筛选进程号，pkill 命令可以直接发送指定信号给筛选结果。如 sudo pkill -u ben该命令将发送 TERM 信号给所有属于用户 ben 的进程。 killall 命令可以通过程序名称杀死指定进程的所有实例。如：sudo killall apache2123456789101112131415161718192021$ pgrep postgres # 筛选 postgres 进程的 PID 号 25874258762587725878258792588025881$ pgrep -a postgres # 筛选 postgres 进程的 PID 号，并输出详细信息25874 /usr/lib/postgresql/10/bin/postgres -D /var/lib/postgresql/10/main -c config_file=/etc/postgresql/10/main/postgresql.conf25876 postgres: 10/main: checkpointer process25877 postgres: 10/main: writer process25878 postgres: 10/main: wal writer process25879 postgres: 10/main: autovacuum launcher process25880 postgres: 10/main: stats collector process25881 postgres: 10/main: bgworker: logical replication launcher$ sudo kill -9 `pgrep postgres` # 杀死 postgres 进程$ sudo pkill postgres # 同上一条命令$ sudo killall postgres # 杀死 postgres 进程的所有实例$ sudo pkill -9 -u postgres # 杀死属于 postgres 用户的所有进程 根据进程 PID 号查找进程可以使用 ps -p &lt;pid&gt; -o comm= 命令 四、进程状态 状态 含义 Runnable 该进程正在（正准备）执行 Sleeping 该进程正等待某些资源 Zombie 该进程正努力尝试结束 Stopped 该进程已挂起（不允许执行） Runnable 表示进程已经获取到了运行所需的所有资源，只是等待相应的 CPU 时间来处理数据。 Sleeping 表示进程处于等待特定事件发生的状态。交互式 Shell 和系统守护进程的大部分时间都是 Sleeping 状态，等待用户输入或网络连接。 Zombies 表示进程已经结束执行，但是还没有收集完所有的状态，在进程表中仍有纪录。 Stopped 表示进程已停止运行，通常是收到了某种停止信号。 五、PS 命令：监控资源ps 命令是系统管理员监控进程的主要工具。该命令可以显示进程的 PID、UID、优先级和控制终端，以及进程占用的内存、消耗的 CPU 时间和当前的状态等信息。 常用的 PS 命令选项组合： 1. ps auxa 选项表示显示所有进程，x 选项表示同时显示没有控制终端的进程（TTY 显示为 ?），u 选项表示使用基于用户的信息输出格式1234567891011121314151617181920212223$ ps auxUSER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMANDroot 1 0.0 0.1 225428 9548 ? Ss 7月30 0:30 /lib/systemd/systemd --system --deserialize 19root 2 0.0 0.0 0 0 ? S 7月30 0:00 [kthreadd]root 4 0.0 0.0 0 0 ? I&lt; 7月30 0:00 [kworker/0:0H]root 6 0.0 0.0 0 0 ? I&lt; 7月30 0:00 [mm_percpu_wq]root 7 0.0 0.0 0 0 ? S 7月30 0:03 [ksoftirqd/0]root 8 0.0 0.0 0 0 ? I 7月30 14:49 [rcu_sched]...starky 6874 0.0 0.1 33016 8556 pts/2 Ss 8月07 0:00 bashstarky 7150 0.0 0.0 33016 6044 pts/2 S+ 8月07 0:00 bashstarky 7151 3.1 16.1 4763784 1227932 pts/2 Sl+ 8月07 272:54 java -Xmx1024M -Xms512M -jar minecraft_server.1.12.2.jar nogui...root 18447 0.0 0.0 107984 7116 ? Ss 13:55 0:00 sshd: starky [priv]starky 18535 0.0 0.0 108092 4268 ? S 13:55 0:00 sshd: starky@pts/1starky 18536 0.0 0.1 33096 8336 pts/1 Ss 13:55 0:00 -bashroot 18761 0.0 0.0 0 0 ? I 13:55 0:00 [kworker/u8:0]root 18799 0.0 0.0 0 0 ? I 14:01 0:00 [kworker/u8:1]root 18805 0.0 0.0 0 0 ? I 14:05 0:00 [kworker/0:2]starky 18874 0.0 0.0 46780 3568 pts/1 R+ 14:10 0:00 ps -auxredis 19235 0.2 0.0 58548 3736 ? Ssl 8月04 30:03 /usr/bin/redis-server 127.0.0.1:6379root 20799 0.0 0.0 107548 7504 ? Ss 8月05 0:00 /usr/sbin/cupsd -lroot 28342 0.0 0.4 535068 36940 ? Ss 8月10 0:16 /usr/sbin/apache2 -k start 其中带中括号的命令（如 [kthreadd]）并不是真正的命令而是内核线程。 ps aux 命令输出的各列信息含义如下： 项目 解释 USER 进程属主的用户名 PID 进程 ID %CPU 进程占用的 CPU 百分比 %MEM 进程使用的内存百分比 VSZ 进程的虚拟大小 RSS 驻留内存大小（内存中的页数） TTY 控制终端 ID STAT 进程当前的状态：R = RunnableD = In uninterruptible sleepS = Sleeping(&lt;20s)T = Traced or stoppedZ = Zombie额外标记：W = Process is swapped out&lt; = 进程有相对于平时更高的优先级N = 进程有相对于平时更低的优先级L = Some pages are locked in cores = Process is a session leader TIME 进程已经消耗的 CPU 时间 COMMAND 进程的命令和命令选项 2. ps laxl 选项表示以详细的格式输出进程信息。1234567891011121314151617181920212223242526$ ps laxF UID PID PPID PRI NI VSZ RSS WCHAN STAT TTY TIME COMMAND4 0 1 0 20 0 225428 9548 - Ss ? 0:30 /lib/systemd/systemd --system --deserialize 191 0 2 0 20 0 0 0 - S ? 0:00 [kthreadd]1 0 4 2 0 -20 0 0 - I&lt; ? 0:00 [kworker/0:0H]1 0 6 2 0 -20 0 0 - I&lt; ? 0:00 [mm_percpu_wq]1 0 7 2 20 0 0 0 - S ? 0:03 [ksoftirqd/0]1 0 8 2 20 0 0 0 - I ? 14:58 [rcu_sched]...0 1000 6874 6871 20 0 33016 8556 wait Ss pts/2 0:00 bash1 1000 7150 6874 20 0 33016 6044 wait S+ pts/2 0:00 bash0 1000 7151 7150 20 0 4763784 1227932 futex_ Sl+ pts/2 275:03 java -Xmx1024M -Xms512M -jar minecraft_server.1.12.2.jar nogui...4 0 18447 619 20 0 107984 7116 - Ss ? 0:00 sshd: starky [priv]5 1000 18535 18447 20 0 108092 4268 - S ? 0:00 sshd: starky@pts/10 1000 18536 18535 20 0 33096 8336 wait Ss pts/1 0:00 -bash1 0 19051 2 20 0 0 0 - I ? 0:00 [kworker/3:0]1 0 19141 2 20 0 0 0 - I ? 0:00 [kworker/2:3]1 115 19235 1 20 0 58548 3736 - Ssl ? 30:22 /usr/bin/redis-server 127.0.0.1:63791 0 19246 2 20 0 0 0 - I ? 0:00 [kworker/2:0]1 0 19291 2 20 0 0 0 - I ? 0:00 [kworker/u8:0]1 0 19312 2 20 0 0 0 - I ? 0:00 [kworker/0:2]1 0 19405 2 20 0 0 0 - I ? 0:00 [kworker/u8:1]0 1000 19417 18536 20 0 36024 1596 - R+ pts/1 0:00 ps -lax4 0 20799 1 20 0 107548 7504 - Ss ? 0:00 /usr/sbin/cupsd -l5 0 28342 1 20 0 535068 36940 - Ss ? 0:16 /usr/sbin/apache2 -k start ps lax 命令的输出包含了父进程 ID（PPID）、nice 值（NI）还有进程正在等待的资源类型（WCHAN）等。 3. ps axjfps axjf 命令能够以树状结构显示各进程间的层级关系f 选项表示用 ASCII 字符显示树状结构，表达程序间的相互关系。12345678910111213141516171819202122$ ps axjf PPID PID PGID SID TTY TPGID STAT UID TIME COMMAND 0 2 0 0 ? -1 S 0 0:00 [kthreadd] 2 4 0 0 ? -1 I&lt; 0 0:00 \_ [kworker/0:0H] 2 6 0 0 ? -1 I&lt; 0 0:00 \_ [mm_percpu_wq] 2 7 0 0 ? -1 S 0 0:02 \_ [ksoftirqd/0] 2 8 0 0 ? -1 I 0 4:26 \_ [rcu_sched]... 1 672 672 672 ? -1 Ss 0 0:00 /usr/sbin/sshd -D 672 27078 27078 27078 ? -1 Ss 0 0:00 \_ sshd: starky [priv]27078 27166 27078 27078 ? -1 S 1000 0:00 \_ sshd: starky@pts/127166 27167 27167 27167 pts/1 27438 Ss 1000 0:00 \_ -bash27167 27438 27438 27167 pts/1 27438 R+ 1000 0:00 \_ ps axjf 1 681 681 681 ? -1 Ssl 115 9:40 /usr/bin/redis-server 127.0.0.1:6379 1 700 700 700 tty1 700 Ss+ 0 0:00 /sbin/agetty -o -p -- \u --noclear tty1 linux 1 710 710 710 ? -1 Ss 0 0:14 /usr/sbin/apache2 -k start 710 25651 710 710 ? -1 S 33 0:00 \_ /usr/sbin/apache2 -k start 710 25652 710 710 ? -1 S 33 0:00 \_ /usr/sbin/apache2 -k start 710 25653 710 710 ? -1 S 33 0:00 \_ /usr/sbin/apache2 -k start 710 25654 710 710 ? -1 S 33 0:00 \_ /usr/sbin/apache2 -k start 710 25655 710 710 ? -1 S 33 0:00 \_ /usr/sbin/apache2 -k start ... 4. ps ops o 命令加上选项可以指定信息的输出格式，同时加上 --sort 选项可指定排序依据如：ps axo pid,ppid,%mem,%cpu,cmd --sort=-%mem上面的命令表示输出进程的 PID、PPID、内存占用、CPU占用和命令选项。并以内存占用大小排序。（--sort=-%mem 中的 - 表示逆向排序，即由大到小排序）1234567891011$ ps axo pid,ppid,%mem,%cpu,cmd --sort=-%mem | head PID PPID %MEM %CPU CMD 1790 1789 14.1 3.8 java -Xmx1024M -Xms512M -jar minecraft_server.1.12.2.jar nogui 1357 1 2.6 0.1 /usr/sbin/mysqld --daemonize --pid-file=/run/mysqld/mysqld.pid 9343 1 2.0 0.0 /usr/bin/python3 /usr/bin/update-manager --no-update --no-focus-on-map 1244 1 1.5 0.0 sogou-qimpanel %U 1024 1 1.0 0.0 /usr/bin/fcitx 1454 1 0.9 0.0 fcitx-qimpanel 7401 1067 0.7 0.0 lxterminal 248 1 0.6 0.0 /lib/systemd/systemd-journald 1119 1 0.6 0.0 nm-applet 可以尝试不同的命令选项组合来获取相应的信息，具体可参考 man ps 六、使用 TOP 命令动态监控进程top 命令可以实时显示系统当前活跃进程的总体信息及其占用的资源。top 命令的 -d 选项可以指定信息刷新的时间间隔。同时还有一些常用的交互命令 命令 描述 h 显示帮助信息 k 终止某个进程 i 忽略闲置和僵死进程（这是一个开关式命令） q 退出 top 程序 r 重新设置某个进程的优先级 s 改变两次刷新之间的延迟时间（单位为s） f 或 F 从当前显示中添加或者删除项目 l 切换显示平均负载和启动时间信息 m 切换显示内存信息 t 切换显示进程和CPU状态信息 c 切换显示命令名称和完整命令 M 根据驻留内存大小进行排序 P 根据CPU使用百分比大小进行排序 T 根据时间/累计时间进行排序 w 将当前设置写入 ~/.toprc 文件中 七、前台/后台进程 前台进程（也称作交互式进程）：由某个终端会话创建和控制的进程。即需要用户控制而不能作为系统服务自动启动。 后台进程（也称作非交互式进程）：不和终端绑定的进程，不等待用户输入。 可以在命令后带上 &amp; 符号，在后台启用一个 Linux 进程执行该命令。并通过 jobs 命令查看当前的任务。使用 fg 命令将后台执行的进程调到前台执行使用 Ctrl + Z 组合键（发送 SIGSTOP 信号）挂起当前进程（前台），并使用 bg 命令令其在后台继续执行1234567891011121314$ python -m SimpleHTTPServer &amp; # 后台启动 python 进程[1] 28036$ Serving HTTP on 0.0.0.0 port 8000 ...$ jobs # 使用 jobs 命令查看后台进行的任务[1]+ 运行中 python -m SimpleHTTPServer &amp;$ fg %1 # 将后台执行的第一个任务调到前台执行（fg %1）python -m SimpleHTTPServer^Z # 使用 Ctrl + Z 组合键（发送 STOP 信号）停止当前进程[1]+ 已停止 python -m SimpleHTTPServer$ bg # 使用 bg 命令将进程调至后台继续执行[1]+ python -m SimpleHTTPServer &amp;$ fg %1python -m SimpleHTTPServer 参考资料UNIX and Linux System Administration Handbook, 4th EditionAll You Need To Know About Processes in Linux]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Admin</tag>
        <tag>System</tag>
        <tag>Tools</tag>
        <tag>Trick</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 文本处理命令详解（tr cut sort uniq）]]></title>
    <url>%2F2018%2F07%2F03%2FLinux-text-manipulation%2F</url>
    <content type="text"><![CDATA[一、tr 命令tr (translate) 命令可以对来自标准输入的字符进行替换、压缩和删除操作。 语法1tr [option] string1 string2 选项tr string1 string2 ：将标准输入中所有属于 string1 的字符替换为 string2 中的字符-d ：删除标准输入中所有属于 string1 的字符-s ：把标准输入中连续的重复字符压缩成一个字符显示-c ：取代或删除标准输入中所有不属于 string1 的字符 示例123456789101112131415# 替换指定字符$ echo &quot;hello world&quot; | tr &apos;h&apos; &apos;H&apos;Hello world# 删除指定字符（数字和空格）$ echo &quot;hello 12345 world&quot; | tr -d &apos;0-9 &apos;helloworld# 压缩指定字符$ echo &quot;heeeeeeeeello world&quot; | tr -s &apos;e&apos;hello world# 字符集补集$ echo &quot;hello 12345 world&quot; | tr -c -d &apos;0-9&apos;12345 字符集格式（string1 / string2）字符集合（即命令参数中的 string1 和 string2）用于指定需要替换或删除的字符。&#39;A-Za-z&#39;、&#39;A-F0-9&#39;、&#39;}.\t&#39; 等都是合法的字符集合。 字符 含义 \b 退格符 \n 新行 \r 回车符 \t Tab 符 CHAR1-CHAR2 从 CHAR1 到 CHAR2 的所有字符（按 ASCII 字符顺序） [:alnum:] 所有字母和数字 [:alpha:] 所有字母 [:blank:] 所有空格和 Tab 字符 [:cntrl:] 所有控制字符 [:graph:] 所有可打印字符，不包括空格 [:lower:] 所有小写字符 [:punct:] 所有标点符号 [:space:] 所有横向或纵向的空白字符 [:upper:] 所有大写字符 二、cut 命令cut 命令用于切割并筛选文本行中的指定部分，其操作对象可以是一个或多个文件，如未指定文件选项或该选项为 “-“，则从标准输入中读取需要操作的内容。 语法cut &lt;option&gt; list [file ...]其中 list 选项为由逗号分隔的数字或 “-“ 号连接的数字范围，用于指定文本行中需要显示的字段。 N-：从第 N 个字节、字符、字段到结尾 N-M：从第 N 个字节、字符、字段到第 M 个（包括 M 在内） -M：从第 1 个字节、字符、字段到第 M 个（包括 M 在内） 选项-b list：list 选项指定的是 byte 的范围-c list：list 选项指定的是字符的范围-d：指定字段的分隔符，默认是 Tab-f list：list 选项指定的是字段的范围 示例用于演示的文件内容如下：12345$ cat students.txtNo Name Mark01 rose 6902 jack 7103 alex 68 使用 -f 选项提取指定字段12345$ cut -f 2,3 students.txtName Markrose 69jack 71alex 68 使用 -d 选项指定字段分隔符12345678910$ cat students2.txtNo,Name,Mark01,rose,6902,jack,7103,alex,68$ cut -f 2,3 -d &quot;,&quot; students2.txtName,Markrose,69jack,71alex,68 使用 -c 选项提取指定字符范围里的内容123456789101112$ cat test.txtabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyz$ cut -c -5 test.txtabcdeabcdeabcde$ cut -c 5- test.txtefghijklmnopqrstuvwxyzefghijklmnopqrstuvwxyzefghijklmnopqrstuvwxyz 三、sort 命令sort 命令用于将文件内容进行排序，并将排序结果打印到标准输出。它将文件的每一行文本视为一个单位，从首字母向后，依次按该字母的 ASCII 码值进行比较，并按升序输出。 选项-b：忽略每行行首的空格字符-c：检查文件是否已按顺序排序-d：排序时，只考虑字母、空格和数字，忽略其它字符-f：排序时，将小写字母视为大写字母（即忽略大小写）-i：排序时，忽略所有非打印字符-M：将前面三个字母按月份的缩写进行排序-n：按照数值的大小进行排序-o：将排序好的结果输出到指定文件中-r：以相反的顺序输出排序后的结果-t：指定排序时使用的栏位分隔符-u：合并显示内容相同的行 示例用于演示的文件内容如下：123456$ cat sort.txtAAA:BBaaa:4ccc:10bbb:20bbb:8 简单排序123456$ sort sort.txtAAA:BBaaa:4bbb:20bbb:8ccc:10 按照 BB 列的数字由大到小排序12345678$ sort -nr -t: -k2 sort.txtbbb:20ccc:10bbb:8aaa:4AAA:BB# -n 表示按照数字大小排序，-r 表示反向排序# -t: 表示冒号作为栏位分隔符，-k2 表示第二栏（即 BB 列）作为排序依据 四、uniq 命令uniq 命令用于报告或过滤文件中内容重复的行 选项-c：在每行输出内容的行首加上该行重复的次数-d：仅显示内容重复的行-u：仅显示内容未重复的行-f：不比较指定的栏位-s：不比较指定的字符 示例1234567891011121314151617181920212223242526# 源文件$ cat uniq.txthelloworldhelloworldhelloshellhellotext# 删除重复行$ uniq uniq.txthelloworldhelloshellhellotext# 只显示未重复的内容$ uniq -u uniq.txthelloshellhellotext# 统计各行出现的次数$ uniq -c uniq.txt 2 helloworld 1 helloshell 1 hellotext# 只显示重复行$ uniq -d uniq.txthelloworld]]></content>
      <categories>
        <category>Tools</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Admin</tag>
        <tag>Tools</tag>
        <tag>Trick</tag>
        <tag>Text</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[gawk 使用方法简介]]></title>
    <url>%2F2018%2F06%2F30%2Fgawk-manual%2F</url>
    <content type="text"><![CDATA[gawk 是最初 Unix 系统上 awk 程序的 GNU 版本。相对于作为流式编辑器的 sed 而言，它提供了更为强大的编程语言特性。 其功能与特性包括： 定义变量来存储数据 通过代数运算符和字符串操作符来处理数据 使用结构化编程语句如 if-then 和循环等 从数据文件中提取出有价值的字段再重新组合以生成结构化的报表 基本语法gawk options program file 构成 gawk 脚本的语句须包含在一对大括号（ {} ）中，而作为命令选项的整个脚本需要包含在一对引号中：1234$ cat test.txtHello World!$ gawk &apos;&#123;print $0&#125;&apos; test.txtHello World! 1. 使用字段变量gawk 会自动地将每行文本中的每个数据字段赋值给一个指定的变量，默认情况下，预先定义的变量为： $0 表示一整行文本 $1 表示该行文本的第一个字段 $2 表示该行文本的第二个字段 $n 表示该行文本的第 n 个字段 文本行中的数据字段是通过预先定义的字段分隔符来分隔开的，默认为空格（包括 TAB ）12345678$ cat data.txtOne line of test text.Two lines of test text. Three lines of test text.$ gawk &apos;&#123;print $1&#125;&apos; data.txt OneTwoThree 可以通过 -F 选项指定另外的分隔符（如 ‘ , ‘ ）12345678$ cat user.csvJack,male,20Rose,female,18Mike,male,24$ gawk -F, &apos;&#123;print $1&#125;&apos; user.csvJackRoseMike 2. 多个命令gawk 语言允许在脚本语句中组合多个命令使用，只需要在各命令之间使用分号（ ; ）分隔开即可12$ echo &quot;My name is Rich&quot; | gawk &apos;&#123;$4=&quot;Christine&quot;; print $0&#125;&apos; My name is Christine 也可以这样：12345$ gawk &apos;&#123;&gt; $4=&quot;Christine&quot;&gt; print $0&#125;&apos;My name is RichMy name is Christine 其中 My name is Rich 是运行时程序获取的用户输入，My name is Christine 是程序运行后的输出 3. BEGIN &amp; END默认情况下，gawk 从输入中读取一行文本，再对该文本执行程序指令。而有时候需要在读取待处理数据之前先执行某些指令，此时就要用到BEGIN关键字。同样的，END 关键字允许你指定在数据处理完成后才执行的脚本。12345678$ gawk &apos;BEGIN &#123;print &quot;The data File Contents:&quot;&#125; &gt; &#123;print $0&#125;&gt; END &#123;print &quot;End of File&quot;&#125;&apos; data.txt The data File Contents:One line of test text.Two lines of test text. Three lines of test text.End of File 4. 从文件中获取脚本gawk 允许先将其程序脚本保存在某个文件中，再通过 -f 选项指定该文件的文件名。而在脚本文件中，各命令不再需要通过 ‘;’ 符号分隔，直接分行列出即可。12345678910111213141516171819$ cat script.gawkBEGIN &#123;print &quot;Users and their age&quot; print &quot;User \t Age&quot;FS=&quot;,&quot;&#125;&#123; print $1 &quot; \t &quot; $3&#125;END &#123;print &quot;There are three people&quot; &#125;$ gawk -f script.gawk user.csvUsers and their ageUser AgeJack 20Rose 18Mike 24There are three people 上述脚本文件中的 FS=”,” 用于定义字段分隔符，效果等同于 -F 选项。 高级特性1. 变量程序语言最重要的特性之一就是定义和引用变量。gawk 语言支持两种类型的变量：内建变量和用户自定义变量。 gawk 程序在处理文本数据时，一次只读取一小段文本，称为 Record 。默认的 Record 分隔符即为换行符。而每条 Record 又可进一步划分成字段（Data Field），并按顺序依次赋值给 $1，$2，$n 等。默认的字段分隔符为空格（包括 TAB） 控制数据字段和 Record 的内建变量： 变量名 描述 FIELDWIDTHS 用一串由空格分隔的数字定义每个数据字段的具体宽度 FS 定义数据字段分割符（输入） RS 定义 Record 分割符（输入） OFS 定义数据字段分割符（输出） ORS 定义 Record 分割符（输出） 默认的 OFS 为空格12345678$ gawk &apos;BEGIN&#123;FS=&quot;,&quot;&#125; &#123;print $1,$2,$3&#125;&apos; user.csvJack male 20Rose female 18Mike male 24$ gawk &apos;BEGIN&#123;FS=&quot;,&quot;;OFS=&quot;:&quot;&#125; &#123;print $1,$2,$3&#125;&apos; user.csvJack:male:20Rose:female:18Mike:male:24 当定义了 FIELDWIDTHS 变量时，gawk 在读取数据时就会忽略字段分割符（FS），转而使用字段宽度来分割数据。12345678$ cat numbers.txt1005.3247596.3711522.349194.0005810.1298100.1$ gawk &apos;BEGIN&#123;FIELDWIDTHS=&quot;3 5 2 5&quot;&#125; &#123;print $1,$2,$3,$4&#125;&apos; numbers.txt100 5.324 75 96.37115 22.34 91 94.00 058 10.12 98 100.1 有些时候会遇到如下组织方式的文本文件：12345678910$ cat people.txtRiley Mullen123 Main StreetChicago, IL 60601(312)555-1234Frank Williams456 Oak StreetIndianapolis, IN 46201(317)555-9876 此时可将字段分隔符（FS）设置为 “\n”，Record 分隔符（RS）设置为空字符串，则 gawk 程序会将空行作为一条 Record 的终止点。123$ gawk &apos;BEGIN&#123;FS=&quot;\n&quot;; RS=&quot;&quot;&#125; &#123;print $1,$4&#125;&apos; people.txtRiley Mullen (312)555-1234Frank Williams (317)555-9876 其他内建变量 变量 描述 ARGC 当前命令行参数的数目 ARGV 由命令行参数组成的数组 CONVFMT 数字的转换格式，默认值为 %.6 g ENVIRON 包含当前系统环境变量的关联数组（字典） ERRNO 当读取或关闭文件出现错误时返回的系统错误 FILENAME gawk 处理的数据文件的文件名 FNR 当前正在处理的 Record 序号 IGNORECASE 设置为非零值时忽略大小写 NF 数据文件中的字段序号 NR 已处理的 Record 总数 OFMT 数字的输出格式，默认为 %.6 g 123456789$ gawk &apos;BEGIN&#123;FS=&quot;,&quot;;&gt; print ARGC,ARGV[0],ARGV[1];&gt; print ENVIRON[&quot;HOME&quot;]&#125;&gt; &#123;print FILENAME,FNR &quot;:&quot; $1&#125;&apos; user.csv2 gawk user.csv/Users/starkyuser.csv 1:Jackuser.csv 2:Roseuser.csv 3:Mike ARGV 的索引是从 0 开始的，表示第一个命令行参数（呃，所以通常就是 gawk 这个命令本身）。程序脚本（引号中的内容）不算在参数内。 用户自定义变量 在脚本中定义变量 1234567891011$ gawk &apos;&gt; BEGIN&#123;&gt; testing = &quot;This is a test&quot; &gt; print testing&gt; testing = 45&gt; print testing&gt; &#125;&apos;This is a test45$ gawk &apos;BEGIN&#123;x = 4; x = x * 2 + 3; print x&#125;&apos;11 在命令行参数中定义变量 1234567$ cat script1.gawkBEGIN&#123;FS = &quot;,&quot;&#125;&#123;print $n&#125;$ gawk -f script1.gawk n=1 user.csvJackRoseMike 2. 数组定义数组：var[index]=element1234567$ gawk &apos;BEGIN&#123;&gt; var[1] = 34&gt; var[2] = 3&gt; total = var[1] + var[2] &gt; print total&gt; &#125;&apos; 37 遍历数组：1234for (var in array) &#123; statements&#125; 1234567891011121314$ gawk &apos;BEGIN&#123;&gt; var[&quot;a&quot;] = 1&gt; var[&quot;g&quot;] = 2&gt; var[&quot;m&quot;] = 3&gt; var[&quot;u&quot;] = 4&gt; for (test in var) &gt; &#123;&gt; print &quot;Index:&quot;,test,&quot; Value:&quot;,var[test] &gt; &#125;&gt; &#125;&apos;Index: u Value: 4Index: m Value: 3Index: a Value: 1Index: g Value: 2 关联数组遍历的顺序是随机的 3. 模式匹配正则表达式123456$ cat user.csvJack,male,20Rose,female,18Mike,male,24$ gawk &apos;BEGIN&#123;FS=&quot;,&quot;&#125; /Jack/&#123;print $0&#125;&apos; user.csvJack,male,20 匹配符（~）匹配符（~）用来对 Record 中的特定字段使用正则表达式。!~ 表示不匹配。1234567891011$ cat dataThis is line 1Another lineline threeThis is line four$ gawk &apos;$3 ~ /line/&#123;print $0&#125;&apos; dataThis is line 1This is line four$ gawk &apos;$3 !~ /line/&#123;print $0&#125;&apos; dataAnother lineline three 数学表达式1234567$ cat user.csvJack,male,20Rose,female,18Mike,male,24$ gawk &apos;BEGIN&#123;FS=&quot;,&quot;&#125; $3 &gt;= 20&#123;print $0&#125;&apos; user.csvJack,male,20Mike,male,24 4. 结构化命令if if (condition) statement123456789101112131415161718192021$ cat numbers246810$ gawk &apos;&#123;&gt; if ($1 &lt; 5)&gt; &#123;&gt; x = $1 - 2&gt; print x&gt; &#125; else&gt; &#123;&gt; x = $1 / 2&gt; print x&gt; &#125;&#125;&apos; numbers02345 while1234while (condition)&#123; statements&#125; 123456789101112131415161718cat number130 120 135160 113 140145 170 215$ gawk &apos;&#123;&gt; total = 0&gt; i = 1&gt; while (i &lt;= 3)&gt; &#123;&gt; total += $i&gt; i++&gt; &#125;&gt; avg = total / 3&gt; print &quot;Average:&quot;,avg&gt; &#125;&apos; numberAverage: 128.333Average: 137.667Average: 176.667 forfor( variable assignment; condition; iteration process)123456789101112$ gawk &apos;&#123;&gt; total = 0&gt; for (i = 1; i &lt; 4; i++)&gt; &#123;&gt; total += $1&gt; &#125;&gt; avg = total / 3&gt; print &quot;Average:&quot;,avg&gt; &#125;&apos; numberAverage: 130Average: 160Average: 145 5. 格式化输出printf 命令格式：printf &quot;format string&quot;, var1, var2 . . .常用格式控制符如下表所示： 控制字符 描述 c 将数字显示为对应的 ASCII 字符 d 或 i 显示整数 e 将数字以科学记数法显示 f 显示浮点数 g 以科学计算法或浮点数显示（看哪种更短） o 以八进制显示 s 显示字符串 x 以十六进制显示 X 以十六进制显示，使用大写的 A-F 12345$ gawk &apos;BEGIN&#123;&gt; x = 10 * 100&gt; printf &quot;The answer is: %e\n&quot;, x&gt; &#125;&apos;The answer is: 1.000000e+03 除控制字符以外，还可以使用另外三种修饰符以对输出进行更多的控制。 width ：该数值用于指定输出的最小宽度。如长度不够，用空格补充 prec ：该数值用于指定浮点数的精确度，或者字符串能包含字符的最大数量 -（减号）：格式化输出时，使用左对齐 1234567$ gawk &apos;BEGIN&#123;FS=&quot;\n&quot;; RS=&quot;&quot;&#125; &#123;printf &quot;%16s %s\n&quot;, $1, $4&#125;&apos; people.txt Riley Mullen (312)555-1234 Frank Williams (317)555-9876$$ gawk &apos;BEGIN&#123;FS=&quot;\n&quot;; RS=&quot;&quot;&#125; &#123;printf &quot;%-16s %s\n&quot;, $1, $4&#125;&apos; people.txtRiley Mullen (312)555-1234Frank Williams (317)555-9876 参考下面的示例， %10.1f 中的 10 用于指定字段的最小宽度（右对齐，前面用空格补），.1 用于指定精确度。123456789101112$ gawk &apos;&#123;&gt; total = 0&gt; for (i = 1; i &lt; 4; i++)&gt; &#123;&gt; total += $i&gt; &#125;&gt; avg = total / 3&gt; printf &quot;Average: %10.1f\n&quot;,avg&gt; &#125;&apos; numberAverage: 128.3Average: 137.7Average: 176.7 6. 内建函数数学函数 函数 描述 atan2(x,y) x / y 的正切 cos(x) x 的余弦 exp(x) x 以 e 为底的指数 int(x) x 的整数部分 log(x) x 的自然对数 rand() 生成介于 0 和 1 之间的随机数 sin(x) x 的正弦 sqrt(x) x 的平方根 srand(x) 指定生成随机数的种子 1234$ gawk &apos;BEGIN&#123;x=exp(100); print x&#125;&apos;26881171418161356094253400435962903554686976$ gawk &apos;BEGIN&#123;x=exp(1000); print x&#125;&apos;inf 字符串函数 函数 描述 gensub(r, s, h [, t]) 该函数用于检索字符串（默认为 $0 ，如 t 指定，则检索字符串 t），用正则表达式 r 进行匹配，并将匹配结果替换为 s 。如 h 为 “g” 或 “G” ，则执行全局替换；如 h 为数字，则只将第 h 个匹配项替换为 s gsub(r, s [,t] 该函数用于检索字符串（默认为 $0 ，如 t 指定，则检索字符串 t），用正则表达式 r 进行匹配，并将匹配结果替换为 s （全局替换） index(s, t) 该函数用于返回字符串 t 在字符串 s 中的位置索引（如 s 不包含 t ，则返回 0） length([s]) 该函数用于返回字符串 s 的长度，如 s 未指定，则返回 $0 的长度 match(s, r ) 该函数用于返回字符串 s 中正则表达式 r 的位置索引 split(s, a [,r]) 该函数用于将字符串 s 根据 FS 符分割后的字段保存在数组 a 中。如已指定正则表达式 r ，则根据 r 而不是 FS 进行分割 sprintf(format, variablies) 该函数用于返回一个格式化后的字符串，该字符串类似 printf 函数的输出 sub(r, s [,t]) 该函数用于检索指定字符串 t （如果未指定 t ，则检索 $0），并使用 s 替换第一个符合条件的匹配结果 tolower(s) 将字符串 s 中的所有字符转换成小写 toupper(s) 将字符串 s 中的所有字符转换成大写 123456789101112131415161718192021222324252627$ gawk &apos;BEGIN&#123;&gt; x = &quot;hello world, hello gawk, hello text&quot;&gt; y = gensub(&quot;hello&quot;,&quot;nihao&quot;,&quot;g&quot;,x)&gt; print y&#125;&apos;nihao world, nihao gawk, nihao text$ gawk &apos;BEGIN&#123;&gt; x = &quot;hello world, hello gawk, hello text&quot;&gt; y = gensub(&quot;hello&quot;,&quot;nihao&quot;,2,x)&gt; print y&#125;&apos;hello world, nihao gawk, hello text$$ gawk &apos;BEGIN&#123;&gt; x = &quot;hello world, hello gawk, hello text&quot;&gt; gsub(&quot;hello&quot;,&quot;nihao&quot;,x)&gt; print x&#125;&apos;nihao world, nihao gawk, nihao text$$ gawk &apos;BEGIN&#123;&gt; x = &quot;hello world, hello gawk, hello text&quot;&gt; split(x,var)&gt; print var[2] var[4] var[6]&#125;&apos;world,gawk,text$ gawk &apos;BEGIN&#123;&gt; x = &quot;hello world, hello gawk, hello text&quot;&gt; split(x,var,&quot;,&quot;)&gt; print var[1] var[2] var[3]&#125;&apos;hello world hello gawk hello text 时间函数 函数 描述 mktime(datespec) 将普通格式（ YYYY MM DD HH MM SS ）的时间日期转换成时间戳 strftime(format [,timestamp]) 将指定时间戳（如未指定，使用当前时间戳）转换成指定的时间日期格式 systime() 返回当前时间的时间戳 123456$ gawk &apos;BEGIN&#123;&gt; date = systime()&gt; day = strftime(&quot;%A, %B %d, %Y&quot;, date)&gt; print day&gt; &#125;&apos;Saturday, June 30, 2018 用户自定义函数定义函数1234function name([variables])&#123; statements&#125; 使用函数1234567891011$ gawk &apos;&gt; function myprint()&gt; &#123;&gt; printf &quot;%-16s - %s\n&quot;, $1, $4&gt; &#125;&gt; BEGIN&#123;FS=&quot;\n&quot;; RS=&quot;&quot;&#125;&gt; &#123;&gt; myprint()&gt; &#125;&apos; people.txtRiley Mullen - (312)555-1234Frank Williams - (317)555-9876 函数库创建函数库12345678910111213$ cat funclibfunction myprint()&#123; printf &quot;%-16s - %s\n&quot;, $1, $4&#125;function myrand(limit)&#123; return int(limit * rand())&#125;function printthird()&#123; print $3&#125; 调用函数库123456789$ cat scriptBEGIN&#123; FS=&quot;\n&quot;; RS=&quot;&quot;&#125;&#123; myprint()&#125;$$ gawk -f funclib -f script people.txtRiley Mullen - (312)555-1234Frank Williams - (317)555-9876 参考书籍Linux Command Line and Shell Scripting Bible 3rd Edition]]></content>
      <categories>
        <category>Programming</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Admin</tag>
        <tag>Tools</tag>
        <tag>Text</tag>
        <tag>Programming</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[emacs 速查手册]]></title>
    <url>%2F2018%2F06%2F29%2Femacs-quick-reference%2F</url>
    <content type="text"><![CDATA[Emacs 是一个历史悠久的文本编辑器（最初版本发布于 1976 年）。它的核心是一个 Emacs Lisp（Lisp 编程语言的一种方言）的解释器，这给予了它近乎变态的扩展性和定制性。Emacs 有着“神之编辑器”和“伪装成操作系统的编辑器”的称号，功能的强大与灵活可想而知。 一、进入 emacs直接在终端输入 emacs 命令即可进入 emacs 编辑器，该命令后面可跟一个或多个需要编辑的文件。使用 --no--init-file 或 -q 选项可以在启动时不使用任何初始配置文件（ ~/.emacs 或 default.el ）当然这个界面，看上去稍微有些年代感……稍微修改一下启动配置文件（~/.emacs），内容如下：123456789101112; .emacs 界面配置(setq default-frame-alist &apos;((vertical-scroll-bars) (top . 25) (left . 45) (background-color . &quot;black&quot;) (foreground-color . &quot;grey&quot;) (cursor-color . &quot;gold1&quot;) (tool-bar-lines . 0) (menu-bar-lines . 1)))(set-default-font &quot;Source Code Pro 16&quot;)(setq inhibit-startup-message t) 修改后的效果： 二、退出 emacs 组合键定义：C 表示 Ctrl 键，M 表示 Windows系统里的 alt 键和 MacOS 系统里的 option 键。 功能 快捷键 命令（按下 M-x 后输入） 挂起 emacs（在图形模式下：最小化/还原） C-z suspend-emacs（suspend-frame） 退出 emacs C-x C-c save-buffers-kill-terminal 三、文件操作 功能 快捷键 命令（按下 M-x 后输入） 读取文件内容到 emacs 中（打开文件） C-x C-f find-file 保存文件 C-x C-s save-buffer 保存所有文件 C-x s save-some-buffers 将另一个文件的内容插入到当前正在编辑的文件中 C-x i insert-file 关闭当前文件并打开另一个文件 C-x C-v find-alternate-file 将当前 buffer 中的内容写入到指定的文件（另存为） C-x C-w write-file 启用/关闭当前 buffer 的只读模式 C-x C-q read-only-mode 四、获取帮助 功能 快捷键 命令 新手指引 C-h t help-with-tutorial 显示匹配指定正则表达式的命令 C-h a apropos-command 显示绑定指定快捷键的函数的帮助信息 C-h k describe-key 显示指定函数（命令）的帮助信息 C-h f describe-function 显示 Mode 相关的信息 C-h m describe-mode 五、错误恢复 功能 快捷键 命令 中断正在输入或执行中的命令 C-g keyboard-quit 恢复由于系统崩溃未保存的文件 M-x recover-session recover-session 撤销不想要的操作 C-x u 或 C-_ 或 C-/ undo 将 buffer 恢复到初始状态 M-x revert-buffer revert-buffer 六、增量搜索 功能 快捷键 命令 向前搜索（向文档底部搜索） C-s isearch-repeat-forward 向后搜索（向文档顶部搜索） C-r isearch-repeat-backward 正则表达式搜索（向文档底部） C-M-s isearch-forward-regexp 正则表达式搜索（想文档顶部） C-M-r isearch-backward-regexp 中断当前搜索 C-g keyboard-quit 重复使用 C-s 或 C-r 可以继续当前方向对同一关键字的搜索（即跳转到下一个匹配项） 七、Buffers 功能 快捷键 命令 列出所有的 Buffer 信息 C-x C-b list-buffers 切换到另一个 Buffer C-x b switch-to-buffer 关闭 Buffer C-x k kill-buffer 八、Shell 命令 功能 快捷键 命令 执行 Shell 命令 M-! shell-command 异步执行 Shell 命令 M-&amp; async-shell-command 开启一个 *Shell* 窗口用于执行命令 M-x shell shell 九、移动 对象 方向向后（文档顶部） 方向向前（文档底部） 字符 C-b C-f 单词 M-b M-f 行 C-p C-n 跳转到行首（行尾） C-a C-e 句子 M-a M-e 段落 M-{ M-} 页面 C-x [ C-x ] 函数 C-M-a C-M-e 跳转到文档开头（结尾） M-&lt; M-&gt; 十、跳转 功能 快捷键 命令 滚动到下（后）一屏 C-v scroll-up-command 滚动到上（前）一屏 M-v scroll-down-command 向左滚动屏幕 C-x &lt; scroll-left 向右滚动屏幕 C-x &gt; scroll-right 将当前行置于屏幕中央/顶部/底部 C-l recenter-top-bottom 跳转到指定行 M-g g goto-line 跳转到指定字符 M-g c goto-char 十一、选择 功能 快捷键 命令 在当前位置设置标记 C-@ set-mark-command 选中整个段落 M-h mark-paragrath 选中整个页面 C-x C-p mark-page 选中整个函数 C-M-h mark-defun 选中整个 Buffer C-x h mark-whole-buffer 十二、搜索替换 功能 快捷键 命令 以交互的方式检索并替换字符串 M-% query-replace 使用正则表达式检索替换 M-x query-replace-regexp query-replace-regexp 交换模式中的合法输入 替换当前匹配并跳转到下一个 SPACE 或 y 替换当前匹配后不做移动 , 跳过当前匹配直接到下一个 DELETE 或 n 替换剩余的所有匹配项 ! 跳转到上一个匹配项 ^ 退出搜索替换模式 ENTER 十三、多窗口 功能 快捷键 命令 关闭其他所有窗口（只显示当前窗口） C-x 1 delete-other-windows 分割当前窗口（上下） C-x 2 split-window-below 关闭当前窗口 C-x 0 delete-window 分割当前窗口（左右） C-x 3 split-window-right 滚动另一个窗口的内容 C-M-v scroll-other-window 移动光标到另一个窗口 C-x o other-window 在另一个窗口打开文件 C-x 4 f find-file-other-window 在另一个窗口运行 Dired C-x 4 d dired-other-window 增大当前窗口高度 C-x ^ enlarge-window 缩减当前窗口宽度 C-x { shrink-window-horizontally 增大当前窗口宽度 C-x } enlarge-window-horizontally 十四、Minibuffer 功能 快捷键 尽可能补全 TAB 补全至一个完整单词 SPACE 补全并执行 ENTER 显示所有可能的补全结果 ? 获取上一个输入 M-p 获取下一个或默认输入 M-n 向后搜索输入历史（正则表达式） M-r 向前搜索输入历史（正则表达式） M-s 中断命令或输入 C-g 十五、简单定制 功能 按键 定义变量或外观 M-x customize 自定义全局按键映射（例子） (global-set-key (kbd “C-c g”) ‘search-forward) 参考资料 &amp; 拓展阅读Emacs Reference CardsGNU Emacs manualAn Introduction to Programming in Emacs LispEmacs Lisp Reference Manual]]></content>
      <tags>
        <tag>Linux</tag>
        <tag>Admin</tag>
        <tag>Tools</tag>
        <tag>Trick</tag>
        <tag>Software</tag>
        <tag>Editor</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Vim 速查手册]]></title>
    <url>%2F2018%2F06%2F06%2Fvim-quick-reference%2F</url>
    <content type="text"><![CDATA[一、移动光标字符级 命令 功能 h 向 左 移动一个字符单位 l 向 右 移动一个字符单位 j 向 下 移动一个字符单位 k 向 上 移动一个字符单位 单词级 命令 功能 w/W 向 右 移动到下一单词开头 e/E 向 右 移动到单词结尾 b/B 向 左 移动到单词开头 PS：所谓“单词”，是按照英文的书写和使用习惯来定义的。用在中文上，一般就会以标点符号和空格作为“单词”的界限。所以常常是一句话一句话地跳转。 块级 命令 功能 0 移动到当前行的第一个字符 ^ 移动到当前行第一个非空白字符 $ 移动到当前行行尾 + 移动到下一行的首字母 - 移动到上一行的首字母 gg 移动到文档第一行 G 移动到文档最后一行 H 移动到当前屏幕的第一行 M 移动到当前屏幕的中间一行 L 移动到当前屏幕的最后一行 : 或 gg 跳转到第 n 行 :+ 或 j 从当前位置向下跳 n 行 :- 或 k 从当前位置向上跳 n 行 滚动屏幕 命令 功能 Ctrl-d 向下滚动半页 Ctrl-u 向上滚动半页 Ctrl-f 向下移滚动一页 Ctrl-b 向上滚动一页 z 将当前行滚动到屏幕顶部 z. 将当前行滚动到屏幕中间 z- 将当前行滚动到屏幕底部 PS：绝大部分命令前都可以加一个数字 n ，通常表示对其后的命令连续执行 n 次。所以 3j 可以表示向下移动 3 个字符单位（即下移 3 行）。3l 表示向右移动 3 个字符单位。相当于连续执行了 l 命令 3 次。而 9z. 则表示光标移动到第 9 行的同时，滚动屏幕并使得第 9 行位于屏幕中间。（这个 9 的含义不同于 3l 命令中的 3） 前面带冒号的命令（如 :+&lt;n&gt;，命令内容会显示在底部）是需要手动输入回车后才执行的。而不带冒号的命令（如 &lt;n&gt;gg）需要在键盘上不间断地按下，之后命令会自动执行。 二、文档操作 插入 命令 功能 i 当前字符前插入 a 当前字符后插入 I 行首插入 A 行尾插入 o 在下一行插入 O 在上一行插入 PS：以上命令会使 vim 进入 insert 模式（即编辑模式），此时输入的任何命令都会作为字符插入到文档中。按 esc 键可退出 insert 模式。 删除（并将删除的内容保存至 vim 剪贴板） 命令 功能 x 删除当前字符（等于 insert 模式下的 Delete） X 删除前一个字符（等于 insert 模式下的 Backspace） dd 删除当前行 d 删除指定内容 D 删除当前光标位置到行尾的所有内容。等于 d$ cc 替换整行内容。即将整行内容删除并进入 insert 模式 c 删除指定内容后，进入 insert 模式。相当于 d&lt;x&gt;a C 替换当前光标位置到行尾的所有内容。等于 c$ 部分用于指定删除的范围。如 j 表示将光标向下移动一个字符单位（即下移一行），则 dj 表示删除当前行和下一行的内容。G 表示将光标移动到文档末尾，则在光标定位在文档首行时，使用 dG 命令可以清空整个文档的内容。 PS：使用以上命令删除的内容会自动保存到 vim 的剪贴板中，即所谓“删除”实际上是“剪切”，使用 p 命令即可粘贴删除的内容。而且该剪贴板限于 vim 内部，不是系统剪贴板。 复制与粘贴 命令 功能 yy 复制当前内容到 vim 剪贴板 y 复制指定内容到 vim 剪贴板 p 在当前位置后粘贴 P 在当前位置前粘贴 J 将当前行与下一行内容合并为一行 y&lt;x&gt; 命令中的 &lt;x&gt; 同样用于指定复制的范围。如 4j 表示将光标下移 4 行，则 y4j 表示将当前行及其后 4 行内容复制到 vim 剪贴板。8gg 表示将光标定位至第 8 行，则 y8gg 表示复制当前行到第 8 行的所有内容。当 vim 剪贴板中的内容为整行时，则粘贴命令（p/P）执行时，也会变成在当前行的前（后）一行粘贴。 查找行内查找 命令 功能 f 当前行向行尾方向查找并定位到字符 x F 当前行向行首方向查找并定位到字符 x t 当前行向行尾方向查找并定位到字符 x 之前 T 当前行向行首方向查找并定位到字符 x 之后 ; 继续向当前方向查找下一个字符 , 向当前方向的相反方向查找下一个字符 文档内查找 命令 功能 * 向后查找光标当前所在单词 # 向前查找光标当前所在单词 / 向后查找指定字符串或模式 ? 向前查找指定字符串或模式 n 继续查找下一个（依照原方向继续查找） N 继续查找上一个（依照原方向进行反向查找） PS：vim 中可使用 % 对括号 ()[]{} 进行匹配查找，当光标位于其中一个符合上时，按下 % 会跳转到与之匹配的另外一个符合上。 替换 命令 功能 r 将当前字符替换为字符 x s 删除当前字符并进入 insert 模式 R 进入 replace 模式，逐字对当前字符进行替换操作，可以移动光标定位需要替换的字符。直到按下 ESC 键退出该模式 ~ 对当前字符进行大小写切换（即大写转小写，小写转大写） gu 将指定的文本转换为小写 gU 将指定的文本转换为大写 g~ 将指定文本进行大小写切换 :,s// 以某个模式（pattern）检索整个文档，并将第 n1 行到第 n2 行中的匹配项替换为指定内容（replace） :%s// 以某个模式（pattern）检索整个文档并将匹配项替换为指定内容（replace）。等同于 :1,$s// PS：:%s/&lt;pattern&gt;/&lt;replace&gt; 命令中的 可以是正则表达式，且该命令只替换每行中的第一个匹配项。如需要全局匹配，可以使用 :%s/&lt;pattern&gt;/&lt;replace&gt;/g 命令 撤销、重做 命令 功能 . 重复执行上一次的命令 u 撤销 U 撤销对当前行的所有操作 Ctrl-r 重做 打开、关闭文档 命令 功能 :e 打开名为 filename 的文件，如文件不存在则创建 :Ex 在 vim 中打开目录树，光标选定后回车打开对应文件（- 命令进入上级目录） :w 保存当前文件 :wa 保存全部文件 :wq 或 ZZ 保存文件并退出 vim :q! 或 ZQ 强制退出 vim ，不保存文件 :r 读入另一个文档（filename）的数据，并将其内容附加到当前文档光标所在行的后面 :saveas &lt;new_filename&gt; 文件另存为 :w &lt;new_name&gt; 另存为一份名为 new_name 的副本并继续编辑原文件 :,w &lt;new_name&gt; 将 n1 行到 n2 行的所有内容保存到名为 new_name 的新文档中 BufferBuffer（缓冲区）指 vim 中打开的文件所占的内存空间，当未写入磁盘时，所有的修改都发生在内存中。vim 打开过的每个文件都会放到一个 Buffer 中，可以随意切换已打开的 Buffer。 命令 功能 :ls 或 :buffers 查看 buffer 列表 :bn 打开缓冲区中下一个文件 :bp 打开缓冲区中上一个文件 :b 打开缓冲区中第 n 个文件 :bdelete 删除需要关闭的缓冲区文件 三、其他技巧缩进 &gt;&gt; 向右缩进当前行 &lt;&lt; 向左缩进当前行 4&gt;&gt; 向右缩进当前行的同时，缩进当前行下面的 3 行内容 &gt;G 向右缩进当前行到文档末尾的所有内容自动排版 == 自动排版当前行 gg=G 自动排版整个文档 &lt;n&gt;== 对从当前行开始的 n 行进行自动排版 =&lt;n&gt;j 对当前行以及下面的 n 行进行自动排版执行 shell 命令:!&lt;command&gt; 可以执行相应的 shell 命令，命令执行完成后按 Enter 回到 vim 界面。如使用 :1,9!sort 命令可以将当前文件中第 1 行到第 9 行的内容重新排序。:r !&lt;command&gt; 可以将相应 shell 命令执行后的输出读取到当前文件中。如使用 :r !date 命令可以将当前详细的时间日期插入到 vim 编辑的文件中。四、分屏与标签页 窗口分屏 分屏方式 :split 或 :sp 或 Ctrl-w s ：上下分屏 :vsplit 或 :vs 或 Ctrl-w v` ：左右分屏 :diffsplit 或 :diffs ：diff 模式打开一个分屏，后面可以加 &lt;filename&gt; 窗口跳转 Ctrl-w w ：激活下一个窗口 Ctrl-w j ：激活下方窗口 Ctrl-w k ：激活上方窗口 Ctrl-w h ：激活左侧窗口 Ctrl-w l ：激活右侧窗口 屏幕缩放 Ctrl-w = ：平均窗口尺寸 Ctrl-w + ：增加当前窗口高度 Ctrl-w - ：缩减窗口高度 Ctrl-w _ ：最大窗口高度 Ctrl-w &gt; ：增加窗口宽度 Ctrl-w &lt; ：缩减窗口宽度 Ctrl-w | ：最大窗口宽度 标签页 创建标签页:tabnew 或 :tabedit 或 :tabe ：打开新标签页该命令包括上面的分屏命令（:sp 或 :vs 等）后面都可以跟 &lt;filename&gt; 选项，用以在新标签页（或窗口）中打开指定文件 切换标签页 gt 或 :tabnext 或 :tabn ：切换到下一个标签页（最后一个会循环到第一个） gT 或 :tabprevious 或 :tabp ：切换到上一个标签页 :tabrewind 或 :tabr 或 :tabfir ：切换到第一个标签页 :tablast 或 :tabl ：切换到最后一个标签页 关闭标签页 :tabclose 或 :tabc ：关闭当前标签页 :-tabc ：关闭上一个标签页 :+tabc ：关闭下一个标签页 :tabonly 或 :tabo ：关闭其他标签页 附录：vim 模式介绍大致上 vim 分为三种模式，分别是命令模式（Command mode），编辑模式（Insert mode）和底线命令模式（Last line mode）。 1. 命令模式vim 启动即进入命令模式。此时敲击键盘动作会被识别为命令，而不是作为字符插入到文档中。如： i 切换到输入模式（在当前字符前插入） : 切换到底线命令模式，此时输入的命令显示在最底下一行， : 符号后面命令模式下只有一些最基本的命令，而底线命令模式下拥有更多的命令。2. 编辑模式在命令模式下键入 i （或 a、e 等）即进入编辑模式在编辑模式下，可以像在记事本中那样，使用键盘输入或修改文档内容。注意编辑完成时，可使用 ESC 键退出编辑模式，回到命令模式。3. 底线命令模式在命令模式下按下 : 即进入底线命令模式该模式下可以输入单个或多个字符的命令，以完成比命令模式下更复杂的操作（如 :wq 保存文件并退出）按 ESC 键可随时退出底线命令模式。 参考书籍vimtutorLearning The Vi And Vim Editors, 7th Edition]]></content>
      <categories>
        <category>Tools</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Admin</tag>
        <tag>Tools</tag>
        <tag>Efficiency</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[命令行使用 gmail 的 SMTP 服务发送电子邮件]]></title>
    <url>%2F2018%2F05%2F29%2Fusing-gmail-smtp-in-shell%2F</url>
    <content type="text"><![CDATA[SMTP 即简单邮件传输协议（Simple Mail Transfer Protocol），是一种基于 TCP 的应用层协议，用于将电子邮件发送到收件人的邮件服务器。不使用 SSL/TLS 加密的 SMTP 协议默认使用 25 端口，通过 SSL/TLS 加密的 SMTP 协议使用 465/587 端口。命令行结合 SMTP 协议主要是为了通过一些简单的脚本，完成在特定情景下通知邮件的自动发送。 MacOS 系统和很多 Linux 发行版默认已经配置好了 postfix 服务，可以直接发送邮件。不过此时使用的发件服务器在本地，发件人为用户名@本地主机名，而该种类型的邮件会被现在绝大多数的邮箱服务封禁。导致邮件发送失败。 而 postfix 本身的配置也较复杂，所以这里讨论通过 QQ 邮箱或 gmail 等第三方 SMTP 服务器进行邮件发送的方式。 一、msmtpmsmtp 是一个非常简单又易用的 SMTP 客户端，且它与 sendmail有很好的兼容性。MacOS 系统和大部分 Linux 都可以使用包管理器进行安装。 软件包安装：brew install msmtp（MacOS）sudo apt-get install msmtp（Ubuntu） Ubuntu 系统下还需要安装 ca-certificates 软件包（证书）sudo apt-get install ca-certificates 软件配置msmtp 配置文件（Linux）如下（~/.msmtprc）：123456789101112131415defaultsaccount gmailhost smtp.gmail.comtls ontls_starttls ontls_trust_file /etc/ssl/certs/ca-certificates.crt tls_certcheck onport 587auth loginfrom username@gmail.comuser username@gmail.compassword ****************account default: gmail 其中 host 项用于配置使用的发件服务器（smtp.gmail.com） tls_starttls on 用于指定启用 STARTTLS 加密，此时 port （端口号）则为 587 。默认的 25 端口不使用加密，在 gmail 等服务器中是不允许访问的 tls_trust_file 用于指定证书文件。MacOS 系统下此路径不存在，则可以注释掉（#）该行配置，并将 tls_certcheck （是否验证证书）设置为 off 。（可以正常发件，但不够安全） from 和 user 为 gmail 邮箱地址，必须为同一个邮箱账号，否则无法通过验证 password 并非登录 gmail 时的密码，而是 Google 账号应用专用密码，可在此地址进行创建。 编辑 /etc/mail.rc 配置文件，令 mail 命令使用 msmtp 作为发件程序（可通过 which msmtp 命令查看 msmtp 程序的具体路径）1set sendmail=/usr/bin/msmtp 配置完成后，即可使用 mail 命令发送邮件了 二、sendemailsendemail 是一个轻量级的命令行下的 SMTP 邮件客户端，用 Perl 语言编写，简单但功能丰富。无需额外配置，只需要配合适当的命令选项和参数，即可使用 gmail 的 SMTP 服务发送电子邮件。MacOS 和 Linux 系统下都可直接使用包管理器进行安装：（brew install sendemail 或 sudo apt-get install sendemail） 命令示例：sendemail -l email.log -f &quot;sender@gmail.com&quot; -u &quot;subject&quot; -t &quot;recipient@xx.com&quot; -s &quot;smtp.gmail.com:587&quot; -o tls=yes -xu &quot;sender@gmail.com&quot; -xp &quot;your_password&quot;可以通过它创建简单的 shell 脚本，作为一个交互式的发件程序。更多用法可参考 man sendemail 三、Python &amp; smtplibPython 语言的内置库中即包含了对电子邮件的支持，比如 email 模块可用于编辑邮件内容，smtplib 模块可用于访问 SMTP 服务。通过 Python 脚本使用 gmail 的 SMTP 服务发送邮件的示例如下：12345678910111213141516171819202122232425262728293031323334353637383940#!/usr/bin/env python3import getpassimport smtplibfrom email.mime.image import MIMEImagefrom email.mime.multipart import MIMEMultipartfrom email.mime.text import MIMETextSMTP_SERVER = &apos;smtp.gmail.com&apos;SMTP_PORT = 587def send_email(sender,recipient): msg = MIMEMultipart() msg[&apos;To&apos;] = recipient msg[&apos;From&apos;] = sender msg[&apos;Subject&apos;] = input(&apos;Subject: &apos;) message = input(&apos;Enter your message. Press Enter when finished:\n&apos;) part = MIMEText(&apos;text&apos;,&quot;plain&quot;) part.set_payload(message) msg.attach(part) session = smtplib.SMTP(SMTP_SERVER, SMTP_PORT)# session.set_debuglevel(1) session.ehlo() session.starttls() session.ehlo# password = getpass.getpass(prompt=&quot;Enter your email password:&quot;) password=&quot;your_password&quot; session.login(sender, password) session.sendmail(sender, recipient, msg.as_string()) print(&quot;Your email is sent to &#123;0&#125;.&quot;.format(recipient)) session.quit()if __name__ == &apos;__main__&apos;: sender=&quot;sender@gmail.com&quot; recipient = input(&quot;Enter recipient address: &quot;) send_email(sender, recipient) 取消 # session.set_debuglevel(1) 前面的注释，可以在程序运行时输出调试信息 参考资料：Learning Python Network Programming by Dr. M. O. Faruque Sarker et al. msmtp - ArchWiki]]></content>
      <categories>
        <category>Admin</category>
      </categories>
      <tags>
        <tag>Admin</tag>
        <tag>Tools</tag>
        <tag>Shell</tag>
        <tag>Python</tag>
        <tag>Script</tag>
        <tag>Programming</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MacOS 系统使用命令安装软件包]]></title>
    <url>%2F2018%2F05%2F17%2Fmacos-install-software%2F</url>
    <content type="text"><![CDATA[Linux 操作系统各发行版都有自己的软件包管理器，如 Ubuntu 的 apt-get ，Fedora 的 yum 及 Arch 的 pacman 等。安装软件不要太方便。而 MacOS 系统可在 Appstore 中获取安装软件，或者从网上下载 pkg 格式的安装包双击运行。其实 MacOS 系统下也有一个很强大的包管理软件 Homebrew （以及 Homebrew-Cask），使用方便，功能强大。但不是内置软件，需要自己手动安装。其实 pkg 格式的安装包，一样可以通过命令（install）来安装。而常见的 dmg 格式的软件包，其实只是将安装文件又打包成了 dmg 磁盘镜像。挂载后即可继续操作。 一、Homebrew &amp; Homebrew-Cask 1. HomebrewHomebrew 是 MacOS 系统里的软件包管理系统，类似于 Ubuntu 中的 apt-get ，这个软件本身安装起来也很简单。 Homebrew 官网提供了安装命令：/usr/bin/ruby -e &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)&quot;将该命令直接复制到 terminal 中并运行，执行完成后即可使用 brew 命令搜索或安装软件了。 2. Homebrew-caskHomebrew-cask 相当于 Homebrew 的扩展，区别在于，brew 命令首先获取程序源码然后编译安装（包括依赖库），并自动做好必要的配置（如环境变量等）；而 brew cask 命令是下载已经编译好的应用包并放在统一的目录中。 安装好 Homebrew 后，可使用 brew tap caskroom/cask 命令直接安装 Homebrew-cask 。 3. 常用命令选项 install &lt;formula&gt; ：安装软件 uninstall &lt;formula&gt; ：卸载软件 update ：使用 git 获取最新版本的 Homebrew list ：列出所有已通过 brew 命令安装的软件 search &lt;text|/text/&gt; ：通过关键字 text 搜索可供安装的软件，如搜索关键字为 /text/ ，则表示由 text 组成的正则表达式 info &lt;formula&gt; ：获取软件包的简要信息 更多用法可参考：man brew 二、安装 pkg 软件包在图形界面下，pkg 软件包可以直接双击运行。而命令行界面下，也可以使用 installer 命令进行安装。命令格式：sudo installer -pkg &lt;package&gt; -target / 三、dmg 格式的软件包dmg 格式的磁盘镜像文件，通常是对应用程序文件或安装程序的打包压缩。所以安装时需要先使用 hdiutil 命令挂载 dmg，再根据文件类型确定需要执行的安装操作。 1. 应用文件像 Tor Browser 这种，下载下来是 dmg 格式的镜像文件，挂载后目录中是已编译好的应用程序文件，直接拖动到 Applications 文件夹即可安装成功。 在命令行中操作时，则需要先使用 hdiutil attach &lt;imgFile&gt; 挂载镜像文件（一般默认会挂载到 /Volumes 目录下），然后直接将应用程序复制到 /Applications 目录下即可。 2. pkg 安装包像 Wireshark 这种，挂载 dmg 文件后，目录中是已编译好的 pkg 安装包，则需要使用 installer 命令进行安装。命令行安装过程如下： 附录Linux 包管理器1. pacman pacman -S &lt;package&gt; 安装软件包 pacman -Ss &lt;regex&gt; 搜索软件包 pacman -Su 更新系统 pacman -Syu 同步源并更新系统 pacman -R &lt;package&gt; 删除软件包 pacman -Rc &lt;package&gt; 删除软件包及依赖该软件的包 pacman -Rs &lt;package&gt; 删除软件包，及其所有未被其他软件包使用的依赖关系 pacman -Rsc &lt;package&gt; 卸载软件及其依赖的包 pacman -Sc 清理 /var/cache/pacman/pkg 目录下的旧软件包 pacman -Scc 清理所有缓存的软件包和数据库 pacman -U &lt;path_to_package&gt; 安装本地的软件包 pacman -Qi &lt;package&gt; 显示已安装软件包的信息大小、安装日期、创建日期、依赖关系、冲突等） pacman -Qip &lt;package.tar.gz&gt; 显示未安装软件包的信息 pacman -Ql &lt;package&gt; 显示软件包所包含的文件列表 2. apt-get apt-cache search &lt;package&gt; 搜索软件包 apt-cache show &lt;package&gt; 获取软件包的信息 apt-get install &lt;package&gt; 安装软件包 apt-get -f install &lt;package&gt; 修复安装 apt-get remove &lt;package&gt; 卸载软件包 apt-get purge &lt;package&gt; 卸载软件包（包括删除配置文件等） apt-get update 更新软件源 apt-get upgrade 更新已安装的软件包 apt-get dist-upgrade 升级系统 3. yum yum update 更新所有软件包 yum update &lt;package&gt; 更新指定的软件包 yum install &lt;package&gt; 安装软件包 yum remove &lt;package&gt; 删除软件包 yum search &lt;pattern&gt; 搜索匹配特定内容的软件包 yum info &lt;package&gt; 查看软件包信息 yum clean 清除缓存和旧的包 yum list installed 列出已安装的软件包]]></content>
      <categories>
        <category>MacOS</category>
      </categories>
      <tags>
        <tag>Admin</tag>
        <tag>System</tag>
        <tag>Tools</tag>
        <tag>MacOS</tag>
        <tag>software</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MacOS 系统使用 dd 命令创建 Linux 启动U盘]]></title>
    <url>%2F2018%2F05%2F14%2Fcreate-startup-disk-using-dd%2F</url>
    <content type="text"><![CDATA[dd) 命令可以从标准输入或文件中读取数据，根据指定的格式来转换数据，再输出到文件、设备或标准输出。同样是对数据的复制转移操作，cp 命令针对的是文件系统级别，操作的是文件和目录；而 dd 针对的是扇区级别，对 blocks 进行操作。所以可以用它来做整盘数据的备份与恢复、备份 MBR 及启动盘的制作等。 比如将 disk1 上的数据使用 dd 命令复制到 disk2 上，则两块硬盘上的数据包括布局都是完全一样的（扇区级别）。而 cp 命令只是将 disk1 上的数据复制到 disk2 上，由于系统写硬盘不是顺序写的，则两块硬盘上相同扇区号上的数据有可能不一样。 dd 命令将原始数据按照数据源的格式原封不动地拷贝到目的地；而cp将文件和目录拷贝到目的地后按照目的地的格式排列新数据。对于不能以文件或目录格式呈现的数据（如引导扇区的数据），cp 是不能操作的。所以需要使用 dd 命令来创建启动 U 盘。 一、镜像格式转换从网上下载的 Linux iso 镜像文件，是不受 MacOS 系统支持的。需要先使用 hdiutil 命令进行格式转换，才可以使用 dd 命令读取。$ hdiutil convert -format UDRW -o &lt;dmg 文件&gt; &lt;iso 文件&gt;-format 指定生成文件的权限，UDRW 表示转换成有 read/write 权限的镜像PS：前面讲过 dd 命令的原理，即无视文件系统，完成扇区级别的数据转移。所以虽然，网上所有的教程都有格式转换这一步。我尝试了不做转换直接将 iso 文件写入磁盘，事实证明也是可行的。 二、卸载U盘查看U盘的设备号$ diskutil list可以通过 diskutil 的 list 选项查看系统当前挂载的磁盘设备。其中的 /dev/disk2 即用来制作启动盘的U盘。可使用 diskutil 的 unmountDisk 选项来卸载U盘。$ diskutil unmountDisk /dev/disk2只有当U盘成功卸载后，才可以使用 dd 命令将镜像文件写入U盘（否则会报 Resource busy 错误） 三、将镜像文件写入U盘$ sudo dd if=lubuntu-16.04.dmg of=/dev/disk2 bs=1m其中 if 选项用于指定输入文件，of 选项用于指定输出文件，bs 选项用于指定块大小。出现以上提示后，则数据写入成功。Linux 启动U盘制作完成。 附：dd 命令介绍dd 命令可以用指定大小的 blocks 复制一个文件，并在复制的同时进行指定的转换。 命令参数 if=文件名：指定输入文件名，缺省为标准输入 of=文件名：指定输出文件名，缺省为标准输出 ibs=bytes：一次读入 bytes 个字节，即指定一个块大小为 bytes 个字节 obs=bytes：一次输出 bytes 个字节，即指定一个块大小为 bytes 个字节 bs=bytes：同时设置读入/输出的块大小为 bytes 个字节 cbs=bytes：一次转换 bytes 个字节，即指定转换缓冲区大小 skip=blocks：从输入文件开头跳过 blocks 个块后再开始复制 seek=blocks：从输出文件开头跳过 blocks 个块后再开始复制 count=blocks：仅复制 blocks 个块，块大小等于 ibs 指定的字节数 命令示例 将 /dev/disk2 全盘数据备份到指定路径的 image 文件dd if=/dev/disk2 of=/home/starky/image 将备份文件恢复到指定磁盘dd if=/home/starky/image of=/dev/disk2 备份 /dev/disk2 全盘数据，并利用 gzip 工具进行压缩，保存到指定路径dd if=/dev/disk2 | gzip &gt; /home/starky/image.gz 将压缩的备份文件恢复到指定盘gzip -dc /home/starky/image.gz | dd of=/dev/disk2 备份磁盘开始的 512 个字节大小的 MBR 信息到指定文件dd if=/dev/disk2 of=/home/starky/image count=1 bs=512count=1 指仅拷贝一个块；bs=512 指块大小为 512 个字节。 拷贝内存内容到硬盘（Linux）dd if=/dev/mem of=/home/starky/mem.bin bs=1024（指定块大小为 1k） 销毁磁盘数据（利用随机的数据填充硬盘）dd if=/dev/urandom of=/dev/disk2 bs=16M 测试 使用 dd 命令将整个U盘（包括分区信息和文件数据）写入 dmg 格式镜像文件上图中的 hdiutil 命令在 MacOS 系统中用于操作 dmg 格式的磁盘镜像（包括挂载、验证、转换、压缩、烧录等）。而 attach 选项则用于挂载 dmg 文件（就像使用 mount 命令挂载物理磁盘一样）。而从截图中可以看出，原 U 盘（disk2）和挂载的 dmg 磁盘镜像（disk3）分区信息是完全一样的，同样的磁盘（分区）大小、分区表类型（mbr）、文件系统（FAT32）和卷标（F01）。dmg 文件就像是对整个 U 盘的克隆，而不只是文件数据的转移。而分区中的文件内容也完全一致。 使用 dd 命令将部分U盘写入 dmg 格式的磁盘镜像文件这种行为，其实当前我也不是很理解。。。 参考资料man diskutilman hdiutilman dd]]></content>
      <categories>
        <category>MacOS</category>
      </categories>
      <tags>
        <tag>Admin</tag>
        <tag>System</tag>
        <tag>Tools</tag>
        <tag>Hardware</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MacOS 磁盘管理工具 diskutil 介绍]]></title>
    <url>%2F2018%2F04%2F27%2Fdiskutil-manual%2F</url>
    <content type="text"><![CDATA[电脑上的操作系统、应用程序和应用数据一般都需要保存在永久存储器中（通常就是硬盘），这样电脑断电后应用数据等就不会丢失。为了更有效地组织磁盘上的数据信息，通常将磁盘预先划分成一个或多个磁盘分区，创建对应的文件系统，以方便计算机对各分区分别进行管理。MacOS 系统自带一个图形化的磁盘管理工具（Disk Utility），同时还有一个命令行版本的 diskutil。通过该命令的使用，可以很快捷地对本地磁盘进行擦除数据、调整分区大小、格式化等操作。 一、verbdiskutil 命令的格式为：diskutil &lt;verb&gt; &lt;options&gt;不带任何选项的 diskutil 命令会列出该命令支持的 verb 及其对应的介绍：123456789101112131415161718192021222324252627282930313233343536373839404142➜ ~ diskutilDisk Utility ToolUtility to manage local disks and volumesUsage: diskutil [quiet] &lt;verb&gt; &lt;options&gt;, where &lt;verb&gt; is as follows: list (List the partitions of a disk) info[rmation] (Get information on a specific disk or partition) listFilesystems (List file systems available for formatting) activity (Continuous log of system-wide disk arbitration) u[n]mount (Unmount a single volume) unmountDisk (Unmount an entire disk (all volumes)) eject (Eject a disk) mount (Mount a single volume) mountDisk (Mount an entire disk (all mountable volumes)) rename[Volume] (Rename a volume) verifyVolume (Verify the file system data structures of a volume) repairVolume (Repair the file system data structures of a volume) verifyDisk (Verify the components of a partition map of a disk) repairDisk (Repair the components of a partition map of a disk) eraseDisk (Erase an existing disk, removing all volumes) eraseVolume (Erase an existing volume) reformat (Erase an existing volume with same name and type) eraseOptical (Erase optical media (CD/RW, DVD/RW, etc.)) zeroDisk (Erase a disk, writing zeros to the media) randomDisk (Erase a disk, writing random data to the media) secureErase (Securely erase a disk or freespace on a volume) partitionDisk ((re)Partition a disk, removing all volumes) resizeVolume (Resize a volume, increasing or decreasing its size) splitPartition (Split an existing partition into two or more) mergePartitions (Combine two or more existing partitions into one) appleRAID &lt;verb&gt; (Perform additional verbs related to AppleRAID) coreStorage &lt;verb&gt; (Perform additional verbs related to CoreStorage) apfs &lt;verb&gt; (Perform additional verbs related to APFS)diskutil &lt;verb&gt; with no options will provide help on that verb 上面列出的 verb 主要分为以下几类： 获取磁盘和分区信息：如 list、info、activity 等 挂（卸）载磁盘或卷：如 mount、eject、mountDisk 等 验证、修复磁盘分区或文件系统：如 verifyVolume、repairDisk 等 分区操作：如 splitPartitions、mergePartitions 等 其他：如 appleRAID、apfs 等 如不清楚某个 verb 的具体命令格式，可以直接使用 diskutil 命令加上该 verb 并且不带任何其他选项，命令行即输出该 verb 的使用介绍。如 eraseDisk 的使用介绍：12345678910➜ ~ diskutil eraseDiskUsage: diskutil eraseDisk format name [APM[Format]|MBR[Format]|GPT[Format]] MountPoint|DiskIdentifier|DeviceNodeCompletely erase an existing whole disk. All volumes on this disk will bedestroyed. Ownership of the affected disk is required.Format is the specific file system name you want to erase it as (HFS+, etc.).Example: diskutil eraseDisk JHFS+ UntitledUFS disk3 二、获取磁盘分区信息1. list可以使用 list 选项简要列出 MacOS 系统的磁盘及分区信息，包括分区类型（TYPE）、分区名（NAME）、容量大小（SIZE）和标志符（IDENTIFIER）等。如此时系统挂载了 dmg 映像文件，其信息也会显示在列表中（下表中的 disk3 ）。1234567891011121314151617181920212223242526➜ ~ diskutil list/dev/disk0 (internal, physical): #: TYPE NAME SIZE IDENTIFIER 0: GUID_partition_scheme *121.3 GB disk0 1: EFI EFI 209.7 MB disk0s1 2: Apple_APFS Container disk1 121.1 GB disk0s2/dev/disk1 (synthesized): #: TYPE NAME SIZE IDENTIFIER 0: APFS Container Scheme - +121.1 GB disk1 Physical Store disk0s2 1: APFS Volume Mac OS 78.4 GB disk1s1 2: APFS Volume Preboot 22.5 MB disk1s2 3: APFS Volume Recovery 517.8 MB disk1s3 4: APFS Volume VM 3.2 GB disk1s4/dev/disk2 (external, physical): #: TYPE NAME SIZE IDENTIFIER 0: FDisk_partition_scheme *7.8 GB disk2 1: Windows_FAT_32 UNTITLED 7.8 GB disk2s1/dev/disk3 (disk image): #: TYPE NAME SIZE IDENTIFIER 0: Apple_partition_scheme +39.1 MB disk3 1: Apple_partition_map 32.3 KB disk3s1 2: Apple_HFS Wireshark 39.1 MB disk3s2 其中的 /dev/disk0 为内置磁盘，/dev/disk2 为外置磁盘（U 盘，已在 Windows系统下格式化为 FAT32 格式），/dev/disk3 为 DMG 映像文件。而 /dev/disk1 其实就是 disk0s2 作为 APFS 文件系统容器的具体信息。 2. infoinfo 选项可以列出指定磁盘或分区的详细信息。如查看 disk2 （即 8 G 优盘）的信息：12345678910111213141516171819202122232425262728~ diskutil info disk2 Device Identifier: disk2 Device Node: /dev/disk2 Whole: Yes Part of Whole: disk2 Device / Media Name: DataTraveler 2.0 Volume Name: Not applicable (no file system) Mounted: Not applicable (no file system) File System: None Content (IOContent): FDisk_partition_scheme OS Can Be Installed: No Media Type: Generic Protocol: USB SMART Status: Not Supported Disk Size: 7.8 GB (7807696896 Bytes) (exactly 15249408 512-Byte-Units) Device Block Size: 512 Bytes Read-Only Media: No Read-Only Volume: Not applicable (no file system) Device Location: External Removable Media: Removable Media Removal: Software-Activated Virtual: No 输出的信息包括设备标志符（Device Identifier）、设备节点（Device Node）、设备名（Device / Media Name）、容量大小（Disk Size）、块大小（Block Size）等。 也可以查看某个分区的详细信息：1234567891011121314151617181920212223242526272829303132333435363738394041~ diskutil info disk1s1 Device Identifier: disk1s1 Device Node: /dev/disk1s1 Whole: No Part of Whole: disk1 Volume Name: Mac OS Mounted: Yes Mount Point: / Partition Type: 41504653-0000-11AA-AA11-00306543ECAC File System Personality: APFS Type (Bundle): apfs Name (User Visible): APFS Owners: Enabled OS Can Be Installed: Yes Booter Disk: disk1s2 Recovery Disk: disk1s3 Media Type: Generic Protocol: PCI SMART Status: Verified Volume UUID: E9D63DEC-29D7-3EE0-B9BB-3614E31EA747 Disk / Partition UUID: E9D63DEC-29D7-3EE0-B9BB-3614E31EA747 Disk Size: 121.1 GB (121123069952 Bytes) (exactly 236568496 512-Byte-Units) Device Block Size: 4096 Bytes Volume Total Space: 121.1 GB (121123069952 Bytes) (exactly 236568496 512-Byte-Units) Volume Used Space: 80.0 GB (79982071808 Bytes) (exactly 156214984 512-Byte-Units) (66.0%) Volume Free Space: 41.1 GB (41140998144 Bytes) (exactly 80353512 512-Byte-Units) (34.0%) Allocation Block Size: 4096 Bytes Read-Only Media: No Read-Only Volume: No Device Location: Internal Removable Media: Fixed Solid State: Yes Hardware AES Support: No 三、擦除磁盘或分区eraseDisk 选项用于擦除整个磁盘并重新格式化。该命令的格式为：diskutil eraseDisk &lt;format&gt; &lt;name&gt; [APM|MBR|GPT] MountPoint|DiskIdentifier|DeviceNodeformat 用于指定擦除数据后需要重新建立的文件系统类型。可以为 %noformat% 来跳过初始化文件系统的操作。其他支持的类型可以通过 listFilesystems 选项查看。123456789101112131415161718192021222324252627➜ ~ diskutil listFilesystemsFormattable file systemsThese file system personalities can be used for erasing and partitioning.When specifying a personality as a parameter to a verb, case is not considered.-------------------------------------------------------------------------------PERSONALITY USER VISIBLE NAME-------------------------------------------------------------------------------APFS APFS (or) APFSICase-sensitive APFS APFS (Case-sensitive)ExFAT ExFATFree Space Free Space (or) FREEMS-DOS MS-DOS (FAT)MS-DOS FAT12 MS-DOS (FAT12)MS-DOS FAT16 MS-DOS (FAT16)MS-DOS FAT32 MS-DOS (FAT32) (or) FAT32HFS+ Mac OS ExtendedCase-sensitive HFS+ Mac OS Extended (Case-sensitive) (or) HFSXCase-sensitive Journaled HFS+ Mac OS Extended (Case-sensitive, Journaled) (or) JHFSXJournaled HFS+ Mac OS Extended (Journaled) (or) JHFS+ 用来测试的优盘如下所示，已在 Windows 下格式化为 FAT32 格式。可以使用 diskutil eraseDisk ExFAT StarkyDisk disk2 命令将优盘数据擦除并格式化为 ExFAT 格式。123456789101112131415161718192021222324➜ ~ diskutil eraseDisk ExFAT StarkyDisk disk2Started erase on disk2Unmounting diskCreating the partition mapWaiting for partitions to activateFormatting disk2s2 as ExFAT with name StarkyDiskVolume name : StarkyDiskPartition offset : 411648 sectors (210763776 bytes)Volume size : 14835712 sectors (7595884544 bytes)Bytes per sector : 512Bytes per cluster: 32768FAT offset : 2048 sectors (1048576 bytes)# FAT sectors : 2048Number of FATs : 1Cluster offset : 4096 sectors (2097152 bytes)# Clusters : 231744Volume Serial # : 5ad7f879Bitmap start : 2Bitmap file size : 28968Upcase start : 3Upcase file size : 5836Root start : 4Mounting diskFinished erase on disk2 此时的优盘信息为：分区表变为 GPT 类型，且多了一个 EFI 分区。 也可以在擦除磁盘时指定分区表类型：12345678910111213141516171819202122232425➜ ~ sudo diskutil eraseDisk ExFAT StarkyDisk MBR disk2Password:Started erase on disk2Unmounting diskCreating the partition mapWaiting for partitions to activateFormatting disk2s1 as ExFAT with name StarkyDiskVolume name : StarkyDiskPartition offset : 2 sectors (1024 bytes)Volume size : 15249406 sectors (7807695872 bytes)Bytes per sector : 512Bytes per cluster: 32768FAT offset : 2048 sectors (1048576 bytes)# FAT sectors : 2048Number of FATs : 1Cluster offset : 4096 sectors (2097152 bytes)# Clusters : 238207Volume Serial # : 5ad80e37Bitmap start : 2Bitmap file size : 29776Upcase start : 3Upcase file size : 5836Root start : 4Mounting diskFinished erase on disk2 此时的优盘分区表变为 MBR 类型： 其他擦除命令如 eraseVolume （完全擦除整个磁盘或某个磁盘分区，创建新的文件系统）、zeroDisk （向整个磁盘或某个分区全部写入 ‘0’）使用 zeroDisk 命令擦除磁盘（该过程会花费很长的时间，我试了）后，该磁盘上的全部信息被抹除，同时也不再包含分区和文件系统信息：则再次插入此优盘会提示你『初始化』或『格式化』该磁盘。 四、创建磁盘分区可以通过 partionDisk 选项完成对磁盘的分区操作。该命令的格式为：1234diskutil partitionDisk MountPoint|DiskIdentifier|DeviceNode [numberOfPartitions] [APM|MBR|GPT] [part1Format part1Name part1Size part2Format part2Name part2Size part3Format part3Name part3Size ...] 命令选项中的 Size 用来指定分区的大小（以扇区数计量），合法的值包括带有指定后缀的浮点数。其中的后缀有 B(ytes), S(512-byte-blocks), K(ilobytes), M(egabytes), G(igabytes), T(erabytes), P(etabytes)，也可以是 % 来表示对整个磁盘的占比。最后一个分区会自动扩展到占用整个磁盘的剩余空间，如果想为最后一个分区指定固定的大小，可在其后再创建一个类型为『free space』的分区。12345678910111213141516171819➜ ~ sudo diskutil partitionDisk disk2 3 MBR MS-DOS F01 3G JHFS+ F02 3G &quot;Free Space&quot; F03 0Started partitioning on disk2Unmounting diskCreating the partition mapWaiting for partitions to activateFormatting disk2s1 as MS-DOS (FAT) with name F01512 bytes per physical sector/dev/rdisk2s1: 5847920 sectors in 730990 FAT32 clusters (4096 bytes/cluster)bps=512 spc=8 res=32 nft=2 mid=0xf8 spt=32 hds=255 hid=2 drv=0x80 bsec=5859376 bspf=5711 rdcl=2 infs=1 bkbs=6Mounting diskFormatting disk2s2 as Mac OS Extended (Journaled) with name F02Initialized /dev/rdisk2s2 as a 3 GB case-insensitive HFS Plus volume with a 8192k journalMounting diskFinished partitioning on disk2/dev/disk2 (external, physical): #: TYPE NAME SIZE IDENTIFIER 0: FDisk_partition_scheme *7.8 GB disk2 1: DOS_FAT_32 F01 3.0 GB disk2s1 2: Apple_HFS F02 3.0 GB disk2s2 上面的命令在优盘（disk2）上创建了 3 个分区，第一个（F01）格式为 FAT32，大小是 3 Gb。第二个（F02）格式为 JHFS+，大小为 3 Gb。最后一个是『自由空间』，大小为剩余的容量。所以实际上只是分了两个区，整体的分区表类型为 MBR。 五、分割/合并磁盘分区splitPartition 选项可以用来将已存在的某个分区再分割成数个更小的分区，注意原分区上的所有数据都会丢失。该选项的第一个参数为需要分割的分区的挂载点/标志符/设备节点，其余参数和使用 partitionDisk 时相同。12345678910111213141516171819202122232425➜ ~ sudo diskutil list | grep disk2/dev/disk2 (external, physical): 0: GUID_partition_scheme *7.8 GB disk2 1: EFI EFI 209.7 MB disk2s1 2: Apple_HFS starky 7.5 GB disk2s2➜ ~ sudo diskutil splitPartition disk2s2 2 MS-DOS F01 3g JHFS+ F02 3gStarted partitioning on disk2s2 starkySplittingUnmounting diskWaiting for partitions to activateFormatting disk2s2 as MS-DOS (FAT) with name F01512 bytes per physical sector/dev/rdisk2s2: 5845824 sectors in 730728 FAT32 clusters (4096 bytes/cluster)bps=512 spc=8 res=32 nft=2 mid=0xf8 spt=32 hds=255 hid=411648 drv=0x80 bsec=5857280 bspf=5709 rdcl=2 infs=1 bkbs=6Mounting diskFormatting disk2s3 as Mac OS Extended (Journaled) with name F02Initialized /dev/rdisk2s3 as a 4 GB case-insensitive HFS Plus volume with a 8192k journalMounting diskFinished partitioning on disk2s2 starky/dev/disk2 (external, physical): #: TYPE NAME SIZE IDENTIFIER 0: GUID_partition_scheme *7.8 GB disk2 1: EFI EFI 209.7 MB disk2s1 2: Microsoft Basic Data F01 3.0 GB disk2s2 3: Apple_HFS F02 4.5 GB disk2s3 上面的命令将优盘的第二个分区（disk2s2）又分割成了两个更小的分区，分别是 FAT32 格式的 F01（disk2s2），和 JHFS+ 格式的 F02（disk2s3）。虽然命令中指定了 F02 的大小是 3G，因为是最后一个分区，所以自动扩展到占用剩余的磁盘空间。最后它的实际大小是 4.5G。 mergePartitions 选项用来将多个已存在的分区合并为一个大的分区。该选项的格式为：diskutil mergePartitions [force] format name DiskIdentifier|DeviceNode DiskIdentifier|DeviceNode第一个分区参数为起始分区，第二个分区参数为结束分区。这两个分区之间的所有分区都将被合并。如果 force 选项没有被指定，且合并前的第一个分区是可调整大小的文件系统（如 JHFS+），则第一个分区上的数据会保留到合并后的分区。 1234567891011121314151617181920➜ ~ sudo diskutil list | grep disk2/dev/disk2 (external, physical): 0: GUID_partition_scheme *7.8 GB disk2 1: EFI EFI 209.7 MB disk2s1 2: Apple_HFS F01 2.9 GB disk2s2 3: Microsoft Basic Data F02 4.5 GB disk2s4➜ ~ sudo diskutil mergePartitions JHFS+ Starky disk2s2 disk2s4Merging partitions into a new partition Start partition: disk2s2 F01 Finish partition: disk2s4 F02Started partitioning on disk2Merging partitionsWaiting for partitions to activateGrowing diskFinished partitioning on disk2/dev/disk2 (external, physical): #: TYPE NAME SIZE IDENTIFIER 0: GUID_partition_scheme *7.8 GB disk2 1: EFI EFI 209.7 MB disk2s1 2: Apple_HFS F01 7.5 GB disk2s2 六、调整分区大小（无损）resizeVolume 选项可以无损调整（增加或缩减）分区大小。 将 disk2s2 分区缩减为 4g 大小，腾出的空间作为『free space』：1234567891011121314151617181920212223242526272829303132➜ ~ diskutil list | grep disk2/dev/disk2 (external, physical): 0: GUID_partition_scheme *7.8 GB disk2 1: EFI EFI 209.7 MB disk2s1 2: Apple_HFS F01 7.5 GB disk2s2➜ ~ sudo diskutil resizeVolume disk2s2 4gResizing to 4000000000 bytesStarted partitioning on disk2s2 F01Verifying the diskVerifying file systemVolume was successfully unmountedPerforming fsck_hfs -fn -x /dev/rdisk2s2Checking Journaled HFS Plus volumeChecking extents overflow fileChecking catalog fileChecking multi-linked filesChecking catalog hierarchyChecking extended attributes fileChecking volume bitmapChecking volume informationThe volume F01 appears to be OKFile system check exit code is 0Restoring the original state found as mountedResizingShrinking file systemModifying partition mapFinished partitioning on disk2s2 F01/dev/disk2 (external, physical): #: TYPE NAME SIZE IDENTIFIER 0: GUID_partition_scheme *7.8 GB disk2 1: EFI EFI 209.7 MB disk2s1 2: Apple_HFS F01 4.0 GB disk2s2 此时 disk2s2 内的文件如下： 将 disk2s2 分区扩展，并尽可能占用所有可用的自由空间。123456789101112131415161718192021222324252627➜ ~ sudo diskutil resizeVolume disk2s2 RResizing to full size (fit to fill)Started partitioning on disk2s2 F01Verifying the diskVerifying file systemVolume was successfully unmountedPerforming fsck_hfs -fn -x /dev/rdisk2s2Checking Journaled HFS Plus volumeChecking extents overflow fileChecking catalog fileChecking multi-linked filesChecking catalog hierarchyChecking extended attributes fileChecking volume bitmapChecking volume informationThe volume F01 appears to be OKFile system check exit code is 0Restoring the original state found as mountedResizingModifying partition mapGrowing file systemFinished partitioning on disk2s2 F01/dev/disk2 (external, physical): #: TYPE NAME SIZE IDENTIFIER 0: GUID_partition_scheme *7.8 GB disk2 1: EFI EFI 209.7 MB disk2s1 2: Apple_HFS F01 7.5 GB disk2s2 此时 disk2s2 内的文件如下： 参考文章man diskutil]]></content>
      <categories>
        <category>MacOS</category>
      </categories>
      <tags>
        <tag>Admin</tag>
        <tag>System</tag>
        <tag>Tools</tag>
        <tag>Hardware</tag>
        <tag>MacOS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu 16.04 搭建 NFS 文件共享服务器]]></title>
    <url>%2F2018%2F04%2F10%2Fubuntu-1604-NFS-server%2F</url>
    <content type="text"><![CDATA[NFS 即网络文件系统（Network File System），是一种分布式文件系统协议，该协议允许客户端主机可以像访问本地文件系统一样通过网络访问服务器端文件，即可以将远程服务器文件直接 mount（挂载）到本地的文件目录结构中进行访问。 一、软件安装服务器端需要安装 nfs-kernel-server 软件包：$ sudo apt-get update$ sudo apt-get install nfs-kernel-server 二、服务器配置默认情况下，NFS 服务器上定义了某个共享目录，则该目录及其子目录下的所有文件都可被访问。出于对安全的考虑，客户端任何需要超级用户（即 root 用户，UID=0 &amp; GID=0）权限的文件操作都默认映射到 UID=65534 和 GID=65534 的用户，即 Ubuntu 系统中的 nobody:nogroup。例如客户端使用 root 权限在挂载的共享目录中创建文件时，该文件的属主和属组自动变为 nobody:nogroup，而非 root:root 。 1. 在服务器端创建共享目录sudo mkdir -p /var/nfs/gernelsudo mkdir -p /var/nfs/publicsudo chown nobody:nogroup /var/nfs/gernel 2. 修改 exports 文件为了使 NFS 服务器定义的共享文件可被指定的客户端主机访问，需要在服务器端的 /etc/exports 文件中添加对应的记录。该文件的格式如下：Directory Host(Options ...) Host(Options) #comment关于 /etc/exports 文件的详细语法格式可参考 man exports 。 文件示例：123/var/nfs/gernel 192.168.56.0/24(rw,insecure,sync,no_subtree_check)/var/nfs/public *(ro,insecure,sync,no_subtree_check)/home/starky 192.168.56.1(rw,insecure,no_root_squash,sync,no_subtree_check) 第一条纪录表示 192.168.56.0/24 子网中的所有主机都可挂载 var/nfs/gernel 目录并拥有读写（rw）权限 第二条纪录表示所有主机都可挂载 /var/nfs/public 目录且拥有只读（ro）权限 第三条纪录表示客户端 IP 地址为 192.168.56.1 的主机可以挂载 /home/starky 目录并拥有读写权限，而且任何 root 权限（UID=0 , GID=0）的文件操作都不默认映射给 nobody:nogroup，而保持属主（组）仍为 root（no_root_squash） insecure 选项：允许通过任意端口的远程访问 sync 选项：强制 NFS 服务器在响应请求之前将文件的改动写入磁盘（强调客户端和服务端文件内容的一致性，但会降低文件操作的效率）。 no_subtree_check 选项：禁用 subtree_check 。subtree_check 用来设置服务器在收到请求时，检查该文件是否在指定目录结构中依旧可用（该选项会在某些情况下发生错误：重命名某文件的同时，该文件在客户端打开）。 三、客户端挂载共享目录列出 nfs 服务器上的共享目录12345$ showmount -e 192.168.56.102Exports list on 192.168.56.102:/home/starky 192.168.56.1/var/nfs/public */var/nfs/gernel 192.168.56.0/24 创建挂载点sudo mkdir -p /mnt/nfs/gernelsudo mkdir -p /mnt/nfs/publicsudo mkdir -p /mnt/nfs/starky 挂载远程目录sudo mount 192.168.56.102:/var/nfs/gernel /mnt/nfs/gernelsudo mount 192.168.56.102:/var/nfs/public /mnt/nfs/publicsudo mount 192.168.56.102:/home/starky /mnt/nfs/starky 权限测试 如截图所示： NFS 的权限设定基于 Linux 文件系统的权限管理，即客户端挂载远程共享目录后，会把它们当成本地磁盘目录一样对待，也是根据文件的属主（组）及其对应的权限设定来限制访问。gernel 目录的属主（组）为 nobody:nogroup（65534:65534），所以虽然该目录为读写权限，非 root 用户无法执行新建操作。而 root 用户由于 NFS 默认的安全机制，会自动映射到 nobody:nogroup。由于我在客户端和服务端都有一个名为 starky 的用户，且它们的 UID:GID 都为1000:1000，所以服务端的 /home/starky 目录可以直接被客户端的 starky 用户访问。且由于 no_root_squash 选项，通过 sudo 命令创建的文件其属主仍为 root（而不会再映射为 nobody）。当然这会导致一些安全问题，比如多个客户端同时都有 UID（GID）为1000的用户（不管用户名是什么），则这些用户会共享服务端 /home/starky 目录里的文件权限。 四、系统启动时自动挂载共享目录可编辑 /etc/fstab 文件令挂载共享目录的 mount 操作成为系统的固定配置（手动输入的 mount 命令属于临时挂载，重启会自动卸载），使得系统重启后可以自动挂载远程文件系统。/etc/fstab 文件的示例内容如下：1234# filesystem mountpoint fstype flags dump fsck192.168.56.102:/var/nfs/gernel /mnt/nfs/gernel nfs rw,bg,intr,hard,nodev,nosuid 0 0192.168.56.102:/var/nfs/public /mnt/nfs/public nfs4 ro,bg,intr,soft,nodev,nosuid 0 0192.168.56.102:/home/starky /mnt/nfs/starky nfs rw,bg,intr,hard,nodev,nosuid 0 0 附录：1. /etc/exports 文件中的 Host 格式/etc/exports 文件的格式为：Directory Host(Options ...) Host(Options) #comment其中的 Host 项用来指定可访问对应共享目录的主机，其格式可分为以下几种： 单个主机Host 项可以为一个或多个单独的 TCP/IP 主机名或 IP 地址 123adminadmin.starky.net192.168.56.101 IP 子网 12310.0.0.0/255.0.0.0 172.16.0.0/255.255.0.0192.168.56.0/24 TCP/IP 域通过使用通配符，可以指定某个特定域中的全部或部分主机 123*.starky.net*craft.starky.net???.starky.net NIS 组可以指定某个 NIS 组中所有主机的访问权限，使用 @group 2. /etc/exports 文件中的 Options 选项 描述 ro 只读权限 rw 读写权限（默认） rw=list 通过 list 指定具有写权限的客户端主机，其他主机则为只读权限 root_squash 将 UID 0 和 GID 0 映射到 anonuid 和 anongid（即 Ubuntu 系统中的 nobody 和 nogroup） no_root_squash 允许需要 root 权限的文件操作，有安全风险 all_squash 将所有的 UID 和 GID 映射到它们的匿名形式，主要针对不信任的主机 anonuid=xxx 指定客户端 root 权限的操作需要映射到的 UID（默认是65534） anongid=xxx 指定客户端 root 权限的操作需要映射到的 GID（默认是65534） insecure 允许通过任意端口的远程访问 async 服务器可以在写入硬盘之前响应客户端的写入请求 wdelay 通过延迟同步多个客户端对文件的更新 sec=flavor 指定共享目录的安全验证方法，包括 sys（UNIX 验证），dh (DES)，krb5i，krb5p 和 none（匿名访问） 3. NFS 挂载选项 选项 描述 rw 以读写模式挂载文件系统（rw 也需在服务端定义） ro 以只读模式挂载文件系统 bg 如挂载失败（服务器无响应），在后台继续尝试并执行其他挂载请求 hard 如果服务器无响应，重复发送请求直到服务器回复 soft 如果服务器无响应，重复发送请求，超过一定时间后返回错误，而不会一直阻塞 intr 允许用户中断阻塞的文件操作（并返回错误） nointr 不允许用户中断客户端的文件操作请求 retrans=n 在 soft 模式下，指定返回错误前重复发送请求的次数 timeo=n 设置超时后重复发送请求的时间间隔（单位 1/10 秒） rsize=n 设置读取 buffer 大小为 n bytes wsize=n 设置写入 buffer 大小为 n bytes sec=flavor 设置安全验证方法 proto=proto 设置传输协议，NFSv4 必须为 TCP 4. NFS 协议讨论传输协议最初的 NFSv2 由于性能原因使用 UDP 协议，虽然 NFS 添加了自己的包序列重组和错误检查功能，但 UDP 和 NFS 都不具备阻塞控制算法，所以在大型的互联网络环境中缺乏足够的性能。NFSv3 提供了 UDP 和 TCP 协议之间的选择。NFSv4 只能使用 TCP 协议。随着 CPU，内存等硬件设备和网络传输速度的提高，最初由于性能需求而倾向 UDP 协议的选择也变得不再必要。 StateNFSv2 和 NFSv3 是无状态的连接，服务端不会跟踪客户端对共享目录的挂载情况，而是使用 “cookie” 来记录一次成功的挂载。”cookie” 不会因为服务器重启而删除，可以用来在服务器挂掉之后保留客户端的连接信息。NFSv4 是有状态的连接，客户端和服务端都会维护文件操作纪录及文件锁的状态。所以不再需要 “cookie” 的使用。 文件锁早期版本的 NFS 协议（v2 &amp; v3）由于是无状态的连接，它们并不清楚哪些主机正在使用哪些文件。但是文件锁的实现又需要获取状态信息。所以早期协议中的文件锁是独立于 NFS 实现的。而 NFSv4 将文件锁的实现整合到了核心协议中，虽然此举增加了复杂度，但同时也解决了早期版本中的很多问题。但是为了兼容使用 V2 和 V3 协议的客户端，独立的 locked 和 statd 守护进程仍旧需要。 安全相关NFS 协议最初在设计时并不关注安全性，NFSv4 通过引入对更强大的安全服务和身份验证的支持，加强了该协议的安全性。 传统的 NFS 协议大多使用 AUTH_SYS 验证方式，基于 UNIX 的用户和组标识。在这种方式下，客户端只需要发送自己的 UID 和 GID 并与服务器上的 /etc/passwd 文件内容作对比，以决定其拥有怎样的权限。所以当多个客户端存在 UID 相同的用户时，这些用户会拥有相同的文件权限。更进一步，拥有 root 权限的用户可以通过 su 命令切换到任意 UID 登录，服务器会因此给予其对应 UID 的权限。为了防止上面的问题出现，服务器可选择使用更健壮的验证机制比如 Kerberos 结合 NFS PRCSEC_GSS。 NFS 共享目录的访问控制基于 /etc/exports 文件中定义的主机名或 IP 地址。但是客户端很容易针对其身份和 IP 地址造假，这也会导致一些安全问题。NFSv4 只使用 TCP 作为自己的传输协议，而且通常只开放 2049 端口进行数据传输。在配置防火墙时，除了放开 2049 端口的限制外，还要时刻注意数据传输的源地址和目标地址。 5. Windows 系统挂载共享目录win10 系统默认不能挂载 NFS 共享目录，需要进入控制面板 - 程序 - 程序和功能 - 启用或关闭 Windows 功能，勾选上 NFS 服务。之后就可以使用 mount 命令挂载共享目录了。只是 Windows 系统并不使用 Linux 那样的用户管理，导致挂载的共享目录只能读取而没有写入的权限。解决办法是在注册表中新建两个 DWORD 值，用作匿名用户的 UID 和 GID。默认参数下的挂载选项，UID 和 GID 都为 -2：可进入注册表编辑器（regedit），定位到 HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\ClientForNFS\CurrentVersion\Default ，新建两个名为 AnonymousUid 和 AnonymousGid 的 DWORD（32位）值，改成自己需要用到的数字（我都改成了 0 ，即对应 Linux 系统中的 root 用户。如需要改为 0 以外的数字，注意先转换成 16 位）。此时的挂载选项变为：如更改未生效，可重启电脑。 参考资料UNIX and Linux System Administration Handbook, 4th EditionHow to Mount an NFS Share Using a Windows 10 Machine]]></content>
      <categories>
        <category>Server</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Admin</tag>
        <tag>Server</tag>
        <tag>Configuration</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[netstat 命令用法详解]]></title>
    <url>%2F2018%2F03%2F29%2Fnetstat-manual%2F</url>
    <content type="text"><![CDATA[Netstat（network statistics）是在内核中访问网络连接状态及其相关信息的命令行程序，可以显示路由表、实际的网络连接和网络接口设备的状态信息，以及与 IP、TCP、UDP 和 ICMP 协议相关的统计数据，一般用于检验本机各端口的网络服务运行状况。 命令选项 显示所有连接。-a 选项会列出 tcp, udp 和 unix 协议下所有套接字的所有连接。 只列出 TCP 或 UDP 协议的连接 使用 -t 选项列出 TCP 协议的连接，可和 -a 选项配合使用 使用 -u 选项列出 UDP 协议的连接 禁用反向域名解析，加快查询速度默认情况下 netstat 会通过反向域名解析查找每个 IP 地址对应的主机名，会降低查找速度。n 选项可以禁用此行为，并且用户 ID 和端口号也优先使用数字显示。 只列出监听中的连接-l 选项可以只列出正在监听的连接（不能和 a 选项同时使用） 获取进程名、进程号以及用户 ID-p 选项可以查看进程信息（此时 netstat 应尽量运行在 root 权限之下，否则不能得到运行在 root 权限下的进程名）-pe 选项可以同时查看进程名（号）和进程所属的用户名 显示路由信息使用 -r 选项打印内核路由信息，与 route 命令输出一样。 网络接口信息-i 选项可以输出网络接口设备的统计信息，结合上 -e 选项，等于 ifconfig 命令的输出。 获取网络协议的统计信息-s 选项可以输出针对不同网络协议的统计信息，包括 Ip、Icmp、Tcp 和 Udp 等。 命令实例 打印 active 状态的连接 查看指定服务是否正常运行 参考文章netstat 的10个基本用法]]></content>
      <categories>
        <category>Tools</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Admin</tag>
        <tag>Tools</tag>
        <tag>Networking</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu 16.04 配置 L2TP over IPSec VPN 服务器]]></title>
    <url>%2F2018%2F03%2F23%2FL2TP-VPN-Server-on-Ubuntu-1604%2F</url>
    <content type="text"><![CDATA[VPN 即 Virtual Private Network（虚拟专用网），简单来说，就是在公共网络上搭建一条虚拟的私有链路，可以通过该链路加入到远程的私有网络环境中。所以常用来帮助员工在办公室外安全地访问企业内部网。创建私有链路需要使用隧道技术，用到的协议包括点对点隧道协议（PPTP），第2层隧道协议（L2TP）等。macOS 系统已经不再支持 PPTP 类型的 VPN。 一、安装软件包$ sudo apt-get install strongswan xl2tpd ppp lsofIPSec 是组建安全的 VPN 时使用的一个加密和认证标准，而 strongSwan 是一个完全支持 IKEv1 和 IKEv2 的 IKE 后台进程。 二、修改配置文件1. 修改系统转发配置在 /etc/sysctl.conf 文件末尾添加以下内容：1234567net.ipv4.ip_forward = 1net.ipv4.conf.all.accept_redirects = 0net.ipv4.conf.all.send_redirects = 0net.ipv4.conf.default.rp_filter = 0net.ipv4.conf.default.accept_source_route = 0net.ipv4.conf.default.send_redirects = 0net.ipv4.icmp_ignore_bogus_error_responses = 1 启用配置：$ sudo sysctl -p 2. 配置 strongswan(IPSec)在 /etc/ipsec.conf 文件末尾添加如下内容：12345678910111213141516171819202122232425262728293031323334version 2 config setupconn L2TP-PSK-noNAT authby=secret #shared secret. Use rsasig for certificates. auto=add #the ipsec tunnel should be started and routes created when the ipsec daemon itself starts. keyingtries=3 #Only negotiate a conn. 3 times. ikelifetime=8h keylife=1h ike=aes256-sha1,aes128-sha1,3des-sha1 type=transport #because we use l2tp as tunnel protocol left=10.2.67.203 # VPN 服务器的 IP 地址，&apos;%any&apos; 表示任意地址 leftprotoport=17/1701 right=%any rightprotoport=17/%any dpddelay=10 # Dead Peer Dectection (RFC 3706) keepalives delay dpdtimeout=20 # length of time (in seconds) we will idle without hearing either an R_U_THERE poll from our peer, or an R_U_THERE_ACK reply. dpdaction=clear # When a DPD enabled peer is declared dead, what action should be taken. clear means the eroute and SA with both be cleared. 配置共享密钥 /etc/ipsec.secrets：1%any : PSK &quot;PASSWORD&quot; %any 针对任意服务器地址，PASSWORD 需要改为足够安全的长密码 3. 配置 xl2tpd在 /etc/xl2tpd/xl2tpd.conf 文件末尾添加如下内容：1234567891011121314151617[global]ipsec saref = yessaref refinfo = 30;debug avp = yes;debug network = yes;debug state = yes;debug tunnel = yes[lns default]ip range = 192.168.100.100 - 192.168.100.200local ip = 192.168.100.1refuse pap = yesrequire authentication = yes;ppp debug = yespppoptfile = /etc/ppp/options.xl2tpdlength bit = yes local ip 表示 VPN 虚拟网络的网关，ip range 表示客户端连接 VPN 服务器时能分配到的 IP 地址在 /etc/ppp/options.xl2tpd 文件中添加如下内容：12345678910111213require-mschap-v2ms-dns 10.2.64.1ms-dns 114.114.114.114authmtu 1200mru 1000crtsctshide-passwordmodemname l2tpdproxyarplcp-echo-interval 30lcp-echo-failure 4 修改 ms-dns 为需要 vpn 客户端使用的 dns 服务器 4. 添加用户修改 /etc/ppp/chap-secrets 文件：12starky l2tpd password1 *bob l2tpd password2 * 格式为：用户名、服务、密码、限制 ip 。 以上的配置完成以后，重启服务就可以使用客户端连接了。不过此时还不能通过该 VPN 访问互联网，需要部署 IP 转发（使用 iptables ）。 三、配置转发输入下面的指令，开启 gre 协议，并打开服务器 47 和 1723 号端口。123$ sudo iptables -A INPUT -p gre -j ACCEPT $ sudo iptables -A INPUT -p tcp --dport 1723 -j ACCEPT $ sudo iptables -A INPUT -p tcp --dport 47 -j ACCEPT 开启一个 NAT 转发$ sudo iptables -t nat -A POSTROUTING -s 192.168.100.0/24 -o wlp4s0 -j MASQUERADEwlp4s0 表示当前服务器使用的网卡设备名。可以通过 ifconfig 命令查看 通过上面的指令，iptables 做了这样一件事：将所有从服务器上传出的源地址为 192.168.100.1-255 的数据包源 ip 改成服务器的 ip 。 四、连接测试首先需要重启服务：12sudo ipsec restartsudo service xl2tpd restart Mac 电脑上在网络偏好设置里新建 VPN 连接，类型选 L2TP over IPSec。验证设置如下图：连接成功图示：在命令行下使用 Tcpdump 的抓包结果： 参考文章Ubuntu 16.04 配置L2TP VPN Server]]></content>
      <categories>
        <category>Server</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Admin</tag>
        <tag>Server</tag>
        <tag>Configuration</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[简单实用的 vim 配置文件]]></title>
    <url>%2F2018%2F03%2F20%2Fvim-configuration%2F</url>
    <content type="text"><![CDATA[Vim 号称『编辑器之神』，既已成神，自然有它凌驾于众生之上的资本。而我只是拿它当个『编辑器』罢了。从来没想过鼓捣些什么神奇的插件，然后摇身一变，成了别人眼里无所不能的『IDE』。在我看来总有些招摇撞骗的感觉。可能折腾的过程才是最值得体味的吧。只是因为它的高效，随手可得，偶尔改改配置文件什么的，感觉很顺手就够了。 所以这儿贴出的配置文件，初衷也只是拿它作为一个再简单不过的编辑器，最起码的要求，就是看上去更合我胃口一点。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697&quot; 关闭对 vi 的兼容性set nocompatible&quot; 使退格键正常处理 indent,eol,start 等set backspace=2&quot; 显示行号set nu&quot; 显示标尺set ruler&quot; 突出显示当前行set cursorline&quot; 高亮显示对应的括号set showmatch&quot; 对应括号的高亮时间（单位 1/10 秒）set matchtime=5&quot; 高亮搜索时的匹配项set hlsearch&quot; 搜索时逐字符匹配set incsearch&quot; 搜索时忽略大小写set ignorecase&quot; 开启语法高亮syntax on&quot; 开启文件类型检测filetype on&quot; 允许载入对应文件类型的插件filetype plugin on&quot; 为特定文件载入对应的缩进文件filetype indent on&quot; 关闭备份功能set nobackup&quot; 文件未保存(或文件只读)时弹出确认set confirm&quot; 切换 buffer 时自动保存当前文件set autowrite&quot; 文件有外部改动时自动载入set autoread&quot; 自动切换为当前文件所在的目录set autochdir&quot; 设置鼠标可用set mouse=aset selection=exclusiveset selectmode=mouse,key&quot; 共享剪贴板set clipboard+=unnamed&quot; 隐藏工具栏和菜单栏（GVim）&quot;set guioptions-=T&quot;set guioptions-=m&quot; tab 宽度和缩进设置set tabstop=4set shiftwidth=4&quot; 不使用空格代替制表符set noexpandtab&quot; 在行和段开始处使用制表符set smarttab&quot; 使用 C 风格的缩进set cindent&quot; 自动缩进（继承上一行的缩进方式）set autoindent&quot; 为 C 程序提供自动缩进set smartindent&quot; 允许折叠set foldenable&quot; 根据语法折叠set fdm=syntax&quot; 手动折叠&quot; set fdm=manual&quot; 设定折叠层数setlocal foldlevel=1&quot; 通过空格键开关折叠nnoremap &lt;space&gt; @=((foldclosed(line(&apos;.&apos;)) &lt; 0) ? &apos;zc&apos; : &apos;zo&apos;)&lt;CR&gt;&quot; 显示状态栏set laststatus=2&quot; 设置状态栏显示的信息set statusline=\ %&lt;%.20F[%1*%M%*%n%R%H]%=\ [%&#123;&amp;ff&#125;:%&#123;&amp;encoding&#125;]\ %Y\ \ \ %l:%v\ %p%%/%L\ \ \ %&#123;strftime(\&quot;%y/%m/%d\ %H:%M\&quot;)&#125;\ \ &quot; 显示输入的命令set showcmd&quot; 增强模式中的命令行自动完成功能set wildmenu&quot; 设置命令行的高度，默认为 1set cmdheight=2&quot; 设置编码set encoding=utf-8set fileencodings=utf-8,usc-bom,shift-jis,gb18030,gbk,gb2312,cp936,utf-16,big-5,euc-jp,latin1set whichwrap=b,s,&lt;,&gt;,[,]&quot; 开启Normal或Visual模式下退格键、空格键、左右方向键，Insert或Replace模式下左右方向键的跳行功能 没有插件，甚至没有自定义组合键，还是觉得没那个必要了。倒是状态栏费了好一阵功夫。。。也许有机会，可以好好陪它玩一下，感受感受神之光芒的洗礼。这会儿，只当它是个工具。对不起咯]]></content>
      <categories>
        <category>Tools</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Tools</tag>
        <tag>Configuration</tag>
        <tag>VIM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tcpdump 命令用法详解]]></title>
    <url>%2F2018%2F03%2F20%2FTcpdump-manual%2F</url>
    <content type="text"><![CDATA[Tcpdump 是信息安全领域常用的嗅探和网络分析工具，运行在命令行下。虽然要用好它需要对 TCP/IP 协议有足够的了解，但从另一个角度讲，多用一下同样也能促进对网络协议的掌握。 大部分 Linux 发行版都内置了 Tcpdump 工具。如果没有，也可以直接使用对应的包管理器进行安装（如：$ sudo apt-get install tcpdump 和 $ sudo yum install tcpdump） 一、命令选项 -i any：监听所有网络接口 -i eth0：监听指定的网络接口（eth0） -D：列出所有可用的网络接口 -n：不解析主机名 -nn：不解析主机名和端口名 -q：输出较少的信息 -t：更便于阅读的时间戳输出 -tttt：最便于阅读的时间戳输出 -X：以 HEX 和 ASCII 模式输出数据包的内容 -XX：与 -X 选项相同，同时还输出 ethernet 头 -v, -vv, -vvv：输出更多数据包的信息 -c：获取到指定数目的数据包后就停止 -s：定义 snaplength (size) ，-s0 表示获取全部 -S：输出绝对序列号 -e：获取 ethernet 头信息 -E：通过提供 key 来解密 IPSEC 流量 二、表达式通过表达式可以对各种不同类型的网络流量进行过滤，以获取到需要的信息。这也是 tcpdump 强大功能的一个体现。主要有 3 种类型的表达式： Type（类型）选项包括 host 、net 和 port Direction（方向）选项包括 src 和 dst 以及它们的组合 Proto（协议）包括 tcp 、udp 、ICMP 和 ah 等 三、应用实例指定网络接口：# tcpdump -i &lt;dev&gt;vboxnet0 是 virtualvox 虚拟机通过 Host-only 方式虚拟的一张网卡 原始信息输出模式：# tcpdump -ttttnnvvS更详细的输出，不解析主机名和端口名，使用绝对序列号，方便阅读的时间戳 通过IP地址过滤：# tcpdump host 10.2.64.110.2.64.1 是我的 DNS 服务器地址 HEX 输出# tcpdump -nnvXSs 0 -c1 icmp 通过源地址和目标地址进行过滤# tcpdump src 10.2.67.203 # tcpdump dst 10.2.67.203 通过子网进行过滤# tcpdump net 10.2.64.0/24 监听指定端口号# tcpdump port 515515 是本地打印机 LPD 服务的端口号 指定协议# tcpdmp icmp 端口范围# tcpdump portrange 21-23 通过包大小过滤# tcpdump less 32# tcpdump greater 64# tcpdump &lt;= 128 写入 PCAP 文件# tcpdump port 80 -w capture_file 读取 PCAP 文件# tcpdump -r capture_file 四、高级功能1. 逻辑运算符可以通过命令选项的不同组合（使用逻辑运算符）完成更复杂的任务。运算符包括以下3种： AND（and 或 &amp;） OR（or 或 ||） EXCEPT （not 或 !） # tcpdump src 10.2.64.29 and dst port 80即捕捉从指定主机（10.2.64.92）发出，且目标端口为 80 的所有网络数据 # tcpdump src net 192.168.0.0/16 and dst net 10.0.0.0/8 or 172.16.0.0/16即捕捉从指定子网（192.168.0.0/16）发送到目标子网（10.0.0.0/8 和 172.16.0.0/16）的所有网络数据 # tcpdump src 192.168.56.1 and not dst port 22即捕捉从指定主机（192.168.56.1）发出，且目标端口不为 22 的所有网络数据 2. 指定 TCP 标志位（Flags）# tcpdump &#39;tcp[13] &amp; 32!=0&#39; 所有 URGENT (URG) 包# tcpdump &#39;tcp[13] &amp; 16!=0&#39; 所有 ACKNOWLEDGE (ACK) 包# tcpdump &#39;tcp[13] &amp; 8!=0&#39; 所有 PUSH (PSH) 包# tcpdump &#39;tcp[13] &amp; 4!=0&#39; 所有 RESET (RST) 包# tcpdump &#39;tcp[13] &amp; 2!=0&#39; 所有 SYNCHRONIZE (SYN) 包# tcpdump &#39;tcp[13] &amp; 1!=0&#39; 所有 FINISH (FIN) 包# tcpdump &#39;tcp[13]=18&#39; 所有 SYNCHRONIZE/ACKNOWLEDGE (SYNACK) 包 其他指定标志位的方式如：# tcpdump &#39;tcp[tcpflags] == tcp-syn&#39;# tcpdump &#39;tcp[tcpflags] == tcp-fin&#39; 一些特殊的用法# tcpdump &#39;tcp[13] = 6&#39; RST 和 SYN 同时启用的数据包（不正常）# tcpdump &#39;tcp[32:4] = 0x47455420&#39; 获取 http GET 请求的文本# tcpdump &#39;tcp[(tcp[12]&gt;&gt;2):4] = 0x5353482D&#39; 获取任何端口的 ssh 连接（通过 banner 信息）# tcpdump &#39;ip[8] &lt; 10&#39; ttl 小于 10 的数据包（出现问题或 traceroute 命令）# tcpdump &#39;ip[6] &amp; 128 != 0&#39; 非常有可能是黑客入侵的情况 参考文章：A tcpdump Tutorial and Primer with Examples]]></content>
      <categories>
        <category>Pentest</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Tools</tag>
        <tag>Security</tag>
        <tag>Networking</tag>
        <tag>Pentest</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在 Ubuntu 16.04 上通过 SSL/TLS 搭建安全的 FTP 服务器]]></title>
    <url>%2F2018%2F03%2F19%2Fsecure-ftp-with-tls-on-ubuntu%2F</url>
    <content type="text"><![CDATA[FTP 是 File Transfer Protocol （文件传输协议）的简称，用于在 Internet 上控制文件的双向传输。作为很古老的一种文件传输协议，简单的服务器搭建和丰富的客户端支持是它的优势，但安全性却是它很大的一个软肋。（在建立连接时使用明文传输用户名和密码等重要信息） 一、安全性测试1. 安装 ftp 服务：ubuntu 16.04 可以使用包管理器直接安装 vsftpd 以建立 ftp 服务器。$ sudo apt-get install vsftpd vsftpd 的配置文件为 /etc/vsftpd.conf，默认即开启了本地用户（local_enable=YES）的登录和下载权限，可以直接使用。如需使用上传功能，可以将配置文件中 #write_enable=YES 前面的注释去掉。因为主要是测试安全性，其他配置选项暂不做改动。 2. 使用 wireshark 抓包Wireshark 是一个功能强大的网络抓包和分析工具。这里使用它来对 FTP 客户端与服务器之间的数据交换进行监控。因为 Ubuntu 服务器安装在 virtualbox 虚拟机中，联网使用的是 Host-only 模式，所以抓包时监控的是 vboxnet0 虚拟网卡。 效果如下：可以看到，FTP 的用户名和密码都是使用明文传输的，可以直接被看到。 3. 更安全的 SFTP 服务SFTP 是 Secure File Transfer Protocol （安全文件传送协议）的缩写，包含在 ssh 服务中。Ubuntu 16.04 系统中，配置好 ssh 服务后，sftp 服务即默认开启。需要注意的是，sftp 使用了加密、解密技术，所以传输效率比普通的FTP要低得多。 可以使用 sudo apt-get openssh-server 命令安装 ssh 服务，如 sftp 服务未默认开启，可以编辑 sshd 配置文件：/etc/ssh/sshd_config，添加上 Subsystem sftp /usr/lib/openssh/sftp-server。 使用 sftp 服务时的抓包截图：可以看到，截获的包内容都变成了 Encrypted packet 二、搭建安全的 FTPS 服务这里要详细说明的是另外一种解决方案，使用 SSL/TLS 对 FTP 的数据传输进行加密。关于 SFTP 与 FTPS 的之间的对比，建议参考此文章：FTPS (FTP over SSL) vs SFTP (SSH File Transfer Protocol)。 1. 创建证书FTPS 是使用安全套接层（SSL）证书的 FTP 技术，也就是使用用户 ID、密码和 SSL 证书进行身份验证。$ sudo openssl req -x509 -nodes -keyout /etc/ssl/private/vsftpd.pem -out /etc/ssl/private/vsftpd.pem -days 365 -newkey rsa:2048上面的命令用于生成证书和 key 并保存在 vsftpd.pem 文件中。运行后会提示你输入相关信息（可以随便填写，不要留空）：1234567Country Name (2 letter code) [AU]:State or Province Name (full name) [Some-State]:Locality Name (eg, city) []:hangzhouOrganization Name (eg, company) [Internet Widgits Pty Ltd]:Organizational Unit Name (eg, section) []:sectionCommon Name (e.g. server FQDN or YOUR name) []:starkyEmail Address []:starky@email.com 2. 修改配置文件如果你有开启 ufw 防火墙（默认是未开启的），需要先在防火墙配置中开放指定端口用于通讯和数据传输。12$ sudo ufw allow 990/tcp$ sudo ufw allow 4000:5000/tcp 我这里没有开启 ufw ，所以不需要以上操作。直接修改 vsftpd 的配置文件（$ sudo vim /etc/vsftpd.conf）。将文件中的对应配置做如下修改（没有就添加）：1234567891011121314151617# 开启 ssl 并指定使用的协议ssl_enable=YESssl_tlsv1=YESssl_sslv2=NOssl_sslv3=NO# 指定证书和 key 文件rsa_cert_file=/etc/ssl/private/vsftpd.pemrsa_private_key_file=/etc/ssl/private/vsftpd.pem# 安全选项allow_anon_ssl=NOforce_local_data_ssl=YESforce_local_logins_ssl=YESrequire_ssl_reuse=NOssl_ciphers=HIGH# 指定主动模式时使用的端口范围pasv_min_port=40000pasv_max_port=50000 修改完成后重启 vsftpd 服务：$ sudo systemctl restart vsftpd 三、连接验证客户端使用 filezilla 连接服务器，新建站点时使用如下配置：12345Host: 192.168.56.102Protocol: FTP – File Transfer ProtocolEncryption: Require explicit FTP over TLSLogon Type: Ask for passwordUser: username 点连接后会跳出输入密码界面和证书信息，确定之后即可成功连接而此时 wireshark 的抓包截图如下：捕捉到的请求和响应信息都是已加密过的密文。 参考文章Setting Up a Secure FTP Server using SSL/TLS on Ubuntu关于 vsftpd 配置文件的详细解释可参考：Vsftpd - Ubuntu中文]]></content>
      <categories>
        <category>Server</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Admin</tag>
        <tag>Server</tag>
        <tag>Ubuntu</tag>
        <tag>FTP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[蚂蚱之死]]></title>
    <url>%2F2018%2F03%2F16%2FThe-Death-of-Grasshopper%2F</url>
    <content type="text"><![CDATA[我小时候是个极恶毒的家伙田野地头儿上走着，每遇蚂蚱蟋蟀之流无论大小，必捕而杀之还是很残忍的那种，处决然后以得胜者的气势扬长而去大有替天行道的侠者风范 以旧的社会主义论调而言，它们是害虫，杀了是为民除害从物竞天择适者生存的观念来讲强者凌驾于弱者之上，又似乎天经地义 于是终于坦然。甚至于有了上瘾的迹象直到很多年过去，到了城里很少再见到它们了竟然又开始想念 我们都是些无能为力的弱者做着些自我欺骗的勾当 小蚂蚱，希望有机会你我可以角色互换]]></content>
      <categories>
        <category>nonsense</category>
      </categories>
      <tags>
        <tag>nonsense</tag>
        <tag>胡言乱语</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 打包 压缩 解压 命令详解]]></title>
    <url>%2F2018%2F03%2F11%2Fuse-tar-to-package-and-compress-files%2F</url>
    <content type="text"><![CDATA[tar 命令是 Linux 环境下最基本的打包工具，注意打包并不等同于压缩。打包只是负责将多个文件整理后合成为一个文件包（即归档，方便传输分享），一般后缀为 .tar。而文件打包后通常都要进行压缩，以节省空间和传输时间。所以打包和压缩是紧密相关但并不相同的两个命令（过程）。用 tar 命令加上适当的选项可以统一执行这两道程序。 一、压缩命令常用的 Linux 压缩工具如下： 压缩工具 对应后缀 对应 tar 选项 gzip .gz z bzip2 .bz 或 .bz2 j compress .Z Z xz .xz J lzma .lzma a PS：lzma 等同于 xz –format=lzma 关于上面表格中的压缩命令，命令选项都大同小异。可以查看相应的帮助信息（如 gzip --help）或手册（如 man xz）。 二、使用 tar 命令打包压缩单独使用压缩命令（如 gzip 或 bzip2）的情形并不常见，多用于单个文件或对多个文件逐个进行压缩。更多的时候使用 tar 命令先打包多个文件或目录后再配合指定的选项执行压缩操作。 tar 命令选项： 第一选项（指定操作方式）： -c ：创建归档文件 -x ：解包归档文件 -t ：列出归档文件的内容列表 -r ：向归档文件中添加（替换）文件（只对未压缩的归档文件有效） -u ：更新归档文件中的内容（只对未压缩的归档文件有效） 压缩选项（指定压缩方法）：-z，-j，-J，-a，分别对应于 gzip，bzip2，xz，lzma 四种压缩命令。 通用选项： -v ：开启更详细的输出信息 -f ：指定归档文件的位置 -w ：交互模式 三、总结 tar 格式：打包：tar -cvf Archive.tar DirName解包：tar -xvf Archive.tar gz 格式：压缩：gzip FileName解压：gzip -d FileName.gz 或 gunzip FileName.gz tar.gz 或 tgz 格式：压缩：tar -czvf FileName.tar.gz DirName解压：tar -xzvf FileName.tar.gz bz 或 bz2 格式：压缩：bzip2 -z FileName解压：bzip2 -d FileName.bz 或 bunzip2 FileName.bz tar.bz 或 tar.bz2 格式：压缩：tar -cjvf FileName.tar.bz DirName解压：tar -xjvf FileName.tar.bz Z 格式：压缩：compress FileName解压：uncompress FileName.Z tar.Z 格式：压缩：tar -cZvf FileName.tar.Z DirName解压：tar -xZvf FileName.tar.Z xz 格式：压缩：xz -z FileName.xz DirName解压：xz -d FileName.xz 或 unxz FileName.xz tar.xz 格式：压缩：tar -cJvf FileName.tar.xz DirName解压：tar -xJvf FileName.tar.xz zip 格式：压缩：zip FileName.zip DirName解压：unzip FileName.zip 更详细的 tar 命令使用方法可参见 man tar]]></content>
      <categories>
        <category>Tools</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Admin</tag>
        <tag>Tools</tag>
        <tag>Trick</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ssh-使用-RSA-密钥完成免密码登录]]></title>
    <url>%2F2018%2F03%2F11%2Fssh-login-by-RSA%2F</url>
    <content type="text"><![CDATA[大多数 Linux 服务器并不包含完善的用户界面，很多时候也并非安装在本地机器上。而是使用 SSH 远程登录的方式来完成对服务器的控制。SSH（Secure Shell）提供两种级别的验证方式。第一种是基于口令的验证，即使用自己的用户名和密码登录远程机器。第二种是基于密钥的验证，即先在本地机器上创建一对密钥（公钥和私钥），将公钥上传到远程服务器，登录时通过对比客户端与远程端的密钥来进行验证。基于密钥的验证是不需要在网络上传输口令的，相对而言更加安全。 Linux 上的 ssh 服务器可以使用 openssh ，使用软件包管理器直接安装即可：$ sudo apt-get install openssh-server 使用密钥进行远程登录还可以实现自动登录（不需要输入密码），以下为详细配置过程。 一、在本地机器上创建密钥$ ssh-keygen -t rsa -C &#39;email@domain.com&#39;-t 选项用来指定密钥类型，默认即为 rsa)；-C 选项用来提供说明文字命令执行成功后会在 ~/.ssh 目录下分别创建公钥（默认为 id_rsa.pub）和私钥（默认为 id_rsa）文件 二、将公钥上传至服务器这一步需要将本地生成的公钥文件（id_rsa.pub）内容添加到服务器上的 ~/.ssh/authorized_keys 文件中。 可使用 scp 命令完成远程复制：1234$ scp ~/.ssh/id_rsa.pub username@hostname:~/ # 将公钥文件上传至服务器用户主目录$ ssh username@hostname # 使用用户名和密码登录至服务器$ mkdir .ssh # 在远程服务器上创建 .ssh 目录$ cat id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys # 将公钥文件内容追加到authorized_keys文件末尾 上面我使用的是 virtualbox 虚拟机安装的 ubuntu 16.04 Server，利用 NAT 的端口转发将宿主机的 8022 端口映射到了虚拟机的 22 端口。-P 8022 用于指定端口号 以上步骤完成后就可以通过 ssh username@hostname 命令直接登录远程主机。 三、配置登录别名可以通过在本地机器上的 ~/.ssh/config 文件中定义别名，然后使用 ssh &lt;alias&gt; 命令即可直接登录远程服务器。该文件格式如下：12345Host alias # 自定义别名HostName hostname # 服务器地址Port port # 远程 ssh 服务的端口号，默认为22User user # 用户名IdentityFile ~/.ssh/id_rsa # 本地公钥文件位置 登录效果： 针对第二步中上传密钥的操作，还可以使用此命令：$ cat ~/.ssh/id_rsa.pub | ssh username@hostname &quot;mkdir ~/.ssh; cat &gt;&gt; ~/.ssh/authorized_keys&quot; 附：配置 github 账号时也需要用到生成密钥的操作，可以参考上面的 ssh-keygen 命令]]></content>
      <categories>
        <category>Admin</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Admin</tag>
        <tag>Tools</tag>
        <tag>Server</tag>
        <tag>Networking</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL Server 5.7 修改 root 密码]]></title>
    <url>%2F2018%2F03%2F09%2FMysql-change-root-password%2F</url>
    <content type="text"><![CDATA[通常安装 MySQL Server 时，如使用包管理器（比如 ubuntu 的 apt-get），可在安装过程中设置 MySQL 的 root 密码。装好后使用 mysql -u root -p 命令登录并管理数据库。而使用其他方式安装的 MySQL 数据库（如 wamp ）默认设置了空的 root 密码。可在需要的时候进行修改。 一、SET PASSWORD使用空密码登录 root 用户并直接重新设置密码1234$ mysql -uroot -p...mysql&gt; SET PASSWORD FOR root@localhost=PASSWORD(&apos;your password&apos;);Query OK, 0 rows affected, 1 warning (0.02 sec) 二、UPDATE mysql使用空密码登录 root 用户并更新 mysql 数据库中的 user 表12345678$ mysql -uroot -p...mysql&gt; UPDATE mysql.user SET authentication_string=PASSWORD(&apos;your password&apos;) -&gt; WHERE user=&apos;root&apos; AND host=&apos;localhost&apos;;Query OK, 0 rows affected, 1 warning (0.00 sec)Rows matched: 1 Changed: 0 Warnings: 1mysql&gt; FLUSH PRIVILEGES;Query OK, 0 rows affected (0.01 sec) 三、创建数据库并授予权限最基本的方法为：使用 root 账号登录并创建数据库，再分配其权限给某个普通用户，再将该数据库与某个网络应用绑定。CREATE DATABASE db_nameGRANT privileges ON db_name.tables TO user@domains IDENTIFIED BY &#39;user_pass&#39;ALL 指代该数据库的所有权限，‘%’ 表示允许该用户从任意主机登录（可修改为指定 IP 地址，默认是 localhost）。starky.* 表示该数据库中的所有表格 附：ubuntu 16.04 取消 MySQL Server 的默认自启动$ sudo systemctl disable mysql取消 mysql 开机自启动$ sudo systemctl start mysql 启动 mysql 服务]]></content>
      <categories>
        <category>Database</category>
      </categories>
      <tags>
        <tag>Admin</tag>
        <tag>Database</tag>
        <tag>Mysql</tag>
        <tag>Security</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[正则表达式学习笔记]]></title>
    <url>%2F2018%2F02%2F25%2Flearn-regex%2F</url>
    <content type="text"><![CDATA[正则表达式（regular expression）用来定义一种 模式，相关的 Linux 工具（如 sed、gawk ）以及 Perl 等编程语言可以通过该模式对文本内容进行匹配，再进行替换等操作。正则表达式并非只有一种实现，在 Linux 环境中，不同的应用程序往往使用不同类型的正则表达式。最常见的两种有基本正则表达式（BRE）引擎和扩展正则表达式（ERE）引擎。大部分 Linux 工具至少遵循 BRE 对规则的定义，编程语言多使用 ERE 引擎。其中 gawk 就使用了 ERE 引擎。这里只介绍一些最常见的正则表达式。 普通文本普通字符包括没有显式指定为元字符的所有可打印和不可打印字符。这包括所有大写和小写字母、数字、标点符号和一些其他符号。正则表达式在匹配文本时，不管其定义的模式（区分大小写）出现在文本的什么位置或者出现几次，只要文本中包含此模式，该文本就会传递给相应的 Linux 工具做进一步的处理。在 sed 和 gawk 等工具中，模式的定义包含在两个 ‘ / ‘ 中，下面的 sed -n &#39;/模式/p&#39;命令类似于 Linux 中的 grep ，对符合正则表达式模式的文本进行打印输出。123456789101112$ echo &quot;This is a test&quot; | sed -n &apos;/test/p&apos; This is a test$ echo &quot;This is a test&quot; | sed -n &apos;/trial/p&apos;$$ sed -n &apos;/ /p&apos; data1.txtThis is a normal line of text.This is a line with two spaces.This is a line with a TAB.$ sed -n &apos;/ /p&apos; data1.txtThis is a line with two spaces.$ gawk &apos;/\t/&#123;print $0&#125;&apos; data1.txtThis is a line with a TAB. 上面的例子中涉及到多个空格的匹配，同时非打印字符如制表符（\t）、换行符（\n）等定义在 ERE 中，所以 sed 命令不再支持，例子中使用的是 gawk 。 特殊字符特殊字符包含以下几种：.*[]^${}\+?|()，它们在正则表达式中有着特殊的含义，如 ‘ . ‘ 匹配任何单字符。而对于特殊符号自身的匹配则需要进行转义（在该字符前加上 ‘ \ ‘ ），即匹配 ‘ . ‘ 需使用 ‘ \. ‘ 。 特殊字符的描述如下表所示： 特殊字符 描述 $ 匹配输入字符串的结尾位置 ( ) 标记一个子表达式的开始和结束位置。子表达式可以获取供以后使用 * 匹配前面的子表达式零次或多次 + 匹配前面的子表达式一次或多次 . 匹配除换行符 \n 之外的任何单字符 [ 标记一个中括号表达式的开始 ? 匹配前面的子表达式零次或一次，或指明一个非贪婪限定符 ^ 匹配输入字符串的开始位置。在方括号中使用时，表示不接受该字符集合 { 标记限定符表达式的开始 管道符 指明两项之间的一个选择 123456789$ cat data2.txtThe cost is $4.003 / 2$ sed -n &apos;/\$/p&apos; data2.txtThe cost is $4.00$ sed -n &apos;/\//p&apos; data2.txt3 / 2$ sed -n &apos;///p&apos; data2.txtsed: 1: &quot;///p&quot;: invalid command code / 1. 定位符定位符用来描述字符串或单词的边界，^ 和 $ 分别指定字符串的开始与结束位置，\b 描述单词的前或后边界，\B 表示非单词边界。当 ^ 并非位于模式开头时，仍当普通字符看待。123456789101112131415161718$ echo &quot;The book store&quot; | sed -n &apos;/^book/p&apos; $$ echo &quot;books are great&quot; | sed -n &apos;/^book/p&apos; Books are great$ echo &quot;This is a good book&quot; | sed -n &apos;/book$/p&apos;This is a good book$ echo &quot;This book is good&quot; | sed -n &apos;/book$/p&apos; $$ echo &quot;This ^ is a test&quot; | sed -n &apos;/s ^/p&apos; This ^ is a test&gt;&gt;&gt; import re&gt;&gt;&gt; re.findall(r&apos;\bbook&apos;,&apos;The book store&apos;)[&apos;book&apos;]&gt;&gt;&gt; re.findall(r&apos;\bbook&apos;,&apos;Thebook store&apos;)[]&gt;&gt;&gt; re.findall(r&apos;book\b&apos;,&apos;Thebook store&apos;)[&apos;book&apos;] \b 和 \B 属于 ERE，上例中使用了 Python 的 re 模块（注意空格如何影响输出） 组合定位符1234567$ cat data4this is a test of using both anchors I said this is a testthis is a testI&apos;m sure this is a test.$ sed -n &apos;/^this is a test$/p&apos; data4 this is a test 过滤空行1234567$ cat data5This is one test line.This is another test line. $ sed &apos;/^$/d&apos; data5This is one test line. This is another test line. 2. ‘ . ‘ 符号‘ . ‘ 匹配除 \n 以外的任何单字符，包括空格。所以 /.at/ 匹配 ‘cat’ 和行中间的 ‘at’ （实际为 ‘ at’ ），却不匹配行首的 ‘at’ 。123456&gt;&gt;&gt; print(text)The cat is sleeping.This test is at line two.at ten o&apos;clock we&apos;ll go home.&gt;&gt;&gt; re.findall(&apos;.at&apos;,text)[&apos;cat&apos;, &apos; at&apos;] 3. 字符集合包裹在一对中括号内的多个字符和数字构成一个字符集合，用来限制该位置只匹配集合中出现的字符。如 /[AaEeIiOoUu]/ 可以匹配所有元音字母1234$ echo &quot;Yes&quot; | sed -n &apos;/[Yy]es/p&apos; Yes$ echo &quot;yes&quot; | sed -n &apos;/[Yy]es/p&apos; yes 字符范围用连字号可以表示一个字符的范围 模式 含义 [a-z] 匹配所有的小写字母 [A-Z] 匹配所有的大写字母 [a-zA-Z] 匹配所有的字母 [0-9] 匹配所有的数字 [0-9\.\-] 匹配所有的数字、小数点和连字符 当在一组方括号里使用 ^ 时，它表示”非”或”排除”的意思，常常用来剔除某个字符。如 [^a-z] 匹配除了小写字母以外的所有字符 此外，还有一些特殊的字符集合。 模式 含义 [[:alpha:]] 匹配所有字母 [[:digit:]] 匹配所有数字 [[:alnum:]] 匹配所有字母和数字 [[:blank:]] 匹配所有空格和 Tab 字符 [[:space:]] 匹配所有空白字符：Space, Tab, NL, FF, VT, CR [[:upper:]] 匹配所有大写字母 [[:lower:]] 匹配所有小写字母 [[:print:]] 匹配所有可打印字符 [[:punct:]] 匹配所有标点符号 [[:xdigit:]] 匹配所有16进制的数字，相当于[0-9a-fA-F] 4. ‘ * ‘ 符号‘ * ‘ 表示匹配前面的字符或子表达式零次或多次12345678$ echo &quot;ik&quot; | sed -n &apos;/ie*k/p&apos; ik$ echo &quot;iek&quot; | sed -n &apos;/ie*k/p&apos; iek$ echo &quot;ieek&quot; | sed -n &apos;/ie*k/p&apos; ieek$ echo &quot;ieeek&quot; | sed -n &apos;/ie*k/p&apos; ieeek ‘ * ‘ 号还可以应用在字符集合上12345678910$ echo &quot;bt&quot; | sed -n &apos;/b[ae]*t/p&apos; bt$ echo &quot;bat&quot; | sed -n &apos;/b[ae]*t/p&apos; bat$ echo &quot;bet&quot; | sed -n &apos;/b[ae]*t/p&apos; bet$ echo &quot;baaeeaeeat&quot; | sed -n &apos;/b[ae]*t/p&apos; baaeeaeeat$ echo &quot;baakeeet&quot; | sed -n &apos;/b[ae]*t/p&apos; $ 5. ‘ ? ‘ 和 ‘ + ‘ 符号‘ ? ‘ 表示匹配前面的字符或子表达式零次或一次123456$ echo &quot;bt&quot; | gawk &apos;/be?t/&#123;print $0&#125;&apos; bt$ echo &quot;bet&quot; | gawk &apos;/be?t/&#123;print $0&#125;&apos; bet$ echo &quot;beet&quot; | gawk &apos;/be?t/&#123;print $0&#125;&apos; $ ‘ + ‘ 表示匹配前面的字符或子表达式一次或多次123456$ echo &quot;beet&quot; | gawk &apos;/be+t/&#123;print $0&#125;&apos; beet$ echo &quot;bet&quot; | gawk &apos;/be+t/&#123;print $0&#125;&apos; bet$ echo &quot;bt&quot; | gawk &apos;/be+t/&#123;print $0&#125;&apos; $ 6. 限定符限定符用来指定正则表达式的一个给定组件必须要出现多少次才能满足匹配。有 或 + 或 ? 或 {n} 或 {n,} 或 {n,m} 共6种。前三种前面已经讲到，除 外其他5种都属于 ERE 。 模式 含义 {n} n 是一个非负整数。匹配确定的 n 次。例如，/o{2}/ 不能匹配 “Bob” 中的 ‘o’，但是能匹配 “food” 中的两个 ‘o’。 {n,} n 是一个非负整数。至少匹配n 次。例如，/o{2,}/ 不能匹配 “Bob” 中的 ‘o’，但能匹配 “foooood” 中的所有 ‘o’。/o{1,}/ 等价于 /o+/ 。/o{0,}/ 则等价于 /o*/ 。 {n,m} m 和 n 均为非负整数，其中 n &lt;= m 。最少匹配 n 次且最多匹配 m 次。例如，/o{1,3}/ 将匹配 “fooooood” 中的前三个 ‘o’ 。/o{0,1}/ 等价于 /o?/ 。 12345678$ echo &quot;bt&quot; | gawk --re-interval &apos;/be&#123;1,2&#125;t/&#123;print $0&#125;&apos; $$ echo &quot;bet&quot; | gawk --re-interval &apos;/be&#123;1,2&#125;t/&#123;print $0&#125;&apos; bet$ echo &quot;beet&quot; | gawk --re-interval &apos;/be&#123;1,2&#125;t/&#123;print $0&#125;&apos; beet$ echo &quot;beeet&quot; | gawk --re-interval &apos;/be&#123;1,2&#125;t/&#123;print $0&#125;&apos; $ /[ae]{1,2}/ 匹配 ‘a’ , ‘e’ , ‘aa’ , ‘ae’ , ‘ee’ 。 * 和 + 限定符都是贪婪的，因为它们会尽可能多的匹配文字，只有在它们的后面加上一个 ? 就可以实现非贪婪或最小匹配。如匹配 HTML 文档里的标签：12345&gt;&gt;&gt; text=&quot;&lt;h1&gt;Head Line&lt;/h1&gt;&quot;&gt;&gt;&gt; re.findall(r&apos;&lt;.*&gt;&apos;,text)[&apos;&lt;h1&gt;Head Line&lt;/h1&gt;&apos;]&gt;&gt;&gt; re.findall(r&apos;&lt;.*?&gt;&apos;,text)[&apos;&lt;h1&gt;&apos;, &apos;&lt;/h1&gt;&apos;] 7. ‘ | ‘ 符号管道（’ | ‘）符号用来定义两个或多个模式，且彼此之间是逻辑或的关系。123456$ echo &quot;The cat is asleep&quot; | gawk &apos;/cat|dog/&#123;print $0&#125;&apos; The cat is asleep$ echo &quot;The dog is asleep&quot; | gawk &apos;/cat|dog/&#123;print $0&#125;&apos; The dog is asleep$ echo &quot;He has a hat.&quot; | gawk &apos;/[ch]at|dog/&#123;print $0&#125;&apos; He has a hat. 8. 分组小括号用于将正则表达式的模式进行分组1234$ echo &quot;Sat&quot; | gawk &apos;/Sat(urday)?/&#123;print $0&#125;&apos;Sat$ echo &quot;Saturday&quot; | gawk &apos;/Sat(urday)?/&#123;print $0&#125;&apos; Saturday 上例中的 /Sat(urday)?/ 用于匹配 ‘Sat’ 或者 ‘Saturday’ ，’urday’ 加上小括号后被当成一个整体，再附上 ? 符号表示该组合出现零次或一次。 附录：运算符优先级 运算符（由高到低） 描述 反斜杠（\） 转义符 (), (?:), (?=), [] 圆括号和方括号 *, +, ?, {n}, {n,}, {n,m} 限定符 ^, $, 任何元字符、任何字符 定位点和序列（即：位置和顺序） 替换运算符（管道符） 替换，”或”操作 字符具有高于替换运算符（|）的优先级，使得 /m|food/ 匹配 ‘m’ 或 ‘food’ 。若要匹配 ‘mood’ 或 ‘food’ ，可使用括号创建子表达式：/(m|f)ood/ 。 参考资料：Linux Command Line and Shell Scripting Bible 3rd Edition正则表达式-菜鸟教程]]></content>
      <categories>
        <category>Programming</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Admin</tag>
        <tag>Tools</tag>
        <tag>Trick</tag>
        <tag>Programming</tag>
        <tag>regex</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[版本管理工具（nvm、virtualenv(wrapper)-和-rbenv-的安装与使用）]]></title>
    <url>%2F2018%2F02%2F02%2Fnvm-virtualenv-rbenv%2F</url>
    <content type="text"><![CDATA[我是个懒惰的人，对于编程着实懂得不多，能clone下来的绝对不自己写。。。但是像 nodejs 和 Python 这俩哥们，本来版本就不少，再加上众多的软件包依赖，常常clone下来了运行还报错。可想而知，配置好相互独立的运行环境（避免软件包版本冲突）是多么必要的一件事。 一、 nvmnvm 是一个跨平台的 Node 版本管理工具。Windows系统下可安装 nvm-windows，有编译好的exe安装包文件提供下载，安装好后可以直接在命令行使用。 Linux 和 Mac 可以使用网站上提供的安装脚本：使用cURL：curl -o- https://raw.githubusercontent.com/creationix/nvm/v0.33.8/install.sh | bash使用wget：wget -qO- https://raw.githubusercontent.com/creationix/nvm/v0.33.8/install.sh | bash 如安装出现问题，请确保以下内容12export NVM_DIR=&quot;$HOME/.nvm&quot;[ -s &quot;$NVM_DIR/nvm.sh&quot; ] &amp;&amp; \. &quot;$NVM_DIR/nvm.sh&quot; 已添加到你的命令行配置文件（~/.bash_profile, ~/.zshrc, ~/.profile, 或 ~/.bashrc）中。具体可参考 nvm 的README。 使用技巧 nvm --help 获取该命令的帮助信息 nvm install [-s] &lt;version&gt; 下载安装指定版本的Node，[-s] 表示从源码安装如：nvm install 8.9 安装最新8.9.x稳定版本的Nodejs nvm ls-remote 查看可供下载安装的所有版本 nvm use &lt;version&gt; 使用指定版本的 Node（即切换当前运行环境为该版本的 Node） nvm run &lt;version&gt; [&lt;args&gt;] 运行指定版本的 Node 程序如：nvm run 8.9.4 app.js 用 8.9.4 版本的 nodejs 执行 app.js nvm which &lt;version&gt; 获取该版本Node可执行文件的路径 nvm ls 列出已安装的版本 nvm alias default &lt;version&gt; 设置进入任何新终端时的默认 Node 版本 nvm current 查看当前配置下的 Node 版本 PS： export NVM_NODEJS_ORG_MIRROR=https://npm.taobao.org/mirrors/node 用于将安装源改为国内镜像 二、virtualenv 和 virtualenvwrapper1.virtualenvvirtualenv 是用来创建独立的 Python 运行环境的工具。可以直接通过 Python 的 pip 命令安装，也可以使用系统的软件包管理器（如 Ubuntu 的 apt-get）。 安装：[sudo] pip install virtualenv 加上 sudo 表示全局安装基本用法：virtualenv -p PYTHON_EXE DEST_DIR -p PYTHON_EXE 指定创建虚拟环境时使用的 Python 可执行程序的路径，默认是用来安装 virtualenv 的 Python。 DEST_DIR 表示将虚拟环境安装至目标文件夹 如：virtualenv -p python3 web表示使用系统 PATH 变量指定的 Python3 程序，将虚拟环境安装至当前目录下的 web 目录。source web/bin/activate命令用于激活此虚拟环境（Windows系统此命令为：web\Scripts\activate）。待操作完成需要退出时，输入deactivate即可。 2.virtualenvwrappervirtualenvwrapper 是针对 virtualenv 的一组扩展工具。Windows 系统环境下可安装 virtualenvwrapper-win 。安装：Linux or Mac：pip install virtualenvwrapperWindows：pip install virtualenvwrapper-win 完成后将以下内容添加到命令行配置文件中123export WORKON_HOME=$HOME/.virtualenvsexport PROJECT_HOME=$HOME/Develsource /usr/local/bin/virtualenvwrapper.sh WORKON_HOME 定义了在什么位置创建虚拟环境PROJECT_HOME 定义了在什么位置创建 Python 代码项目 PS：如果安装 virtualenvwrapper 时使用的 python 并非 PATH 环境变量中定义的默认值，可以额外添加一行export VIRTUALENVWRAPPER_PYTHON = &lt;PYTHON_EXE&gt; 使用技巧 mkvirtualenv ：在 WORKON_HOME 定义的路径下创建新的运行环境语法：mkvirtualenv [-a project_path] [-i package] [-r requirements_file] [virtualenv options] ENVNAME例：mkvirtualenv -i flask -p /usr/local/bin/python3 env_flask rmvirtualenv ：删除 WORKON_HOME 里已安装的某个运行环境语法：rmvirtualenv ENVNAME workon :列出或切换虚拟运行环境 mkproject ：在 WORK_ON 目录里创建虚拟环境的同时在 PROJECT_HOME 目录下创建项目文件夹其他命令及用法可参考 virtualenvwrapper 文档 三、rbenvrbenv 是 ruby 语言的版本管理工具。MacOS 系统可直接使用 brew 命令安装：brew install rbenv Linux系统可使用 git 命令手动安装。在命令行界面依此输入以下命令：1234$ git clone https://github.com/rbenv/rbenv.git ~/.rbenv$ echo &apos;export PATH=&quot;$HOME/.rbenv/bin:$PATH&quot;&apos; &gt;&gt; ~/.bashrc$ echo &apos;eval &quot;$(rbenv init -)&quot;&apos; &gt;&gt; ~/.bashrc$ source ~/.bashrc 以上命令将 rbenv 的项目文件 clone 到本地同时更新 PATH 环境变量。之后输入 如显示版本信息和帮助内容，则安装成功。123此时的 rbenv 命令是没有 install 选项的，如需要使用 rbenv install 命令下载并编译安装其他版本的 ruby，需要再安装用来编译 Ruby 源代码的工具 [ruby-build](https://github.com/rbenv/ruby-build) 。因为是编译安装 ruby，需要先安装好编译用到的依赖库，然后将 ruby-build 项目文件 clone 到本地即可： $ sudo apt-get install autoconf bison build-essential libssl-dev libyaml-dev libreadline6 libreadline6-dev zlib1g zlib1g-dev$ git clone https://github.com/rbenv/ruby-build.git ~/.rbenv/plugins/ruby-build` 命令简介 rbenv install -l 列出所有可供安装的 ruby 版本 rbenv install &lt;version&gt; 编译安装指定版本的 ruby rbenv versions 查看已安装的所有 ruby 版本 rbenv global &lt;version&gt; 设置全局版本 rbenv local &lt;version&gt; 设置本地版本（针对特定的项目，会在当前目录下创建 .rbenv-version 文件覆盖全局版本配置） rbenv shell &lt;version&gt; 设置当前终端版本。优先级最高。 参考文章：How To Install Ruby on Rails with rbenv on Ubuntu 16.04]]></content>
      <categories>
        <category>Programming</category>
      </categories>
      <tags>
        <tag>Admin</tag>
        <tag>Tools</tag>
        <tag>Python</tag>
        <tag>Programming</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[cURL-基本使用方法介绍]]></title>
    <url>%2F2018%2F01%2F11%2FcURL-Manual%2F</url>
    <content type="text"><![CDATA[cURL是一个利用URL语法在命令行下工作的文件传输工具，支持文件上传和下载，是综合传输工具。其源代码可在 Github 上阅读和获取（包括libcurl），这里只简单地介绍下常见的使用方法。 一、获取文件语法：curl -option URL不加任何选项时，默认将结果打印到标准输出（STDOUT）-o filename 选项：将获取到的内容以指定文件名（filename）保存至本地-O 选项：使用URL中的文件名将其保存至本地–progress 选项表示仅用『#』和百分比表示下载进度，若下载中断，可以添加-C - 选项断点续传–limit-rate 选项对下载文件时的速度进行限制 二、获取响应头信息-i 选项：输出时响应头和文档内容都显示-I 选项：只显示响应头信息不显示文档内容 可以看出，默认情况下 cURL 不会发送 HTTP Location headers（重定向），即遇到需要跳转的网页不自动跳转。curl www.jianshu.com会得到 『301 Moved Permanently』，而不会跳转至 https://www.jianshu.com。 可以通过添加 -L 选项进行重定向。（图中的 -s 选项表示静默模式） 三、自定义User-AgentUser-Agent 是浏览器的身份标识，远程服务器通过它可以获取客户端使用的操作系统、浏览器版本等信息。（写过爬虫的都知道……）-A 选项可以自定义 User-Agent 信息，默认是 curl/版本号访问上图中的网站时会返回浏览者的 User-Agent 信息–header 选项可以自定义其他请求头信息如curl --header &quot;Content-Type:application/json&quot; URL 关于HTTP消息头（包括请求头和响应头）的简介，可参考这篇文章https://itbilu.com/other/relate/EJ3fKUwUx.html 四、cookie信息Cookie 是访问的远程站点存储在客户端计算机上的一段信息，通常储存着用户对某个站点的设置，比如偏好的语言或地理位置，也包括个人身份识别信息。–cookie 选项可以附加上 cookie 信息-c cookie-file 可以保存服务器返回的 cookie 到文件-b cookie-file 可以使用该文件作为 cookie 信息 五、HTTP动词默认无选项的 curl 命令即使用了 GET（获取） 动词，另外还有 POST（新建），PUT（更新），DELETE（删除）等方法。通过这些动词可以很方便的访问 Restful 架构的 API。命令格式为：curl -d data -X method URL-d 选项指定要传输的数据，-X 选项指定使用的方法。只附加 -d 选项则默认使用 POST 方法，可以通过此命令完成简单的表单验证操作 附录：参考文章：Using curl to automate HTTP jobs]]></content>
      <categories>
        <category>Tools</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Admin</tag>
        <tag>Tools</tag>
        <tag>Trick</tag>
        <tag>Networking</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Nmap-网络扫描实战（2）--端口扫描]]></title>
    <url>%2F2017%2F11%2F29%2Fscan-network-using-Nmap-2%2F</url>
    <content type="text"><![CDATA[开放的端口意味着远程主机上正在运行着联网的服务，而某些难以察觉的编程错误或实现缺陷会使这些服务容易受到攻击，有时甚至成为侵入整个系统的突破口。这些网络服务往往使用 TCP 或 UDP 作为传输协议。TCP（Transmission Control Protocol）是一种连接导向的协议，相对更常用。而 UDP（User Datagram Protocol）是一种非连接导向的协议，多用于对传输速度的要求高于数据完整性的服务。而通过检测端口及服务来入侵远程系统的渗透测试方法即为端口扫描。 一、TCP 端口扫描技术1. Connect 扫描Connect 扫描对每一个端口都尝试建立完整的 TCP 连接（三步握手），如果连接创建成功，则该端口被判定为开放的。 2. Stealth 扫描Stealth 扫描也常被称作 SYN 扫描或半开放扫描。该技术向每一个被扫描的端口发送一个单独的 SYN 包，如果收到 SYN+ACK 回复，则该端口被判定为开放的。这之后不会再遵照三步握手的程序向目标主机发送 ACK 包，所以并没有开启完整的TCP连接，而目标主机的日志系统一般也不会记录这类扫描的痕迹。 3. Zombie 扫描『僵尸』扫描背后的实现原理比较负责。整个过程如下图所示：简单来说， 先找一个远程系统作为『僵尸』主机，该主机与网络中的其他主机之间不存在活跃的网络连接。 向『僵尸』主机发送一个 SYN+ACK 包并记录下该主机初始的 IPID 值。 伪装成『僵尸』主机（将报文中的源IP地址替换为『僵尸』主机的）向目标主机发送一个 SYN 数据包。 如果目标端口是开放的，则目标主机会向『僵尸』主机返回一个 SYN+ACK 包，而『僵尸』主机（觉得很懵逼。。。）则返回一个 RST 包并把自己的 IPID 值增加 1。如果目标端口是关闭的，则目标主机会向『僵尸』主机返回 RST 响应，而收到 RST 的『僵尸』主机（依然不清楚发生了什么）则不做任何动作，IPID 值也不会增加。 向『僵尸』主机发送另一个 SYN+ACK 包，从返回的 RST 包中获取最终的 IPID 值。如果该值比第 2 步时增加了 1，则目标端口是关闭的。如果最终增加了 2，则目标端口是开放的。 一点儿也不简单哈。。。 命令示例在特权用户下执行时，nmap 默认采用 SYN 扫描方式（即不开启完整的TCP连接）以节省扫描时间。同时这种扫描行为也不易被目标主机的日志系统记录到。而普通用户不具有修改原始数据包的权限，所以只能通过 connect系统调用 打开完整的 TCP 连接以完成对远程系统的扫描。所以从效率和安全的角度出发，应优先选择 SYN 扫描而非 connect 扫描。具体可参考 nmap 官方文档——端口扫描技术12345$ sudo nmap 10.2.64.1 -p 80# 特权用户 SYN 扫描$ nmap 10.2.64.1 -p 80# 普通用户 connect 扫描 也可以显式地指定采用哪种扫描方式，-sT 表示 connect 扫描，-sS 表示 SYN 扫描。-p 选项用来指定扫描的端口号或端口范围（一共有65535个端口可供扫描，默认扫描1000个常用的端口）。如：12$ sudo nmap 10.2.64.1 -p 21,22,80,443$ sudo nmap 10.2.64.1 -p 20-25 从上述命令的输出结果中可以看出，该主机的 22,80 端口是开放的（很可能运行着ssh服务和http服务），21 和 443 端口是关闭的。 二、UDP 端口扫描技术UDP 扫描相对显得更有难度，同时也更乏味和耗时。其中一种方式依赖于ICMP端口不可达响应，即假设每一个被扫描的 UDP 端口在不开放时都会回复 ICMP 端口不可达的响应，而收不到该响应时则判定端口为开放的。但有时候被扫描主机不允许生成端口不可达响应，或该响应被限制在一定频率内，也可能是被主机的防火墙隔离。此种方法就会产生不准确的结果。另一种方式是通过发送服务相关的请求来探测远程主机上对应的服务。此种方法可信度更高，但同时消耗的时间也更多。open|filtered 表示该端口可能是开放的，也可能被防火墙屏蔽掉了。]]></content>
      <categories>
        <category>Pentest</category>
      </categories>
      <tags>
        <tag>Tools</tag>
        <tag>Security</tag>
        <tag>Networking</tag>
        <tag>Pentest</tag>
        <tag>Hacking</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[grep 命令使用技巧]]></title>
    <url>%2F2017%2F11%2F23%2Fgrep-Tricks%2F</url>
    <content type="text"><![CDATA[grep（globally search a regular expression and print）是一个强大的文本搜索工具。常常出现在管道符（|）身后，对大段的文本输出进行过滤，打印出与特定模式相匹配的内容。 1. 基本用法$ grep pattern filename或 $ cat filename | grep pattern或 $ grep pattern file1 file2 …（搜索多个文件） 示例文件（numbers.txt）：123456789101 1 1 1 1 12 2 2 2 2 23 3 3 3 3 34 4 4 4 4 45 5 5 5 5 5six six sixseven seven seveneight eight eightnine nine nine10 10 10 10 命令输出： 2. -v（打印 不包含 匹配项的行）其中 -E 选项表示开启扩展正则表达式（grep -E 等同于 egrep）添加上 -v 选项后输出的是不匹配的内容 3. -o（只输出匹配项而不是默认的整行内容） 4. -c（统计包含匹配项的行数）如$ grep 1 numbers.txt输出为 2，（即第一行 1 1 1 1 1 1 1 和最后一行 10 10 10 10，计算的是行数） 5. -n（打印输出时额外显示行号） 6. -i （搜索时忽略匹配模式中的大小写）grep 默认是大小写敏感的，加 -i 选项可以在匹配时不区分大小写。 7. -e（多个匹配模式）注意格式 8. 打印匹配文本之前或之后的内容-A n ：额外打印匹配文本之后n行内容-B n ：额外打印匹配文本之前n行内容-C n ：额外打印匹配文本前后n行内容 9. -l（搜索多个文件并查找匹配文本在哪些文件中）$ grep -l pattern file1 file2 …该命令的输出为包含 pattern 的文件名同时可以使用 -r 选项对目录进行递归搜索$ grep -r pattern dir]]></content>
      <categories>
        <category>Tools</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Admin</tag>
        <tag>Tools</tag>
        <tag>Trick</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sed 命令详解]]></title>
    <url>%2F2017%2F11%2F09%2Fsed-manual%2F</url>
    <content type="text"><![CDATA[sed 即 stream editor，一个简单而强大的文本解析转换工具，1973-1974年期间由贝尔实验室的 Lee E. McMahon 开发，能够完美的配合正则表达式使用。处理文本时，当前处理的行会先存储在临时缓冲区中，称为模式空间（pattern space），操作完成后再把缓冲区的内容送往屏幕。接着处理下一行，直到文件末尾。文件内容并没有改变，除非使用重定向存储输出。sed 主要用来自动编辑一个或多个文件，简化对文件的反复操作，编写转换程序等。 1. 使用 s 命令替换基本使用sed 默认情况下通过 STDIN 输入流读取数据，可以利用管道符（|）进行重定向。如下：12$ echo &quot;This is a test&quot; | sed &apos;s/test/big test/&apos;This is a big test s 替换命令的格式为：s/pattern/replacement/flags ，如下：1234567891011$ cat test.txtThe quick brown fox jumps over the lazy dog.The quick brown fox jumps over the lazy dog.The quick brown fox jumps over the lazy dog.The quick brown fox jumps over the lazy dog.$$ sed &apos;s/dog/cat/&apos; test.txtThe quick brown fox jumps over the lazy cat.The quick brown fox jumps over the lazy cat.The quick brown fox jumps over the lazy cat.The quick brown fox jumps over the lazy cat. 使用 s 命令进行替换时，默认只替换每行中的第一个匹配。全局替换需增加 g 选项。12345$ sed &apos;s/o/O/g&apos; test.txtThe quick brOwn fOx jumps Over the lazy dOg.The quick brOwn fOx jumps Over the lazy dOg.The quick brOwn fOx jumps Over the lazy dOg.The quick brOwn fOx jumps Over the lazy dOg. sed 默认只将处理后的内容输出，而不对原文件的内容做任何改动。如需写入文件，可使用重定向：$ sed &#39;s/dog/cat/&#39; test.txt &gt; test2.txt或通过 -i 选项直接修改文件内容：$ sed -i &#39;s/dog/cat/&#39; test.txt address如果需要将命令应用到特定的一行或多行内容，可以使用 line addressing。格式为 [address[,address]][!]command。address 可以是数字或模式，也可以通过逗号分隔两个 address 表示一个区间范围。如只将第2行中的 dog 替换为 cat：12345$ sed &apos;2s/dog/cat/&apos; test.txtThe quick brown fox jumps over the lazy dog.The quick brown fox jumps over the lazy cat.The quick brown fox jumps over the lazy dog.The quick brown fox jumps over the lazy dog. 或者$ sed &#39;1,3s/dog/cat/&#39; test.txt，替换1-3行的内容。$ sed &#39;2,$s/dog/cat/&#39; test.txt，替换第2行到最后一行的内容。 ! 表示完成匹配后是否在该行执行替换命令。加上 ! 表示不执行。如下：1234567891011121314151617$ cat lines.txtThis is line oneThis is line twoThis is line threeThis is line four$$ sed &apos;/one/,2s/line/LINE/&apos; lines.txtThis is LINE oneThis is LINE twoThis is line threeThis is line four$$ sed &apos;/one/,2!s/line/LINE/&apos; lines.txtThis is line oneThis is line twoThis is LINE threeThis is LINE four 替换选项sed 的替换命令除了可以附加 g 选项（全局替换）外，还有更多选项适用于不同的情形。 数字。表示只替换每行中的第 n 个匹配项，如：12345$ sed &apos;s/o/O/2&apos; test.txtThe quick brown fOx jumps over the lazy dog.The quick brown fOx jumps over the lazy dog.The quick brown fOx jumps over the lazy dog.The quick brown fOx jumps over the lazy dog. 或$ sed &#39;s/o/O/2g&#39; test.txt 替换每行中从第二个开始的所有匹配项（即替换每行中从第二个 o 开始直到该行行尾的所有 o 字符） p，表示打印处理前的原始内容，结合上 -n 选项（不打印输出）则可以只输出处理过的内容。 123456789$ sed &apos;s/three/3/p&apos; lines.txtThis is line oneThis is line twoThis is line 3This is line 3This is line four$$ sed -n &apos;s/three/3/p&apos; lines.txtThis is line 3 w，将处理过的内容写入文件 1234$ sed &apos;s/three/3/w line3.txt&apos; lines.txt$$ cat line3.txtThis is line 3 sed 命令选项 删除：d$ sed &#39;2,$d&#39; lines.txt 删除 lines.txt 中第2行到最后一行的内容。$ sed &#39;/two/d&#39; lines.txt 删除 lines.txt 中包含 two 的行。 追加（行下）：a 1234567$ sed &apos;/two/a\&gt; This is a line behind line 2&apos; lines.txtThis is line oneThis is line twoThis is a line behind line 2This is line threeThis is line four 插入（行上）：i 12$ sed &apos;2i\&gt; This is a line above line 2&apos; lines.txt 在第2行以上插入内容。 修改：c12$ sed &apos;/two/c\&gt; Line 2&apos; lines.txt 将包含 two 的行修改为 Line 2（整行内容替换为 Line 2）。 字符转换：y格式为：[address]y/inchars/outchars/输入文件中所有包含在 inchars 中的字符都将替换为 outchars 中对应的字符。1234567891011$ cat line_number.txtThis is line 1.This is line 2.This is line 3.This is another line 1.$$ sed &apos;y/123/456/&apos; line_number.txtThis is line 4.This is line 5.This is line 6.This is another line 4. 其他用法 打印输出sed 的 p 命令可以达到类似 grep 的效果，结合上 address 功能还可以完成更复杂的筛选打印操作。 12345678910111213$ sed -n &apos;p&apos; lines.txtThis is line oneThis is line twoThis is line threeThis is line four$$ sed -n &apos;2,3p&apos; lines.txtThis is line twoThis is line three$$sed -n &apos;/two/,/three/p&apos; lines.txtThis is line twoThis is line three 多个匹配可以通过 -e 选项实现多个匹配的替换 12345$ sed -e &apos;s/fox/kangaroo/;s/dog/cat/&apos; test.txtThe quick brown kangaroo jumps over the lazy cat.The quick brown kangaroo jumps over the lazy cat.The quick brown kangaroo jumps over the lazy cat.The quick brown kangaroo jumps over the lazy cat. 也可以这样：12345678$ sed -e &apos;&gt; s/brown/red/&gt; s/fox/kangaroo/&gt; s/dog/cat/&apos; test.txtThe quick red kangaroo jumps over the lazy cat.The quick red kangaroo jumps over the lazy cat.The quick red kangaroo jumps over the lazy cat.The quick red kangaroo jumps over the lazy cat. 从脚本文件中读取命令 12345678910$ cat script.seds/brown/red/s/fox/kangaroo/s/dog/cat$$ sed -f script.sed test.txtThe quick red kangaroo jumps over the lazy cat.The quick red kangaroo jumps over the lazy cat.The quick red kangaroo jumps over the lazy cat.The quick red kangaroo jumps over the lazy cat. 可以使用 &amp; 变量表示前面的匹配项，如： 12345$ sed &apos;s/fox/&amp;es/&apos; test.txtThe quick brown foxes jumps over the lazy dog.The quick brown foxes jumps over the lazy dog.The quick brown foxes jumps over the lazy dog.The quick brown foxes jumps over the lazy dog. 参考书籍和文章：Linux Command Line and Shell Scripting Bible, 3rd EditionSED 简明教程 by 陈皓Linux 命令大全 / SED 命令]]></content>
      <categories>
        <category>Tools</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Admin</tag>
        <tag>Tools</tag>
        <tag>Trick</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux-系统管理（1）----获取硬件信息]]></title>
    <url>%2F2017%2F11%2F02%2FLinux-get-System-Infomation%2F</url>
    <content type="text"><![CDATA[当系统启动时，Linux 内核会自动检测硬件设备并加载相应的驱动程序。但这些硬件检测信息在启动时很快就滚动消失，所以需要在进入系统后再次显示这类信息，以发现潜在的问题。 demsgdmesg 命令可以列出系统启动时硬件检测和驱动加载的情况，以及后续由内核生成的其他信息。dmesg 命令的输出示例：从输出信息中可以找到 Linux 内核版本、内核命令行选项以及连接到此计算机的硬件设备，如串口、鼠标接口、CD驱动器、网卡和并行端口等。 tips:dmesg 命令的输出确实有点长，所以常通过管道符（|）将其导向 more 或 less 命令单页显示。或者输送给 head、tail、grep等文字处理工具。如：# dmesg | head -20 只输出前 20 行日志# dmesg | tail -20 只输出最后 20 行日志# dmesg | grep -i usb 只输出包含 “usb” 字符串的日志行（-i 选项表示忽略大小写）常用作筛选的字符串包括 usb、dma、tty、memory、eth、sda等，也可以多个关键字相组合。# dmesg | grep -E -i &quot;eth|sda&quot; 筛选包含字符串 “eth” 或 “sda” 的日志行 lspcilspci 命令可以列出系统中的 PCI 总线及其连接的设备。该命令的输出如下：输出信息中包括声音（Multimedia audio controller），USB设备（USB controller），视频输出（VGA compatible controller），有线网卡（Ethernet controller）和磁盘（SATA controller）等。如需获取更详尽的信息，还可以在该命令后增加一至多个 -v 选项（如 -vvv）。 lsusb如果只对USB设备感兴趣，可以通过 lsusb 命令来获知USB设备的情况（同样可以通过增加 -v 选项来获取更详细的输出）。以上面截图中第三行为例： Bus 002 指明设备连接到哪条总线 Device 003 表明这是连接到某条总线的第三台设备 ID 指设备 id Logitech, Inc. Unifying Receiver 表示生产商和设备名 tips：找出连接了多少 USB 设备：find /dev/bus 列出某台 USB 设备的详细信息，如：lsusb -D /dev/bus/usb/001/002以树层结构列出 USB 设备：lsusb -t其中 12M 和 480M 指 USB 类型的传输速率。 12M 意味着 USB1.0/1.1 的速率是12Mbit/s 480M 意味着 USB2.0 的速率是 480Mbit/s 5.0G 意味着 USB3.0 的速率是 5.0 Gbit/s lsblklsblk 命令用于列出所有可用块设备的信息，还能显示他们之间的依赖关系。块设备有硬盘，闪存盘，cd-ROM等等。 NAME ：块设备名。 MAJ:MIN ：主要和次要设备号。 RM ：设备是否属于可移动设备。如 sdb 的 RM 值为1，是可移动设备。 SIZE ：设备的容量大小信息。 RO ：设备是否为只读。 TYPE ：设备是磁盘还是磁盘上的一个分区。在本例中，sda 和 sdb 是磁盘，而 sr0 是只读存储（rom）。 MOUNTPOINT ：设备的挂载点。 lscpulscpu 命令可以获得 CPU 的详细信息，如CPU架构（如 i686 或 x86_64）、运算模式（32bit 或 64bit）、核心数、每颗核心的线程数、型号、主频等。 lsmod如果你新增加了硬件，却不能被系统自动识别。可能你需要手动地加载对应的内核模块。内核模块安装在 /lib/modules/ 的子目录下，该子目录的名字对应内核的版本。即 /lib/modules/4.10.0-19-generic 目录下包含了 4.10.0-19-generic 内核的驱动程序。lsmod 命令用于列出当前系统下已经载入的内核模块。获取已加载模块的信息，可以使用 modinfo 命令。另外，还可以使用 modprobe 命令加载模块，rmmod 命令移除模块。 参考书目和文章：Linux Bible，9th EditionLinux lsblk 命令详解Linux中显示系统中USB信息的lsusb命令]]></content>
      <categories>
        <category>Admin</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Admin</tag>
        <tag>System</tag>
        <tag>Tools</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Nmap-网络扫描实战（1）--主机发现]]></title>
    <url>%2F2017%2F11%2F02%2Fscan-network-using-Nmap-1%2F</url>
    <content type="text"><![CDATA[主机发现指的是从网络中寻找活跃主机的过程。该过程的关注点不在于如何获取目标的详细信息，而是在尽量减少资源消耗的情况下，获得目标们在逻辑上的分布。 OSI 模型 OSI 模型（即 Open Systems Interconnection model 开放式系统互联通信参考模型），由国际标准化组织（ISO）提出，是一个试图使各种网络设备在世界范围内互联通信的标准框架。该模型将计算机网络体系结构划分为7层，如下图所示：本文涉及到的主机探测方法，主要利用了定义在 Layer 2，3 和 4 上的网络协议。 1. Layer 2（ARP） 优势 非常快速 高可信度 劣势 不能探测远程系统（ARP 属于不可路由协议） ARP（Address Resolution Protocol 地址解析协议）是通过解析网路层（Layer 3）地址来找寻数据链路层（Layer 2）地址的一个重要的网络传输协议，即将 Layer 3 中逻辑性的 IP 地址翻译成 Layer 2 中物理性的 MAC 地址。发起查询的主机先在本地网络中广播一条 ARP 请求，符合该查询的主机则直接返回一条包含其 MAC 地址的 ARP 回复，收到回复后的主机会更新保存在本地的 ARP 缓存并开启两者间的网络通信。该过程不使用任何形式的身份验证和授权。 命令示例# nmap -sn 10.1.87.0-255# nmap -sn 10.1.87.0/24# nmap -iL iplist.txt -sn -sn 选项同 -sP，即关闭默认的端口扫描，只执行 Ping 扫描，这在主机发现时可以节省很多时间。从表面上看，执行的 Ping 扫描属于 Layer 3 上的协议。这里 nmap 会根据提供的 IP 地址自动判定是否为本地子网，对本地子网发送 ARP 请求，对远程网络则执行 Ping 扫描。如下图（本地子网）： 输出内容包括活跃主机的 IP 地址、网络延迟、MAC 地址及对应的硬件厂商。WireShark 捕获到的流量信息： 当探测的 IP 地址不属于本地子网时，则执行 Ping 扫描（输出信息无 MAC 地址），输出如下： WireShark 捕获到的流量信息： 2. Layer 3（ICMP） 优势 可探测远程系统（可路由协议） 相对较快 劣势 比 ARP 慢 可能被防火墙拦截 Layer 3 上的 ICMP 探测是最为人熟知的扫描手段（ping 命令总用过的吧）。ICMP 代表 Internet Control Message Protocol（互联网控制消息协议），它的功能之一，就是通过向目标主机发送 ICMP echo 请求，看是否收到 echo 响应，来确定其是否处于活跃状态。如 Ping 命令的输出：返回结果中包括信号往返时间和(信息)包丢失情况的统计信息。 前面已经提到过，nmap 的 Ping 扫描可以根据 IP 地址自行在 ARP 和 ICMP 协议间切换，不需要额外的选项。 3. Layer 4（TCP） 优势 可探测远程系统 比 ICMP 更可靠 劣势 有时防火墙会导致异常结果 全局扫描会非常耗时 TCP 扫描的原理在于，一条单独的 TCP Finish(FIN) 包或者 Acknowledge(ACK) 包通常会触发远程主机的 Reset(RST) 响应，而发送至远程主机的 Synchronize(SYN) 包通常会触发 SYN+ACK 或者 RST 响应（取决于被扫描端口的开放情况）。重点在于，来自指定主机的任何响应都可以确定该主机处于存活状态。 当目标主机所有的 TCP 服务都被防火墙隔离时，UDP 扫描有时就显得极为有效。但有些 UDP 服务回复的是 ICMP 端口不可达响应，而这类响应会被高防护的防火墙阻止。另一些 UDP 服务则只对特定的请求作出响应，使得有效的 UDP 扫描需要针对不同的服务采用针对性的技术。 命令示例 UDP 扫描# nmap 10.1.87.0-255 -PU53 -sn TCP 扫描（发送 ACK 包）# nmap 10.1.87.0/24 -PA80 -sn 附：1. TCP &amp; UDP在 OSI 模型中，IP 协议位于网路层（Layer 3），它可以提供寻址、数据报路由及其他功能，将一台设备连接至另外一台设备。TCP 协议位于传输层（Layer 4），负责管理连接以及各设备间数据的稳定传输。 TCP 是一种连接导向（connection-oriented）的协议。在 TCP 传输数据之前，客户端和服务端之间需要通过一种三步握手的机制来创建连接。过程如下： 客户端通过向服务端发送一个包含 SYN（synchronize）标志的数据包来初始化连接，在 SYN 字段中包含了一个随机的初始序列号（ISN）。 接着服务端向客户端同样回复一个 SYN 包（其中有它自己的 ISN）。同时服务端发送一个 ACK（acknowledge）包来告诉客户端自己收到了前面的信息。该 ACK 报文中包含了客户端的 ISN+1 的值。 客户端发送一个 ACK 包（包含 服务端的 ISN+1）来告知服务端，自己已收到消息。此时，两者之间才开始交换数据。 当连接关闭时： 客户端发送一个包含 FIN（finish）标志的数据包。 服务端发送 ACK 包表示自己已收到。 当服务端准备好关闭时，它会发送一个 FIN 包。 客户端发送 ACK 包表示自己已收到。开启连接的过程可参考下图： TCP 是一种强调数据完整性的协议。通过上述机制，在传输过程中若有一个包丢失，TCP 会自动重新传输（直到接收者发出 ACK 包）。如果数据包到达时次序错误，TCP 会对它们进行重排，之后才交给应用程序。所以传输文件或者重要数据如 HTTP 和 FTP 都是用的 TCP 协议。 UDP 属于非连接导向（connectionless）的协议。在传输数据之前，他不需要先创建一个 UDP 连接。如果发生丢包，UDP 也不会重复发送（丢失包的重发根据情况由应用程序来完成）。所以像视频流等可以承受丢包损失的多媒体应用常通过 UDP 传输。常见的使用 UDP 的应用：DNS、DHCP、SNMP 等。 2. TCP表头格式 参考书籍及文章：Kali Linux Network Scanning Cookbook by Justin HutchensTCP 三向交握 (Three-way Handshake)]]></content>
      <categories>
        <category>Pentest</category>
      </categories>
      <tags>
        <tag>Tools</tag>
        <tag>Security</tag>
        <tag>Networking</tag>
        <tag>Pentest</tag>
        <tag>Hacking</tag>
        <tag>TCP/IP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ncat--网络工具箱里的『瑞士军刀』]]></title>
    <url>%2F2017%2F10%2F19%2FNcat-document%2F</url>
    <content type="text"><![CDATA[Ncat is a general-purpose command-line tool for reading, writing, redirecting, and encrypting data across a network. It aims to be your network Swiss Army knife, handling a wide variety of security testing and administration tasks. 以前在手机上装 busybox ，总听人讲到什么『瑞士军刀』。哥们儿小地方的娃，没见过大场面，这回总算在网上找到张图片。总的来说，Ncat 还是对得起这个称号的，小巧但功能强悍。这里只简要地介绍下有趣的功能。关于安装，Ncat 其实是 nmap 项目对传统的 Netcat（即 nc 命令）的重写，是包含在 nmap 安装包里的，具体可参考官网。不做赘述。 一、Ncat 作为浏览器命令示例：ncat -C scanme.nmap.org 80其中 -C 是格式化选项，80 指代端口号（ web 服务监听）。输出结果如下：需要注意的是，该命令是以交互的方式执行的。即输入 ncat -C scanme.nmap.org 80 和 回车 后，接着继续输入 GET / HTTP/1.0 ，再敲击两次 回车 。即可获取目标网站的 HTML 文档内容。 二、监听模式（模拟 web 服务器）命令示例：ncat -l 8080 &lt; hello.http其中 hello.http 的文件内容：1234567HTTP/1.0 200 OK&lt;html&gt; &lt;body&gt; &lt;h1&gt;Hello, world!&lt;/h1&gt; &lt;/body&gt;&lt;/html&gt; -l 即 –listen（监听）。实际效果如下：需要注意的是，访问一次后程序即自动退出。 三、执行命令（远程 shell）命令示例：ncat -l 8080 --exec &quot;/bin/echo Hello.&quot;效果如下：进一步，可以开启一个远程 shell 供其他设备连接。命令如下：ncat -l 8022 --exec &quot;/bin/bash -i&quot;效果如下： Ncat 同时还支持 sh 脚本（ –sh-exec ）和 lua 脚本（ –lua-exec ） 注：以上情形均支持局域网远程访问，访问时将 IP 地址改为对应数字。截图中为了方便，只在本地测试。 四、访问控制 只允许指定客户端连接：ncat -l --allow 10.2.67.204 只拒绝指定客户端连接：ncat -l --deny 10.2.67.204 只允许指定网段的本地 IP： ncat -l --allow 10.2.67.0/24 ncat -l --allow 10.2.67.0-255 从文件中获取允许访问的地址列表：ncat -l --allowfile trusted_hosts.txt 设置最大连接数为5：ncat -l –max-conns 5五、文件传输 #####传输单个文件 接收者监听：receiver$ ncat -l &gt; outputfilesender$ ncat --send-only receiver_ip &lt; inputfile 发送者监听：sender$ ncat -l --send-only &lt; inputfilereceiver$ ncat sender_ip &gt; outputfile #####传输目录receiver$ ncat -l | tar xzvf -sender$ tar czvf - dirname | ncat --send-only receiver_ip效果如下： #####传输磁盘镜像（压缩）receiver$ ncat -l | bzip2 -d &gt; sender-hda.imagesender$ cat /dev/hda | bzip2 | ncat --send-only receiver_ip六、聊天 #####双人聊天host1$ ncat -lhost2$ ncat host1 #####多人聊天server$ ncat -l --chatclients$ ncat server_ip效果如下：七、简易 web 服务器 Linux 用户：ncat -lk -p 8080 --sh-exec &quot;echo -e &#39;HTTP/1.1 200 OK\r\n&#39;; cat index.html&quot; Windows 用户：ncat -lk -p 8080 --sh-exec &quot;echo HTTP/1.1 200 OK&amp; echo(&amp;type index.html&quot;八、流媒体视频server$ $cat video.avi | ncat -lclient$ ncat server_ip | mplayer -vo x11 -cache 3000 -效果如下：]]></content>
      <categories>
        <category>Tools</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Admin</tag>
        <tag>Tools</tag>
        <tag>Trick</tag>
        <tag>Networking</tag>
      </tags>
  </entry>
</search>
